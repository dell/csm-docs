[{"body":"User can migrate existing pre-provisioned volumes to another storage array by using the array migration feature.\nNOTE: Currently only migration of standalone block volumes is supported.\nPrerequisites This feature needs to be planned in a controlled host environment.\nIf the user have native multipathing, the user has to run multipath list commands on all nodes to ensure that there are no faulty paths on the host. If any faulty paths exist, the user has to flush the paths, and have a clean setup before migration is triggered using the following command:\nrescan-scsi-bus.sh --remove On Storage Array User has to configure physical SRDF connection between source array (where the volumes are currently provisioned) and target array (where the volumes should be migrated).\nIn Kubernetes User need to ensure that migration group CRD is installed.\nTo install CRD, user can run the command as below:\nkubectl create -f deploy/replicationcrds.all.yaml Support Matrix PowerMax PowerStore PowerScale PowerFlex Unity Yes No No No No Installing Driver With sidecars Dell-csi-migrator and dell-csi-node-rescanner sidecars are installed alongside with the driver, the user can enable it in the driver’s myvalues.yaml file.\nSample: # CSM module attributes # Set this to true to enable migration # Allowed values: # \"true\" - migration is enabled # \"false\" - migration is disabled # Default value: \"false\" migration: enabled: true # migrationPrefix: Determine if migration is enabled # Default value: \"migration.storage.dell.com\" # Examples: \"migration.storage.dell.com\" migrationPrefix: \"migration.storage.dell.com\" Target array configuration and endpoint needs to be updated in the driver’s myvalues.yaml file as shown below:\n########################## # PLATFORM ATTRIBUTES ########################## # The CSI PowerMax ReverseProxy section to fill out the required configuration defaultCredentialsSecret: powermax-creds storageArrays: - storageArrayId: \"000000000000\" endpoint: https://00.000.000.00:0000 # backupEndpoint: https://backup-1.unisphe.re:8443 - storageArrayId: \"000120001178\" endpoint: https://00.000.000.00:0000 # backupEndpoint: https://backup-2.unisphe.re:8443 After enabling the migration module the user can continue to install the CSI driver following usual installation procedure.\nPowerMax Support CSM for PowerMax supports the following migrations:\nFrom a VMAX3 array to VMAX All Flash, or PowerMax array.\nFrom a PowerMax array to another PowerMax array.\nBasic Usage To trigger array migration procedure, the user need to create the migration group for the required source and target array.\nCreating the migration group will trigger reconcile action on the migrator sidecar that will call ArrayMigrate() on the CSI driver with actions migrate or commit. After the migrated state, the migration group will trigger reconcile on the node-rescanner sidecar.\nManual Migration Group Creation User can find sample migration group manifest in the driver repository here. A sample is provided below, for convenience:\napiVersion: \"replication.storage.dell.com/v1\" kind: DellCSIMigrationGroup metadata: # custom name of the migration group # Default value: pmax-migration name: pmax-migration spec: # driverName: exact name of CSI PowerMax driver driverName: \"csi-powermax.dellemc.com\" # sourceID: source ArrayID sourceID: \"000000001234\" # targetID: target ArrayID targetID: \"000000005678\" migrationGroupAttributes: action: \"migrate\" To create the migration group, use the below command:\nkubectl -create -f \u003cmanifest.yaml\u003e After completion of migration, the migration group comes to deleting state after which the admin can manually delete the migration group with the below command:\nkubectl -delete -f \u003cmanifest.yaml\u003e Post migration The PV/PVCs will be mounted, up and running after migration and the pod can continue service as before.\nLIMITATION: Any control operations like expansion, snapshot creation, replication workflows on migrated PV/PVCs will not be supported.\n","categories":"","description":"Support for Array Migration of Volumes between arrays\n","excerpt":"Support for Array Migration of Volumes between arrays\n","ref":"/csm-docs/docs/replication/migration/migrating-volumes-diff-array/","tags":"","title":"Between Storage Arrays"},{"body":"User can migrate existing pre-provisioned volumes to another storage array by using the array migration feature.\nNOTE: Currently only migration of standalone block volumes is supported.\nPrerequisites This feature needs to be planned in a controlled host environment.\nIf the user have native multipathing, the user has to run multipath list commands on all nodes to ensure that there are no faulty paths on the host. If any faulty paths exist, the user has to flush the paths, and have a clean setup before migration is triggered using the following command:\nrescan-scsi-bus.sh --remove On Storage Array User has to configure physical SRDF connection between source array (where the volumes are currently provisioned) and target array (where the volumes should be migrated).\nIn Kubernetes User need to ensure that migration group CRD is installed.\nTo install CRD, user can run the command as below:\nkubectl create -f deploy/replicationcrds.all.yaml Support Matrix PowerMax PowerStore PowerScale PowerFlex Unity Yes No No No No Installing Driver With sidecars Dell-csi-migrator and dell-csi-node-rescanner sidecars are installed alongside with the driver, the user can enable it in the driver’s myvalues.yaml file.\nSample: # CSM module attributes # Set this to true to enable migration # Allowed values: # \"true\" - migration is enabled # \"false\" - migration is disabled # Default value: \"false\" migration: enabled: true # Change this to use any specific version of the dell-csi-migrator sidecar # Default value: None nodeRescanSidecarImage: dellemc/dell-csi-node-rescanner:v1.0.0 image: dellemc/dell-csi-migrator:v1.1.0 # migrationPrefix: Determine if migration is enabled # Default value: \"migration.storage.dell.com\" # Examples: \"migration.storage.dell.com\" migrationPrefix: \"migration.storage.dell.com\" Target array configuration and endpoint needs to be updated in the driver’s myvalues.yaml file as shown below:\n########################## # PLATFORM ATTRIBUTES ########################## # The CSI PowerMax ReverseProxy section to fill out the required configuration defaultCredentialsSecret: powermax-creds storageArrays: - storageArrayId: \"000000000000\" endpoint: https://00.000.000.00:0000 # backupEndpoint: https://backup-1.unisphe.re:8443 - storageArrayId: \"000120001178\" endpoint: https://00.000.000.00:0000 # backupEndpoint: https://backup-2.unisphe.re:8443 After enabling the migration module the user can continue to install the CSI driver following usual installation procedure.\nPowerMax Support CSM for PowerMax supports the following migrations:\nFrom a VMAX3 array to VMAX All Flash, or PowerMax array.\nFrom a PowerMax array to another PowerMax array.\nBasic Usage To trigger array migration procedure, the user need to create the migration group for the required source and target array.\nCreating the migration group will trigger reconcile action on the migrator sidecar that will call ArrayMigrate() on the CSI driver with actions migrate or commit. After the migrated state, the migration group will trigger reconcile on the node-rescanner sidecar.\nManual Migration Group Creation User can find sample migration group manifest in the driver repository here. A sample is provided below, for convenience:\napiVersion: \"replication.storage.dell.com/v1\" kind: DellCSIMigrationGroup metadata: # custom name of the migration group # Default value: pmax-migration name: pmax-migration spec: # driverName: exact name of CSI PowerMax driver driverName: \"csi-powermax.dellemc.com\" # sourceID: source ArrayID sourceID: \"000000001234\" # targetID: target ArrayID targetID: \"000000005678\" migrationGroupAttributes: action: \"migrate\" To create the migration group, use the below command:\nkubectl -create -f \u003cmanifest.yaml\u003e After completion of migration, the migration group comes to deleting state after which the admin can manually delete the migration group with the below command:\nkubectl -delete -f \u003cmanifest.yaml\u003e Post migration The PV/PVCs will be mounted, up and running after migration and the pod can continue service as before.\nLIMITATION: Any control operations like expansion, snapshot creation, replication workflows on migrated PV/PVCs will not be supported.\n","categories":"","description":"Support for Array Migration of Volumes between arrays\n","excerpt":"Support for Array Migration of Volumes between arrays\n","ref":"/csm-docs/v1/replication/migration/migrating-volumes-diff-array/","tags":"","title":"Between Storage Arrays"},{"body":"User can migrate existing pre-provisioned volumes to another storage array by using the array migration feature.\nNOTE: Currently only migration of standalone volumes is supported.\nPrerequisites This feature needs to be planned in a controlled host environment.\nIf the user have native multipathing, the user has to run multipath list commands on all nodes to ensure that there are no faulty paths on the host. If any faulty paths exist, the user has to flush the paths, and have a clean setup before migration is triggered using the following command:\nrescan-scsi-bus.sh --remove On Storage Array User has to configure physical SRDF connection between source array (where the volumes are currently provisioned) and target array (where the volumes should be migrated).\nIn Kubernetes User need to ensure that migration group CRD is installed.\nTo install CRD, user can run the command as below:\nkubectl create -f deploy/replicationcrds.all.yaml Support Matrix PowerMax PowerStore PowerScale PowerFlex Unity Yes No No No No Installing Driver With sidecars Dell-csi-migrator and dell-csi-node-rescanner sidecars are installed alongside with the driver, the user can enable it in the driver’s myvalues.yaml file.\nSample: # CSM module attributes # Set this to true to enable migration # Allowed values: # \"true\" - migration is enabled # \"false\" - migration is disabled # Default value: \"false\" migration: enabled: true # Change this to use any specific version of the dell-csi-migrator sidecar # Default value: None nodeRescanSidecarImage: dellemc/dell-csi-node-rescanner:v1.0.0 image: dellemc/dell-csi-migrator:v1.1.0 # migrationPrefix: Determine if migration is enabled # Default value: \"migration.storage.dell.com\" # Examples: \"migration.storage.dell.com\" migrationPrefix: \"migration.storage.dell.com\" Target array configuration and endpoint needs to be updated in the driver’s myvalues.yaml file as shown below:\n########################## # PLATFORM ATTRIBUTES ########################## # The CSI PowerMax ReverseProxy section to fill out the required configuration defaultCredentialsSecret: powermax-creds storageArrays: - storageArrayId: \"000000000000\" endpoint: https://00.000.000.00:0000 # backupEndpoint: https://backup-1.unisphe.re:8443 - storageArrayId: \"000120001178\" endpoint: https://00.000.000.00:0000 # backupEndpoint: https://backup-2.unisphe.re:8443 After enabling the migration module the user can continue to install the CSI driver following usual installation procedure.\nPowerMax Support CSM for PowerMax supports the following migrations:\nFrom a VMAX3 array to VMAX All Flash, or PowerMax array.\nFrom a PowerMax array to another PowerMax array.\nBasic Usage To trigger array migration procedure, the user need to create the migration group for the required source and target array.\nCreating the migration group will trigger reconcile action on the migrator sidecar that will call ArrayMigrate() on the CSI driver with actions migrate or commit. After the migrated state, the migration group will trigger reconcile on the node-rescanner sidecar.\nManual Migration Group Creation User can find sample migration group manifest in the driver repository here. A sample is provided below, for convenience:\napiVersion: \"replication.storage.dell.com/v1\" kind: DellCSIMigrationGroup metadata: # custom name of the migration group # Default value: pmax-migration name: pmax-migration spec: # driverName: exact name of CSI PowerMax driver driverName: \"csi-powermax.dellemc.com\" # sourceID: source ArrayID sourceID: \"000000001234\" # targetID: target ArrayID targetID: \"000000005678\" migrationGroupAttributes: action: \"migrate\" To create the migration group, use the below command:\nkubectl -create -f \u003cmanifest.yaml\u003e After completion of migration, the migration group comes to deleting state after which the admin can manually delete the migration group with the below command:\nkubectl -delete -f \u003cmanifest.yaml\u003e Post migration The PV/PVCs will be mounted, up and running after migration and the pod can continue service as before.\nLIMITATION: Any control operations like expansion, snapshot creation, replication workflows on migrated PV/PVCs will not be supported.\n","categories":"","description":"Support for Array Migration of Volumes between arrays\n","excerpt":"Support for Array Migration of Volumes between arrays\n","ref":"/csm-docs/v2/replication/migration/migrating-volumes-diff-array/","tags":"","title":"Between Storage Arrays"},{"body":"User can migrate existing pre-provisioned volumes to another storage array by using the array migration feature.\nNOTE: Currently only migration of standalone volumes is supported.\nPrerequisites This feature needs to be planned in a controlled host environment.\nIf the user have native multipathing, the user has to run multipath list commands on all nodes to ensure that there are no faulty paths on the host. If any faulty paths exist, the user has to flush the paths, and have a clean setup before migration is triggered using the following command:\nrescan-scsi-bus.sh --remove\nOn Storage Array User has to configure physical SRDF connection between source array (where the volumes are currently provisioned) and target array (where the volumes should be migrated).\nIn Kubernetes User need to ensure that migration group CRD is installed.\nTo install CRD, user can run the command as below:\nkubectl create -f deploy/replicationcrds.all.yaml\nSupport Matrix PowerMax PowerStore PowerScale PowerFlex Unity Yes No No No No Installing Driver With sidecars Dell-csi-migrator and dell-csi-node-rescanner sidecars are installed alongside with the driver, the user can enable it in the driver’s myvalues.yaml file.\nSample: # CSM module attributes # Set this to true to enable migration # Allowed values: # \"true\" - migration is enabled # \"false\" - migration is disabled # Default value: \"false\" migration: enabled: true # Change this to use any specific version of the dell-csi-migrator sidecar # Default value: None nodeRescanSidecarImage: dellemc/dell-csi-node-rescanner:v1.0.0 image: dellemc/dell-csi-migrator:v1.1.0 # migrationPrefix: Determine if migration is enabled # Default value: \"migration.storage.dell.com\" # Examples: \"migration.storage.dell.com\" migrationPrefix: \"migration.storage.dell.com\" Target array configuration and endpoint needs to be updated in the driver’s myvalues.yaml file as shown below:\n########################## # PLATFORM ATTRIBUTES ########################## # The CSI PowerMax ReverseProxy section to fill out the required configuration defaultCredentialsSecret: powermax-creds storageArrays: - storageArrayId: \"000000000000\" endpoint: https://00.000.000.00:0000 # backupEndpoint: https://backup-1.unisphe.re:8443 - storageArrayId: \"000120001178\" endpoint: https://00.000.000.00:0000 # backupEndpoint: https://backup-2.unisphe.re:8443 After enabling the migration module the user can continue to install the CSI driver following usual installation procedure.\nPowerMax Support CSM for PowerMax supports the following migrations:\nFrom a VMAX3 array to VMAX All Flash, or PowerMax array.\nFrom a PowerMax array to another PowerMax array.\nBasic Usage To trigger array migration procedure, the user need to create the migration group for the required source and target array.\nCreating the migration group will trigger reconcile action on the migrator sidecar that will call ArrayMigrate() on the CSI driver with actions migrate or commit. After the migrated state, the migration group will trigger reconcile on the node-rescanner sidecar.\nManual Migration Group Creation User can find sample migration group manifest in the driver repository here. A sample is provided below, for convenience:\napiVersion: \"replication.storage.dell.com/v1\" kind: DellCSIMigrationGroup metadata: # custom name of the migration group # Default value: pmax-migration name: pmax-migration spec: # driverName: exact name of CSI Powermax driver driverName: \"csi-powermax.dellemc.com\" # sourceID: source ArrayID sourceID: \"000000001234\" # targetID: target ArrayID targetID: \"000000005678\" migrationGroupAttributes: action: \"migrate\" To create the migration group, use the below command:\nkubectl -create -f \u003cmanifest.yaml\u003e\nAfter completion of migration, the migration group comes to deleting state after which the admin can manually delete the migration group with the below command:\nkubectl -delete -f \u003cmanifest.yaml\u003e\nPost migration The PV/PVCs will be mounted, up and running after migration and the pod can continue service as before.\nLIMITATION: Any control operations like expansion, snapshot creation, replication workflows on migrated PV/PVCs will not be supported.\n","categories":"","description":"Support for Array Migration of Volumes between arrays\n","excerpt":"Support for Array Migration of Volumes between arrays\n","ref":"/csm-docs/v3/replication/migration/migrating-volumes-diff-array/","tags":"","title":"Between Storage Arrays"},{"body":"dellctl is a common command line interface(CLI) used to interact with and manage your Container Storage Modules (CSM) resources. This document outlines all dellctl commands, their intended use, options that can be provided to alter their execution, and expected output from those commands.\nCommand Description dellctl dellctl is used to interact with Container Storage Modules dellctl cluster Manipulate one or more k8s cluster configurations dellctl cluster add Add a k8s cluster to be managed by dellctl dellctl cluster remove Removes a k8s cluster managed by dellctl dellctl cluster get List all clusters currently being managed by dellctl dellctl backup Allows you to manipulate application backups/clones dellctl backup create Create an application backup/clones dellctl backup delete Delete application backups dellctl backup get Get application backups dellctl restore Allows you to manipulate application restores dellctl restore create Restore an application backup dellctl restore delete Delete application restores dellctl restore get Get application restores dellctl schedule Allows you to manipulate schedules dellctl schedule create Create a schedule dellctl schedule create for-backup Create a schedule for application backups dellctl schedule delete Delete schedules dellctl schedule get Get schedules dellctl encryption rekey Rekey an encrypted volume dellctl encryption rekey-status Get status of an encryption rekey operation dellctl images List the container images needed by csi driver dellctl volume get Gets PowerFlex volume infomation for a given tenant on a local cluster Installation instructions Download dellctl from here. chmod +x dellctl Move dellctl to /usr/local/bin or add dellctl’s containing directory path to PATH environment variable. Run dellctl --help to know available commands or run dellctl command --help to know more about a specific command. By default, the dellctl runs against local cluster(referenced by KUBECONFIG environment variable or by a kube config file present at default location). The user can register one or more remote clusters for dellctl, and run any dellctl command against these clusters by specifying the registered cluster id to the command.\nGeneral Commands dellctl dellctl is a CLI tool for managing Dell Container Storage Resources.\nFlags -h, --help help for dellctl -v, --version version for dellctl Output Outputs help text\ndellctl cluster Allows you to manipulate one or more k8s cluster configurations\nAvailable Commands add Adds a k8s cluster to be managed by dellctl remove Removes a k8s cluster managed by dellctl get List all clusters currently being managed by dellctl Flags -h, --help help for cluster Output Outputs help text\ndellctl cluster add Add one or more k8s clusters to be managed by dellctl\nFlags Flags: -n, --names strings cluster names -f, --files strings paths for kube config files -u, --uids strings uids of the kube-system namespaces in the clusters --force forcefully add cluster -h, --help help for add Output dellctl cluster add -n cluster1 -f ~/kubeconfigs/cluster1-kubeconfig INFO Adding clusters ... INFO Cluster: cluster1 INFO Successfully added cluster cluster1 in /root/.dellctl/clusters/cluster1 folder. Add a cluster with it’s uid\ndellctl cluster add -n cluster2 -f ~/kubeconfigs/cluster2-kubeconfig -u \"035133aa-5b65-4080-a813-34a7abe48180\" INFO Adding clusters ... INFO Cluster: cluster2 INFO Successfully added cluster cluster2 in /root/.dellctl/clusters/cluster2 folder. dellctl cluster remove Removes a k8s cluster by name from the list of clusters being managed by dellctl\nAliases remove, rm Flags -h, --help help for remove -n, --name string cluster name Output dellctl cluster remove -n cluster1 INFO Removing cluster with id cluster1 INFO Removed cluster with id cluster1 dellctl cluster get List all clusters currently being managed by dellctl\nAliases get, ls Flags -h, --help help for get Output dellctl cluster get CLUSTER ID VERSION URL UID cluster1 v1.22 https://1.2.3.4:6443 cluster2 v1.22 https://1.2.3.5:6443 035133aa-5b65-4080-a813-34a7abe48180 Commands related to application mobility operations dellctl backup Allows you to manipulate application backups/clones\nAvailable Commands create Create an application backup/clones delete Delete application backups get Get application backups Flags -h, --help help for backup Output Outputs help text\ndellctl backup create Create an application backup/clones\nFlags --cluster-id string Id of the cluster managed by dellctl --exclude-namespaces stringArray List of namespace names to exclude from the backup. --include-namespaces stringArray List of namespace names to include in the backup (use '*' for all namespaces). (default *) --ttl duration Backup retention period. (default 720h0m0s) --exclude-resources stringArray Resources to exclude from the backup, formatted as resource.group, such as storageclasses.storage.k8s.io. --include-resources stringArray Resources to include in the backup, formatted as resource.group, such as storageclasses.storage.k8s.io (use '*' for all resources). --backup-location string Storage location where k8s resources and application data will be backed up to. (default \"default\") --data-mover string Data mover to be used to backup application data. (default \"Restic\") --include-cluster-resources optionalBool[=true] Include cluster-scoped resources in the backup -l, --label-selector labelSelector Only backup resources matching this label selector. (default \u003cnone\u003e) -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") --clones stringArray Creates an application clone into target clusters managed by dellctl. Specify optional namespace mappings where the clone is created. Example: 'cluster1/sourceNamespace1:targetNamespace1', 'cluster1/sourceNamespace1:targetNamespace1;cluster2/sourceNamespace2:targetNamespace2' -h, --help help for create Output Create a backup of the applications running in namespace demo1\ndellctl backup create backup1 --include-namespaces demo1 INFO Backup request \"backup1\" submitted successfully. INFO Run 'dellctl backup get backup1' for more details. Create clones of the application running in namespace demo1, on clusters with id cluster1 and cluster2\ndellctl backup create demo-app-clones --include-namespaces demo1 --clones \"cluster1/demo1:restore-ns1\" --clones \"cluster2/demo1:restore-ns1\" INFO Clone request \"demo-app-clones\" submitted successfully. INFO Run 'dellctl backup get demo-app-clones' for more details. Take backup of application running in namespace demo3 on remote cluster with id cluster2\ndellctl backup create backup4 --include-namespaces demo3 --cluster-id cluster2 INFO Backup request \"backup4\" submitted successfully. INFO Run 'dellctl backup get backup4' for more details. dellctl backup delete Delete one or more application backups\nFlags --all Delete all backups --cluster-id string Id of the cluster managed by dellctl --confirm Confirm deletion -h, --help help for delete -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output dellctl backup delete backup1 Are you sure you want to continue (Y/N)? Y INFO Request to delete backup \"backup1\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. Delete multiple backups\ndellctl backup delete backup1 backup2 Are you sure you want to continue (Y/N)? Y INFO Request to delete backup \"backup1\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. INFO Request to delete backup \"backup2\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. Delete all backups without asking for user confirmation\ndellctl backup delete --all --confirm INFO Request to delete backup \"backup4\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. INFO Request to delete backup \"demo-app-clones\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. dellctl backup get Get application backups\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help help for get -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output dellctl backup get NAME STATUS CREATED EXPIRES STORAGE LOCATION DATA MOVER CLONED TARGET CLUSTERS backup1 Completed 2022-07-27 11:51:00 -0400 EDT 2022-08-26 11:51:00 -0400 EDT default Restic false backup2 Completed 2022-07-27 11:59:24 -0400 EDT 2022-08-26 11:59:42 -0400 EDT default Restic false backup4 Completed 2022-07-27 12:02:54 -0400 EDT NA default Restic false demo-app-clones Restored 2022-07-27 11:53:37 -0400 EDT 2022-08-26 11:53:37 -0400 EDT default Restic true cluster1, cluster2 Get backups from remote cluster with id cluster2\ndellctl backup get --cluster-id cluster2 NAME STATUS CREATED EXPIRES STORAGE LOCATION DATA MOVER CLONED TARGET CLUSTERS backup1 Completed 2022-07-27 11:52:42 -0400 EDT NA default Restic false backup2 Completed 2022-07-27 12:02:29 -0400 EDT NA default Restic false backup4 Completed 2022-07-27 12:01:49 -0400 EDT 2022-08-26 12:01:49 -0400 EDT default Restic false demo-app-clones Completed 2022-07-27 11:54:55 -0400 EDT NA default Restic true cluster1, cluster2 Get backups with their names\ndellctl backup get backup1 demo-app-clones NAME STATUS CREATED EXPIRES STORAGE LOCATION DATA MOVER CLONED TARGET CLUSTERS backup1 Completed 2022-07-27 11:51:00 -0400 EDT 2022-08-26 11:51:00 -0400 EDT default Restic false demo-app-clones Completed 2022-07-27 11:53:37 -0400 EDT 2022-08-26 11:53:37 -0400 EDT default Restic true cluster1, cluster2 dellctl restore Allows you to manipulate application restores\nAvailable Commands create Restore an application backup delete Delete application restores get Get application restores Flags -h, --help help for restore Output Outputs help text\ndellctl restore create Restore an application backup\nFlags --cluster-id string Id of the cluster managed by dellctl --from-backup string Backup to restore from --namespace-mappings mapStringString Map of source namespace names to target namespace names to restore into in the form src1:dst1,src2:dst2,... --exclude-namespaces stringArray List of namespace names to exclude from the backup. --include-namespaces stringArray List of namespace names to include in the backup (use '*' for all namespaces). (default *) --exclude-resources stringArray Resources to exclude from the backup, formatted as resource.group, such as storageclasses.storage.k8s.io. --include-resources stringArray Resources to include in the backup, formatted as resource.group, such as storageclasses.storage.k8s.io (use '*' for all resources). --restore-volumes optionalBool[=true] Whether to restore volumes from snapshots. --include-cluster-resources optionalBool[=true] Include cluster-scoped resources in the backup -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") -h, --help help for create Output Restore application backup backup1 on local cluster in namespace restorens1\ndellctl restore create restore1 --from-backup backup1 --namespace-mappings \"demo1:restorens1\" INFO Restore request \"restore1\" submitted successfully. INFO Run 'dellctl restore get restore1' for more details. Restore application backup backup1 on remote cluster cluster2 in namespace demo1\ndellctl restore create restore1 --from-backup backup1 --cluster-id cluster2 INFO Restore request \"restore1\" submitted successfully. INFO Run 'dellctl restore get restore1' for more details. dellctl restore delete Delete one or more application restores\nFlags --all Delete all restores --cluster-id string Id of the cluster managed by dellctl --confirm Confirm deletion -h, --help help for delete -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output Delete a restore created on remote cluster with id cluster2\ndellctl restore delete restore1 --cluster-id cluster2 Are you sure you want to continue (Y/N)? Y INFO Request to delete restore \"restore1\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. Delete multiple restores\ndellctl restore delete restore1 restore4 Are you sure you want to continue (Y/N)? Y INFO Request to delete restore \"restore1\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. INFO Request to delete restore \"restore4\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. Delete all restores without asking for user confirmation\ndellctl restore delete --all --confirm INFO Request to delete restore \"restore1\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. INFO Request to delete restore \"restore2\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. dellctl restore get Get application restores\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help help for get -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output Get all the application restores created on local cluster\ndellctl restore get NAME BACKUP STATUS CREATED COMPLETED restore1 backup1 Completed 2022-07-27 12:35:29 -0400 EDT restore4 backup1 Completed 2022-07-27 12:39:42 -0400 EDT Get all the application restores created on remote cluster with id cluster2\ndellctl restore get --cluster-id cluster2 NAME BACKUP STATUS CREATED COMPLETED restore1 backup1 Completed 2022-07-27 12:38:43 -0400 EDT Get restores with their names\ndellctl restore get restore1 NAME BACKUP STATUS CREATED COMPLETED restore1 backup1 Completed 2022-07-27 12:35:29 -0400 EDT dellctl schedule Allows you to manipulate schedules\nAvailable Commands create Create a schedule delete Delete schedules get Get schedules Flags -h, --help Help for schedule Output Outputs help text\ndellctl schedule create Create a schedule\nAvailable Commands for-backup Create a schedule for application backups Flags --cluster-id string Id of the cluster managed by dellctl -h, --help Help for create --name string Name for the schedule --schedule string A cron expression representing when to create the application backup Output Outputs help text\ndellctl schedule create for-backup Create a schedule for application backups\nFlags --exclude-namespaces stringArray List of namespace names to exclude from the backup. --include-namespaces stringArray List of namespace names to include in the backup (use '*' for all namespaces). (default *) --ttl duration Backup retention period. (default 720h0m0s) --exclude-resources stringArray Resources to exclude from the backup, formatted as resource.group, such as storageclasses.storage.k8s.io. --include-resources stringArray Resources to include in the backup, formatted as resource.group, such as storageclasses.storage.k8s.io (use '*' for all resources). --backup-location string Storage location where k8s resources and application data will be backed up to. (default \"default\") --data-mover string Data mover to be used to backup application data. (default \"Restic\") --include-cluster-resources optionalBool[=true] Include cluster-scoped resources in the backup -l, --label-selector labelSelector Only backup resources matching this label selector. (default \u003cnone\u003e) --set-owner-references-in-backup optionalBool[=false] Specifies whether to set OwnerReferences on backups created by this schedule. -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") -h, --help Help for for-backup Global Flags --cluster-id string Id of the cluster managed by dellctl --name string Name for the schedule --schedule string A cron expression representing when to create the application backup Output Create a schedule to backup namespace demo, every 1hour\ndellctl schedule create for-backup --name schedule1 --schedule \"@every 1h\" --include-namespaces demo INFO schedule request \"schedule1\" submitted successfully. INFO Run 'dellctl schedule get schedule1' for more details. Create a schedule to backup namespace demo, once a day at midnight and set OwnerReferences on backups created by this schedule\ndellctl schedule create for-backup --name schedule2 --schedule \"@daily\" --include-namespaces demo --set-owner-references-in-backup INFO schedule request \"schedule2\" submitted successfully. INFO Run 'dellctl schedule get schedule2' for more details. Create a schedule to backup namespace demo, at 23:00(11:00 pm) every saturday\ndellctl schedule create for-backup --name schedule3 --schedule \"00 23 * * 6\" --include-namespaces demo INFO schedule request \"schedule3\" submitted successfully. INFO Run 'dellctl schedule get schedule3' for more details. dellctl schedule delete Delete one or more schedules\nFlags --all Delete all schedules --cluster-id string Id of the cluster managed by dellctl --confirm Confirm deletion -h, --help Help for delete -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output Delete a schedule with name\ndellctl schedule delete schedule1 Are you sure you want to continue (Y/N)? y INFO Request to delete schedule \"schedule1\" submitted successfully. Delete multiple schedules\ndellctl schedule delete schedule1 schedule2 Are you sure you want to continue (Y/N)? y INFO Request to delete schedule \"schedule1\" submitted successfully. INFO Request to delete schedule \"schedule2\" submitted successfully. Delete all schedules without asking for user confirmation\ndellctl schedule delete --confirm --all INFO Request to delete schedule \"schedule1\" submitted successfully. INFO Request to delete schedule \"schedule2\" submitted successfully. dellctl schedule get Get schedules\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help Help for get -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output Get all the application schedules created on local cluster\ndellctl schedule get NAME STATUS CREATED PAUSED SCHEDULE LAST BACKUP TIME schedule1 Enabled 2022-11-04 08:33:35 +0000 UTC false @every 1h NA schedule2 Enabled 2022-11-04 08:35:57 +0000 UTC false @daily NA Get schedules with their names\ndellctl schedule get schedule1 NAME STATUS CREATED PAUSED SCHEDULE LAST BACKUP TIME schedule1 Enabled 2022-11-04 08:33:35 +0000 UTC false @every 1h NA dellctl encryption rekey Encryption rekey with a name for the rekey object and volume name of an encrypted volume\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help help for get Output dellctl encryption rekey myrekey k8s-5d2cc565d4 INFO rekey request \"myrekey\" submitted successfully for persistent volume \"k8s-5d2cc565d4\". INFO Run 'dellctl encryption rekey-status myrekey' for more details. dellctl encryption rekey-status Encryption rekey status with name of the rekey object\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help help for get Output dellctl encryption rekey-status myrekey INFO Status of rekey request myrekey = completed dellctl images List the container images needed by csm components\nNOTE.:\nSupported CSM Components [csi-vxflexos,csi-isilon,csi-powerstore,csi-unity,csi-powermax,csm-authorization]\nAliases images,imgs Flags Flags: -c, --component string csm-component name -h, --help help for images Output dellctl images --component csi-vxflexos Driver/Module Image Supported Orchestrator Versions Sidecar Images dellemc/csi-vxflexos:v2.9.0 k8s1.28,k8s1.27,k8s1.26,ocp4.14,ocp4.13 registry.k8s.io/sig-storage/csi-attacher:v4.4.2 registry.k8s.io/sig-storage/csi-provisioner:v3.6.2 registry.k8s.io/sig-storage/csi-external-health-monitor-controller:v0.10.0 registry.k8s.io/sig-storage/csi-snapshotter:v6.3.2 registry.k8s.io/sig-storage/csi-resizer:v1.9.2 registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1 dellemc/sdc:4.5 dellemc/csi-vxflexos:v2.8.0 k8s1.27,k8s1.26,k8s1.25,ocp4.13,ocp4.12 registry.k8s.io/sig-storage/csi-attacher:v4.3.0 registry.k8s.io/sig-storage/csi-provisioner:v3.5.0 registry.k8s.io/sig-storage/csi-external-health-monitor-controller:v0.9.0 registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2 registry.k8s.io/sig-storage/csi-resizer:v1.8.0 registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0 dellemc/sdc:3.6.1 dellemc/csi-vxflexos:v2.7.0 k8s1.27,k8s1.26,k8s1.25,ocp4.12,ocp4.11 registry.k8s.io/sig-storage/csi-attacher:v4.3.0 registry.k8s.io/sig-storage/csi-provisioner:v3.5.0 registry.k8s.io/sig-storage/csi-external-health-monitor-controller:v0.9.0 registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2 registry.k8s.io/sig-storage/csi-resizer:v1.8.0 registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0 dellemc/sdc:3.6.0.6 dellctl images --component csm-authorization Driver/Module Image Supported Orchestrator Versions Sidecar Images dellemc/csm-authorization-sidecar:v1.9.0 k8s1.28,k8s1.27,k8s1.26 jetstack/cert-manager-cainjector:v1.6.1 jetstack/cert-manager-controller:v1.6.1 jetstack/cert-manager-webhook:v1.6.1 ingress-nginx/controller:v1.4.0 ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343 dellemc/csm-authorization-sidecar:v1.8.0 k8s1.27,k8s1.26,k8s1.25 jetstack/cert-manager-cainjector:v1.6.1 jetstack/cert-manager-controller:v1.6.1 jetstack/cert-manager-webhook:v1.6.1 ingress-nginx/controller:v1.4.0 ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343 dellemc/csm-authorization-sidecar:v1.7.0 k8s1.27,k8s1.26,k8s1.25 jetstack/cert-manager-cainjector:v1.6.1 jetstack/cert-manager-controller:v1.6.1 jetstack/cert-manager-webhook:v1.6.1 ingress-nginx/controller:v1.4.0 ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343 dellctl volume get Gets PowerFlex volume infomation for a given tenant on a local cluster\nAliases get, ls, list\nFlags -h, --help help for get --insecure optionalBool[=true] provide flag to skip certificate validation --namespace string namespace of the secret for the given tenant --proxy string auth proxy endpoint to use Output Gets PowerFlex volume infomation for a given tenant on a local cluster. The namespace is the namespace where tenant secret is created.\nNote: This was output was generated using Authorization Proxy version 1.5.1. Please ensure you are using version 1.5.1 or greater.\ndellctl volume get --proxy \u003cproxy.dell.com\u003e --namespace vxflexos # dellctl volume get --proxy \u003cproxy.dell.com/proxy/volumes\u003e --namespace vxflexos NAME VOLUME ID SIZE POOL SYSTEM ID PV NAME PV STATUS STORAGE CLASS PVC NAME NAMESPACE k8s-e7c8b39112 a69bf18e00000008 8.000000 mypool 636468e3638c840f k8s-e7c8b39112 Released vxflexos demo-claim10 default k8s-e6e2b46103 a69bf18f00000009 8.000000 mypool 636468e3638c840f k8s-e6e2b46103 Bound vxflexos demo-claim11 default k8s-b1abb817d3 a69bf19000000001 8.000000 mypool 636468e3638c840f k8s-b1abb817d3 Bound vxflexos demo-claim13 default k8s-28e4184f41 c6b2280d0000009a 8.000000 mypool 636468e3638c840f k8s-28e4184f41 Available local-storage k8s-7296621062 a69b554f00000004 8.000000 mypool 636468e3638c840f ","categories":"","description":"CLI for Dell Container Storage Modules (CSM)\n","excerpt":"CLI for Dell Container Storage Modules (CSM)\n","ref":"/csm-docs/docs/references/cli/","tags":"","title":"CLI"},{"body":"dellctl is a common command line interface(CLI) used to interact with and manage your Container Storage Modules (CSM) resources. This document outlines all dellctl commands, their intended use, options that can be provided to alter their execution, and expected output from those commands.\nCommand Description dellctl dellctl is used to interact with Container Storage Modules dellctl cluster Manipulate one or more k8s cluster configurations dellctl cluster add Add a k8s cluster to be managed by dellctl dellctl cluster remove Removes a k8s cluster managed by dellctl dellctl cluster get List all clusters currently being managed by dellctl dellctl backup Allows you to manipulate application backups/clones dellctl backup create Create an application backup/clones dellctl backup delete Delete application backups dellctl backup get Get application backups dellctl restore Allows you to manipulate application restores dellctl restore create Restore an application backup dellctl restore delete Delete application restores dellctl restore get Get application restores dellctl schedule Allows you to manipulate schedules dellctl schedule create Create a schedule dellctl schedule create for-backup Create a schedule for application backups dellctl schedule delete Delete schedules dellctl schedule get Get schedules dellctl encryption rekey Rekey an encrypted volume dellctl encryption rekey-status Get status of an encryption rekey operation dellctl images List the container images needed by csi driver dellctl volume get Gets PowerFlex volume infomation for a given tenant on a local cluster Installation instructions Download dellctl from here. chmod +x dellctl Move dellctl to /usr/local/bin or add dellctl’s containing directory path to PATH environment variable. Run dellctl --help to know available commands or run dellctl command --help to know more about a specific command. By default, the dellctl runs against local cluster(referenced by KUBECONFIG environment variable or by a kube config file present at default location). The user can register one or more remote clusters for dellctl, and run any dellctl command against these clusters by specifying the registered cluster id to the command.\nGeneral Commands dellctl dellctl is a CLI tool for managing Dell Container Storage Resources.\nFlags -h, --help help for dellctl -v, --version version for dellctl Output Outputs help text\ndellctl cluster Allows you to manipulate one or more k8s cluster configurations\nAvailable Commands add Adds a k8s cluster to be managed by dellctl remove Removes a k8s cluster managed by dellctl get List all clusters currently being managed by dellctl Flags -h, --help help for cluster Output Outputs help text\ndellctl cluster add Add one or more k8s clusters to be managed by dellctl\nFlags Flags: -n, --names strings cluster names -f, --files strings paths for kube config files -u, --uids strings uids of the kube-system namespaces in the clusters --force forcefully add cluster -h, --help help for add Output dellctl cluster add -n cluster1 -f ~/kubeconfigs/cluster1-kubeconfig INFO Adding clusters ... INFO Cluster: cluster1 INFO Successfully added cluster cluster1 in /root/.dellctl/clusters/cluster1 folder. Add a cluster with it’s uid\ndellctl cluster add -n cluster2 -f ~/kubeconfigs/cluster2-kubeconfig -u \"035133aa-5b65-4080-a813-34a7abe48180\" INFO Adding clusters ... INFO Cluster: cluster2 INFO Successfully added cluster cluster2 in /root/.dellctl/clusters/cluster2 folder. dellctl cluster remove Removes a k8s cluster by name from the list of clusters being managed by dellctl\nAliases remove, rm Flags -h, --help help for remove -n, --name string cluster name Output dellctl cluster remove -n cluster1 INFO Removing cluster with id cluster1 INFO Removed cluster with id cluster1 dellctl cluster get List all clusters currently being managed by dellctl\nAliases get, ls Flags -h, --help help for get Output dellctl cluster get CLUSTER ID VERSION URL UID cluster1 v1.22 https://1.2.3.4:6443 cluster2 v1.22 https://1.2.3.5:6443 035133aa-5b65-4080-a813-34a7abe48180 Commands related to application mobility operations dellctl backup Allows you to manipulate application backups/clones\nAvailable Commands create Create an application backup/clones delete Delete application backups get Get application backups Flags -h, --help help for backup Output Outputs help text\ndellctl backup create Create an application backup/clones\nFlags --cluster-id string Id of the cluster managed by dellctl --exclude-namespaces stringArray List of namespace names to exclude from the backup. --include-namespaces stringArray List of namespace names to include in the backup (use '*' for all namespaces). (default *) --ttl duration Backup retention period. (default 720h0m0s) --exclude-resources stringArray Resources to exclude from the backup, formatted as resource.group, such as storageclasses.storage.k8s.io. --include-resources stringArray Resources to include in the backup, formatted as resource.group, such as storageclasses.storage.k8s.io (use '*' for all resources). --backup-location string Storage location where k8s resources and application data will be backed up to. (default \"default\") --data-mover string Data mover to be used to backup application data. (default \"Restic\") --include-cluster-resources optionalBool[=true] Include cluster-scoped resources in the backup -l, --label-selector labelSelector Only backup resources matching this label selector. (default \u003cnone\u003e) -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") --clones stringArray Creates an application clone into target clusters managed by dellctl. Specify optional namespace mappings where the clone is created. Example: 'cluster1/sourceNamespace1:targetNamespace1', 'cluster1/sourceNamespace1:targetNamespace1;cluster2/sourceNamespace2:targetNamespace2' -h, --help help for create Output Create a backup of the applications running in namespace demo1\ndellctl backup create backup1 --include-namespaces demo1 INFO Backup request \"backup1\" submitted successfully. INFO Run 'dellctl backup get backup1' for more details. Create clones of the application running in namespace demo1, on clusters with id cluster1 and cluster2\ndellctl backup create demo-app-clones --include-namespaces demo1 --clones \"cluster1/demo1:restore-ns1\" --clones \"cluster2/demo1:restore-ns1\" INFO Clone request \"demo-app-clones\" submitted successfully. INFO Run 'dellctl backup get demo-app-clones' for more details. Take backup of application running in namespace demo3 on remote cluster with id cluster2\ndellctl backup create backup4 --include-namespaces demo3 --cluster-id cluster2 INFO Backup request \"backup4\" submitted successfully. INFO Run 'dellctl backup get backup4' for more details. dellctl backup delete Delete one or more application backups\nFlags --all Delete all backups --cluster-id string Id of the cluster managed by dellctl --confirm Confirm deletion -h, --help help for delete -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output dellctl backup delete backup1 Are you sure you want to continue (Y/N)? Y INFO Request to delete backup \"backup1\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. Delete multiple backups\ndellctl backup delete backup1 backup2 Are you sure you want to continue (Y/N)? Y INFO Request to delete backup \"backup1\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. INFO Request to delete backup \"backup2\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. Delete all backups without asking for user confirmation\ndellctl backup delete --all --confirm INFO Request to delete backup \"backup4\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. INFO Request to delete backup \"demo-app-clones\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. dellctl backup get Get application backups\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help help for get -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output dellctl backup get NAME STATUS CREATED EXPIRES STORAGE LOCATION DATA MOVER CLONED TARGET CLUSTERS backup1 Completed 2022-07-27 11:51:00 -0400 EDT 2022-08-26 11:51:00 -0400 EDT default Restic false backup2 Completed 2022-07-27 11:59:24 -0400 EDT 2022-08-26 11:59:42 -0400 EDT default Restic false backup4 Completed 2022-07-27 12:02:54 -0400 EDT NA default Restic false demo-app-clones Restored 2022-07-27 11:53:37 -0400 EDT 2022-08-26 11:53:37 -0400 EDT default Restic true cluster1, cluster2 Get backups from remote cluster with id cluster2\ndellctl backup get --cluster-id cluster2 NAME STATUS CREATED EXPIRES STORAGE LOCATION DATA MOVER CLONED TARGET CLUSTERS backup1 Completed 2022-07-27 11:52:42 -0400 EDT NA default Restic false backup2 Completed 2022-07-27 12:02:29 -0400 EDT NA default Restic false backup4 Completed 2022-07-27 12:01:49 -0400 EDT 2022-08-26 12:01:49 -0400 EDT default Restic false demo-app-clones Completed 2022-07-27 11:54:55 -0400 EDT NA default Restic true cluster1, cluster2 Get backups with their names\ndellctl backup get backup1 demo-app-clones NAME STATUS CREATED EXPIRES STORAGE LOCATION DATA MOVER CLONED TARGET CLUSTERS backup1 Completed 2022-07-27 11:51:00 -0400 EDT 2022-08-26 11:51:00 -0400 EDT default Restic false demo-app-clones Completed 2022-07-27 11:53:37 -0400 EDT 2022-08-26 11:53:37 -0400 EDT default Restic true cluster1, cluster2 dellctl restore Allows you to manipulate application restores\nAvailable Commands create Restore an application backup delete Delete application restores get Get application restores Flags -h, --help help for restore Output Outputs help text\ndellctl restore create Restore an application backup\nFlags --cluster-id string Id of the cluster managed by dellctl --from-backup string Backup to restore from --namespace-mappings mapStringString Map of source namespace names to target namespace names to restore into in the form src1:dst1,src2:dst2,... --exclude-namespaces stringArray List of namespace names to exclude from the backup. --include-namespaces stringArray List of namespace names to include in the backup (use '*' for all namespaces). (default *) --exclude-resources stringArray Resources to exclude from the backup, formatted as resource.group, such as storageclasses.storage.k8s.io. --include-resources stringArray Resources to include in the backup, formatted as resource.group, such as storageclasses.storage.k8s.io (use '*' for all resources). --restore-volumes optionalBool[=true] Whether to restore volumes from snapshots. --include-cluster-resources optionalBool[=true] Include cluster-scoped resources in the backup -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") -h, --help help for create Output Restore application backup backup1 on local cluster in namespace restorens1\ndellctl restore create restore1 --from-backup backup1 --namespace-mappings \"demo1:restorens1\" INFO Restore request \"restore1\" submitted successfully. INFO Run 'dellctl restore get restore1' for more details. Restore application backup backup1 on remote cluster cluster2 in namespace demo1\ndellctl restore create restore1 --from-backup backup1 --cluster-id cluster2 INFO Restore request \"restore1\" submitted successfully. INFO Run 'dellctl restore get restore1' for more details. dellctl restore delete Delete one or more application restores\nFlags --all Delete all restores --cluster-id string Id of the cluster managed by dellctl --confirm Confirm deletion -h, --help help for delete -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output Delete a restore created on remote cluster with id cluster2\ndellctl restore delete restore1 --cluster-id cluster2 Are you sure you want to continue (Y/N)? Y INFO Request to delete restore \"restore1\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. Delete multiple restores\ndellctl restore delete restore1 restore4 Are you sure you want to continue (Y/N)? Y INFO Request to delete restore \"restore1\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. INFO Request to delete restore \"restore4\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. Delete all restores without asking for user confirmation\ndellctl restore delete --all --confirm INFO Request to delete restore \"restore1\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. INFO Request to delete restore \"restore2\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. dellctl restore get Get application restores\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help help for get -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output Get all the application restores created on local cluster\ndellctl restore get NAME BACKUP STATUS CREATED COMPLETED restore1 backup1 Completed 2022-07-27 12:35:29 -0400 EDT restore4 backup1 Completed 2022-07-27 12:39:42 -0400 EDT Get all the application restores created on remote cluster with id cluster2\ndellctl restore get --cluster-id cluster2 NAME BACKUP STATUS CREATED COMPLETED restore1 backup1 Completed 2022-07-27 12:38:43 -0400 EDT Get restores with their names\ndellctl restore get restore1 NAME BACKUP STATUS CREATED COMPLETED restore1 backup1 Completed 2022-07-27 12:35:29 -0400 EDT dellctl schedule Allows you to manipulate schedules\nAvailable Commands create Create a schedule delete Delete schedules get Get schedules Flags -h, --help Help for schedule Output Outputs help text\ndellctl schedule create Create a schedule\nAvailable Commands for-backup Create a schedule for application backups Flags --cluster-id string Id of the cluster managed by dellctl -h, --help Help for create --name string Name for the schedule --schedule string A cron expression representing when to create the application backup Output Outputs help text\ndellctl schedule create for-backup Create a schedule for application backups\nFlags --exclude-namespaces stringArray List of namespace names to exclude from the backup. --include-namespaces stringArray List of namespace names to include in the backup (use '*' for all namespaces). (default *) --ttl duration Backup retention period. (default 720h0m0s) --exclude-resources stringArray Resources to exclude from the backup, formatted as resource.group, such as storageclasses.storage.k8s.io. --include-resources stringArray Resources to include in the backup, formatted as resource.group, such as storageclasses.storage.k8s.io (use '*' for all resources). --backup-location string Storage location where k8s resources and application data will be backed up to. (default \"default\") --data-mover string Data mover to be used to backup application data. (default \"Restic\") --include-cluster-resources optionalBool[=true] Include cluster-scoped resources in the backup -l, --label-selector labelSelector Only backup resources matching this label selector. (default \u003cnone\u003e) --set-owner-references-in-backup optionalBool[=false] Specifies whether to set OwnerReferences on backups created by this schedule. -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") -h, --help Help for for-backup Global Flags --cluster-id string Id of the cluster managed by dellctl --name string Name for the schedule --schedule string A cron expression representing when to create the application backup Output Create a schedule to backup namespace demo, every 1hour\ndellctl schedule create for-backup --name schedule1 --schedule \"@every 1h\" --include-namespaces demo INFO schedule request \"schedule1\" submitted successfully. INFO Run 'dellctl schedule get schedule1' for more details. Create a schedule to backup namespace demo, once a day at midnight and set OwnerReferences on backups created by this schedule\ndellctl schedule create for-backup --name schedule2 --schedule \"@daily\" --include-namespaces demo --set-owner-references-in-backup INFO schedule request \"schedule2\" submitted successfully. INFO Run 'dellctl schedule get schedule2' for more details. Create a schedule to backup namespace demo, at 23:00(11:00 pm) every saturday\ndellctl schedule create for-backup --name schedule3 --schedule \"00 23 * * 6\" --include-namespaces demo INFO schedule request \"schedule3\" submitted successfully. INFO Run 'dellctl schedule get schedule3' for more details. dellctl schedule delete Delete one or more schedules\nFlags --all Delete all schedules --cluster-id string Id of the cluster managed by dellctl --confirm Confirm deletion -h, --help Help for delete -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output Delete a schedule with name\ndellctl schedule delete schedule1 Are you sure you want to continue (Y/N)? y INFO Request to delete schedule \"schedule1\" submitted successfully. Delete multiple schedules\ndellctl schedule delete schedule1 schedule2 Are you sure you want to continue (Y/N)? y INFO Request to delete schedule \"schedule1\" submitted successfully. INFO Request to delete schedule \"schedule2\" submitted successfully. Delete all schedules without asking for user confirmation\ndellctl schedule delete --confirm --all INFO Request to delete schedule \"schedule1\" submitted successfully. INFO Request to delete schedule \"schedule2\" submitted successfully. dellctl schedule get Get schedules\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help Help for get -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output Get all the application schedules created on local cluster\ndellctl schedule get NAME STATUS CREATED PAUSED SCHEDULE LAST BACKUP TIME schedule1 Enabled 2022-11-04 08:33:35 +0000 UTC false @every 1h NA schedule2 Enabled 2022-11-04 08:35:57 +0000 UTC false @daily NA Get schedules with their names\ndellctl schedule get schedule1 NAME STATUS CREATED PAUSED SCHEDULE LAST BACKUP TIME schedule1 Enabled 2022-11-04 08:33:35 +0000 UTC false @every 1h NA dellctl encryption rekey Encryption rekey with a name for the rekey object and volume name of an encrypted volume\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help help for get Output dellctl encryption rekey myrekey k8s-5d2cc565d4 INFO rekey request \"myrekey\" submitted successfully for persistent volume \"k8s-5d2cc565d4\". INFO Run 'dellctl encryption rekey-status myrekey' for more details. dellctl encryption rekey-status Encryption rekey status with name of the rekey object\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help help for get Output dellctl encryption rekey-status myrekey INFO Status of rekey request myrekey = completed dellctl images List the container images needed by csm components\nNOTE.:\nSupported CSM Components [csi-vxflexos,csi-isilon,csi-powerstore,csi-unity,csi-powermax,csm-authorization]\nAliases images,imgs Flags Flags: -c, --component string csm-component name -h, --help help for images Output dellctl images --component csi-vxflexos Driver/Module Image Supported Orchestrator Versions Sidecar Images dellemc/csi-vxflexos:v2.8.0 k8s1.27,k8s1.26,k8s1.25,ocp4.13,ocp4.12 registry.k8s.io/sig-storage/csi-attacher:v4.3.0 registry.k8s.io/sig-storage/csi-provisioner:v3.5.0 registry.k8s.io/sig-storage/csi-external-health-monitor-controller:v0.9.0 registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2 registry.k8s.io/sig-storage/csi-resizer:v1.8.0 registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0 dellemc/sdc:3.6.1 dellemc/csi-vxflexos:v2.7.0 k8s1.27,k8s1.26,k8s1.25,ocp4.12,ocp4.11 registry.k8s.io/sig-storage/csi-attacher:v4.3.0 registry.k8s.io/sig-storage/csi-provisioner:v3.5.0 registry.k8s.io/sig-storage/csi-external-health-monitor-controller:v0.9.0 registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2 registry.k8s.io/sig-storage/csi-resizer:v1.8.0 registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0 dellemc/sdc:3.6.0.6 dellemc/csi-vxflexos:v2.6.0 k8s1.26,k8s1.25,k8s1.24,ocp4.11,ocp4.10 registry.k8s.io/sig-storage/csi-attacher:v4.0.0 registry.k8s.io/sig-storage/csi-provisioner:v3.3.0 registry.k8s.io/sig-storage/csi-external-health-monitor-controller:v0.7.0 registry.k8s.io/sig-storage/csi-snapshotter:v6.1.0 registry.k8s.io/sig-storage/csi-resizer:v1.6.0 registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.6.0 dellemc/sdc:3.6.0.6 dellctl images --component csm-authorization Driver/Module Image Supported Orchestrator Versions Sidecar Images dellemc/csm-authorization-sidecar:v1.8.0 k8s1.27,k8s1.26,k8s1.25 jetstack/cert-manager-cainjector:v1.6.1 jetstack/cert-manager-controller:v1.6.1 jetstack/cert-manager-webhook:v1.6.1 ingress-nginx/controller:v1.4.0 ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343 dellemc/csm-authorization-sidecar:v1.7.0 k8s1.27,k8s1.26,k8s1.25 jetstack/cert-manager-cainjector:v1.6.1 jetstack/cert-manager-controller:v1.6.1 jetstack/cert-manager-webhook:v1.6.1 ingress-nginx/controller:v1.4.0 ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343 dellemc/csm-authorization-sidecar:v1.6.0 k8s1.26,k8s1.25,k8s1.24 jetstack/cert-manager-cainjector:v1.6.1 jetstack/cert-manager-controller:v1.6.1 jetstack/cert-manager-webhook:v1.6.1 ingress-nginx/controller:v1.4.0 ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343 dellctl volume get Gets PowerFlex volume infomation for a given tenant on a local cluster\nAliases get, ls, list\nFlags -h, --help help for get --insecure optionalBool[=true] provide flag to skip certificate validation --namespace string namespace of the secret for the given tenant --proxy string auth proxy endpoint to use Output Gets PowerFlex volume infomation for a given tenant on a local cluster. The namespace is the namespace where tenant secret is created.\nNote: This was output was generated using Authorization Proxy version 1.5.1. Please ensure you are using version 1.5.1 or greater.\ndellctl volume get --proxy \u003cproxy.dell.com\u003e --namespace vxflexos # dellctl volume get --proxy \u003cproxy.dell.com/proxy/volumes\u003e --namespace vxflexos NAME VOLUME ID SIZE POOL SYSTEM ID PV NAME PV STATUS STORAGE CLASS PVC NAME NAMESPACE k8s-e7c8b39112 a69bf18e00000008 8.000000 mypool 636468e3638c840f k8s-e7c8b39112 Released vxflexos demo-claim10 default k8s-e6e2b46103 a69bf18f00000009 8.000000 mypool 636468e3638c840f k8s-e6e2b46103 Bound vxflexos demo-claim11 default k8s-b1abb817d3 a69bf19000000001 8.000000 mypool 636468e3638c840f k8s-b1abb817d3 Bound vxflexos demo-claim13 default k8s-28e4184f41 c6b2280d0000009a 8.000000 mypool 636468e3638c840f k8s-28e4184f41 Available local-storage k8s-7296621062 a69b554f00000004 8.000000 mypool 636468e3638c840f ","categories":"","description":"CLI for Dell Container Storage Modules (CSM)\n","excerpt":"CLI for Dell Container Storage Modules (CSM)\n","ref":"/csm-docs/v1/references/cli/","tags":"","title":"CLI"},{"body":"dellctl is a common command line interface(CLI) used to interact with and manage your Container Storage Modules (CSM) resources. This document outlines all dellctl commands, their intended use, options that can be provided to alter their execution, and expected output from those commands.\nCommand Description dellctl dellctl is used to interact with Container Storage Modules dellctl cluster Manipulate one or more k8s cluster configurations dellctl cluster add Add a k8s cluster to be managed by dellctl dellctl cluster remove Removes a k8s cluster managed by dellctl dellctl cluster get List all clusters currently being managed by dellctl dellctl backup Allows you to manipulate application backups/clones dellctl backup create Create an application backup/clones dellctl backup delete Delete application backups dellctl backup get Get application backups dellctl restore Allows you to manipulate application restores dellctl restore create Restore an application backup dellctl restore delete Delete application restores dellctl restore get Get application restores dellctl schedule Allows you to manipulate schedules dellctl schedule create Create a schedule dellctl schedule create for-backup Create a schedule for application backups dellctl schedule delete Delete schedules dellctl schedule get Get schedules dellctl encryption rekey Rekey an encrypted volume dellctl encryption rekey-status Get status of an encryption rekey operation dellctl images List the container images needed by csi driver dellctl volume get Gets PowerFlex volume infomation for a given tenant on a local cluster Installation instructions Download dellctl from here. chmod +x dellctl Move dellctl to /usr/local/bin or add dellctl’s containing directory path to PATH environment variable. Run dellctl --help to know available commands or run dellctl command --help to know more about a specific command. By default, the dellctl runs against local cluster(referenced by KUBECONFIG environment variable or by a kube config file present at default location). The user can register one or more remote clusters for dellctl, and run any dellctl command against these clusters by specifying the registered cluster id to the command.\nGeneral Commands dellctl dellctl is a CLI tool for managing Dell Container Storage Resources.\nFlags -h, --help help for dellctl -v, --version version for dellctl Output Outputs help text\ndellctl cluster Allows you to manipulate one or more k8s cluster configurations\nAvailable Commands add Adds a k8s cluster to be managed by dellctl remove Removes a k8s cluster managed by dellctl get List all clusters currently being managed by dellctl Flags -h, --help help for cluster Output Outputs help text\ndellctl cluster add Add one or more k8s clusters to be managed by dellctl\nFlags Flags: -n, --names strings cluster names -f, --files strings paths for kube config files -u, --uids strings uids of the kube-system namespaces in the clusters --force forcefully add cluster -h, --help help for add Output dellctl cluster add -n cluster1 -f ~/kubeconfigs/cluster1-kubeconfig INFO Adding clusters ... INFO Cluster: cluster1 INFO Successfully added cluster cluster1 in /root/.dellctl/clusters/cluster1 folder. Add a cluster with it’s uid\ndellctl cluster add -n cluster2 -f ~/kubeconfigs/cluster2-kubeconfig -u \"035133aa-5b65-4080-a813-34a7abe48180\" INFO Adding clusters ... INFO Cluster: cluster2 INFO Successfully added cluster cluster2 in /root/.dellctl/clusters/cluster2 folder. dellctl cluster remove Removes a k8s cluster by name from the list of clusters being managed by dellctl\nAliases remove, rm Flags -h, --help help for remove -n, --name string cluster name Output dellctl cluster remove -n cluster1 INFO Removing cluster with id cluster1 INFO Removed cluster with id cluster1 dellctl cluster get List all clusters currently being managed by dellctl\nAliases get, ls Flags -h, --help help for get Output dellctl cluster get CLUSTER ID VERSION URL UID cluster1 v1.22 https://1.2.3.4:6443 cluster2 v1.22 https://1.2.3.5:6443 035133aa-5b65-4080-a813-34a7abe48180 Commands related to application mobility operations dellctl backup Allows you to manipulate application backups/clones\nAvailable Commands create Create an application backup/clones delete Delete application backups get Get application backups Flags -h, --help help for backup Output Outputs help text\ndellctl backup create Create an application backup/clones\nFlags --cluster-id string Id of the cluster managed by dellctl --exclude-namespaces stringArray List of namespace names to exclude from the backup. --include-namespaces stringArray List of namespace names to include in the backup (use '*' for all namespaces). (default *) --ttl duration Backup retention period. (default 720h0m0s) --exclude-resources stringArray Resources to exclude from the backup, formatted as resource.group, such as storageclasses.storage.k8s.io. --include-resources stringArray Resources to include in the backup, formatted as resource.group, such as storageclasses.storage.k8s.io (use '*' for all resources). --backup-location string Storage location where k8s resources and application data will be backed up to. (default \"default\") --data-mover string Data mover to be used to backup application data. (default \"Restic\") --include-cluster-resources optionalBool[=true] Include cluster-scoped resources in the backup -l, --label-selector labelSelector Only backup resources matching this label selector. (default \u003cnone\u003e) -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") --clones stringArray Creates an application clone into target clusters managed by dellctl. Specify optional namespace mappings where the clone is created. Example: 'cluster1/sourceNamespace1:targetNamespace1', 'cluster1/sourceNamespace1:targetNamespace1;cluster2/sourceNamespace2:targetNamespace2' -h, --help help for create Output Create a backup of the applications running in namespace demo1\ndellctl backup create backup1 --include-namespaces demo1 INFO Backup request \"backup1\" submitted successfully. INFO Run 'dellctl backup get backup1' for more details. Create clones of the application running in namespace demo1, on clusters with id cluster1 and cluster2\ndellctl backup create demo-app-clones --include-namespaces demo1 --clones \"cluster1/demo1:restore-ns1\" --clones \"cluster2/demo1:restore-ns1\" INFO Clone request \"demo-app-clones\" submitted successfully. INFO Run 'dellctl backup get demo-app-clones' for more details. Take backup of application running in namespace demo3 on remote cluster with id cluster2\ndellctl backup create backup4 --include-namespaces demo3 --cluster-id cluster2 INFO Backup request \"backup4\" submitted successfully. INFO Run 'dellctl backup get backup4' for more details. dellctl backup delete Delete one or more application backups\nFlags --all Delete all backups --cluster-id string Id of the cluster managed by dellctl --confirm Confirm deletion -h, --help help for delete -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output dellctl backup delete backup1 Are you sure you want to continue (Y/N)? Y INFO Request to delete backup \"backup1\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. Delete multiple backups\ndellctl backup delete backup1 backup2 Are you sure you want to continue (Y/N)? Y INFO Request to delete backup \"backup1\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. INFO Request to delete backup \"backup2\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. Delete all backups without asking for user confirmation\ndellctl backup delete --all --confirm INFO Request to delete backup \"backup4\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. INFO Request to delete backup \"demo-app-clones\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. dellctl backup get Get application backups\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help help for get -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output dellctl backup get NAME STATUS CREATED EXPIRES STORAGE LOCATION DATA MOVER CLONED TARGET CLUSTERS backup1 Completed 2022-07-27 11:51:00 -0400 EDT 2022-08-26 11:51:00 -0400 EDT default Restic false backup2 Completed 2022-07-27 11:59:24 -0400 EDT 2022-08-26 11:59:42 -0400 EDT default Restic false backup4 Completed 2022-07-27 12:02:54 -0400 EDT NA default Restic false demo-app-clones Restored 2022-07-27 11:53:37 -0400 EDT 2022-08-26 11:53:37 -0400 EDT default Restic true cluster1, cluster2 Get backups from remote cluster with id cluster2\ndellctl backup get --cluster-id cluster2 NAME STATUS CREATED EXPIRES STORAGE LOCATION DATA MOVER CLONED TARGET CLUSTERS backup1 Completed 2022-07-27 11:52:42 -0400 EDT NA default Restic false backup2 Completed 2022-07-27 12:02:29 -0400 EDT NA default Restic false backup4 Completed 2022-07-27 12:01:49 -0400 EDT 2022-08-26 12:01:49 -0400 EDT default Restic false demo-app-clones Completed 2022-07-27 11:54:55 -0400 EDT NA default Restic true cluster1, cluster2 Get backups with their names\ndellctl backup get backup1 demo-app-clones NAME STATUS CREATED EXPIRES STORAGE LOCATION DATA MOVER CLONED TARGET CLUSTERS backup1 Completed 2022-07-27 11:51:00 -0400 EDT 2022-08-26 11:51:00 -0400 EDT default Restic false demo-app-clones Completed 2022-07-27 11:53:37 -0400 EDT 2022-08-26 11:53:37 -0400 EDT default Restic true cluster1, cluster2 dellctl restore Allows you to manipulate application restores\nAvailable Commands create Restore an application backup delete Delete application restores get Get application restores Flags -h, --help help for restore Output Outputs help text\ndellctl restore create Restore an application backup\nFlags --cluster-id string Id of the cluster managed by dellctl --from-backup string Backup to restore from --namespace-mappings mapStringString Map of source namespace names to target namespace names to restore into in the form src1:dst1,src2:dst2,... --exclude-namespaces stringArray List of namespace names to exclude from the backup. --include-namespaces stringArray List of namespace names to include in the backup (use '*' for all namespaces). (default *) --exclude-resources stringArray Resources to exclude from the backup, formatted as resource.group, such as storageclasses.storage.k8s.io. --include-resources stringArray Resources to include in the backup, formatted as resource.group, such as storageclasses.storage.k8s.io (use '*' for all resources). --restore-volumes optionalBool[=true] Whether to restore volumes from snapshots. --include-cluster-resources optionalBool[=true] Include cluster-scoped resources in the backup -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") -h, --help help for create Output Restore application backup backup1 on local cluster in namespace restorens1\ndellctl restore create restore1 --from-backup backup1 --namespace-mappings \"demo1:restorens1\" INFO Restore request \"restore1\" submitted successfully. INFO Run 'dellctl restore get restore1' for more details. Restore application backup backup1 on remote cluster cluster2 in namespace demo1\ndellctl restore create restore1 --from-backup backup1 --cluster-id cluster2 INFO Restore request \"restore1\" submitted successfully. INFO Run 'dellctl restore get restore1' for more details. dellctl restore delete Delete one or more application restores\nFlags --all Delete all restores --cluster-id string Id of the cluster managed by dellctl --confirm Confirm deletion -h, --help help for delete -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output Delete a restore created on remote cluster with id cluster2\ndellctl restore delete restore1 --cluster-id cluster2 Are you sure you want to continue (Y/N)? Y INFO Request to delete restore \"restore1\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. Delete multiple restores\ndellctl restore delete restore1 restore4 Are you sure you want to continue (Y/N)? Y INFO Request to delete restore \"restore1\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. INFO Request to delete restore \"restore4\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. Delete all restores without asking for user confirmation\ndellctl restore delete --all --confirm INFO Request to delete restore \"restore1\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. INFO Request to delete restore \"restore2\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. dellctl restore get Get application restores\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help help for get -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output Get all the application restores created on local cluster\ndellctl restore get NAME BACKUP STATUS CREATED COMPLETED restore1 backup1 Completed 2022-07-27 12:35:29 -0400 EDT restore4 backup1 Completed 2022-07-27 12:39:42 -0400 EDT Get all the application restores created on remote cluster with id cluster2\ndellctl restore get --cluster-id cluster2 NAME BACKUP STATUS CREATED COMPLETED restore1 backup1 Completed 2022-07-27 12:38:43 -0400 EDT Get restores with their names\ndellctl restore get restore1 NAME BACKUP STATUS CREATED COMPLETED restore1 backup1 Completed 2022-07-27 12:35:29 -0400 EDT dellctl schedule Allows you to manipulate schedules\nAvailable Commands create Create a schedule delete Delete schedules get Get schedules Flags -h, --help Help for schedule Output Outputs help text\ndellctl schedule create Create a schedule\nAvailable Commands for-backup Create a schedule for application backups Flags --cluster-id string Id of the cluster managed by dellctl -h, --help Help for create --name string Name for the schedule --schedule string A cron expression representing when to create the application backup Output Outputs help text\ndellctl schedule create for-backup Create a schedule for application backups\nFlags --exclude-namespaces stringArray List of namespace names to exclude from the backup. --include-namespaces stringArray List of namespace names to include in the backup (use '*' for all namespaces). (default *) --ttl duration Backup retention period. (default 720h0m0s) --exclude-resources stringArray Resources to exclude from the backup, formatted as resource.group, such as storageclasses.storage.k8s.io. --include-resources stringArray Resources to include in the backup, formatted as resource.group, such as storageclasses.storage.k8s.io (use '*' for all resources). --backup-location string Storage location where k8s resources and application data will be backed up to. (default \"default\") --data-mover string Data mover to be used to backup application data. (default \"Restic\") --include-cluster-resources optionalBool[=true] Include cluster-scoped resources in the backup -l, --label-selector labelSelector Only backup resources matching this label selector. (default \u003cnone\u003e) --set-owner-references-in-backup optionalBool[=false] Specifies whether to set OwnerReferences on backups created by this schedule. -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") -h, --help Help for for-backup Global Flags --cluster-id string Id of the cluster managed by dellctl --name string Name for the schedule --schedule string A cron expression representing when to create the application backup Output Create a schedule to backup namespace demo, every 1hour\ndellctl schedule create for-backup --name schedule1 --schedule \"@every 1h\" --include-namespaces demo INFO schedule request \"schedule1\" submitted successfully. INFO Run 'dellctl schedule get schedule1' for more details. Create a schedule to backup namespace demo, once a day at midnight and set OwnerReferences on backups created by this schedule\ndellctl schedule create for-backup --name schedule2 --schedule \"@daily\" --include-namespaces demo --set-owner-references-in-backup INFO schedule request \"schedule2\" submitted successfully. INFO Run 'dellctl schedule get schedule2' for more details. Create a schedule to backup namespace demo, at 23:00(11:00 pm) every saturday\ndellctl schedule create for-backup --name schedule3 --schedule \"00 23 * * 6\" --include-namespaces demo INFO schedule request \"schedule3\" submitted successfully. INFO Run 'dellctl schedule get schedule3' for more details. dellctl schedule delete Delete one or more schedules\nFlags --all Delete all schedules --cluster-id string Id of the cluster managed by dellctl --confirm Confirm deletion -h, --help Help for delete -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output Delete a schedule with name\ndellctl schedule delete schedule1 Are you sure you want to continue (Y/N)? y INFO Request to delete schedule \"schedule1\" submitted successfully. Delete multiple schedules\ndellctl schedule delete schedule1 schedule2 Are you sure you want to continue (Y/N)? y INFO Request to delete schedule \"schedule1\" submitted successfully. INFO Request to delete schedule \"schedule2\" submitted successfully. Delete all schedules without asking for user confirmation\ndellctl schedule delete --confirm --all INFO Request to delete schedule \"schedule1\" submitted successfully. INFO Request to delete schedule \"schedule2\" submitted successfully. dellctl schedule get Get schedules\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help Help for get -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output Get all the application schedules created on local cluster\ndellctl schedule get NAME STATUS CREATED PAUSED SCHEDULE LAST BACKUP TIME schedule1 Enabled 2022-11-04 08:33:35 +0000 UTC false @every 1h NA schedule2 Enabled 2022-11-04 08:35:57 +0000 UTC false @daily NA Get schedules with their names\ndellctl schedule get schedule1 NAME STATUS CREATED PAUSED SCHEDULE LAST BACKUP TIME schedule1 Enabled 2022-11-04 08:33:35 +0000 UTC false @every 1h NA dellctl encryption rekey Encryption rekey with a name for the rekey object and volume name of an encrypted volume\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help help for get Output dellctl encryption rekey myrekey k8s-5d2cc565d4 INFO rekey request \"myrekey\" submitted successfully for persistent volume \"k8s-5d2cc565d4\". INFO Run 'dellctl encryption rekey-status myrekey' for more details. dellctl encryption rekey-status Encryption rekey status with name of the rekey object\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help help for get Output dellctl encryption rekey-status myrekey INFO Status of rekey request myrekey = completed dellctl images List the container images needed by csm components\nNOTE.:\nSupported CSM Components [csi-vxflexos,csi-isilon,csi-powerstore,csi-unity,csi-powermax,csm-authorization]\nAliases images,imgs Flags Flags: -c, --component string csm-component name -h, --help help for images Output dellctl images --component csi-vxflexos Driver/Module Image Supported Orchestrator Versions Sidecar Images dellemc/csi-vxflexos:v2.7.0 k8s1.27,k8s1.26,k8s1.25,ocp4.12,ocp4.11 registry.k8s.io/sig-storage/csi-attacher:v4.2.0 registry.k8s.io/sig-storage/csi-provisioner:v3.4.0 registry.k8s.io/sig-storage/csi-external-health-monitor-controller:v0.8.0 registry.k8s.io/sig-storage/csi-snapshotter:v6.2.1 registry.k8s.io/sig-storage/csi-resizer:v1.7.0 registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.6.3 dellemc/sdc:3.6.0.6 dellemc/csi-vxflexos:v2.6.0 k8s1.26,k8s1.25,k8s1.24,ocp4.11,ocp4.10 registry.k8s.io/sig-storage/csi-attacher:v4.0.0 registry.k8s.io/sig-storage/csi-provisioner:v3.3.0 registry.k8s.io/sig-storage/csi-external-health-monitor-controller:v0.7.0 registry.k8s.io/sig-storage/csi-snapshotter:v6.1.0 registry.k8s.io/sig-storage/csi-resizer:v1.6.0 registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.6.0 dellemc/sdc:3.6.0.6 dellemc/csi-vxflexos:v2.5.0 k8s1.25,k8s1.24,k8s1.23,ocp4.11,ocp4.10 registry.k8s.io/sig-storage/csi-attacher:v4.0.0 registry.k8s.io/sig-storage/csi-provisioner:v3.3.0 registry.k8s.io/sig-storage/csi-external-health-monitor-controller:v0.7.0 registry.k8s.io/sig-storage/csi-snapshotter:v6.1.0 registry.k8s.io/sig-storage/csi-resizer:v1.6.0 registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.6.0 dellemc/sdc:3.6.0.6 dellctl images --component csm-authorization Driver/Module Image Supported Orchestrator Versions Sidecar Images dellemc/csm-authorization-sidecar:v1.7.0 k8s1.27,k8s1.26,k8s1.25 jetstack/cert-manager-cainjector:v1.6.1 jetstack/cert-manager-controller:v1.6.1 jetstack/cert-manager-webhook:v1.6.1 ingress-nginx/controller:v1.4.0 ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343 dellemc/csm-authorization-sidecar:v1.6.0 k8s1.26,k8s1.25,k8s1.24 jetstack/cert-manager-cainjector:v1.6.1 jetstack/cert-manager-controller:v1.6.1 jetstack/cert-manager-webhook:v1.6.1 ingress-nginx/controller:v1.4.0 ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343 dellemc/csm-authorization-sidecar:v1.5.0 k8s1.24,k8s1.23,k8s1.22 jetstack/cert-manager-cainjector:v1.6.1 jetstack/cert-manager-controller:v1.6.1 jetstack/cert-manager-webhook:v1.6.1 ingress-nginx/controller:v1.4.0 ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343 dellemc/csm-authorization-sidecar:v1.4.0 k8s1.24,k8s1.23,k8s1.22 jetstack/cert-manager-cainjector:v1.6.1 jetstack/cert-manager-controller:v1.6.1 jetstack/cert-manager-webhook:v1.6.1 ingress-nginx/controller:v1.4.0 ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343 dellctl volume get Gets PowerFlex volume infomation for a given tenant on a local cluster\nAliases get, ls, list\nFlags -h, --help help for get --insecure optionalBool[=true] provide flag to skip certificate validation --namespace string namespace of the secret for the given tenant --proxy string auth proxy endpoint to use Output Gets PowerFlex volume infomation for a given tenant on a local cluster. The namespace is the namespace where tenant secret is created.\nNote: This was output was generated using Authorization Proxy version 1.5.1. Please ensure you are using version 1.5.1 or greater.\ndellctl volume get --proxy \u003cproxy.dell.com\u003e --namespace vxflexos # dellctl volume get --proxy \u003cproxy.dell.com/proxy/volumes\u003e --namespace vxflexos NAME VOLUME ID SIZE POOL SYSTEM ID PV NAME PV STATUS STORAGE CLASS PVC NAME NAMESPACE k8s-e7c8b39112 a69bf18e00000008 8.000000 mypool 636468e3638c840f k8s-e7c8b39112 Released vxflexos demo-claim10 default k8s-e6e2b46103 a69bf18f00000009 8.000000 mypool 636468e3638c840f k8s-e6e2b46103 Bound vxflexos demo-claim11 default k8s-b1abb817d3 a69bf19000000001 8.000000 mypool 636468e3638c840f k8s-b1abb817d3 Bound vxflexos demo-claim13 default k8s-28e4184f41 c6b2280d0000009a 8.000000 mypool 636468e3638c840f k8s-28e4184f41 Available local-storage k8s-7296621062 a69b554f00000004 8.000000 mypool 636468e3638c840f ","categories":"","description":"CLI for Dell Container Storage Modules (CSM)\n","excerpt":"CLI for Dell Container Storage Modules (CSM)\n","ref":"/csm-docs/v2/references/cli/","tags":"","title":"CLI"},{"body":"dellctl is a common command line interface(CLI) used to interact with and manage your Container Storage Modules (CSM) resources. This document outlines all dellctl commands, their intended use, options that can be provided to alter their execution, and expected output from those commands.\nCommand Description dellctl dellctl is used to interact with Container Storage Modules dellctl cluster Manipulate one or more k8s cluster configurations dellctl cluster add Add a k8s cluster to be managed by dellctl dellctl cluster remove Removes a k8s cluster managed by dellctl dellctl cluster get List all clusters currently being managed by dellctl dellctl backup Allows you to manipulate application backups/clones dellctl backup create Create an application backup/clones dellctl backup delete Delete application backups dellctl backup get Get application backups dellctl restore Allows you to manipulate application restores dellctl restore create Restore an application backup dellctl restore delete Delete application restores dellctl restore get Get application restores dellctl schedule Allows you to manipulate schedules dellctl schedule create Create a schedule dellctl schedule create for-backup Create a schedule for application backups dellctl schedule delete Delete schedules dellctl schedule get Get schedules dellctl encryption rekey Rekey an encrypted volume dellctl encryption rekey-status Get status of an encryption rekey operation dellctl images List the container images needed by csi driver dellctl volume get Gets PowerFlex volume infomation for a given tenant on a local cluster Installation instructions Download dellctl from here. chmod +x dellctl Move dellctl to /usr/local/bin or add dellctl’s containing directory path to PATH environment variable. Run dellctl --help to know available commands or run dellctl command --help to know more about a specific command. By default, the dellctl runs against local cluster(referenced by KUBECONFIG environment variable or by a kube config file present at default location). The user can register one or more remote clusters for dellctl, and run any dellctl command against these clusters by specifying the registered cluster id to the command.\nGeneral Commands dellctl dellctl is a CLI tool for managing Dell Container Storage Resources.\nFlags -h, --help help for dellctl -v, --version version for dellctl Output Outputs help text\ndellctl cluster Allows you to manipulate one or more k8s cluster configurations\nAvailable Commands add Adds a k8s cluster to be managed by dellctl remove Removes a k8s cluster managed by dellctl get List all clusters currently being managed by dellctl Flags -h, --help help for cluster Output Outputs help text\ndellctl cluster add Add one or more k8s clusters to be managed by dellctl\nFlags Flags: -n, --names strings cluster names -f, --files strings paths for kube config files -u, --uids strings uids of the kube-system namespaces in the clusters --force forcefully add cluster -h, --help help for add Output # dellctl cluster add -n cluster1 -f ~/kubeconfigs/cluster1-kubeconfig INFO Adding clusters ... INFO Cluster: cluster1 INFO Successfully added cluster cluster1 in /root/.dellctl/clusters/cluster1 folder. Add a cluster with it’s uid\n# dellctl cluster add -n cluster2 -f ~/kubeconfigs/cluster2-kubeconfig -u \"035133aa-5b65-4080-a813-34a7abe48180\" INFO Adding clusters ... INFO Cluster: cluster2 INFO Successfully added cluster cluster2 in /root/.dellctl/clusters/cluster2 folder. dellctl cluster remove Removes a k8s cluster by name from the list of clusters being managed by dellctl\nAliases remove, rm Flags -h, --help help for remove -n, --name string cluster name Output # dellctl cluster remove -n cluster1 INFO Removing cluster with id cluster1 INFO Removed cluster with id cluster1 dellctl cluster get List all clusters currently being managed by dellctl\nAliases get, ls Flags -h, --help help for get Output # dellctl cluster get CLUSTER ID VERSION URL UID cluster1 v1.22 https://1.2.3.4:6443 cluster2 v1.22 https://1.2.3.5:6443 035133aa-5b65-4080-a813-34a7abe48180 Commands related to application mobility operations dellctl backup Allows you to manipulate application backups/clones\nAvailable Commands create Create an application backup/clones delete Delete application backups get Get application backups Flags -h, --help help for backup Output Outputs help text\ndellctl backup create Create an application backup/clones\nFlags --cluster-id string Id of the cluster managed by dellctl --exclude-namespaces stringArray List of namespace names to exclude from the backup. --include-namespaces stringArray List of namespace names to include in the backup (use '*' for all namespaces). (default *) --ttl duration Backup retention period. (default 720h0m0s) --exclude-resources stringArray Resources to exclude from the backup, formatted as resource.group, such as storageclasses.storage.k8s.io. --include-resources stringArray Resources to include in the backup, formatted as resource.group, such as storageclasses.storage.k8s.io (use '*' for all resources). --backup-location string Storage location where k8s resources and application data will be backed up to. (default \"default\") --data-mover string Data mover to be used to backup application data. (default \"Restic\") --include-cluster-resources optionalBool[=true] Include cluster-scoped resources in the backup -l, --label-selector labelSelector Only backup resources matching this label selector. (default \u003cnone\u003e) -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") --clones stringArray Creates an application clone into target clusters managed by dellctl. Specify optional namespace mappings where the clone is created. Example: 'cluster1/sourceNamespace1:targetNamespace1', 'cluster1/sourceNamespace1:targetNamespace1;cluster2/sourceNamespace2:targetNamespace2' -h, --help help for create Output Create a backup of the applications running in namespace demo1\n# dellctl backup create backup1 --include-namespaces demo1 INFO Backup request \"backup1\" submitted successfully. INFO Run 'dellctl backup get backup1' for more details. Create clones of the application running in namespace demo1, on clusters with id cluster1 and cluster2\n# dellctl backup create demo-app-clones --include-namespaces demo1 --clones \"cluster1/demo1:restore-ns1\" --clones \"cluster2/demo1:restore-ns1\" INFO Clone request \"demo-app-clones\" submitted successfully. INFO Run 'dellctl backup get demo-app-clones' for more details. Take backup of application running in namespace demo3 on remote cluster with id cluster2\n# dellctl backup create backup4 --include-namespaces demo3 --cluster-id cluster2 INFO Backup request \"backup4\" submitted successfully. INFO Run 'dellctl backup get backup4' for more details. dellctl backup delete Delete one or more application backups\nFlags --all Delete all backups --cluster-id string Id of the cluster managed by dellctl --confirm Confirm deletion -h, --help help for delete -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output # dellctl backup delete backup1 Are you sure you want to continue (Y/N)? Y INFO Request to delete backup \"backup1\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. Delete multiple backups\n# dellctl backup delete backup1 backup2 Are you sure you want to continue (Y/N)? Y INFO Request to delete backup \"backup1\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. INFO Request to delete backup \"backup2\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. Delete all backups without asking for user confirmation\n# dellctl backup delete --all --confirm INFO Request to delete backup \"backup4\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. INFO Request to delete backup \"demo-app-clones\" submitted successfully. INFO The backup will be fully deleted after all associated data (backup files, pod volume data, restores, velero backup) are removed. dellctl backup get Get application backups\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help help for get -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output # dellctl backup get NAME STATUS CREATED EXPIRES STORAGE LOCATION DATA MOVER CLONED TARGET CLUSTERS backup1 Completed 2022-07-27 11:51:00 -0400 EDT 2022-08-26 11:51:00 -0400 EDT default Restic false backup2 Completed 2022-07-27 11:59:24 -0400 EDT 2022-08-26 11:59:42 -0400 EDT default Restic false backup4 Completed 2022-07-27 12:02:54 -0400 EDT NA default Restic false demo-app-clones Restored 2022-07-27 11:53:37 -0400 EDT 2022-08-26 11:53:37 -0400 EDT default Restic true cluster1, cluster2 Get backups from remote cluster with id cluster2\n# dellctl backup get --cluster-id cluster2 NAME STATUS CREATED EXPIRES STORAGE LOCATION DATA MOVER CLONED TARGET CLUSTERS backup1 Completed 2022-07-27 11:52:42 -0400 EDT NA default Restic false backup2 Completed 2022-07-27 12:02:29 -0400 EDT NA default Restic false backup4 Completed 2022-07-27 12:01:49 -0400 EDT 2022-08-26 12:01:49 -0400 EDT default Restic false demo-app-clones Completed 2022-07-27 11:54:55 -0400 EDT NA default Restic true cluster1, cluster2 Get backups with their names\n# dellctl backup get backup1 demo-app-clones NAME STATUS CREATED EXPIRES STORAGE LOCATION DATA MOVER CLONED TARGET CLUSTERS backup1 Completed 2022-07-27 11:51:00 -0400 EDT 2022-08-26 11:51:00 -0400 EDT default Restic false demo-app-clones Completed 2022-07-27 11:53:37 -0400 EDT 2022-08-26 11:53:37 -0400 EDT default Restic true cluster1, cluster2 dellctl restore Allows you to manipulate application restores\nAvailable Commands create Restore an application backup delete Delete application restores get Get application restores Flags -h, --help help for restore Output Outputs help text\ndellctl restore create Restore an application backup\nFlags --cluster-id string Id of the cluster managed by dellctl --from-backup string Backup to restore from --namespace-mappings mapStringString Map of source namespace names to target namespace names to restore into in the form src1:dst1,src2:dst2,... --exclude-namespaces stringArray List of namespace names to exclude from the backup. --include-namespaces stringArray List of namespace names to include in the backup (use '*' for all namespaces). (default *) --exclude-resources stringArray Resources to exclude from the backup, formatted as resource.group, such as storageclasses.storage.k8s.io. --include-resources stringArray Resources to include in the backup, formatted as resource.group, such as storageclasses.storage.k8s.io (use '*' for all resources). --restore-volumes optionalBool[=true] Whether to restore volumes from snapshots. --include-cluster-resources optionalBool[=true] Include cluster-scoped resources in the backup -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") -h, --help help for create Output Restore application backup backup1 on local cluster in namespace restorens1\n# dellctl restore create restore1 --from-backup backup1 --namespace-mappings \"demo1:restorens1\" INFO Restore request \"restore1\" submitted successfully. INFO Run 'dellctl restore get restore1' for more details. Restore application backup backup1 on remote cluster cluster2 in namespace demo1\n# dellctl restore create restore1 --from-backup backup1 --cluster-id cluster2 INFO Restore request \"restore1\" submitted successfully. INFO Run 'dellctl restore get restore1' for more details. dellctl restore delete Delete one or more application restores\nFlags --all Delete all restores --cluster-id string Id of the cluster managed by dellctl --confirm Confirm deletion -h, --help help for delete -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output Delete a restore created on remote cluster with id cluster2\n# dellctl restore delete restore1 --cluster-id cluster2 Are you sure you want to continue (Y/N)? Y INFO Request to delete restore \"restore1\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. Delete multiple restores\n# dellctl restore delete restore1 restore4 Are you sure you want to continue (Y/N)? Y INFO Request to delete restore \"restore1\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. INFO Request to delete restore \"restore4\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. Delete all restores without asking for user confirmation\n# dellctl restore delete --all --confirm INFO Request to delete restore \"restore1\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. INFO Request to delete restore \"restore2\" submitted successfully. INFO The restore will be fully deleted after all associated data (restore files, velero restore) are removed. dellctl restore get Get application restores\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help help for get -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output Get all the application restores created on local cluster\n# dellctl restore get NAME BACKUP STATUS CREATED COMPLETED restore1 backup1 Completed 2022-07-27 12:35:29 -0400 EDT restore4 backup1 Completed 2022-07-27 12:39:42 -0400 EDT Get all the application restores created on remote cluster with id cluster2\n# dellctl restore get --cluster-id cluster2 NAME BACKUP STATUS CREATED COMPLETED restore1 backup1 Completed 2022-07-27 12:38:43 -0400 EDT Get restores with their names\n# dellctl restore get restore1 NAME BACKUP STATUS CREATED COMPLETED restore1 backup1 Completed 2022-07-27 12:35:29 -0400 EDT dellctl schedule Allows you to manipulate schedules\nAvailable Commands create Create a schedule delete Delete schedules get Get schedules Flags -h, --help Help for schedule Output Outputs help text\ndellctl schedule create Create a schedule\nAvailable Commands for-backup Create a schedule for application backups Flags --cluster-id string Id of the cluster managed by dellctl -h, --help Help for create --name string Name for the schedule --schedule string A cron expression representing when to create the application backup Output Outputs help text\ndellctl schedule create for-backup Create a schedule for application backups\nFlags --exclude-namespaces stringArray List of namespace names to exclude from the backup. --include-namespaces stringArray List of namespace names to include in the backup (use '*' for all namespaces). (default *) --ttl duration Backup retention period. (default 720h0m0s) --exclude-resources stringArray Resources to exclude from the backup, formatted as resource.group, such as storageclasses.storage.k8s.io. --include-resources stringArray Resources to include in the backup, formatted as resource.group, such as storageclasses.storage.k8s.io (use '*' for all resources). --backup-location string Storage location where k8s resources and application data will be backed up to. (default \"default\") --data-mover string Data mover to be used to backup application data. (default \"Restic\") --include-cluster-resources optionalBool[=true] Include cluster-scoped resources in the backup -l, --label-selector labelSelector Only backup resources matching this label selector. (default \u003cnone\u003e) --set-owner-references-in-backup optionalBool[=false] Specifies whether to set OwnerReferences on backups created by this schedule. -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") -h, --help Help for for-backup Global Flags --cluster-id string Id of the cluster managed by dellctl --name string Name for the schedule --schedule string A cron expression representing when to create the application backup Output Create a schedule to backup namespace demo, every 1hour\n# dellctl schedule create for-backup --name schedule1 --schedule \"@every 1h\" --include-namespaces demo INFO schedule request \"schedule1\" submitted successfully. INFO Run 'dellctl schedule get schedule1' for more details. Create a schedule to backup namespace demo, once a day at midnight and set OwnerReferences on backups created by this schedule\n# dellctl schedule create for-backup --name schedule2 --schedule \"@daily\" --include-namespaces demo --set-owner-references-in-backup INFO schedule request \"schedule2\" submitted successfully. INFO Run 'dellctl schedule get schedule2' for more details. Create a schedule to backup namespace demo, at 23:00(11:00 pm) every saturday\n# dellctl schedule create for-backup --name schedule3 --schedule \"00 23 * * 6\" --include-namespaces demo INFO schedule request \"schedule3\" submitted successfully. INFO Run 'dellctl schedule get schedule3' for more details. dellctl schedule delete Delete one or more schedules\nFlags --all Delete all schedules --cluster-id string Id of the cluster managed by dellctl --confirm Confirm deletion -h, --help Help for delete -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output Delete a schedule with name\n# dellctl schedule delete schedule1 Are you sure you want to continue (Y/N)? y INFO Request to delete schedule \"schedule1\" submitted successfully. Delete multiple schedules\n# dellctl schedule delete schedule1 schedule2 Are you sure you want to continue (Y/N)? y INFO Request to delete schedule \"schedule1\" submitted successfully. INFO Request to delete schedule \"schedule2\" submitted successfully. Delete all schedules without asking for user confirmation\n# dellctl schedule delete --confirm --all INFO Request to delete schedule \"schedule1\" submitted successfully. INFO Request to delete schedule \"schedule2\" submitted successfully. dellctl schedule get Get schedules\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help Help for get -n, --namespace string The namespace in which application mobility service should operate. (default \"app-mobility-system\") Output Get all the application schedules created on local cluster\n# dellctl schedule get NAME STATUS CREATED PAUSED SCHEDULE LAST BACKUP TIME schedule1 Enabled 2022-11-04 08:33:35 +0000 UTC false @every 1h NA schedule2 Enabled 2022-11-04 08:35:57 +0000 UTC false @daily NA Get schedules with their names\n# dellctl schedule get schedule1 NAME STATUS CREATED PAUSED SCHEDULE LAST BACKUP TIME schedule1 Enabled 2022-11-04 08:33:35 +0000 UTC false @every 1h NA dellctl encryption rekey Encryption rekey with a name for the rekey object and volume name of an encrypted volume\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help help for get Output # dellctl encryption rekey myrekey k8s-5d2cc565d4 INFO rekey request \"myrekey\" submitted successfully for persistent volume \"k8s-5d2cc565d4\". INFO Run 'dellctl encryption rekey-status myrekey' for more details. dellctl encryption rekey-status Encryption rekey status with name of the rekey object\nFlags --cluster-id string Id of the cluster managed by dellctl -h, --help help for get Output # dellctl encryption rekey-status myrekey INFO Status of rekey request myrekey = completed dellctl images List the container images needed by csi driver\nNOTE.: dellctl images currently supports csi-vxflexos driver only.\nAliases images,imgs Flags Flags: -d, --driver string csi driver name -h, --help help for images Output # dellctl images --driver csi-vxflexos Driver Image Supported Orchestrator Versions Sidecar Images dellemc/csi-vxflexos:v2.5.0 k8s1.25,k8s1.24,k8s1.23,ocp4.11,ocp4.10 k8s.gcr.io/sig-storage/csi-attacher:v4.0.0 k8s.gcr.io/sig-storage/csi-provisioner:v3.3.0 dellemc/csi-volumegroup-snapshotter:v1.2.0 k8s.gcr.io/sig-storage/csi-external-health-monitor-controller:v0.7.0 k8s.gcr.io/sig-storage/csi-snapshotter:v6.1.0 k8s.gcr.io/sig-storage/csi-resizer:v1.6.0 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.6.0 dellemc/sdc:3.6.0.6 dellemc/csi-vxflexos:v2.4.0 k8s1.24,k8s1.23,k8s1.22,ocp4.10,ocp4.9 k8s.gcr.io/sig-storage/csi-attacher:v3.5.0 k8s.gcr.io/sig-storage/csi-provisioner:v3.2.1 dellemc/csi-volumegroup-snapshotter:v1.2.0 k8s.gcr.io/sig-storage/csi-external-health-monitor-controller:v0.6.0 k8s.gcr.io/sig-storage/csi-snapshotter:v6.0.1 k8s.gcr.io/sig-storage/csi-resizer:v1.5.0 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.1 dellemc/sdc:3.6.0.6 dellemc/csi-vxflexos:v2.3.0 k8s1.24,k8s1.23,k8s1.22,ocp4.10,ocp4.9 k8s.gcr.io/sig-storage/csi-attacher:v3.4.0 k8s.gcr.io/sig-storage/csi-provisioner:v3.1.0 dellemc/csi-volumegroup-snapshotter:v1.0.1 gcr.io/k8s-staging-sig-storage/csi-external-health-monitor-controller:v0.5.0 k8s.gcr.io/sig-storage/csi-snapshotter:v5.0.1 k8s.gcr.io/sig-storage/csi-resizer:v1.4.0 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.1 dellemc/sdc:3.6.0.6 dellctl volume get Gets PowerFlex volume infomation for a given tenant on a local cluster\nAliases get, ls, list\nFlags -h, --help help for get --insecure optionalBool[=true] provide flag to skip certificate validation --namespace string namespace of the secret for the given tenant --proxy string auth proxy endpoint to use Output Gets PowerFlex volume infomation for a given tenant on a local cluster. The namespace is the namespace where tenant secret is created.\nNote: This was output was generated using Authorization Proxy version 1.5.1. Please ensure you are using version 1.5.1 or greater.\n# dellctl volume get --proxy \u003cproxy.dell.com/proxy/volumes\u003e --namespace vxflexos NAME VOLUME ID SIZE POOL SYSTEM ID PV NAME PV STATUS STORAGE CLASS PVC NAME NAMESPACE k8s-e7c8b39112 a69bf18e00000008 8.000000 mypool 636468e3638c840f k8s-e7c8b39112 Released vxflexos demo-claim10 default k8s-e6e2b46103 a69bf18f00000009 8.000000 mypool 636468e3638c840f k8s-e6e2b46103 Bound vxflexos demo-claim11 default k8s-b1abb817d3 a69bf19000000001 8.000000 mypool 636468e3638c840f k8s-b1abb817d3 Bound vxflexos demo-claim13 default k8s-28e4184f41 c6b2280d0000009a 8.000000 mypool 636468e3638c840f k8s-28e4184f41 Available local-storage k8s-7296621062 a69b554f00000004 8.000000 mypool 636468e3638c840f ","categories":"","description":"CLI for Dell Container Storage Modules (CSM)\n","excerpt":"CLI for Dell Container Storage Modules (CSM)\n","ref":"/csm-docs/v3/references/cli/","tags":"","title":"CLI"},{"body":"Dell COSI Driver Configuration Schema This configuration file is used to specify the settings for the Dell COSI Driver, which is responsible for managing connections to the Dell ObjectScale platform. The configuration file is written in YAML format and based on the JSON schema and adheres to its specification.\nYAML files can have comments, which are lines in the file that begin with the # character. Comments can be used to provide context and explanations for the data in the file, and they are ignored by parsers when reading the YAML data.\nConfiguration file example # This is an example of a configuration file. You MUST edit the file before using it in your environment. # List of connections to object storage platforms that is used for object storage provisioning. connections: # Configuration specific to the Dell ObjectScale platform. - objectscale: # Default, unique identifier for the single connection. # # It MUST NOT contain any hyphens '-'. # # REQUIRED id: example.id # Credentials used for authentication to object storage provider. # # REQUIRED credentials: # Username used to login to ObjectScale Management API # # REQUIRED username: testuser # Password used to login to ObjectScale Management API # # REQUIRED password: testpassword # Namespace associated with the user/tenant that is allowed to access the bucket. # It can be retrieved from the ObjectScale Portal, under the Accounts tab. # # How to: # 1. Login into ObjectScale Portal; # 2. Select Accounts tab in the panel on the left side of your screen; # 3. You should now see list of accounts. Select one of the values from column called 'Account ID'. # # REQUIRED namespace: osaia3382ab190a7a3df # The ID of the ObjectScale the driver should communicate with. # It can be retrieved from the ObjectScale Portal, under the ObjectScale tab. # # How to: # 1. Login into ObjectScale Portal; # 2. From the menu on left side of the screen select 'Administration' tab; # 3. Expand the 'Administration tab and select 'ObjectScale'; # 4. Select 'Federation' tab; # 5. In the table you will see value under 'ObjectScale ID' column. # # REQUIRED objectscale-id: osci809ccd51aade874b # The ID of the Objectstore under specific ObjectScale, with which the driver should communicate. # It can be retrieved from the ObjectScale Portal, under the ObjectScale tab. # # How to: # 1. Login into ObjectScale Portal; # 2. From the menu on left side of the screen select 'Administration' tab; # 3. Expand the 'Administration tab and select 'ObjectScale'; # 4. Select one of the object stores visible in the table, and click its name; # 5. You should see 'Summary' of that object store. # 6. In the 'General' section, you will see value under 'Object store ID' column. # # REQUIRED objectstore-id: ostibd2054393c389b1a # Endpoint of the ObjectScale Gateway Internal service. # It can be retrieved from the ObjectScale Portal, under the ObjectScale tab. # # How to: # 1. Login into ObjectScale Portal; # 2. From the menu on left side of the screen select 'Administration' tab; # 3. Expand the 'Administration tab and select 'ObjectScale'; # 4. Select 'Federation' tab; # 5. In the table you will see one or more values, expand the selected value; # 6. In the table, you will now see 'External Endpoint' value associated with 'objectscale-gateway-internal'. # # Valid values: # - https://\u003cIP-ADDRESS\u003e:443 # - https://\u003cEXTERNAL-HOSTNAME\u003e # # REQUIRED objectscale-gateway: https://gateway.objectscale.test:443 # Endpoint of the ObjectScale ObjectStore Management Gateway service. # It can be retrieved from the ObjectScale Portal, under the ObjectScale tab. # # How to: # 1. Login into ObjectScale Portal; # 2. From the menu on left side of the screen select 'Administration' tab; # 3. Expand the 'Administration' tab, and select 'ObjectScale'; # 4. Select one of the object stores visible in the table, and click its name; # 5. You should see 'Summary' of that object store. # 6. In the 'Management Service details' section, you will see value under 'IP address' column. # # Valid values: # - https://\u003cIP-ADDRESS\u003e:4443 # - https://\u003cEXTERNAL-HOSTNAME\u003e # # REQUIRED objectstore-gateway: https://gateway.objectstore.test:4443 # Identity and Access Management (IAM) API specific field. # It points to the region in which object storage provider is installed. # # OPTIONAL region: us-east-1 # Indicates if the contents of the bucket should be emptied as part of the deletion process # # Possible values: # - true - bucket will be emptied during the deletion. # - false - default - deletion of bucket will fail if the bucket is not empty. # All contents of the bucket must be cleared manually. # # OPTIONAL emptyBucket: false # Protocols supported by the connection # # Valid values: # s3 (property) # # REQUIRED protocols: # S3 configuration # # REQUIRED s3: # Endpoint of the S3 service. # It can be retrieved from the ObjectScale Portal, under the ObjectScale tab. # # How to: # 1. Login into ObjectScale Portal; # 2. From the menu on left side of the screen select 'Administration' tab; # 3. Expand the 'Administration tab and select 'ObjectScale'; # 4. Select one of the object stores visible in the table, and click its name; # 5. You should see 'Summary' of that object store. # 6. In the 'S3 Service details' section, you will see value under 'IP address' column. # # Valid values: # - https://\u003cIP-ADDRESS\u003e:443 # - https://\u003cEXTERNAL-HOSTNAME\u003e # - http://\u003cIP-ADDRESS\u003e:80 # - http://\u003cEXTERNAL-HOSTNAME\u003e # # REQUIRED endpoint: https://s3.objectstore.test # TLS configuration details # # REQUIRED tls: # Controls whether a client verifies the server's certificate chain and host name. # # Possible values: # - true - default # - false # # REQUIRED insecure: false # Base64 encoded content of the root certificate authority file. # # How To: # 1. Fetch the certificate from the ObjectScale: # $ openssl s_client -showcerts -connect [ObjectScale IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e root.crt # 2. Encode the data using the following commands: # $ cat root.crt | base64 \u003e root.crt.b64 # 3. Open the 'root.crt.b64' file, copy it contents, and paste to the configuration file # # REQUIRED: # + if insecure is set to false root-cas: |- LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUU1RENDQXN5Z0F3SUJBZ0lCQVRBTkJna3Fo a2lHOXcwQkFRc0ZBREFTTVJBd0RnWURWUVFERXdkMFpYTjAKTFdOaE1CNFhEVEl6TURNeE56RXlN ek16TTFvWERUSTBNRGt4TnpFeU5ETXpNRm93RWpFUU1BNEdBMVVFQXhNSApkR1Z6ZEMxallUQ0NB aUl3RFFZSktvWklodmNOQVFFQkJRQURnZ0lQQURDQ0Fnb0NnZ0lCQU9oUmc1Um95UXdxCmVtQ1VN TDU3cXVLSXJjMWZXdGdlSGRpbVRSamFsVERQMStqYUhGeG56d2M1MTRwOWNLNzcxRWZ2bDRjZW9Q VWsKWnRhNSsxckRxdlBkd25BMnE2TXI5cFB2aWQyRkRiZVZPdXNIaHNQSG1kMDVxa1pnNGNXUGdp eXlSM3BmNTF0bApVYkxyNU1tL0FIK0JvRHVMbFo1UG5SVUw1b0hFd1hQa3BXc0UyMXJDc2xSdmJv WWZJYlplUzlsOHhlYURMVmdDCk53UmFHRjgxTFpoZjVrTDA0SXJUV0dETzdlbVF0S2tpN0dSZ1Ex bHIxRHR3SXZpa0puakhBeEJiOTJ3WDN1WnoKcGdMQksxU2RsUlY1bjY2VTZtUklzMGo1MkVyTG1h TDdUSHJxRVZHRXNvczFIbEZFQ2NJMlNhQjZZdmltaTdZawpmT1lOS2NPaE5BcXlXcWhlUERHQ0dq d3l4RHR3OWN2Z2FJSTlTOFFUa2w5Z1JiL056dFlMREptejlEYXZiRWNjCjRDelZBdUVmdUVtWUNi aFRrUVUyWitZczlKdXgwdmc4WXFFTExlRzlNZHc1cmZJQkkwNmRMRDVkU0JUVFc1Y08KYjRNN0h1 ODhrZUdIWnlNZXU2cVMyR2czUUFTVEM3RkpFcWFYTkRDc095aCs2Uk14UnkyZy9idEZMRm5VdmlG QQo0NktKZHk0QWVjOEpXVkc1OFlLYkd2QlJrekkzY1BNWE1oWFpDS3pZb0tnUWoxMnFOMWM0SkVp TUFPK2F2ZW9RCjB0dnJmd3MxMlF3d3ZIZm40SCtYVnlDZGpMcDE5dlhlY0FSRFJyaGlkRW1CbEFD cVJVdTFLSGhzejZ2TmxzUzIKSlZiWU9BYW5ISzYzNzdYT211OUthL2x1TmxSVDdmckxBZ01CQUFH alJUQkRNQTRHQTFVZER3RUIvd1FFQXdJQgpCakFTQmdOVkhSTUJBZjhFQ0RBR0FRSC9BZ0VBTUIw R0ExVWREZ1FXQkJSbDk4cG1valVUQ3RZb3phTDl6L0hSCmJIUkdkREFOQmdrcWhraUc5dzBCQVFz RkFBT0NBZ0VBNUVxL09ocGs0MUxLa3R2ajlmSWNFWXI5Vi9QUit6Z1UKQThRTUtvOVBuZSsrdW1N dEZEK1R1M040b1lxV0srTmg0ajdHTm80RFJXejNnbWpZdUlDdklhMGo5emppT1FkWgo1Q2xVQkdk YUlScFoyRG5CblBUM2tJbnhSd0hmU0JMVlVTRXRTcXh4YkV2dk5LWkZWY0lsRUV5ODZodnJ5OUZD CjhFOWRXWEw5VDhMd29uVXpqSjBxZ242cGRjNHpjdEtUMDFjaDQvWGw2UjBVQkR5Q1NoSGFyU29C eTkvSk1NTXIKajBoeEZSN3Izb052a2N3QWl6T1RsQ3BWdTZaNHF2cng3NndCc0hIanV6elNiODJL dUxnelJUNElWbjFjbzRrVQpSaTlBRkNaRlh6QklaQlEwTUZ6NU03bzJkN0ovN3ZMOFhYRlhwWlpy K3RibWE1L3BCSmZhcXliK3FPRXViWGdUCjFsSDZGeFNVcWt0TktQNlZoeWdQY2ZSMlR4YWtHZ0cw Ny9qVWZWRmhpVXM5aFBlejh6Sjg2RWMrd283VEVQbEsKSlRnMHZmMDM4MTROR3ZuWmlpTnBFWVBM S0ZhcHlDMWJONVdFTGFTWFVBaVFPZDJjK01xVHAyN21vV1RZa29TOApzRFczRTMraEN6c1djdmFY RW1nMjZJTjQybmVUWFBuNS9QajNpcUVoT0pQYkJsY3l6dDBZL1BYeU1jR3JtbUs1CkhxOUMzTndl VUV3M09rY09BOXlCdC9kLzZ5S3c3QmovSlFQZGI0aDlWWjNGN09wemFpeXQ5cFhvSXRQMHNUSHUK S2ZKbDBCRUFYV29SR2lWM2EyeUlUcGp0a0pkQVBoS0xpSkkrWWowZEVEU05WZnlENFhJTXdQSmpV eFpsd2FROQorQUtkVDFBdlplbz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= # Base64 encoded content of the clients's certificate file. # # How To: # Considering client certificate file is named 'client.crt', you can obtain the data using the following commands: # cat client.crt | base64 \u003e client.crt.b64 # You can then open the 'client.crt.b64' file, copy it contents, and paste to the configuration file # # It is required only if the server requires client authentication. # It is mutually required if the field client-key has a value. # # REQUIRED: # + if insecure is set to false # AND # + the client-key field is not empty client-cert: |- LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUVORENDQWh5Z0F3SUJBZ0lSQU9JSlZ2NnB3 a0lIK0p1NTNKSEFuam93RFFZSktvWklodmNOQVFFTEJRQXcKRWpFUU1BNEdBMVVFQXhNSGRHVnpk QzFqWVRBZUZ3MHlNekF6TVRjeE1qTTJNelphRncweU5EQTVNVGN4TWpRegpNamxhTUJFeER6QU5C Z05WQkFNVEJtTnNhV1Z1ZERDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDCkFRb0Nn Z0VCQU5LVFNHeEEyV2RyNmtCR0N3RjY5c1JVZElPV0xqeTUvN3QyRktKWDVVenNyMDlFWW9tS0sr bVQKdWF2eWJIMWhsbTYzdG5kb3VFOHFIQnVhYmYvUGIzSlRTQ0twR0NRdHR2NmQzeGc3MHFZVWIx cUZKT2o5andlNgpRZW0xb2RIVFpLc0xMc2J1N1Fzei91MGtseUovMHNYcFQ5K2JXK1M0OHMrL3pK dHNDR21SdVhlRjE2Y1FqOWErCkFFejNqVzhrdExMYi9nS25GWGRSS2FiY2RWLzNzN2RLNWx0SXpS ZlRvUWw0bzBpckpOa3Z4eXIrYUtMMTR4NUQKc3g2Wm9DUHJhRFYrWWlRS0ZSenFjQ1RYcWdRb3BY LzFINFRMV3RkeG14M25IdmhZdzB0VlBZSXZsa245NmpJUwpKdVE2K1VMbVAzZDNzNWJadlhQeUZD bENKSENxaWZNQ0F3RUFBYU9CaFRDQmdqQU9CZ05WSFE4QkFmOEVCQU1DCkE3Z3dIUVlEVlIwbEJC WXdGQVlJS3dZQkJRVUhBd0VHQ0NzR0FRVUZCd01DTUIwR0ExVWREZ1FXQkJTRWVIOTEKVnBhdDlV SWlrRUdkc1ljdUI2dWxOakFmQmdOVkhTTUVHREFXZ0JSbDk4cG1valVUQ3RZb3phTDl6L0hSYkhS RwpkREFSQmdOVkhSRUVDakFJZ2daamJHbGxiblF3RFFZSktvWklodmNOQVFFTEJRQURnZ0lCQUQv TnZVNWRSajlHCmMzYzVVQ3VLcDI4U2lacjAySE40M091WU5QVlk4L1c5QnZUSk5yMXRPRDFscnhE eFMzTkpVdzdGaTNidmU5enMKSzA0a09peUxpVjRLd0g2eitpVm8xZU9GUzJLd1BRaGxsaDlobVBB dXZ4Zm5Fd2k2ZEdXZm5nNExmQ1FvbXFkTgpmbkFCODJBbTViZTBubGJvaGdLcFJUWnVBZjR4dVY4 SWxlQ1pjVHdFL1hBbERhNVhHaDNvWlE3REYrQnFLSkNUCk1pYS9MT0JPYXRoRVh5ZGJmbndOUUhy UWlQZzk4c2NMc3FTZEFQMFNGYjMrMmdscFJZT1JrQlFvOWRoa1pGZXkKc2tUakVhbk9YaUhqWldq aXZRS2Z2WEUvK1l2eGpCcEJqREE2NnYyeUgzSlJqZEM5ZTR2cnE2R0t6VXZML3ltOQpVOGdVWnho L2ZmeFp4TVA5UmxXajQ0U1NGUVpZNGxUNFF5U2lteFpGdVBTamwzV29QME12UHVvUzFUUzhQUk5s CnVGeXBVell5SEtlbHpLUnRJZmlnWG9XQi9uR2hSV0RMN2FZS0xYZWRIU0ZrdXBmZm9YM1hHQThM ZVAwQ01PaEsKUUJaUkxIeXU0VjhvRG1lakFIcFoyVjlpY2E1emtmcnJWVXFvSzF1VjYvdHd3cEZG WDErN0w1bk0ybDJDQWxvegpaVHFUZzNCdVdYd2VkYzZQbkpuU2xQSDNadFhqcGFJUWhXdU85TUlG WFVtVFBlSkZ2WGxKeWRsdUxtMlQzanVqCldiVENGcEhyMXBrMGk3K1J4ZVRBcFY0RTk2S09DOXEw ZGREOG1waTM0cnkyZjFmQ2RZekhQM0s4bW5od3BPWmkKaG1Xd3VWVDV3em5kVWVBRGNWYUY2UlhU UENKSElLd24KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= # Base64 encoded content of the clients's key certificate file. # # How To: # Considering client certificate file is named 'client.key', you can obtain the data using the following commands: # cat client.key | base64 \u003e client.key.b64 # You can then open the 'client.key.b64' file, copy it contents, and paste to the configuration file # # It is required only if the server requires client authentication. # It is mutually required if the field client-cert has a value. # # REQUIRED: # + if insecure is set to false # AND # + the client-cert field is not empty client-key: |- LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcFFJQkFBS0NBUUVBMHBOSWJFRFpa MnZxUUVZTEFYcjJ4RlIwZzVZdVBMbi91M1lVb2xmbFRPeXZUMFJpCmlZb3I2Wk81cS9Kc2ZXR1di cmUyZDJpNFR5b2NHNXB0Lzg5dmNsTklJcWtZSkMyMi9wM2ZHRHZTcGhSdldvVWsKNlAyUEI3cEI2 YldoMGROa3F3c3V4dTd0Q3pQKzdTU1hJbi9TeGVsUDM1dGI1TGp5ejcvTW0yd0lhWkc1ZDRYWApw eENQMXI0QVRQZU5ieVMwc3R2K0FxY1ZkMUVwcHR4MVgvZXp0MHJtVzBqTkY5T2hDWGlqU0tzazJT L0hLdjVvCm92WGpIa096SHBtZ0krdG9OWDVpSkFvVkhPcHdKTmVxQkNpbGYvVWZoTXRhMTNHYkhl Y2UrRmpEUzFVOWdpK1cKU2YzcU1oSW01RHI1UXVZL2QzZXpsdG05Yy9JVUtVSWtjS3FKOHdJREFR QUJBb0lCQUJFSVVzSlcySDd5RHFlVwpRc3VpMjVUejA5elU1L2FIZ1BUenp5VjJnSmloU0dqYitq QnYyYTl5QUlHMUFTdC9Ha0RvWVR6MVhuc2d4OWMvCnZZZ0VpbG92L0ZTNVlyZUNieHZYUHpWaG1W OVBwZFlua04yN3JMY09UTWlQcFlBb1hpc3JvMlA1N1hpTGd5SkIKWkd3bzlLNkhlYXQza0k1R20z Vk1hVXRsQ0tVcE84cUwzcEZ4S1AwMVVwbGh6ZjhMbXJpTUJQMDlxdFFJejBydQpiR1l5eUdVdk9a a0RKZFJycmlSWGJWK0RNMFlmbVpqU1Q4aEI0UDlsOEhwMEZRNUp2TWVGREpzRFFaZjVBZnJmClFI WE55SlFUeTNTeXJ1bGd5N0p4MGY1T2JpVWRMRWViQVRpN3VLR3Y5UEZRRUJmSzdFdE4vZ1ZibGsx MzRzNUIKWEhkNXU1a0NnWUVBNDBVMjhONko4QXIwY2puYnNLUUJtOGhURWlJSjk3TEJPOU5kOTlJ M1dJYklZVzIzVE5wVwo0M2R4K1JHelA4eVMzYzZhN00wbzR1dUl6TXFDSkV3cVNJUjAvVGZaWWdx cGtwcFZPalp2VFdCUDFtSUlKUFpwCll1SFk0UVRJdkdhcVFNNnFWQXA4MW9YdXoxTmNmQWpTLzNJ Z1BWdGVZeDNKd0pmNWVqenZQclVDZ1lFQTdUSEwKR3VCTWpqTWVhaWk1ZU1sU1BndkYxMHJISUs0 RzZCZUJDTFFXU2ViNmNOT2x2a1RaOTNqdlFiWko1L3JBTGNWNgpaTVdqbWY5Tkl0NWdDdyt2K2dM Qm9BZXM3WEk2K2Rpdk1DYXE0dUFmWkhJWjBYbXpIOGx1a0o5ZUtyK2NyR2FzClNhWkdKRnlyQTZz WGdOc1ZJUm85RkFsR3V1dGZnd2hSUmo1eFp3Y0NnWUVBZ241MWcyeGtDMTVlNlU5clkwdG8KV1Fo M0dreE5LTnFNdFVzeUExL0N3NlB3WG5EZTlOUFJYQjV6WkszVEhHamNVMXVUL1MvM3NBUEpzcno4 YU5jSwoyRVNsMzljM2pHSE82QXlScnpFZVMzRm5waEwzMWpGZVpaYUVMdi9PT3M5QUpxSURqdW5P c0dhS3JxU1F6KzlKCko3OWgzNWtjNHhCeGpaSTFmd2lKM3BrQ2dZRUFwUnBOMkExYy9IWlVxMnho ZmRRVXJSK2d2TFZPV2s4SWU3RXcKbmhCTW0zQnR6dTlqcFVkanVVQ3l1YmpiUk9CanVQaUdzM0pt NktDdTNxQ1BsZU43aUxrMmNlQWwzTG53bDB6ZQoxTk4xaTZxWjcxOEUzYXlxcEd1ZnpJZENFdHVC Z1BlTzRVMGQ4ZDJYSkZ5SlphWVoxUXJnalB2UUFmZ29hWnIyCmg4Q2JTeTBDZ1lFQW1VQ3BqR0JW MGNpVnlmUXNmOGdsclNOdWx6NzBiaVJWQzVSeno0dVJEMkhsYVM2eC8wc0IKQzltSUhpdWgwR0Zp dEVFRlg4TzdlZ1ppNWJKMGFuQWYyakk1R1RnTjJOYzFpVlZnWldxcHh2aXpuckpKcENSYgpaejB0 M2thTkkyNjg0WTNxS2JxeG8ramRNK05hMG1qd2ErTEFOcEdCUDNwb2c0RHJ4eTNNSFdZPQotLS0t LUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo= ","categories":"","description":"Description of configuration file for ObjectScale","excerpt":"Description of configuration file for ObjectScale","ref":"/csm-docs/docs/cosidriver/installation/configuration_file/","tags":"","title":"Configuration File"},{"body":" Notational Conventions\nThe keywords “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD NOT”, “RECOMMENDED”, “NOT RECOMMENDED”, “MAY”, and “OPTIONAL” are to be interpreted as described in RFC 2119 (Bradner, S., “Key words for use in RFCs to Indicate Requirement Levels”, BCP 14, RFC 2119, March 1997).\nDell COSI Driver Configuration Schema This configuration file is used to specify the settings for the Dell COSI Driver, which is responsible for managing connections to the Dell ObjectScale platform. The configuration file is written in YAML format and based on the JSON schema and adheres to its specification.\nYAML files can have comments, which are lines in the file that begin with the # character. Comments can be used to provide context and explanations for the data in the file, and they are ignored by parsers when reading the YAML data.\nConfiguration file example # This is an example of a configuration file. You MUST edit the file before using it in your environment. # List of connections to object storage platforms that is used for object storage provisioning. connections: # Configuration specific to the Dell ObjectScale platform. - objectscale: # Default, unique identifier for the single connection. # # It MUST NOT contain any hyphens '-'. # # REQUIRED id: example.id # Credentials used for authentication to object storage provider. # # REQUIRED credentials: # Username used to login to ObjectScale Management API # # REQUIRED username: testuser # Password used to login to ObjectScale Management API # # REQUIRED password: testpassword # Namespace associated with the user/tenant that is allowed to access the bucket. # It can be retrieved from the ObjectScale Portal, under the Accounts tab. # # How to: # 1. Login into ObjectScale Portal; # 2. Select Accounts tab in the panel on the left side of your screen; # 3. You should now see list of accounts. Select one of the values from column called 'Account ID'. # # REQUIRED namespace: osaia3382ab190a7a3df # The ID of the ObjectScale the driver should communicate with. # It can be retrieved from the ObjectScale Portal, under the ObjectScale tab. # # How to: # 1. Login into ObjectScale Portal; # 2. From the menu on left side of the screen select 'Administration' tab; # 3. Expand the 'Administration tab and select 'ObjectScale'; # 4. Select 'Federation' tab; # 5. In the table you will see value under 'ObjectScale ID' column. # # REQUIRED objectscale-id: osci809ccd51aade874b # The ID of the Objectstore under specific ObjectScale, with which the driver should communicate. # It can be retrieved from the ObjectScale Portal, under the ObjectScale tab. # # How to: # 1. Login into ObjectScale Portal; # 2. From the menu on left side of the screen select 'Administration' tab; # 3. Expand the 'Administration tab and select 'ObjectScale'; # 4. Select one of the object stores visible in the table, and click its name; # 5. You should see 'Summary' of that object store. # 6. In the 'General' section, you will see value under 'Object store ID' column. # # REQUIRED objectstore-id: ostibd2054393c389b1a # Endpoint of the ObjectScale Gateway Internal service. # It can be retrieved from the ObjectScale Portal, under the ObjectScale tab. # # How to: # 1. Login into ObjectScale Portal; # 2. From the menu on left side of the screen select 'Administration' tab; # 3. Expand the 'Administration tab and select 'ObjectScale'; # 4. Select 'Federation' tab; # 5. In the table you will see one or more values, expand the selected value; # 6. In the table, you will now see 'External Endpoint' value associated with 'objectscale-gateway-internal'. # # Valid values: # - https://\u003cIP-ADDRESS\u003e:443 # - https://\u003cEXTERNAL-HOSTNAME\u003e # # REQUIRED objectscale-gateway: https://gateway.objectscale.test:443 # Endpoint of the ObjectScale ObjectStore Management Gateway service. # It can be retrieved from the ObjectScale Portal, under the ObjectScale tab. # # How to: # 1. Login into ObjectScale Portal; # 2. From the menu on left side of the screen select 'Administration' tab; # 3. Expand the 'Administration' tab, and select 'ObjectScale'; # 4. Select one of the object stores visible in the table, and click its name; # 5. You should see 'Summary' of that object store. # 6. In the 'Management Service details' section, you will see value under 'IP address' column. # # Valid values: # - https://\u003cIP-ADDRESS\u003e:4443 # - https://\u003cEXTERNAL-HOSTNAME\u003e # # REQUIRED objectstore-gateway: https://gateway.objectstore.test:4443 # Identity and Access Management (IAM) API specific field. # It points to the region in which object storage provider is installed. # # OPTIONAL region: us-east-1 # Indicates if the contents of the bucket should be emptied as part of the deletion process # # Possible values: # - true - bucket will be emptied during the deletion. # - false - default - deletion of bucket will fail if the bucket is not empty. # All contents of the bucket must be cleared manually. # # OPTIONAL emptyBucket: false # Protocols supported by the connection # # Valid values: # s3 (property) # # REQUIRED protocols: # S3 configuration # # REQUIRED s3: # Endpoint of the S3 service. # It can be retrieved from the ObjectScale Portal, under the ObjectScale tab. # # How to: # 1. Login into ObjectScale Portal; # 2. From the menu on left side of the screen select 'Administration' tab; # 3. Expand the 'Administration tab and select 'ObjectScale'; # 4. Select one of the object stores visible in the table, and click its name; # 5. You should see 'Summary' of that object store. # 6. In the 'S3 Service details' section, you will see value under 'IP address' column. # # Valid values: # - https://\u003cIP-ADDRESS\u003e:443 # - https://\u003cEXTERNAL-HOSTNAME\u003e # - http://\u003cIP-ADDRESS\u003e:80 # - http://\u003cEXTERNAL-HOSTNAME\u003e # # REQUIRED endpoint: https://s3.objectstore.test # TLS configuration details # # REQUIRED tls: # Controls whether a client verifies the server's certificate chain and host name. # # Possible values: # - true - default # - false # # REQUIRED insecure: false # Base64 encoded content of the root certificate authority file. # # How To: # 1. Fetch the certificate from the ObjectScale: # $ openssl s_client -showcerts -connect [ObjectScale IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e root.crt # 2. Encode the data using the following commands: # $ cat root.crt | base64 \u003e root.crt.b64 # 3. Open the 'root.crt.b64' file, copy it contents, and paste to the configuration file # # REQUIRED: # + if insecure is set to false root-cas: |- LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUU1RENDQXN5Z0F3SUJBZ0lCQVRBTkJna3Fo a2lHOXcwQkFRc0ZBREFTTVJBd0RnWURWUVFERXdkMFpYTjAKTFdOaE1CNFhEVEl6TURNeE56RXlN ek16TTFvWERUSTBNRGt4TnpFeU5ETXpNRm93RWpFUU1BNEdBMVVFQXhNSApkR1Z6ZEMxallUQ0NB aUl3RFFZSktvWklodmNOQVFFQkJRQURnZ0lQQURDQ0Fnb0NnZ0lCQU9oUmc1Um95UXdxCmVtQ1VN TDU3cXVLSXJjMWZXdGdlSGRpbVRSamFsVERQMStqYUhGeG56d2M1MTRwOWNLNzcxRWZ2bDRjZW9Q VWsKWnRhNSsxckRxdlBkd25BMnE2TXI5cFB2aWQyRkRiZVZPdXNIaHNQSG1kMDVxa1pnNGNXUGdp eXlSM3BmNTF0bApVYkxyNU1tL0FIK0JvRHVMbFo1UG5SVUw1b0hFd1hQa3BXc0UyMXJDc2xSdmJv WWZJYlplUzlsOHhlYURMVmdDCk53UmFHRjgxTFpoZjVrTDA0SXJUV0dETzdlbVF0S2tpN0dSZ1Ex bHIxRHR3SXZpa0puakhBeEJiOTJ3WDN1WnoKcGdMQksxU2RsUlY1bjY2VTZtUklzMGo1MkVyTG1h TDdUSHJxRVZHRXNvczFIbEZFQ2NJMlNhQjZZdmltaTdZawpmT1lOS2NPaE5BcXlXcWhlUERHQ0dq d3l4RHR3OWN2Z2FJSTlTOFFUa2w5Z1JiL056dFlMREptejlEYXZiRWNjCjRDelZBdUVmdUVtWUNi aFRrUVUyWitZczlKdXgwdmc4WXFFTExlRzlNZHc1cmZJQkkwNmRMRDVkU0JUVFc1Y08KYjRNN0h1 ODhrZUdIWnlNZXU2cVMyR2czUUFTVEM3RkpFcWFYTkRDc095aCs2Uk14UnkyZy9idEZMRm5VdmlG QQo0NktKZHk0QWVjOEpXVkc1OFlLYkd2QlJrekkzY1BNWE1oWFpDS3pZb0tnUWoxMnFOMWM0SkVp TUFPK2F2ZW9RCjB0dnJmd3MxMlF3d3ZIZm40SCtYVnlDZGpMcDE5dlhlY0FSRFJyaGlkRW1CbEFD cVJVdTFLSGhzejZ2TmxzUzIKSlZiWU9BYW5ISzYzNzdYT211OUthL2x1TmxSVDdmckxBZ01CQUFH alJUQkRNQTRHQTFVZER3RUIvd1FFQXdJQgpCakFTQmdOVkhSTUJBZjhFQ0RBR0FRSC9BZ0VBTUIw R0ExVWREZ1FXQkJSbDk4cG1valVUQ3RZb3phTDl6L0hSCmJIUkdkREFOQmdrcWhraUc5dzBCQVFz RkFBT0NBZ0VBNUVxL09ocGs0MUxLa3R2ajlmSWNFWXI5Vi9QUit6Z1UKQThRTUtvOVBuZSsrdW1N dEZEK1R1M040b1lxV0srTmg0ajdHTm80RFJXejNnbWpZdUlDdklhMGo5emppT1FkWgo1Q2xVQkdk YUlScFoyRG5CblBUM2tJbnhSd0hmU0JMVlVTRXRTcXh4YkV2dk5LWkZWY0lsRUV5ODZodnJ5OUZD CjhFOWRXWEw5VDhMd29uVXpqSjBxZ242cGRjNHpjdEtUMDFjaDQvWGw2UjBVQkR5Q1NoSGFyU29C eTkvSk1NTXIKajBoeEZSN3Izb052a2N3QWl6T1RsQ3BWdTZaNHF2cng3NndCc0hIanV6elNiODJL dUxnelJUNElWbjFjbzRrVQpSaTlBRkNaRlh6QklaQlEwTUZ6NU03bzJkN0ovN3ZMOFhYRlhwWlpy K3RibWE1L3BCSmZhcXliK3FPRXViWGdUCjFsSDZGeFNVcWt0TktQNlZoeWdQY2ZSMlR4YWtHZ0cw Ny9qVWZWRmhpVXM5aFBlejh6Sjg2RWMrd283VEVQbEsKSlRnMHZmMDM4MTROR3ZuWmlpTnBFWVBM S0ZhcHlDMWJONVdFTGFTWFVBaVFPZDJjK01xVHAyN21vV1RZa29TOApzRFczRTMraEN6c1djdmFY RW1nMjZJTjQybmVUWFBuNS9QajNpcUVoT0pQYkJsY3l6dDBZL1BYeU1jR3JtbUs1CkhxOUMzTndl VUV3M09rY09BOXlCdC9kLzZ5S3c3QmovSlFQZGI0aDlWWjNGN09wemFpeXQ5cFhvSXRQMHNUSHUK S2ZKbDBCRUFYV29SR2lWM2EyeUlUcGp0a0pkQVBoS0xpSkkrWWowZEVEU05WZnlENFhJTXdQSmpV eFpsd2FROQorQUtkVDFBdlplbz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= # Base64 encoded content of the clients's certificate file. # # How To: # Considering client certificate file is named 'client.crt', you can obtain the data using the following commands: # cat client.crt | base64 \u003e client.crt.b64 # You can then open the 'client.crt.b64' file, copy it contents, and paste to the configuration file # # It is required only if the server requires client authentication. # It is mutually required if the field client-key has a value. # # REQUIRED: # + if insecure is set to false # AND # + the client-key field is not empty client-cert: |- LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUVORENDQWh5Z0F3SUJBZ0lSQU9JSlZ2NnB3 a0lIK0p1NTNKSEFuam93RFFZSktvWklodmNOQVFFTEJRQXcKRWpFUU1BNEdBMVVFQXhNSGRHVnpk QzFqWVRBZUZ3MHlNekF6TVRjeE1qTTJNelphRncweU5EQTVNVGN4TWpRegpNamxhTUJFeER6QU5C Z05WQkFNVEJtTnNhV1Z1ZERDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDCkFRb0Nn Z0VCQU5LVFNHeEEyV2RyNmtCR0N3RjY5c1JVZElPV0xqeTUvN3QyRktKWDVVenNyMDlFWW9tS0sr bVQKdWF2eWJIMWhsbTYzdG5kb3VFOHFIQnVhYmYvUGIzSlRTQ0twR0NRdHR2NmQzeGc3MHFZVWIx cUZKT2o5andlNgpRZW0xb2RIVFpLc0xMc2J1N1Fzei91MGtseUovMHNYcFQ5K2JXK1M0OHMrL3pK dHNDR21SdVhlRjE2Y1FqOWErCkFFejNqVzhrdExMYi9nS25GWGRSS2FiY2RWLzNzN2RLNWx0SXpS ZlRvUWw0bzBpckpOa3Z4eXIrYUtMMTR4NUQKc3g2Wm9DUHJhRFYrWWlRS0ZSenFjQ1RYcWdRb3BY LzFINFRMV3RkeG14M25IdmhZdzB0VlBZSXZsa245NmpJUwpKdVE2K1VMbVAzZDNzNWJadlhQeUZD bENKSENxaWZNQ0F3RUFBYU9CaFRDQmdqQU9CZ05WSFE4QkFmOEVCQU1DCkE3Z3dIUVlEVlIwbEJC WXdGQVlJS3dZQkJRVUhBd0VHQ0NzR0FRVUZCd01DTUIwR0ExVWREZ1FXQkJTRWVIOTEKVnBhdDlV SWlrRUdkc1ljdUI2dWxOakFmQmdOVkhTTUVHREFXZ0JSbDk4cG1valVUQ3RZb3phTDl6L0hSYkhS RwpkREFSQmdOVkhSRUVDakFJZ2daamJHbGxiblF3RFFZSktvWklodmNOQVFFTEJRQURnZ0lCQUQv TnZVNWRSajlHCmMzYzVVQ3VLcDI4U2lacjAySE40M091WU5QVlk4L1c5QnZUSk5yMXRPRDFscnhE eFMzTkpVdzdGaTNidmU5enMKSzA0a09peUxpVjRLd0g2eitpVm8xZU9GUzJLd1BRaGxsaDlobVBB dXZ4Zm5Fd2k2ZEdXZm5nNExmQ1FvbXFkTgpmbkFCODJBbTViZTBubGJvaGdLcFJUWnVBZjR4dVY4 SWxlQ1pjVHdFL1hBbERhNVhHaDNvWlE3REYrQnFLSkNUCk1pYS9MT0JPYXRoRVh5ZGJmbndOUUhy UWlQZzk4c2NMc3FTZEFQMFNGYjMrMmdscFJZT1JrQlFvOWRoa1pGZXkKc2tUakVhbk9YaUhqWldq aXZRS2Z2WEUvK1l2eGpCcEJqREE2NnYyeUgzSlJqZEM5ZTR2cnE2R0t6VXZML3ltOQpVOGdVWnho L2ZmeFp4TVA5UmxXajQ0U1NGUVpZNGxUNFF5U2lteFpGdVBTamwzV29QME12UHVvUzFUUzhQUk5s CnVGeXBVell5SEtlbHpLUnRJZmlnWG9XQi9uR2hSV0RMN2FZS0xYZWRIU0ZrdXBmZm9YM1hHQThM ZVAwQ01PaEsKUUJaUkxIeXU0VjhvRG1lakFIcFoyVjlpY2E1emtmcnJWVXFvSzF1VjYvdHd3cEZG WDErN0w1bk0ybDJDQWxvegpaVHFUZzNCdVdYd2VkYzZQbkpuU2xQSDNadFhqcGFJUWhXdU85TUlG WFVtVFBlSkZ2WGxKeWRsdUxtMlQzanVqCldiVENGcEhyMXBrMGk3K1J4ZVRBcFY0RTk2S09DOXEw ZGREOG1waTM0cnkyZjFmQ2RZekhQM0s4bW5od3BPWmkKaG1Xd3VWVDV3em5kVWVBRGNWYUY2UlhU UENKSElLd24KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= # Base64 encoded content of the clients's key certificate file. # # How To: # Considering client certificate file is named 'client.key', you can obtain the data using the following commands: # cat client.key | base64 \u003e client.key.b64 # You can then open the 'client.key.b64' file, copy it contents, and paste to the configuration file # # It is required only if the server requires client authentication. # It is mutually required if the field client-cert has a value. # # REQUIRED: # + if insecure is set to false # AND # + the client-cert field is not empty client-key: |- LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcFFJQkFBS0NBUUVBMHBOSWJFRFpa MnZxUUVZTEFYcjJ4RlIwZzVZdVBMbi91M1lVb2xmbFRPeXZUMFJpCmlZb3I2Wk81cS9Kc2ZXR1di cmUyZDJpNFR5b2NHNXB0Lzg5dmNsTklJcWtZSkMyMi9wM2ZHRHZTcGhSdldvVWsKNlAyUEI3cEI2 YldoMGROa3F3c3V4dTd0Q3pQKzdTU1hJbi9TeGVsUDM1dGI1TGp5ejcvTW0yd0lhWkc1ZDRYWApw eENQMXI0QVRQZU5ieVMwc3R2K0FxY1ZkMUVwcHR4MVgvZXp0MHJtVzBqTkY5T2hDWGlqU0tzazJT L0hLdjVvCm92WGpIa096SHBtZ0krdG9OWDVpSkFvVkhPcHdKTmVxQkNpbGYvVWZoTXRhMTNHYkhl Y2UrRmpEUzFVOWdpK1cKU2YzcU1oSW01RHI1UXVZL2QzZXpsdG05Yy9JVUtVSWtjS3FKOHdJREFR QUJBb0lCQUJFSVVzSlcySDd5RHFlVwpRc3VpMjVUejA5elU1L2FIZ1BUenp5VjJnSmloU0dqYitq QnYyYTl5QUlHMUFTdC9Ha0RvWVR6MVhuc2d4OWMvCnZZZ0VpbG92L0ZTNVlyZUNieHZYUHpWaG1W OVBwZFlua04yN3JMY09UTWlQcFlBb1hpc3JvMlA1N1hpTGd5SkIKWkd3bzlLNkhlYXQza0k1R20z Vk1hVXRsQ0tVcE84cUwzcEZ4S1AwMVVwbGh6ZjhMbXJpTUJQMDlxdFFJejBydQpiR1l5eUdVdk9a a0RKZFJycmlSWGJWK0RNMFlmbVpqU1Q4aEI0UDlsOEhwMEZRNUp2TWVGREpzRFFaZjVBZnJmClFI WE55SlFUeTNTeXJ1bGd5N0p4MGY1T2JpVWRMRWViQVRpN3VLR3Y5UEZRRUJmSzdFdE4vZ1ZibGsx MzRzNUIKWEhkNXU1a0NnWUVBNDBVMjhONko4QXIwY2puYnNLUUJtOGhURWlJSjk3TEJPOU5kOTlJ M1dJYklZVzIzVE5wVwo0M2R4K1JHelA4eVMzYzZhN00wbzR1dUl6TXFDSkV3cVNJUjAvVGZaWWdx cGtwcFZPalp2VFdCUDFtSUlKUFpwCll1SFk0UVRJdkdhcVFNNnFWQXA4MW9YdXoxTmNmQWpTLzNJ Z1BWdGVZeDNKd0pmNWVqenZQclVDZ1lFQTdUSEwKR3VCTWpqTWVhaWk1ZU1sU1BndkYxMHJISUs0 RzZCZUJDTFFXU2ViNmNOT2x2a1RaOTNqdlFiWko1L3JBTGNWNgpaTVdqbWY5Tkl0NWdDdyt2K2dM Qm9BZXM3WEk2K2Rpdk1DYXE0dUFmWkhJWjBYbXpIOGx1a0o5ZUtyK2NyR2FzClNhWkdKRnlyQTZz WGdOc1ZJUm85RkFsR3V1dGZnd2hSUmo1eFp3Y0NnWUVBZ241MWcyeGtDMTVlNlU5clkwdG8KV1Fo M0dreE5LTnFNdFVzeUExL0N3NlB3WG5EZTlOUFJYQjV6WkszVEhHamNVMXVUL1MvM3NBUEpzcno4 YU5jSwoyRVNsMzljM2pHSE82QXlScnpFZVMzRm5waEwzMWpGZVpaYUVMdi9PT3M5QUpxSURqdW5P c0dhS3JxU1F6KzlKCko3OWgzNWtjNHhCeGpaSTFmd2lKM3BrQ2dZRUFwUnBOMkExYy9IWlVxMnho ZmRRVXJSK2d2TFZPV2s4SWU3RXcKbmhCTW0zQnR6dTlqcFVkanVVQ3l1YmpiUk9CanVQaUdzM0pt NktDdTNxQ1BsZU43aUxrMmNlQWwzTG53bDB6ZQoxTk4xaTZxWjcxOEUzYXlxcEd1ZnpJZENFdHVC Z1BlTzRVMGQ4ZDJYSkZ5SlphWVoxUXJnalB2UUFmZ29hWnIyCmg4Q2JTeTBDZ1lFQW1VQ3BqR0JW MGNpVnlmUXNmOGdsclNOdWx6NzBiaVJWQzVSeno0dVJEMkhsYVM2eC8wc0IKQzltSUhpdWgwR0Zp dEVFRlg4TzdlZ1ppNWJKMGFuQWYyakk1R1RnTjJOYzFpVlZnWldxcHh2aXpuckpKcENSYgpaejB0 M2thTkkyNjg0WTNxS2JxeG8ramRNK05hMG1qd2ErTEFOcEdCUDNwb2c0RHJ4eTNNSFdZPQotLS0t LUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo= ","categories":"","description":"Description of configuration file for ObjectScale","excerpt":"Description of configuration file for ObjectScale","ref":"/csm-docs/v1/cosidriver/installation/configuration_file/","tags":"","title":"Configuration File"},{"body":"(Optional) Volume Snapshot Requirements On Upstream Kubernetes clusters, ensure that to install\nVolumeSnapshot CRDs - Install v1 VolumeSnapshot CRDs External Volume Snapshot Controller For detailed snapshot setup procedure, click here.\nNOTE: This step can be skipped with OpenShift.\nInstalling CSI Driver via Operator Refer PowerScale Driver to install the driver via Operator Refer PowerFlex Driver to install the driver via Operator Refer PowerMax Driver to install the driver via Operator Refer PowerStore Driver to install the driver via Operator Refer Unity XT Driver to install the driver via Operator NOTE: If you are using an OLM based installation, example manifests are available in OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the driver installation Once the driver Custom Resource (CR) is created, you can verify the installation as mentioned below\nCheck if ContainerStorageModule CR is created successfully using the command below: kubectl get csm/\u003cname-of-custom-resource\u003e -n \u003cdriver-namespace\u003e -o yaml Check the status of the CR to verify if the driver installation is in the Succeeded state. If the status is not Succeeded, see the Troubleshooting guide for more information. Update CSI Drivers The CSI Drivers and CSM Modules installed by the Dell CSM Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include:\nModifying the installation directly via kubectl edit For example - If the name of the installed PowerScale driver is powerscale, then run #Replace driver-namespace with the namespace where the PowerScale driver is installed kubectl edit csm/powerscale -n \u003cdriver-namespace\u003e and modify the installation Modify the API object in-place via kubectl patch Supported modifications Changing environment variable values for driver Updating the image of the driver Upgrading the driver version NOTES:\nIf you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required. driver: configVersion: v2.9.1 Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures. Uninstall CSI Driver The CSI Drivers and CSM Modules can be uninstalled by deleting the Custom Resource.\nFor e.g.\nkubectl delete csm/powerscale -n \u003cdriver-namespace\u003e By default, the forceRemoveDriver option is set to true which will uninstall the CSI Driver and CSM Modules when the Custom Resource is deleted. Setting this option to false is not recommended.\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. The only exception to this is modifications requested by the documentation, for example, filling in blank IPs or other such system-specific data. Any modifications not specifically requested by the documentation should be only done after consulting with Dell support.\n","categories":"","description":"Installation of Dell CSI Drivers using Dell CSM Operator","excerpt":"Installation of Dell CSI Drivers using Dell CSM Operator","ref":"/csm-docs/docs/deployment/csmoperator/drivers/","tags":"","title":"CSI Drivers"},{"body":"(Optional) Volume Snapshot Requirements On Upstream Kubernetes clusters, ensure that to install\nVolumeSnapshot CRDs - Install v1 VolumeSnapshot CRDs External Volume Snapshot Controller For detailed snapshot setup procedure, click here.\nNOTE: This step can be skipped with OpenShift.\nInstalling CSI Driver via Operator Refer PowerScale Driver to install the driver via Operator Refer PowerFlex Driver to install the driver via Operator Refer PowerMax Driver to install the driver via Operator Refer PowerStore Driver to install the driver via Operator Refer Unity XT Driver to install the driver via Operator NOTE: If you are using an OLM based installation, example manifests are available in OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the driver installation Once the driver Custom Resource (CR) is created, you can verify the installation as mentioned below\nCheck if ContainerStorageModule CR is created successfully using the command below: kubectl get csm/\u003cname-of-custom-resource\u003e -n \u003cdriver-namespace\u003e -o yaml Check the status of the CR to verify if the driver installation is in the Succeeded state. If the status is not Succeeded, see the Troubleshooting guide for more information. Update CSI Drivers The CSI Drivers and CSM Modules installed by the Dell CSM Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include:\nModifying the installation directly via kubectl edit For example - If the name of the installed PowerScale driver is powerscale, then run #Replace driver-namespace with the namespace where the PowerScale driver is installed kubectl edit csm/powerscale -n \u003cdriver-namespace\u003e and modify the installation Modify the API object in-place via kubectl patch Supported modifications Changing environment variable values for driver Updating the image of the driver Upgrading the driver version NOTES:\nIf you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required. driver: configVersion: v2.8.0 Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures. Uninstall CSI Driver The CSI Drivers and CSM Modules can be uninstalled by deleting the Custom Resource.\nFor e.g.\nkubectl delete csm/powerscale -n \u003cdriver-namespace\u003e By default, the forceRemoveDriver option is set to true which will uninstall the CSI Driver and CSM Modules when the Custom Resource is deleted. Setting this option to false is not recommended.\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. The only exception to this is modifications requested by the documentation, for example, filling in blank IPs or other such system-specific data. Any modifications not specifically requested by the documentation should be only done after consulting with Dell support.\n","categories":"","description":"Installation of Dell CSI Drivers using Dell CSM Operator","excerpt":"Installation of Dell CSI Drivers using Dell CSM Operator","ref":"/csm-docs/v1/deployment/csmoperator/drivers/","tags":"","title":"CSI Drivers"},{"body":"(Optional) Volume Snapshot Requirements On Upstream Kubernetes clusters, ensure that to install\nVolumeSnapshot CRDs - Install v1 VolumeSnapshot CRDs External Volume Snapshot Controller For detailed snapshot setup procedure, click here.\nNOTE: This step can be skipped with OpenShift.\nInstalling CSI Driver via Operator Refer PowerScale Driver to install the driver via Operator Refer PowerFlex Driver to install the driver via Operator Refer PowerMax Driver to install the driver via Operator Refer PowerStore Driver to install the driver via Operator Refer Unity XT Driver to install the driver via Operator NOTE: If you are using an OLM based installation, example manifests are available in OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the driver installation Once the driver Custom Resource (CR) is created, you can verify the installation as mentioned below\nCheck if ContainerStorageModule CR is created successfully using the command below: kubectl get csm/\u003cname-of-custom-resource\u003e -n \u003cdriver-namespace\u003e -o yaml Check the status of the CR to verify if the driver installation is in the Succeeded state. If the status is not Succeeded, see the Troubleshooting guide for more information. Update CSI Drivers The CSI Drivers and CSM Modules installed by the Dell CSM Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include:\nModifying the installation directly via kubectl edit For example - If the name of the installed PowerScale driver is powerscale, then run #Replace driver-namespace with the namespace where the PowerScale driver is installed kubectl edit csm/powerscale -n \u003cdriver-namespace\u003e and modify the installation Modify the API object in-place via kubectl patch Supported modifications Changing environment variable values for driver Updating the image of the driver Upgrading the driver version NOTES:\nIf you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required. driver: configVersion: v2.7.0 Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures. Uninstall CSI Driver The CSI Drivers and CSM Modules can be uninstalled by deleting the Custom Resource.\nFor e.g.\nkubectl delete csm/powerscale -n \u003cdriver-namespace\u003e By default, the forceRemoveDriver option is set to true which will uninstall the CSI Driver and CSM Modules when the Custom Resource is deleted. Setting this option to false is not recommended.\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. The only exception to this is modifications requested by the documentation, for example, filling in blank IPs or other such system-specific data. Any modifications not specifically requested by the documentation should be only done after consulting with Dell support.\n","categories":"","description":"Installation of Dell CSI Drivers using Dell CSM Operator","excerpt":"Installation of Dell CSI Drivers using Dell CSM Operator","ref":"/csm-docs/v2/deployment/csmoperator/drivers/","tags":"","title":"CSI Drivers"},{"body":"Pre-requisites for installation of the CSI Drivers On Upstream Kubernetes clusters, ensure that to install\nVolumeSnapshot CRDs - Install v1 VolumeSnapshot CRDs External Volume Snapshot Controller Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\nA common snapshot controller A CSI external-snapshotter sidecar The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here\nNOTE:\nThe manifests available on GitHub install the snapshotter image: quay.io/k8scsi/csi-snapshotter:v6.2.1 The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration. Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\nIt is recommended to use 6.2.1 version of snapshotter/snapshot-controller. Installing CSI Driver via Operator Refer PowerScale Driver to install the driver via Operator Refer PowerFlex Driver to install the driver via Operator Refer PowerStore Driver to install the driver via Operator Note: If you are using an OLM based installation, example manifests are available in OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the driver installation Once the driver Custom Resource (CR) is created, you can verify the installation as mentioned below\nCheck if ContainerStorageModule CR is created successfully using the command below: $ kubectl get csm/\u003cname-of-custom-resource\u003e -n \u003cdriver-namespace\u003e -o yaml Check the status of the CR to verify if the driver installation is in the Succeeded state. If the status is not Succeeded, see the Troubleshooting guide for more information. Update CSI Drivers The CSI Drivers and CSM Modules installed by the Dell CSM Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include:\nModifying the installation directly via kubectl edit For example - If the name of the installed PowerScale driver is powerscale, then run # Replace driver-namespace with the namespace where the PowerScale driver is installed $ kubectl edit csm/powerscale -n \u003cdriver-namespace\u003e and modify the installation Modify the API object in-place via kubectl patch Supported modifications Changing environment variable values for driver Updating the image of the driver Upgrading the driver version NOTES:\nIf you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required. driver: configVersion: v2.6.0 Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures. Uninstall CSI Driver The CSI Drivers and CSM Modules can be uninstalled by deleting the Custom Resource.\nFor e.g.\n$ kubectl delete csm/powerscale -n \u003cdriver-namespace\u003e By default, the forceRemoveDriver option is set to true which will uninstall the CSI Driver and CSM Modules when the Custom Resource is deleted. Setting this option to false is not recommended.\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. The only exception to this is modifications requested by the documentation, for example, filling in blank IPs or other such system-specific data. Any modifications not specifically requested by the documentation should be only done after consulting with Dell support.\n","categories":"","description":"Installation of Dell CSI Drivers using Dell CSM Operator","excerpt":"Installation of Dell CSI Drivers using Dell CSM Operator","ref":"/csm-docs/v3/deployment/csmoperator/drivers/","tags":"","title":"CSI Drivers"},{"body":"The Dell Container Storage Modules Installation Wizard is a webpage that generates a manifest file for installing Dell CSI Drivers and its supported CSM Modules, based on input from the user. It generates a single manifest file to install both Dell CSI Drivers and its supported CSM Modules, thereby eliminating the need to download individual Helm charts for drivers and modules. The user can enable or disable the necessary modules through the UI, and a manifest file is generated accordingly without manually editing the helm charts.\nNOTE: The CSM Installation Wizard supports Helm and Operator based manifest file generation.\nSupported Dell CSI Drivers CSI Driver Version Helm Operator CSI PowerStore 2.9.1 ✔️ ✔️ CSI PowerStore 2.8.0 ✔️ ✔️ CSI PowerStore 2.7.0 ✔️ ✔️ CSI PowerMax 2.9.1 ✔️ ✔️ CSI PowerMax 2.8.0 ✔️ ✔️ CSI PowerMax 2.7.0 ✔️ ✔️ CSI PowerFlex 2.9.1 ✔️ ❌ CSI PowerFlex 2.8.0 ✔️ ❌ CSI PowerFlex 2.7.0 ✔️ ❌ CSI PowerScale 2.9.1 ✔️ ✔️ CSI PowerScale 2.8.0 ✔️ ✔️ CSI PowerScale 2.7.0 ✔️ ✔️ CSI Unity XT 2.9.1 ✔️ ❌ CSI Unity XT 2.8.0 ✔️ ❌ CSI Unity XT 2.7.0 ✔️ ❌ NOTE: The Installation Wizard currently does not support operator-based manifest file generation for Unity XT and PowerFlex drivers.\nSupported Dell CSM Modules CSM Modules Version CSM Observability 1.7.0 CSM Replication 1.7.1 CSM Resiliency 1.8.1 Installation Open the CSM Installation Wizard. Select the Installation Type as Helm/Operator. Select the Array. Enter the Image Repository. The default value is dellemc. Select the CSM Version. Select the modules for installation. If there are module specific inputs, enter their values. If needed, modify the Controller Pods Count. If needed, select Install Controller Pods on Control Plane and/or Install Node Pods on Control Plane. Enter the Namespace. The default value is csi-\u003carray\u003e. Click on Generate YAML. A manifest file, values.yaml will be generated and downloaded. A section Run the following commands to install will be displayed. Run the commands displayed to install Dell CSI Driver and Modules using the generated manifest file. Installation Using Helm Chart Steps\nNOTE: Ensure that the namespace and secrets are created before installing the Helm chart.\nAdd the Dell Helm Charts repository.\nOn your terminal, run each of the commands below:\nhelm repo add dell https://dell.github.io/helm-charts helm repo update Copy the downloaded values.yaml file.\nLook over all the fields in the generated values.yaml and fill in/adjust any as needed.\nNOTE: The CSM Installation Wizard generates values.yaml with the minimal inputs required to install the CSM. To configure additional parameters in values.yaml, you can follow the steps outlined in PowerStore, PowerMax, PowerScale, PowerFlex, Unity XT, Observability, Replication, Resiliency.\nWhen the PowerFlex driver is installed using values generated by installation wizard, the user needs to update the secret for driver by patching the MDM keys, as follows:\n`echo -n '\u003cMDM_IPS\u003e' | base64` `kubectl patch secret vxflexos-config -n vxflexos -p \"{\\\"data\\\": { \\\"MDM\\\": \\\"\u003cGENERATED_BASE64\u003e\\\"}}\"` If Observability is checked in the wizard, refer to Observability to export metrics to Prometheus and load the Grafana dashboards.\nIf Authorization is checked in the wizard, only the sidecar is enabled. Refer to Authorization to install and configure the CSM Authorization Proxy Server.\nIf Replication is checked in the wizard, refer to Replication on configuring communication between Kubernetes clusters.\nIf your Kubernetes distribution doesn’t have the Volume Snapshot feature enabled, refer to this section to install the Volume Snapshot CRDs and the default snapshot controller.\nInstall the Helm chart.\nOn your terminal, run this command:\nhelm install \u003crelease-name\u003e dell/container-storage-modules -n \u003cnamespace\u003e --version \u003ccontainer-storage-module chart-version\u003e -f \u003cvalues.yaml location\u003e Example: helm install powerstore dell/container-storage-modules -n csi-powerstore --version 1.2.1 -f values.yaml Installation Using Operator Steps\nNOTE: Ensure that the csm-operator is installed and that the namespace, secrets, and config.yaml are created as prerequisites.\nCopy the downloaded values.yaml file.\nLook over all the fields in the generated values.yaml and fill in/adjust any as needed.\nNOTE: The CSM Installation Wizard generates values.yaml with the minimal inputs required to install the CSM. To configure additional parameters in values.yaml, you can follow the steps outlined in PowerStore, PowerMax, PowerScale, Resiliency.\nIf Observability is checked in the wizard, refer to Observability to export metrics to Prometheus and load the Grafana dashboards.\nIf Authorization is checked in the wizard, only the sidecar is enabled. Refer to Authorization to install and configure the CSM Authorization Proxy Server.\nIf Replication is checked in the wizard, refer to Replication for the necessary prerequisites required for this module.\nInstall the Operator.\nOn your terminal, run this command:\nkubectl create -f values.yaml ","categories":"","description":"Container Storage Modules Installation Wizard","excerpt":"Container Storage Modules Installation Wizard","ref":"/csm-docs/docs/deployment/csminstallationwizard/","tags":"","title":"CSM Installation Wizard"},{"body":"The Dell Container Storage Modules Installation Wizard is a webpage that generates a manifest file for installing Dell CSI Drivers and its supported CSM Modules, based on input from the user. It generates a single manifest file to install both Dell CSI Drivers and its supported CSM Modules, thereby eliminating the need to download individual Helm charts for drivers and modules. The user can enable or disable the necessary modules through the UI, and a manifest file is generated accordingly without manually editing the helm charts.\nNOTE: The CSM Installation Wizard supports Helm and Operator based manifest file generation.\nSupported Dell CSI Drivers CSI Driver Version Helm Operator CSI PowerStore 2.8.0 ✔️ ✔️ CSI PowerStore 2.7.0 ✔️ ✔️ CSI PowerMax 2.8.0 ✔️ ✔️ CSI PowerMax 2.7.0 ✔️ ✔️ CSI PowerFlex 2.8.0 ✔️ ❌ CSI PowerFlex 2.7.0 ✔️ ❌ CSI PowerScale 2.8.0 ✔️ ✔️ CSI PowerScale 2.7.0 ✔️ ✔️ CSI Unity XT 2.8.0 ✔️ ❌ CSI Unity XT 2.7.0 ✔️ ❌ NOTE: The Installation Wizard currently does not support operator-based manifest file generation for Unity XT and PowerFlex drivers.\nSupported Dell CSM Modules CSM Modules Version CSM Observability 1.6.0 CSM Replication 1.6.0 CSM Resiliency 1.7.0 Installation Open the CSM Installation Wizard. Select the Installation Type as Helm/Operator. Select the Array. Enter the Image Repository. The default value is dellemc. Select the CSM Version. Select the modules for installation. If there are module specific inputs, enter their values. If needed, modify the Controller Pods Count. If needed, select Install Controller Pods on Control Plane and/or Install Node Pods on Control Plane. Enter the Namespace. The default value is csi-\u003carray\u003e. Click on Generate YAML. A manifest file, values.yaml will be generated and downloaded. A section Run the following commands to install will be displayed. Run the commands displayed to install Dell CSI Driver and Modules using the generated manifest file. Installation Using Helm Chart Steps\nNOTE: Ensure that the namespace and secrets are created before installing the Helm chart.\nAdd the Dell Helm Charts repository.\nOn your terminal, run each of the commands below:\nhelm repo add dell https://dell.github.io/helm-charts helm repo update Copy the downloaded values.yaml file.\nLook over all the fields in the generated values.yaml and fill in/adjust any as needed.\nNOTE: The CSM Installation Wizard generates values.yaml with the minimal inputs required to install the CSM. To configure additional parameters in values.yaml, you can follow the steps outlined in PowerStore, PowerMax, PowerScale, PowerFlex, Unity XT, Observability, Replication, Resiliency.\nWhen the PowerFlex driver is installed using values generated by installation wizard, the user needs to update the secret for driver by patching the MDM keys, as follows:\n`echo -n '\u003cMDM_IPS\u003e' | base64` `kubectl patch secret vxflexos-config -n vxflexos -p \"{\\\"data\\\": { \\\"MDM\\\": \\\"\u003cGENERATED_BASE64\u003e\\\"}}\"` If Observability is checked in the wizard, refer to Observability to export metrics to Prometheus and load the Grafana dashboards.\nIf Authorization is checked in the wizard, only the sidecar is enabled. Refer to Authorization to install and configure the CSM Authorization Proxy Server.\nIf Replication is checked in the wizard, refer to Replication on configuring communication between Kubernetes clusters.\nIf your Kubernetes distribution doesn’t have the Volume Snapshot feature enabled, refer to this section to install the Volume Snapshot CRDs and the default snapshot controller.\nInstall the Helm chart.\nOn your terminal, run this command:\nhelm install \u003crelease-name\u003e dell/container-storage-modules -n \u003cnamespace\u003e --version \u003ccontainer-storage-module chart-version\u003e -f \u003cvalues.yaml location\u003e Example: helm install powerstore dell/container-storage-modules -n csi-powerstore --version 1.1.0 -f values.yaml Installation Using Operator Steps\nNOTE: Ensure that the csm-operator is installed and that the namespace, secrets, and config.yaml are created as prerequisites.\nCopy the downloaded values.yaml file.\nLook over all the fields in the generated values.yaml and fill in/adjust any as needed.\nNOTE: The CSM Installation Wizard generates values.yaml with the minimal inputs required to install the CSM. To configure additional parameters in values.yaml, you can follow the steps outlined in PowerStore, PowerMax, PowerScale, Resiliency.\nIf Observability is checked in the wizard, refer to Observability to export metrics to Prometheus and load the Grafana dashboards.\nIf Authorization is checked in the wizard, only the sidecar is enabled. Refer to Authorization to install and configure the CSM Authorization Proxy Server.\nIf Replication is checked in the wizard, refer to Replication for the necessary prerequisites required for this module.\nInstall the Operator.\nOn your terminal, run this command:\nkubectl create -f values.yaml ","categories":"","description":"Container Storage Modules Installation Wizard","excerpt":"Container Storage Modules Installation Wizard","ref":"/csm-docs/v1/deployment/csminstallationwizard/","tags":"","title":"CSM Installation Wizard"},{"body":"The Dell Container Storage Modules Installation Wizard is a webpage that generates a manifest file for installing Dell CSI Drivers and its supported CSM Modules, based on input from the user. It generates a single manifest file to install both Dell CSI Drivers and its supported CSM Modules, thereby eliminating the need to download individual Helm charts for drivers and modules. The user can enable or disable the necessary modules through the UI, and a manifest file is generated accordingly without manually editing the helm charts.\nNOTE: The CSM Installation Wizard currently supports Helm based manifest file generation only.\nSupported Dell CSI Drivers CSI Driver Version CSI PowerStore 2.7.0 CSI PowerMax 2.7.0 CSI PowerFlex 2.7.1 CSI PowerScale 2.7.0 CSI Unity XT 2.7.0 Supported Dell CSM Modules CSM Modules Version CSM Observability 1.5.0 CSM Replication 1.5.0 CSM Resiliency 1.6.0 Installation Open the CSM Installation Wizard. Select the Installation Type as Helm. Select the Array. Enter the Image Repository. The default value is dellemc. Select the CSM Version. Select the modules for installation. If there are module specific inputs, enter their values. If needed, modify the Controller Pods Count. If needed, select Install Controller Pods on Control Plane and/or Install Node Pods on Control Plane. Enter the Namespace. The default value is csi-\u003carray\u003e. Click on Generate YAML. A manifest file, values.yaml will be generated and downloaded. A section Run the following commands to install will be displayed. Run the commands displayed to install Dell CSI Driver and Modules using the generated manifest file. Install Helm Chart Steps\nNOTE: Ensure that the namespace and secrets are created before installing the Helm chart.\nAdd the Dell Helm Charts repository.\nOn your terminal, run each of the commands below:\nhelm repo add dell https://dell.github.io/helm-charts helm repo update Copy the downloaded values.yaml file.\nLook over all the fields in the generated values.yaml and fill in/adjust any as needed.\nFor the Observability module, please refer Observability to install the post installation dependencies.\nIf Authorization is enabled , please refer to Authorization for the installation and configuration of the Proxy Server.\nNOTE: Only the Authorization sidecar is enabled by the CSM Installation Wizard. The Proxy Server has to be installed and configured separately.\nIf the Volume Snapshot feature is enabled, please refer to Volume Snapshot for PowerStore and Volume Snapshot for PowerMax to install the Volume Snapshot CRDs and the default snapshot controller. NOTE: The CSM Installation Wizard generates values.yaml with the minimal inputs required to install the CSM. To configure additional parameters in values.yaml, please follow the steps outlined in PowerStore, PowerMax, PowerScale, PowerFlex, Unity XT, Observability, Replication, Resiliency.\nInstall the Helm chart.\nOn your terminal, run this command:\nhelm install \u003crelease-name\u003e dell/container-storage-modules -n \u003cnamespace\u003e --version \u003ccontainer-storage-module chart-version\u003e -f \u003cvalues.yaml location\u003e Example: helm install powerstore dell/container-storage-modules -n csi-powerstore --version 1.0.1 -f values.yaml ","categories":"","description":"Container Storage Modules Installation Wizard","excerpt":"Container Storage Modules Installation Wizard","ref":"/csm-docs/v2/deployment/csminstallationwizard/","tags":"","title":"CSM Installation Wizard"},{"body":"The Dell Container Storage Modules Installation Wizard is a webpage that generates a manifest file for installing Dell CSI Drivers and its supported CSM Modules, based on input from the user. It generates a single manifest file to install both Dell CSI Drivers and its supported CSM Modules, thereby eliminating the need to download individual Helm charts for drivers and modules. The user can enable or disable the necessary modules through the UI, and a manifest file is generated accordingly without manually editing the helm charts.\nNOTE: The CSM Installation Wizard currently supports Helm based manifest file generation only.\nSupported Dell CSI Drivers CSI Driver Version CSI PowerStore 2.6.0 CSI PowerMax 2.6.0 Supported Dell CSM Modules CSM Modules Version Application Mobility 0.3.0 CSM Observability 1.5.0 CSM Replication 1.4.0 CSM Resiliency 1.5.0 Installation Open the CSM Installation Wizard. Select the Installation Type as Helm. Select the Array. Enter the Image Repository. The default value is dellemc. Select the CSM Version. Select the modules for installation. If there are module specific inputs, enter their values. If needed, modify the Controller Pods Count. If needed, select Install Controller Pods on Control Plane and/or Install Node Pods on Control Plane. Select Single Namespace if the Dell CSI Driver and Modules should be installed in the same namespace. Enter the Driver Namespace. The default value is csi-\u003carray\u003e. Enter the Module Namespace. The default value is csm-module. Click on Generate YAML. A manifest file, values.yaml will be generated and downloaded. A section Run the following commands to install will be displayed. Run the commands displayed to install Dell CSI Driver and Modules using the generated manifest file. Install Helm Chart Steps\nNOTE: Ensure that the namespaces and secrets are created before installing the Helm chart.\nAdd the Dell Helm Charts repository.\nOn your terminal, run each of the commands below:\nhelm repo add dell https://dell.github.io/helm-charts helm repo update Copy the downloaded values.yaml file.\nLook over all the fields in the generated values.yaml and fill in/adjust any as needed.\nFor the Observability module, please refer Observability to install the post installation dependencies.\nIf Authorization is enabled , please refer to Authorization for the installation and configuration of the Proxy Server.\nNOTE: Only the Authorization sidecar is enabled by the CSM Installation Wizard. The Proxy Server has to be installed and configured separately.\nIf the Volume Snapshot feature is enabled, please refer to Volume Snapshot for PowerStore and Volume Snapshot for PowerMax to install the Volume Snapshot CRDs and the default snapshot controller. NOTE: The CSM Installation Wizard generates values.yaml with the minimal inputs required to install the CSM. To configure additional parameters in values.yaml, please follow the steps outlined in PowerStore, PowerMax, Observability, Replication, Resiliency, and Application Mobility.\nInstall the Helm chart.\nOn your terminal, run this command:\nhelm install \u003crelease-name\u003e dell/container-storage-modules -f \u003cvalues.yaml location\u003e Example: helm install powerstore dell/container-storage-modules -f values.yaml ","categories":"","description":"Container Storage Modules Installation Wizard","excerpt":"Container Storage Modules Installation Wizard","ref":"/csm-docs/v3/deployment/csminstallationwizard/","tags":"","title":"CSM Installation Wizard"},{"body":"The Dell Container Storage Modules Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers and CSM Modules provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. The operator can be installed using OLM (Operator Lifecycle Manager) or manually.\nSupport Matrix Dell CSM Operator has been tested and qualified on Upstream Kubernetes and OpenShift. Supported versions are listed below:\nKubernetes Version OpenShift Version 1.26, 1.27, 1.28 4.13, 4.14 NOTE:\nAuthorization module is only supported on Kubernetes platforms. Supported CSM Components The table below lists the driver and modules versions installable with the CSM Operator:\nCSI Driver Version CSM Authorization CSM Replication CSM Observability CSM Resiliency CSI PowerScale 2.9.1 ✔ 1.9.1 ✔ 1.7.1 ✔ 1.7.0 ✔ 1.8.1 CSI PowerScale 2.8.0 ✔ 1.8.0 ✔ 1.6.0 ✔ 1.6.0 ✔ 1.7.0 CSI PowerScale 2.7.0 ✔ 1.7.0 ✔ 1.5.0 ✔ 1.5.0 ✔ 1.6.0 CSI PowerFlex 2.9.2 ✔ 1.9.1 ✔ 1.7.1 ✔ 1.7.0 ✔ 1.8.1 CSI PowerFlex 2.8.0 ✔ 1.8.0 ✔ 1.6.0 ✔ 1.6.0 ✔ 1.7.0 CSI PowerFlex 2.7.0 ✔ 1.7.0 ✔ 1.5.0 ✔ 1.5.0 ✔ 1.6.0 CSI PowerStore 2.9.1 ❌ ❌ ❌ ✔ 1.8.0 CSI PowerStore 2.8.0 ❌ ❌ ❌ ✔ 1.7.0 CSI PowerStore 2.7.0 ❌ ❌ ❌ ✔ 1.6.0 CSI PowerMax 2.9.1 ✔ 1.9.1 ✔ 1.7.1 ✔ 1.7.0 ❌ CSI PowerMax 2.8.0 ✔ 1.8.0 ✔ 1.6.0 ✔ 1.6.0 ❌ CSI PowerMax 2.7.0 ✔ 1.7.0 ✔ 1.5.0 ❌ ❌ CSI Unity XT 2.9.1 ❌ ❌ ❌ ❌ CSI Unity XT 2.8.0 ❌ ❌ ❌ ❌ CSI Unity XT 2.7.0 ❌ ❌ ❌ ❌ NOTE:\nRefer to sample files here. Installation Dell CSM Operator can be installed manually or via Operator Hub.\nOnce installed you will be able to deploy drivers and modules from the Operator.\nOpenShift Installation via Operator Hub dell-csm-operator can be installed via Operator Hub on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the operator to:\nAutomatic - If you want the operator to be automatically installed or upgraded (once an upgrade is available). Manual - If you want a cluster administrator to manually review and approve the InstallPlan for installation/upgrades. NOTE: Dell CSM Operator is distributed as both Certified \u0026 Community editions. Both editions have the same codebase and are supported by Dell Technologies, the only difference is that the Certified version is validated by RedHat. The Certified version is often released couple of days/weeks after the Community version.\nManual Installation on a cluster without OLM Install volume snapshot CRDs. For detailed snapshot setup procedure, click here. Clone and checkout the required csm-operator version using git clone -b v1.4.3 https://github.com/dell/csm-operator.git cd csm-operator (Optional) If using a local Docker image, edit the deploy/operator.yaml file and set the image name for the CSM Operator Deployment. (Optional) If CSM Replication is planned for use and will be deployed using two clusters in an environment where the DNS is not configured, and cluster API endpoints are FQDNs, in order to resolve queries to remote API endpoints, it is necessary to edit the deploy/operator.yaml file and add the hostAliases field and associated \u003cFQDN\u003e:\u003cIP\u003e mappings to the CSM Operator Controller Manager Deployment under spec.template.spec. More information on host aliases can be found, here. # example config apiVersion: apps/v1 kind: Deployment metadata: name: dell-csm-operator-controller-manager spec: template: spec: hostAliases: - hostnames: - \"remote.FQDN\" ip: \"255.255.255.1\" Run bash scripts/install.sh to install the operator. NOTE: Dell CSM Operator will be installed in the dell-csm-operator namespace.\nRun the command to validate the installation. kubectl get pods -n dell-csm-operator If installed successfully, you should be able to see the operator pod in the dell-csm-operator namespace.\nOffline Bundle Installation on a cluster without OLM The csm-offline-bundle.sh script can be used to create a package usable for offline installation of Dell CSI Drivers via CSM Operator\nDependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\nOne Linux-based system, with Internet access, will be used to create the bundle. This involves the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry If one Linux system has both Internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\nDependency Usage docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry. One of these will be required on both the system building the offline bundle as well as the system preparing for installation. Tested version(s) are docker 24.0.5 and podman 4.4.1 git git will be used to manually clone one of the above repositories in order to create an offline bundle. This is only needed on the system preparing the offline bundle. Tested version(s) are git 2.39.3 but any version should work. Workflow To perform an offline installation, the following steps should be performed:\nBuild an offline bundle Unpack the offline bundle created in Step 1 and prepare for installation Perform operator installation using the files obtained after unpacking in Step 2 Perform driver installation using the files obtained after unpacking in Step 2 NOTE: It is recommended to use the same build tool for packing and unpacking of images (either docker or podman).\nBuilding an offline bundle This needs to be performed on a Linux system with access to the Internet as a git repo will need to be cloned, and container images pulled from public registries.\nTo build an offline bundle, the following steps are needed:\nClone and checkout the required csm-operator version using git clone -b v1.4.3 https://github.com/dell/csm-operator.git cd csm-operator Run the csm-offline-bundle.sh script which will be found in the scripts directory with an argument of -c in order to create an offline bundle bash scripts/csm-offline-bundle.sh -c The script will perform the following steps:\nDetermine required images by parsing CSM Operator configuration files Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to install the Operator and drivers. Here is the output of a request to build an offline bundle for the Dell CSM Operator:\n* * Building image manifest file Processing file /root/csm-operator/operatorconfig/driverconfig/common/default.yaml Processing file /root/csm-operator/bundle/manifests/dell-csm-operator.clusterserviceversion.yaml * * Pulling and saving container images dellemc/csi-isilon:v2.9.1 dellemc/csi-metadata-retriever:v1.6.1 dellemc/csipowermax-reverseproxy:v2.8.1 dellemc/csi-powermax:v2.9.1 dellemc/csi-powerstore:v2.9.1 dellemc/csi-unity:v2.9.1 dellemc/csi-vxflexos:v2.9.2 dellemc/csm-authorization-sidecar:v1.9.1 dellemc/csm-metrics-powerflex:v1.7.0 dellemc/csm-metrics-powerscale:v1.4.0 dellemc/csm-topology:v1.7.0 dellemc/dell-csi-replicator:v1.7.0 dellemc/dell-replication-controller:v1.7.0 dellemc/sdc:4.5 docker.io/dellemc/dell-csm-operator:v1.4.3 gcr.io/kubebuilder/kube-rbac-proxy:v0.8.0 nginxinc/nginx-unprivileged:1.20 otel/opentelemetry-collector:0.42.0 registry.k8s.io/sig-storage/csi-attacher:v4.4.2 registry.k8s.io/sig-storage/csi-external-health-monitor-controller:v0.10.0 registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.2 registry.k8s.io/sig-storage/csi-provisioner:v3.6.2 registry.k8s.io/sig-storage/csi-resizer:v1.9.2 registry.k8s.io/sig-storage/csi-snapshotter:v6.3.2 * * Copying necessary files /root/csm-operator/deploy /root/csm-operator/operatorconfig /root/csm-operator/samples /root/csm-operator/scripts /root/csm-operator/README.md /root/csm-operator/LICENSE * * Compressing release dell-csm-operator-bundle/ dell-csm-operator-bundle/deploy/ dell-csm-operator-bundle/deploy/operator.yaml dell-csm-operator-bundle/deploy/crds/ dell-csm-operator-bundle/deploy/crds/storage.dell.com_containerstoragemodules.yaml dell-csm-operator-bundle/deploy/olm/ dell-csm-operator-bundle/deploy/olm/operator_community.yaml ... ... dell-csm-operator-bundle/README.md dell-csm-operator-bundle/LICENSE * * Complete Offline bundle file is: /root/csm-operator/dell-csm-operator-bundle.tar.gz The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nUnpacking the offline bundle and preparing for installation This step needs to be performed on a Linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for Operator installation, the following steps need to be performed:\nCopy the offline bundle file created from the previous step to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e tar xvfz dell-csm-operator-bundle.tar.gz Here is the output of untar\ndell-csm-operator-bundle/ dell-csm-operator-bundle/deploy/ dell-csm-operator-bundle/deploy/operator.yaml dell-csm-operator-bundle/deploy/crds/ dell-csm-operator-bundle/deploy/crds/storage.dell.com_containerstoragemodules.yaml dell-csm-operator-bundle/deploy/olm/ dell-csm-operator-bundle/deploy/olm/operator_community.yaml ... ... dell-csm-operator-bundle/README.md dell-csm-operator-bundle/LICENSE Run the csm-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option cd dell-csm-operator-bundle bash scripts/csm-offline-bundle.sh -p -r localregistry:5000/dell-csm-operator/ The script will then perform the following steps:\nLoad the required container images into the local system Tag the images according to the user-supplied registry information Push the newly tagged images to the registry Modify the Operator configuration to refer to the newly tagged/pushed images Here is the output for preparing the bundle for installation (localregistry:5000 refers to an image registry accessible to Kubernetes/OpenShift. dell-csm-operator refers to the folder created within the registry.):\nPreparing a offline bundle for installation * * Loading docker images Loaded image: docker.io/dellemc/csi-powerstore:v2.9.1 Loaded image: docker.io/dellemc/csi-isilon:v2.9.1 ... ... Loaded image: registry.k8s.io/sig-storage/csi-resizer:v1.9.2 Loaded image: registry.k8s.io/sig-storage/csi-snapshotter:v6.3.2 * * Tagging and pushing images dellemc/csi-isilon:v2.9.1 -\u003e localregistry:5000/dell-csm-operator/csi-isilon:v2.9.1 dellemc/csi-metadata-retriever:v1.6.0 -\u003e localregistry:5000/dell-csm-operator/csi-metadata-retriever:v1.6.0 ... ... registry.k8s.io/sig-storage/csi-resizer:v1.9.2 -\u003e localregistry:5000/dell-csm-operator/csi-resizer:v1.9.2 registry.k8s.io/sig-storage/csi-snapshotter:v6.3.2 -\u003e localregistry:5000/dell-csm-operator/csi-snapshotter:v6.3.2 * * Preparing files within /root/dell-csm-operator-bundle changing: dellemc/csi-isilon:v2.9.1 -\u003e localregistry:5000/dell-csm-operator/csi-isilon:v2.9.1 changing: dellemc/csi-metadata-retriever:v1.6.1 -\u003e localregistry:5000/dell-csm-operator/csi-metadata-retriever:v1.6.1 ... ... changing: registry.k8s.io/sig-storage/csi-resizer:v1.9.2 -\u003e localregistry:5000/dell-csm-operator/csi-resizer:v1.9.2 changing: registry.k8s.io/sig-storage/csi-snapshotter:v6.3.2 -\u003e localregistry:5000/dell-csm-operator/csi-snapshotter:v6.3.2 * * Complete Perform Operator installation Now that the required images are available and the Operator configuration updated, you can proceed to install the operator by executing install.sh script.\nbash scripts/install.sh NOTE: Dell CSM Operator would install to the ‘dell-csm-operator’ namespace by default.\nPerform Driver installation Now that the required images are available and the Operator is installed, you can proceed to install the driver by executing kubectl create -f \u003cmanifest-name\u003e. Manifests for all the supported drivers will be available inside the samples directory. Using Unity XT as an example\nkubectl create -f samples/storage_csm_unity_v280.yaml NOTE: Offline bundle supports install of only the latest version of Dell CSI Drivers\nNOTE:\nOffline bundle installation is only supported with manual installs i.e. without using Operator Lifecycle Manager (OLM). Install/uninstall of operator and drivers should be done using the files that are obtained after unpacking the offline bundle (dell-csm-operator-bundle.tar.gz) as that is where the image tags in the manifests are modified to point to the specified internal registry. Uninstall Operator uninstallation on a cluster without OLM To uninstall a CSM operator, run bash scripts/uninstall.sh. This will uninstall the operator in dell-csm-operator namespace.\nUpgrade Dell CSM Operator Dell CSM Operator can be upgraded in 2 ways:\nUsing Operator Lifecycle Manager (OLM)\nUsing script (for non-OLM based installation)\nUsing OLM The upgrade of the Dell CSM Operator is done via Operator Lifecycle Manager.\nThe Update approval (InstallPlan in OLM terms) strategy plays a role while upgrading dell-csm-operator on OpenShift. This option can be set during installation of dell-csm-operator on OpenShift via the console and can be either set to Manual or Automatic.\nIf the Update approval is set to Automatic, OpenShift automatically detects whenever the latest version of dell-csm-operator is available in the Operator hub, and upgrades it to the latest available version. If the upgrade policy is set to Manual, OpenShift notifies of an available upgrade. This notification can be viewed by the user in the Installed Operators section of the OpenShift console. Clicking on the hyperlink to Approve the installation would trigger the dell-csm-operator upgrade process. NOTE: The recommended version of OLM for Upstream Kubernetes is v0.25.0.\nUsing Installation Script Clone and checkout the required csm-operator version using git clone -b v1.4.3 https://github.com/dell/csm-operator.git cd csm-operator Execute bash scripts/install.sh --upgrade . This command will install the latest version of the operator. NOTE: Dell CSM Operator would install to the ‘dell-csm-operator’ namespace by default.\nUpgrade driver using Dell CSM Operator: The CSI Drivers installed by the Dell CSM Operator can be updated like any Kubernetes resource.\nModifying the installation directly via kubectl edit $ kubectl get \u003cdriver-object\u003e -n \u003cdriver-namespace\u003e For example - If the CSI PowerStore driver is installed then run this command to get the object name # Replace driver-namespace with the namespace where the CSI PowerStore driver is installed $ kubectl get csm -n \u003cdriver-namespace\u003e use the object name in kubectl edit command. $ kubectl edit csm \u003cdriver-object\u003e/\u003cobject-name\u003e -n \u003cdriver-namespace\u003e For example - If the object name is powerstore then use the name as powerstore # Replace object-name with the powerstore $ kubectl edit csm powerstore -n \u003cdriver-namespace\u003e and modify the installation. The usual fields to edit are the version of drivers, sidecars and the environment variables. The following notes explain some of the general items to take care of. NOTE:\nIf you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field. driver: configVersion: v2.9.1 Custom Resource Definitions As part of the Dell CSM Operator installation, a CRD representing configuration for the CSI Driver and CSM Modules is also installed. containerstoragemodule CRD is installed in API Group storage.dell.com.\nDrivers and modules can be installed by creating a customResource.\nCustom Resource Specification Each CSI Driver and CSM Module installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.Below is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - refer here for appropriate config version.\nreplicas - Number of replicas for controller plugin - must be set to 1 for all drivers.\ndnsPolicy - Determines the dnsPolicy for the node daemonset. Accepted values are Default, ClusterFirst, ClusterFirstWithHostNet, None.\ncommon - This field is mandatory and is used to specify common properties for both controller and the node plugin.\nimage - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values Optional fields controller - List of environment variables and values which are applicable only for controller.\nnode - List of environment variables and values which are applicable only for node.\nsideCars - Specification for CSI sidecar containers.\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver.\ntolerations - List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet. It should be set separately in the controller and node sections if you want separate set of tolerations for them.\nnodeSelector - Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet.\nNOTE: The image field should point to the correct image tag for version of the driver you are installing.\n","categories":"","description":"Container Storage Modules Operator","excerpt":"Container Storage Modules Operator","ref":"/csm-docs/docs/deployment/csmoperator/","tags":"","title":"CSM Operator"},{"body":"The Dell Container Storage Modules Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers and CSM Modules provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. The operator can be installed using OLM (Operator Lifecycle Manager) or manually.\nSupport Matrix Dell CSM Operator has been tested and qualified on Upstream Kubernetes and OpenShift. Supported versions are listed below:\nKubernetes Version OpenShift Version 1.25, 1.26, 1.27 4.12, 4.12 EUS, 4.13 NOTE:\nAuthorization module is only supported on Kubernetes platforms. Supported CSM Components The table below lists the driver and modules versions installable with the CSM Operator:\nCSI Driver Version CSM Authorization CSM Replication CSM Observability CSM Resiliency CSI PowerScale 2.8.0 ✔ 1.8.0 ✔ 1.6.0 ✔ 1.6.0 ✔ 1.7.0 CSI PowerScale 2.7.0 ✔ 1.7.0 ✔ 1.5.0 ✔ 1.5.0 ✔ 1.6.0 CSI PowerScale 2.6.0 ✔ 1.6.0 ✔ 1.4.0 ✔ 1.5.0 ❌ CSI PowerFlex 2.8.0 ✔ 1.8.0 ✔ 1.6.0 ✔ 1.6.0 ✔ 1.7.0 CSI PowerFlex 2.7.0 ✔ 1.7.0 ✔ 1.5.0 ✔ 1.5.0 ✔ 1.6.0 CSI PowerFlex 2.6.0 ✔ 1.6.0 ❌ ✔ 1.4.0 ❌ CSI PowerStore 2.8.0 ❌ ❌ ❌ ✔ 1.7.0 CSI PowerStore 2.7.0 ❌ ❌ ❌ ✔ 1.6.0 CSI PowerStore 2.6.0 ❌ ❌ ❌ ❌ CSI PowerMax 2.8.0 ✔ 1.8.0 ✔ 1.6.0 ✔ 1.6.0 ❌ CSI PowerMax 2.7.0 ✔ 1.7.0 ✔ 1.5.0 ❌ ❌ CSI Unity XT 2.8.0 ❌ ❌ ❌ ❌ CSI Unity XT 2.7.0 ❌ ❌ ❌ ❌ CSI Unity XT 2.6.0 ❌ ❌ ❌ ❌ NOTE:\nRefer to sample files here. Installation Dell CSM Operator can be installed manually or via Operator Hub.\nOnce installed you will be able to deploy drivers and modules from the Operator.\nOpenShift Installation via Operator Hub dell-csm-operator can be installed via Operator Hub on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the operator to:\nAutomatic - If you want the operator to be automatically installed or upgraded (once an upgrade is available). Manual - If you want a cluster administrator to manually review and approve the InstallPlan for installation/upgrades. NOTE: Dell CSM Operator is distributed as both Certified \u0026 Community editions. Both editions have the same codebase and are supported by Dell Technologies, the only difference is that the Certified version is validated by RedHat. The Certified version is often released couple of days/weeks after the Community version.\nManual Installation on a cluster without OLM Install volume snapshot CRDs. For detailed snapshot setup procedure, click here. Clone and checkout the required csm-operator version using git clone -b v1.3.0 https://github.com/dell/csm-operator.git cd csm-operator (Optional) If using a local Docker image, edit the deploy/operator.yaml file and set the image name for the CSM Operator Deployment. (Optional) If CSM Replication is planned for use and will be deployed using two clusters in an environment where the DNS is not configured, and cluster API endpoints are FQDNs, in order to resolve queries to remote API endpoints, it is necessary to edit the deploy/operator.yaml file and add the hostAliases field and associated \u003cFQDN\u003e:\u003cIP\u003e mappings to the CSM Operator Controller Manager Deployment under spec.template.spec. More information on host aliases can be found, here. # example config apiVersion: apps/v1 kind: Deployment metadata: name: dell-csm-operator-controller-manager spec: template: spec: hostAliases: - hostnames: - \"remote.FQDN\" ip: \"255.255.255.1\" Run bash scripts/install.sh to install the operator. NOTE: Dell CSM Operator will be installed in the dell-csm-operator namespace.\nRun the command to validate the installation. kubectl get pods -n dell-csm-operator If installed successfully, you should be able to see the operator pod in the dell-csm-operator namespace.\nOffline Bundle Installation on a cluster without OLM The csm-offline-bundle.sh script can be used to create a package usable for offline installation of Dell CSI Drivers via CSM Operator\nDependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\nOne Linux-based system, with Internet access, will be used to create the bundle. This involves the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry If one Linux system has both Internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\nDependency Usage docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry. One of these will be required on both the system building the offline bundle as well as the system preparing for installation. Tested version(s) are docker 24.0.5 and podman 4.4.1 git git will be used to manually clone one of the above repositories in order to create an offline bundle. This is only needed on the system preparing the offline bundle. Tested version(s) are git 2.39.3 but any version should work. Workflow To perform an offline installation, the following steps should be performed:\nBuild an offline bundle Unpack the offline bundle created in Step 1 and prepare for installation Perform operator installation using the files obtained after unpacking in Step 2 Perform driver installation using the files obtained after unpacking in Step 2 NOTE: It is recommended to use the same build tool for packing and unpacking of images (either docker or podman).\nBuilding an offline bundle This needs to be performed on a Linux system with access to the Internet as a git repo will need to be cloned, and container images pulled from public registries.\nTo build an offline bundle, the following steps are needed:\nClone and checkout the required csm-operator version using git clone -b v1.3.0 https://github.com/dell/csm-operator.git cd csm-operator Run the csi-offline-bundle.sh script which will be found in the scripts directory with an argument of -c in order to create an offline bundle bash scripts/csm-offline-bundle.sh -c The script will perform the following steps:\nDetermine required images by parsing CSM Operator configuration files Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to install the Operator and drivers. Here is the output of a request to build an offline bundle for the Dell CSM Operator:\n* * Building image manifest file Processing file /root/csm-operator/operatorconfig/driverconfig/common/default.yaml Processing file /root/csm-operator/bundle/manifests/dell-csm-operator.clusterserviceversion.yaml * * Pulling and saving container images dellemc/csi-isilon:v2.8.0 dellemc/csi-metadata-retriever:v1.5.0 dellemc/csipowermax-reverseproxy:v2.6.0 dellemc/csi-powermax:v2.8.0 dellemc/csi-powerstore:v2.8.0 dellemc/csi-unity:v2.8.0 dellemc/csi-vxflexos:v2.8.0 dellemc/csm-authorization-sidecar:v1.7.0 dellemc/csm-metrics-powerflex:v1.5.0 dellemc/csm-metrics-powerscale:v1.2.0 dellemc/csm-topology:v1.5.0 dellemc/dell-csi-replicator:v1.6.0 dellemc/dell-replication-controller:v1.6.0 dellemc/sdc:3.6.1 docker.io/dellemc/dell-csm-operator:v1.3.0 gcr.io/kubebuilder/kube-rbac-proxy:v0.8.0 nginxinc/nginx-unprivileged:1.20 otel/opentelemetry-collector:0.42.0 registry.k8s.io/sig-storage/csi-attacher:v4.3.0 registry.k8s.io/sig-storage/csi-external-health-monitor-controller:v0.9.0 registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0 registry.k8s.io/sig-storage/csi-provisioner:v3.5.0 registry.k8s.io/sig-storage/csi-resizer:v1.8.0 registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2 * * Copying necessary files /root/csm-operator/deploy /root/csm-operator/operatorconfig /root/csm-operator/samples /root/csm-operator/scripts /root/csm-operator/README.md /root/csm-operator/LICENSE * * Compressing release dell-csm-operator-bundle/ dell-csm-operator-bundle/deploy/ dell-csm-operator-bundle/deploy/operator.yaml dell-csm-operator-bundle/deploy/crds/ dell-csm-operator-bundle/deploy/crds/storage.dell.com_containerstoragemodules.yaml dell-csm-operator-bundle/deploy/olm/ dell-csm-operator-bundle/deploy/olm/operator_community.yaml ... ... dell-csm-operator-bundle/README.md dell-csm-operator-bundle/LICENSE * * Complete Offline bundle file is: /root/csm-operator/dell-csm-operator-bundle.tar.gz The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nUnpacking the offline bundle and preparing for installation This step needs to be performed on a Linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for Operator installation, the following steps need to be performed:\nCopy the offline bundle file created from the previous step to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e tar xvfz dell-csm-operator-bundle.tar.gz Here is the output of untar\ndell-csm-operator-bundle/ dell-csm-operator-bundle/deploy/ dell-csm-operator-bundle/deploy/operator.yaml dell-csm-operator-bundle/deploy/crds/ dell-csm-operator-bundle/deploy/crds/storage.dell.com_containerstoragemodules.yaml dell-csm-operator-bundle/deploy/olm/ dell-csm-operator-bundle/deploy/olm/operator_community.yaml ... ... dell-csm-operator-bundle/README.md dell-csm-operator-bundle/LICENSE Run the csm-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option cd dell-csm-operator-bundle bash scripts/csm-offline-bundle.sh -p -r localregistry:5000/dell-csm-operator/ The script will then perform the following steps:\nLoad the required container images into the local system Tag the images according to the user-supplied registry information Push the newly tagged images to the registry Modify the Operator configuration to refer to the newly tagged/pushed images Here is the output for preparing the bundle for installation (localregistry:5000 refers to an image registry accessible to Kubernetes/OpenShift. dell-csm-operator refers to the folder created within the registry.):\nPreparing a offline bundle for installation * * Loading docker images Loaded image: docker.io/dellemc/csi-powerstore:v2.8.0 Loaded image: docker.io/dellemc/csi-isilon:v2.8.0 ... ... Loaded image: registry.k8s.io/sig-storage/csi-resizer:v1.8.0 Loaded image: registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2 * * Tagging and pushing images dellemc/csi-isilon:v2.8.0 -\u003e localregistry:5000/dell-csm-operator/csi-isilon:v2.8.0 dellemc/csi-metadata-retriever:v1.5.0 -\u003e localregistry:5000/dell-csm-operator/csi-metadata-retriever:v1.5.0 ... ... registry.k8s.io/sig-storage/csi-resizer:v1.8.0 -\u003e localregistry:5000/dell-csm-operator/csi-resizer:v1.8.0 registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2 -\u003e localregistry:5000/dell-csm-operator/csi-snapshotter:v6.2.2 * * Preparing files within /root/dell-csm-operator-bundle changing: dellemc/csi-isilon:v2.8.0 -\u003e localregistry:5000/dell-csm-operator/csi-isilon:v2.8.0 changing: dellemc/csi-metadata-retriever:v1.5.0 -\u003e localregistry:5000/dell-csm-operator/csi-metadata-retriever:v1.5.0 ... ... changing: registry.k8s.io/sig-storage/csi-resizer:v1.8.0 -\u003e localregistry:5000/dell-csm-operator/csi-resizer:v1.8.0 changing: registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2 -\u003e localregistry:5000/dell-csm-operator/csi-snapshotter:v6.2.2 * * Complete Perform Operator installation Now that the required images are available and the Operator configuration updated, you can proceed to install the operator by executing install.sh script.\nbash scripts/install.sh NOTE: Dell CSM Operator would install to the ‘dell-csm-operator’ namespace by default.\nPerform Driver installation Now that the required images are available and the Operator is installed, you can proceed to install the driver by executing kubectl create -f \u003cmanifest-name\u003e. Manifests for all the supported drivers will be available inside the samples directory. Using Unity XT as an example\nkubectl create -f samples/storage_csm_unity_v280.yaml NOTE: Offline bundle supports install of only the latest version of Dell CSI Drivers\nNOTE:\nOffline bundle installation is only supported with manual installs i.e. without using Operator Lifecycle Manager (OLM). Install/uninstall of operator and drivers should be done using the files that are obtained after unpacking the offline bundle (dell-csm-operator-bundle.tar.gz) as that is where the image tags in the manifests are modified to point to the specified internal registry. Uninstall Operator uninstallation on a cluster without OLM To uninstall a CSM operator, run bash scripts/uninstall.sh. This will uninstall the operator in dell-csm-operator namespace.\nUpgrade Dell CSM Operator Dell CSM Operator can be upgraded in 2 ways:\nUsing Operator Lifecycle Manager (OLM)\nUsing script (for non-OLM based installation)\nUsing OLM The upgrade of the Dell CSM Operator is done via Operator Lifecycle Manager.\nThe Update approval (InstallPlan in OLM terms) strategy plays a role while upgrading dell-csm-operator on OpenShift. This option can be set during installation of dell-csm-operator on OpenShift via the console and can be either set to Manual or Automatic.\nIf the Update approval is set to Automatic, OpenShift automatically detects whenever the latest version of dell-csm-operator is available in the Operator hub, and upgrades it to the latest available version. If the upgrade policy is set to Manual, OpenShift notifies of an available upgrade. This notification can be viewed by the user in the Installed Operators section of the OpenShift console. Clicking on the hyperlink to Approve the installation would trigger the dell-csm-operator upgrade process. NOTE: The recommended version of OLM for Upstream Kubernetes is v0.25.0.\nUsing Installation Script Clone and checkout the required csm-operator version using git clone -b v1.3.0 https://github.com/dell/csm-operator.git cd csm-operator Execute bash scripts/install.sh --upgrade . This command will install the latest version of the operator. NOTE: Dell CSM Operator would install to the ‘dell-csm-operator’ namespace by default.\nUpgrade driver using Dell CSM Operator: The CSI Drivers installed by the Dell CSM Operator can be updated like any Kubernetes resource.\nModifying the installation directly via kubectl edit $ kubectl get \u003cdriver-object\u003e -n \u003cdriver-namespace\u003e For example - If the CSI PowerStore driver is installed then run this command to get the object name # Replace driver-namespace with the namespace where the CSI PowerStore driver is installed $ kubectl get csm -n \u003cdriver-namespace\u003e use the object name in kubectl edit command. $ kubectl edit csm \u003cdriver-object\u003e/\u003cobject-name\u003e -n \u003cdriver-namespace\u003e For example - If the object name is powerstore then use the name as powerstore # Replace object-name with the powerstore $ kubectl edit csm powerstore -n \u003cdriver-namespace\u003e and modify the installation. The usual fields to edit are the version of drivers, sidecars and the environment variables. The following notes explain some of the general items to take care of. NOTE:\nIf you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field. driver: configVersion: v2.8.0 Custom Resource Definitions As part of the Dell CSM Operator installation, a CRD representing configuration for the CSI Driver and CSM Modules is also installed. containerstoragemodule CRD is installed in API Group storage.dell.com.\nDrivers and modules can be installed by creating a customResource.\nCustom Resource Specification Each CSI Driver and CSM Module installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.Below is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - refer here for appropriate config version.\nreplicas - Number of replicas for controller plugin - must be set to 1 for all drivers.\ndnsPolicy - Determines the dnsPolicy for the node daemonset. Accepted values are Default, ClusterFirst, ClusterFirstWithHostNet, None.\ncommon - This field is mandatory and is used to specify common properties for both controller and the node plugin.\nimage - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values Optional fields controller - List of environment variables and values which are applicable only for controller.\nnode - List of environment variables and values which are applicable only for node.\nsideCars - Specification for CSI sidecar containers.\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver.\ntolerations - List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet. It should be set separately in the controller and node sections if you want separate set of tolerations for them.\nnodeSelector - Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet.\nNOTE: The image field should point to the correct image tag for version of the driver you are installing.\n","categories":"","description":"Container Storage Modules Operator","excerpt":"Container Storage Modules Operator","ref":"/csm-docs/v1/deployment/csmoperator/","tags":"","title":"CSM Operator"},{"body":" CSM 1.7.1 is applicable to helm based installations of PowerFlex driver.\nThe Dell Container Storage Modules Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers and CSM Modules provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. The operator can be installed using OLM (Operator Lifecycle Manager) or manually.\nSupport Matrix Dell CSM Operator has been tested and qualified on Upstream Kubernetes and OpenShift. Supported versions are listed below:\nKubernetes Version OpenShift Version 1.25, 1.26, 1.27 4.11, 4.12, 4.12 EUS NOTE:\nAuthorization module is only supported on Kubernetes platforms. Supported CSM Components The table below lists the driver and modules versions installable with the CSM Operator:\nCSI Driver Version CSM Authorization CSM Replication CSM Observability CSM Resiliency CSI PowerScale 2.7.0 ✔ 1.7.0 ✔ 1.5.0 ✔ 1.5.0 ✔ 1.6.0 CSI PowerScale 2.6.0 ✔ 1.6.0 ✔ 1.4.0 ✔ 1.5.0 ❌ CSI PowerScale 2.5.0 ✔ 1.5.0 ✔ 1.3.0 ✔ 1.4.0 ❌ CSI PowerFlex 2.7.0 ✔ 1.7.0 ✔ 1.5.0 ✔ 1.5.0 ✔ 1.6.0 CSI PowerFlex 2.6.0 ✔ 1.6.0 ✔ 1.4.0 ✔ 1.5.0 ❌ CSI PowerFlex 2.5.0 ✔ 1.5.0 ❌ ✔ 1.4.0 ❌ CSI PowerStore 2.7.0 ❌ ❌ ❌ ✔ 1.6.0 CSI PowerStore 2.6.0 ❌ ❌ ❌ ❌ CSI PowerMax 2.7.0 ✔ 1.7.0 ✔ 1.5.0 ❌ ❌ CSI Unity XT 2.7.0 ❌ ❌ ❌ ❌ NOTE:\nRefer to sample files here. Installation Dell CSM Operator can be installed manually or via Operator Hub.\nOnce installed you will be able to deploy drivers and modules from the Operator.\nOpenShift Installation via Operator Hub dell-csm-operator can be installed via Operator Hub on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the operator to:\nAutomatic - If you want the operator to be automatically installed or upgraded (once an upgrade is available). Manual - If you want a cluster administrator to manually review and approve the InstallPlan for installation/upgrades. NOTE: Dell CSM Operator is distributed as a Certified \u0026 Community versions. Both are the exact same code and supported by Dell Technologies, the only difference is that the Certified version is validated by RedHat. The Certified version often release couple of days/weeks after the Community version.\nManual Installation on a cluster without OLM Clone and checkout the required csm-operator version using git clone -b v1.2.0 https://github.com/dell/csm-operator.git cd csm-operator (Optional) If using a local Docker image, edit the deploy/operator.yaml file and set the image name for the CSM Operator Deployment. Run bash scripts/install.sh to install the operator. NOTE: Dell CSM Operator will be installed in the dell-csm-operator namespace.\nRun the command to validate the installation. kubectl get pods -n dell-csm-operator If installed successfully, you should be able to see the operator pod in the dell-csm-operator namespace.\nUninstall Operator uninstallation on a cluster without OLM To uninstall a CSM operator, run bash scripts/uninstall.sh. This will uninstall the operator in dell-csm-operator namespace.\nUpgrade Dell CSM Operator Dell CSM Operator can be upgraded in 2 ways:\nUsing Operator Lifecycle Manager (OLM)\nUsing script (for non-OLM based installation)\nUsing OLM The upgrade of the Dell CSM Operator is done via Operator Lifecycle Manager.\nThe Update approval (InstallPlan in OLM terms) strategy plays a role while upgrading dell-csm-operator on OpenShift. This option can be set during installation of dell-csm-operator on OpenShift via the console and can be either set to Manual or Automatic.\nIf the Update approval is set to Automatic, OpenShift automatically detects whenever the latest version of dell-csm-operator is available in the Operator hub, and upgrades it to the latest available version. If the upgrade policy is set to Manual, OpenShift notifies of an available upgrade. This notification can be viewed by the user in the Installed Operators section of the OpenShift console. Clicking on the hyperlink to Approve the installation would trigger the dell-csm-operator upgrade process. NOTE: The recommended version of OLM for Upstream Kubernetes is v0.18.3.\nUsing Installation Script Clone and checkout the required csm-operator version using git clone -b v1.2.0 https://github.com/dell/csm-operator.git cd csm-operator Execute bash scripts/install.sh --upgrade . This command will install the latest version of the operator. NOTE: Dell CSM Operator would install to the ‘dell-csm-operator’ namespace by default.\nUpgrade driver using Dell CSM Operator: The CSI Drivers installed by the Dell CSM Operator can be updated like any Kubernetes resource.\nModifying the installation directly via kubectl edit $ kubectl get \u003cdriver-object\u003e -n \u003cdriver-namespace\u003e For example - If the CSI PowerStore driver is installed then run this command to get the object name # Replace driver-namespace with the namespace where the CSI PowerStore driver is installed $ kubectl get csm -n \u003cdriver-namespace\u003e use the object name in kubectl edit command. $ kubectl edit csm \u003cdriver-object\u003e/\u003cobject-name\u003e -n \u003cdriver-namespace\u003e For example - If the object name is powerstore then use the name as powerstore # Replace object-name with the powerstore $ kubectl edit csm powerstore -n \u003cdriver-namespace\u003e and modify the installation. The usual fields to edit are the version of drivers, sidecars and the environment variables. The following notes explain some of the general items to take care of. NOTES:\nIf you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field. driver: configVersion: v2.7.0 Custom Resource Definitions As part of the Dell CSM Operator installation, a CRD representing configuration for the CSI Driver and CSM Modules is also installed. containerstoragemodule CRD is installed in API Group storage.dell.com.\nDrivers and modules can be installed by creating a customResource.\nCustom Resource Specification Each CSI Driver and CSM Module installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.Below is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - refer here for appropriate config version.\nreplicas - Number of replicas for controller plugin - must be set to 1 for all drivers.\ndnsPolicy - Determines the dnsPolicy for the node daemonset. Accepted values are Default, ClusterFirst, ClusterFirstWithHostNet, None.\ncommon - This field is mandatory and is used to specify common properties for both controller and the node plugin.\nimage - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values Optional fields controller - List of environment variables and values which are applicable only for controller.\nnode - List of environment variables and values which are applicable only for node.\nsideCars - Specification for CSI sidecar containers.\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver.\ntolerations - List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet. It should be set separately in the controller and node sections if you want separate set of tolerations for them.\nnodeSelector - Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet.\nNOTE: The image field should point to the correct image tag for version of the driver you are installing.\n","categories":"","description":"Container Storage Modules Operator","excerpt":"Container Storage Modules Operator","ref":"/csm-docs/v2/deployment/csmoperator/","tags":"","title":"CSM Operator"},{"body":"The Dell Container Storage Modules Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers and CSM Modules provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. The operator can be installed using OLM (Operator Lifecycle Manager) or manually.\nSupported Platforms Dell CSM Operator has been tested and qualified on Upstream Kubernetes and OpenShift. Supported versions are listed below.\nKubernetes Version OpenShift Version 1.24, 1.25, 1.26 4.10, 4.10 EUS, 4.11 NOTE:\nAuthorization module is only supported on Kubernetes platforms. Supported CSM Components The table below lists the driver and modules versions installable with the CSM Operator:\nCSI Driver Version CSM Authorization CSM Replication CSM Observability CSM Resiliency CSI PowerScale 2.6.0 1.6.0 1.4.0 1.5.0 N/A CSI PowerFlex 2.6.0 1.6.0 1.4.0 1.5.0 N/A CSI PowerStore 2.6.0 N/A N/A N/A N/A NOTE:\nRefer to sample files here for prior versions of CSM. Installation Dell CSM Operator can be installed manually or via Operator Hub.\nManual Installation Operator Installation on a cluster without OLM Clone and checkout the required csm-operator version using git clone -b v1.1.0 https://github.com/dell/csm-operator.git cd csm-operator (Optional) If using a local Docker image, edit the deploy/operator.yaml file and set the image name for the CSM Operator Deployment. Run bash scripts/install.sh to install the operator. NOTE: Dell CSM Operator will be installed in the dell-csm-operator namespace.\nRun the command kubectl get pods -n dell-csm-operator to validate the installation. If installed successfully, you should be able to see the operator pod in the dell-csm-operator namespace. Installation via Operator Hub dell-csm-operator can be installed via Operator Hub on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the operator to:\nAutomatic - If you want the operator to be automatically installed or upgraded (once an upgrade is available). Manual - If you want a cluster administrator to manually review and approve the InstallPlan for installation/upgrades. Uninstall Operator uninstallation on a cluster without OLM To uninstall a CSM operator, run bash scripts/uninstall.sh. This will uninstall the operator in dell-csm-operator namespace.\nTo upgrade Dell CSM Operator, perform the following steps. Dell CSM Operator can be upgraded in 2 ways:\n1.Using script (for non-OLM based installation)\n2.Using Operator Lifecycle Manager (OLM)\nUsing Installation Script Clone and checkout the required csm-operator version using git clone -b v1.1.0 https://github.com/dell/csm-operator.git cd csm-operator Execute bash scripts/install.sh --upgrade . This command will install the latest version of the operator. Note: Dell CSM Operator would install to the ‘dell-csm-operator’ namespace by default.\nUsing OLM The upgrade of the Dell CSM Operator is done via Operator Lifecycle Manager.\nThe Update approval (InstallPlan in OLM terms) strategy plays a role while upgrading dell-csm-operator on OpenShift. This option can be set during installation of dell-csm-operator on OpenShift via the console and can be either set to Manual or Automatic.\nIf the Update approval is set to Automatic, OpenShift automatically detects whenever the latest version of dell-csm-operator is available in the Operator hub, and upgrades it to the latest available version. If the upgrade policy is set to Manual, OpenShift notifies of an available upgrade. This notification can be viewed by the user in the Installed Operators section of the OpenShift console. Clicking on the hyperlink to Approve the installation would trigger the dell-csm-operator upgrade process. NOTE: The recommended version of OLM for Upstream Kubernetes is v0.18.3.\nCustom Resource Definitions As part of the Dell CSM Operator installation, a CRD representing configuration for the CSI Driver and CSM Modules is also installed. containerstoragemodule CRD is installed in API Group storage.dell.com.\nDrivers and modules can be installed by creating a customResource.\nCustom Resource Specification Each CSI Driver and CSM Module installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.Below is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - refer here for appropriate config version.\nreplicas - Number of replicas for controller plugin - must be set to 1 for all drivers.\ndnsPolicy - Determines the dnsPolicy for the node daemonset. Accepted values are Default, ClusterFirst, ClusterFirstWithHostNet, None.\ncommon - This field is mandatory and is used to specify common properties for both controller and the node plugin.\nimage - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values Optional fields controller - List of environment variables and values which are applicable only for controller.\nnode - List of environment variables and values which are applicable only for node.\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver.\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver.\ntolerations - List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet. It should be set separately in the controller and node sections if you want separate set of tolerations for them.\nnodeSelector - Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet.\nNote: The image field should point to the correct image tag for version of the driver you are installing.\n","categories":"","description":"Container Storage Modules Operator","excerpt":"Container Storage Modules Operator","ref":"/csm-docs/v3/deployment/csmoperator/","tags":"","title":"CSM Operator"},{"body":" The Dell CSI Operator is no longer actively maintained or supported. It will be deprecated in CSM 1.9. It is highly recommended that you use CSM Operator going forward.\nTo upgrade Dell CSI Operator, perform the following steps. Dell CSI Operator can be upgraded based on the supported platforms in one of the 2 ways:\nUsing script (for non-OLM based installation) Using Operator Lifecycle Manager (OLM) Using Installation Script Clone and checkout the required dell-csi-operator version using git clone -b v1.12.0 https://github.com/dell/dell-csi-operator.git. cd dell-csi-operator Execute bash scripts/install.sh --upgrade. This command will install the latest version of the operator. Using OLM The upgrade of the Dell CSI Operator is done via Operator Lifecycle Manager.\nThe Update approval (InstallPlan in OLM terms) strategy plays a role while upgrading dell-csi-operator on OpenShift. This option can be set during installation of dell-csi-operator on OpenShift via the console and can be either set to Manual or Automatic.\nIf the Update approval is set to Automatic, OpenShift automatically detects whenever the latest version of dell-csi-operator is available in the Operator hub, and upgrades it to the latest available version. If the upgrade policy is set to Manual, OpenShift notifies of an available upgrade. This notification can be viewed by the user in the Installed Operators section of the OpenShift console. Clicking on the hyperlink to Approve the installation would trigger the dell-csi-operator upgrade process. NOTE: The recommended version of OLM for Upstream Kubernetes is v0.18.3 when upgrading operator to v1.12.0.\n","categories":"","description":"Upgrade Dell CSI Operator","excerpt":"Upgrade Dell CSI Operator","ref":"/csm-docs/v1/csidriver/upgradation/drivers/operator/","tags":["upgrade","csi-driver"],"title":"Dell CSI Operator"},{"body":"To upgrade Dell CSI Operator, perform the following steps. Dell CSI Operator can be upgraded based on the supported platforms in one of the 2 ways:\nUsing script (for non-OLM based installation) Using Operator Lifecycle Manager (OLM) Using Installation Script Clone and checkout the required dell-csi-operator version using git clone -b v1.12.0 https://github.com/dell/dell-csi-operator.git. cd dell-csi-operator Execute bash scripts/install.sh --upgrade. This command will install the latest version of the operator. Using OLM The upgrade of the Dell CSI Operator is done via Operator Lifecycle Manager.\nThe Update approval (InstallPlan in OLM terms) strategy plays a role while upgrading dell-csi-operator on OpenShift. This option can be set during installation of dell-csi-operator on OpenShift via the console and can be either set to Manual or Automatic.\nIf the Update approval is set to Automatic, OpenShift automatically detects whenever the latest version of dell-csi-operator is available in the Operator hub, and upgrades it to the latest available version. If the upgrade policy is set to Manual, OpenShift notifies of an available upgrade. This notification can be viewed by the user in the Installed Operators section of the OpenShift console. Clicking on the hyperlink to Approve the installation would trigger the dell-csi-operator upgrade process. NOTE: The recommended version of OLM for Upstream Kubernetes is v0.18.3 when upgrading operator to v1.12.0.\n","categories":"","description":"Upgrade Dell CSI Operator","excerpt":"Upgrade Dell CSI Operator","ref":"/csm-docs/v2/csidriver/upgradation/drivers/operator/","tags":["upgrade","csi-driver"],"title":"Dell CSI Operator"},{"body":"To upgrade Dell CSI Operator, perform the following steps. Dell CSI Operator can be upgraded based on the supported platforms in one of the 2 ways:\nUsing script (for non-OLM based installation) Using Operator Lifecycle Manager (OLM) Using Installation Script Clone and checkout the required dell-csi-operator version using git clone -b v1.11.0 https://github.com/dell/dell-csi-operator.git. cd dell-csi-operator Execute bash scripts/install.sh --upgrade. This command will install the latest version of the operator. Using OLM The upgrade of the Dell CSI Operator is done via Operator Lifecycle Manager.\nThe Update approval (InstallPlan in OLM terms) strategy plays a role while upgrading dell-csi-operator on OpenShift. This option can be set during installation of dell-csi-operator on OpenShift via the console and can be either set to Manual or Automatic.\nIf the Update approval is set to Automatic, OpenShift automatically detects whenever the latest version of dell-csi-operator is available in the Operator hub, and upgrades it to the latest available version. If the upgrade policy is set to Manual, OpenShift notifies of an available upgrade. This notification can be viewed by the user in the Installed Operators section of the OpenShift console. Clicking on the hyperlink to Approve the installation would trigger the dell-csi-operator upgrade process. NOTE: The recommended version of OLM for Upstream Kubernetes is v0.18.3 when upgrading operator to v1.11.0.\n","categories":"","description":"Upgrade Dell CSI Operator","excerpt":"Upgrade Dell CSI Operator","ref":"/csm-docs/v3/csidriver/upgradation/drivers/operator/","tags":["upgrade","csi-driver"],"title":"Dell CSI Operator"},{"body":"Pre-requisites Request a License for Application Mobility Object store bucket accessible by both the source and target clusters Installation Repeat the following steps on both clusters:\nCreate a namespace where Application Mobility will be installed. kubectl create ns application-mobility Edit the license Secret file (see Pre-requisites above) and set the correct namespace (ex: namespace: application-mobility) Create the Secret containing a license file kubectl apply -f license.yml Add the Dell Helm Charts repository helm repo add dell https://dell.github.io/helm-charts Either create a values.yml file or provide the --set options to the helm install to override default values from the Configuration section. Install the helm chart helm install application-mobility -n application-mobility dell/csm-application-mobility Configuration This table lists the configurable parameters of the Application Mobility Helm chart and their default values.\nParameter Description Required Default replicaCount Number of replicas for the Application Mobility controllers Yes 1 image.pullPolicy Image pull policy for the Application Mobility controller images Yes IfNotPresent controller.image Location of the Application Mobility Docker image Yes dellemc/csm-application-mobility-controller:v0.3.0 cert-manager.enabled If set to true, cert-manager will be installed during Application Mobility installation Yes false veleroNamespace If Velero is already installed, set to the namespace where Velero is installed No velero licenseName Name of the Secret that contains the License for Application Mobility Yes license objectstore.secretName If velero is already installed on the cluster, specify the name of the secret in velero namespace that has credentials to access object store No velero.enabled If set to true, Velero will be installed during Application Mobility installation Yes true velero.use-volume-snapshots If set to true, Velero will use volume snapshots Yes false velero.deployRestic If set to true, Velero will also deploy Restic Yes true velero.cleanUpCRDs If set to true, Velero CRDs will be cleaned up Yes true velero.credentials.existingSecret Optionally, specify the name of the pre-created secret in the release namespace that holds the object store credentials. Either this or secretContents should be specified No velero.credentials.name Optionally, specify the name to be used for secret that will be created to hold object store credentials. Used in conjunction with secretContents. No velero.credentials.secretContents Optionally, specify the object store access credentials to be stored in a secret with key “cloud”. Either this or existingSecret should be provided. No velero.configuration.provider Provider to use for Velero. Yes aws velero.configuration.backupStorageLocation.name Name of the backup storage location for Velero. Yes default velero.configuration.backupStorageLocation.bucket Name of the object store bucket to use for backups. Yes velero-bucket velero.configuration.backupStorageLocation.config Additional provider-specific configuration. See https://velero.io/docs/v1.9/api-types/backupstoragelocation/ for specific details. Yes velero.initContainers List of plugins used by Velero. Dell Velero plugin is required and plugins for other providers can be added. Yes velero.initContainers[0].name Name of the Dell Velero plugin. Yes dell-custom-velero-plugin velero.initContainers[0].image Location of the Dell Velero plugin image. Yes dellemc/csm-application-mobility-velero-plugin:v0.3.0 velero.initContainers[0].volumeMounts[0].mountPath Mount path of the volume mount. Yes /target velero.initContainers[0].volumeMounts[0].name Name of the volume mount. Yes plugins velero.restic.privileged If set to true, Restic Pods will be run in privileged mode. Note: Set to true when using Red Hat OpenShift No false ","categories":"","description":"Deployment\n","excerpt":"Deployment\n","ref":"/csm-docs/docs/applicationmobility/deployment/","tags":"","title":"Deployment"},{"body":"The Container Storage Modules along with the required CSI Drivers can each be deployed using CSM operator.\nCSM Operator Dell CSM Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers and CSM Modules provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. The operator can be installed using OLM (Operator Lifecycle Manager) or manually. …More on installation instructions Supported drivers: PowerScale, PowerStore, PowerFlex, PowerMax, Unity XT Supported modules: Authorization, Replication, Observability The Container Storage Modules and the required CSI Drivers can each be deployed following the links below:\nDell CSI Drivers Installation via Helm Dell CSI Helm installer installs the CSI Driver components using the provided Helm charts. …More on installation instructions Installs PowerStore PowerMax PowerScale PowerFlex Unity XT CSM Installation Wizard CSM Installation Wizard generates manifest files to install Dell CSI Drivers and supported modules. …More on installation instructions Generates manifest file for installation Dell CSI Drivers Installation via offline installer Both Helm and Dell CSM operator supports offline installation of the Dell CSI Storage Providers via csi-offline-bundle.sh or csm-offline-bundle.sh script, respectively, by creating a usable package. …More on installation instructions Offline installation for all drivers Offline installation with Operator Dell Container Storage Module for Observability CSM for Observability can be deployed either via Helm/CSM operator/CSM for Observability Installer/CSM for Observability Offline Installer …More on installation instructions Installs Observability Module Dell Container Storage Module for Authorization CSM Authorization can be installed by using the provided Helm v3 charts on Kubernetes platforms or CSM operator. …More on installation instructions Installs Authorization Module Dell Container Storage Module for Resiliency CSI drivers that support Helm chart installation allow CSM for Resiliency to be optionally installed by variables in the chart. It can be updated via podmon block specified in the values.yaml. It can be installed via CSM operator as well. …More on installation instructions Installs Resiliency Module Dell Container Storage Module for Replication Replication module can be installed by installing repctl,Container Storage Modules (CSM) for Replication Controller,CSI driver after enabling replication. It can be installed via CSM operator as well. …More on installation instructions Installs Replication Module Dell Container Storage Module for Application Mobility Application mobility module can be installed via helm charts. This is a tech preview release and it requires a license for installation. …More on installation instructions Installs Application Mobility Module Dell Container Storage Module for Encryption Encryption can be optionally installed via the PowerScale CSI driver Helm chart. …More on installation instructions Installs Encryption Module ","categories":"","description":"Deployment of CSM","excerpt":"Deployment of CSM","ref":"/csm-docs/docs/deployment/","tags":"","title":"Deployment"},{"body":"","categories":"","description":"Installation for Dell Container Storage Module (CSM) for Replication\n","excerpt":"Installation for Dell Container Storage Module (CSM) for Replication\n","ref":"/csm-docs/docs/replication/deployment/","tags":"","title":"Deployment"},{"body":"","categories":"","description":"Installation for Dell Container Storage Module (CSM) for Resiliency\n","excerpt":"Installation for Dell Container Storage Module (CSM) for Resiliency\n","ref":"/csm-docs/docs/resiliency/deployment/","tags":"","title":"Deployment"},{"body":"Encryption for Dell Container Storage Modules is enabled via the Dell CSI driver installation. The drivers can be installed either by a Helm chart or by the Dell CSM Operator. In the tech preview release, Encryption can only be enabled via Helm chart installation.\nExcept for additional Encryption related configuration outlined on this page, the rest of the deployment process is described in the correspondent CSI driver documentation.\nVault Server Hashicorp Vault must be pre-configured to support Encryption. The Vault server’s IP address and port must be accessible from the Kubernetes cluster where the CSI driver is to be deployed.\nRekey Controller The Encryption Rekey CRD Controller is an optional component that, if installed, allows encrypted volumes rekeying in a Kubernetes cluster. Please refer to Rekey Configuration for the Rekey Controller installation details.\nHelm Chart Values The drivers that support Encryption via Helm chart have an encryption block in their values.yaml file that looks like this:\nencryption: # enabled: Enable/disable volume encryption feature. enabled: false # pluginName: The name of the provisioner to use for encrypted volumes. pluginName: \"sec-isilon.dellemc.com\" # logLevel: Log level of the encryption driver. # Allowed values: \"error\", \"warning\", \"info\", \"debug\", \"trace\". logLevel: \"error\" # apiPort: TCP port number used by the REST API server. apiPort: 3838 # logLevel: Log level of the encryption driver. # Allowed values: \"error\", \"warning\", \"info\", \"debug\", \"trace\". logLevel: \"debug\" # livenessPort: HTTP liveness probe port number. # Leave empty to disable the liveness probe. # Example: 8080 livenessPort: # ocp: Enable when running on OpenShift Container Platform with CoreOS worker nodes. ocp: false # ocpCoreID: User ID and group ID of user core on CoreOS worker nodes. # Ignored when ocp is set to false. ocpCoreID: \"1000:1000\" # extraArgs: Extra command line parameters to pass to the encryption driver. # Allowed values: # --sharedStorage - may be required by some applications to work properly. # When set, performance is reduced and hard links cannot be created. # See the gocryptfs documentation for more details. extraArgs: [] Parameter Description Required Default enabled Enable/disable volume encryption feature. No false pluginName The name of the provisioner to use for encrypted volumes. No “sec-isilon.dellemc.com” image Encryption driver image name. No “dellemc/csm-encryption:v0.3.0” logLevel Log level of the encryption driver.Allowed values: “error”, “warning”, “info”, “debug”, “trace”. No “error” apiPort TCP Port number used by the REST API Server. No 3838 livenessPort HTTP liveness probe port number. Leave empty to disable the liveness probe. No ocp Enable when running an OCP Platform with CoreOS worker nodes. No false ocpCoreID User ID and group ID of user core on CoreOS worker nodes. Ignored when ocp is set to false. No “1000:1000” extraArgs Extra command line parameters to pass to the encryption driver.Allowed values:\"--sharedStorage\" - may be required by some applications to work properly.When set, performance is reduced and hard links cannot be created.See the gocryptfs documentation for more details. No [] Secrets and Config Maps Apart from any secrets and config maps described in the CSI driver documentation, these resources should be created for Encryption:\nSecret encryption-license Request a trial license following instructions on the License page. You will be provided with a YAML file similar to:\napiVersion: v1 data: license: k1FXzMDZodGNnK4I12Alo4UvuhLd+ithRhuLz2eoIxlcMSfW0xJYWnBiNMvTUl8VdGmR5fsvs2L6KqPfpIJk4wOzCxQ9wfDIJuYqrwV0wi2F2lzb1Hkk7O7/4r8cblPdCRJWfbg8QFc2BVtl4PZ/pFkHZoZVCbhGDD1MsbI1CiKqva9r9TBfswSFnqv7p3QXgbqQov8/q/j2+sHcvFF3j4kx+q1PzXoRNxwuTQaP4VAvipsQNAU5yV2dos2hs4Y/Ltbtreu/vrRGUaxvPbass1vUtIOJnvKkfbp53j8PFJGGISMYvYylUiD7TpoamxT/1I6mkjgRds+tEciMvutqDpmKEtdyp3vBjt4Sgd07ptvsdBJlyRAYb8ZPX9vXr4Ws kind: Secret metadata: name: edit_name namespace: edit_namespace Set name to \"encryption-license\" and namespace to your driver namespace and apply the file:\nkubectl apply -f \u003clicense yaml file name\u003e Secret vault-auth A secret with the AppRole credentials used by Encryption to authenticate to the Vault server.\nSet role_id and secret_id to the values provided by the Vault server administrator.\nIf a self-managed test Vault instance is used, generate role ID and secret ID following these steps.\ncat \u003eauth.json \u003c\u003cEOF { \"role_id\": \"\u003crole ID\u003e\", \"secret_id\": \"\u003csecret ID\u003e\" } EOF kubectl create secret generic vault-auth -n \u003cdriver namespace\u003e --from-file=auth.json -o yaml --dry-run=client | kubectl apply -f - rm -f auth.json In this release, Encryption does not pick up modifications to this secret while the CSI driver is running, unless it needs to re-login which happens at:\nCSI Driver startup an authentication error from the Vault server client token expiration In all other cases, to apply new values in the secret (e.g., to use another role), the CSI driver must be restarted.\nSecret vault-cert A secret with TLS certificates used by Encryption to communicate with the Vault server.\nFiles server-ca.crt, client.crt and client.key should be in PEM format.\nkubectl create secret generic vault-cert -n \u003cdriver namespace\u003e \\ --from-file=server-ca.crt --from-file=client.crt --from-file=client.key \\ -o yaml --dry-run=client | kubectl apply -f - In this release, Encryption does not pick up modifications to this secret while the CSI driver is running. To apply new values in the secret (e.g., to update the client certificate), the CSI driver must be restarted.\nConfigMap vault-client-conf A config map with settings used by Encryption to communicate with the Vault server.\nPopulate client.json with your settings.\ncat \u003eclient.json \u003c\u003cEOF { \"auth_type\": \"approle\", \"auth_conf_file\": \"/etc/dea/vault/auth.json\", \"vault_addr\": \"https://\u003cvault server address\u003e:8400\", \"kv_engine_path\": \"/dea-keys\", \"tls_config\": { \"client_crt\": \"/etc/dea/vault/client.crt\", \"client_key\": \"/etc/dea/vault/client.key\", \"server_ca\": \"/etc/dea/vault/server-ca.crt\" } } EOF kubectl create configmap vault-client-conf -n \u003cdriver namespace\u003e \\ --from-file=client.json -o yaml --dry-run=client | kubectl apply -f - rm -f client.json These fields are available for use in client.json:\nclient.json field Description Required Default auth_type Authentication type used to authenticate to the Vault server. Currently, the only supported type is “approle”. Yes auth_conf_file Set to “/etc/dea/vault/auth.json” Yes auth_timeout Defines in how many seconds key requests to the Vault server fail if there is no valid authentication token. No 5 lease_duration_margin Defines how many seconds in advance the authentication token lease will be renewed. This value should accommodate network and processing delays. No 15 lease_increase Defines the number of seconds used in the authentication token renew call. This value is advisory and may be disregarded by the server. No 3600 vault_addr URL to use for REST calls to the Vault server. It must start with “https”. Yes kv_engine_path The path to which the Key/Value secret engine is mounted on the Vault server. Yes tls_config.client_crt Set to “/etc/dea/vault/client.crt” Yes tls_config.client_key Set to “/etc/dea/vault/client.key” Yes tls_config.client_ca Set to “/etc/dea/vault/server-ca.crt” Yes ","categories":"","description":"Deployment\n","excerpt":"Deployment\n","ref":"/csm-docs/docs/secure/encryption/deployment/","tags":"","title":"Deployment"},{"body":"Pre-requisites Request a License for Application Mobility Object store bucket accessible by both the source and target clusters Installation Repeat the following steps on both clusters:\nCreate a namespace where Application Mobility will be installed. kubectl create ns application-mobility Edit the license Secret file (see Pre-requisites above) and set the correct namespace (ex: namespace: application-mobility) Create the Secret containing a license file kubectl apply -f license.yml Add the Dell Helm Charts repository helm repo add dell https://dell.github.io/helm-charts Either create a values.yml file or provide the --set options to the helm install to override default values from the Configuration section. Install the helm chart helm install application-mobility -n application-mobility dell/csm-application-mobility Configuration This table lists the configurable parameters of the Application Mobility Helm chart and their default values.\nParameter Description Required Default replicaCount Number of replicas for the Application Mobility controllers Yes 1 image.pullPolicy Image pull policy for the Application Mobility controller images Yes IfNotPresent controller.image Location of the Application Mobility Docker image Yes dellemc/csm-application-mobility-controller:v0.3.0 cert-manager.enabled If set to true, cert-manager will be installed during Application Mobility installation Yes false veleroNamespace If Velero is already installed, set to the namespace where Velero is installed No velero licenseName Name of the Secret that contains the License for Application Mobility Yes license objectstore.secretName If velero is already installed on the cluster, specify the name of the secret in velero namespace that has credentials to access object store No velero.enabled If set to true, Velero will be installed during Application Mobility installation Yes true velero.use-volume-snapshots If set to true, Velero will use volume snapshots Yes false velero.deployRestic If set to true, Velero will also deploy Restic Yes true velero.cleanUpCRDs If set to true, Velero CRDs will be cleaned up Yes true velero.credentials.existingSecret Optionally, specify the name of the pre-created secret in the release namespace that holds the object store credentials. Either this or secretContents should be specified No velero.credentials.name Optionally, specify the name to be used for secret that will be created to hold object store credentials. Used in conjunction with secretContents. No velero.credentials.secretContents Optionally, specify the object store access credentials to be stored in a secret with key “cloud”. Either this or existingSecret should be provided. No velero.configuration.provider Provider to use for Velero. Yes aws velero.configuration.backupStorageLocation.name Name of the backup storage location for Velero. Yes default velero.configuration.backupStorageLocation.bucket Name of the object store bucket to use for backups. Yes velero-bucket velero.configuration.backupStorageLocation.config Additional provider-specific configuration. See https://velero.io/docs/v1.9/api-types/backupstoragelocation/ for specific details. Yes velero.initContainers List of plugins used by Velero. Dell Velero plugin is required and plugins for other providers can be added. Yes velero.initContainers[0].name Name of the Dell Velero plugin. Yes dell-custom-velero-plugin velero.initContainers[0].image Location of the Dell Velero plugin image. Yes dellemc/csm-application-mobility-velero-plugin:v0.3.0 velero.initContainers[0].volumeMounts[0].mountPath Mount path of the volume mount. Yes /target velero.initContainers[0].volumeMounts[0].name Name of the volume mount. Yes plugins velero.restic.privileged If set to true, Restic Pods will be run in privileged mode. Note: Set to true when using Red Hat OpenShift No false ","categories":"","description":"Deployment\n","excerpt":"Deployment\n","ref":"/csm-docs/v1/applicationmobility/deployment/","tags":"","title":"Deployment"},{"body":"The Container Storage Modules along with the required CSI Drivers can each be deployed using CSM operator.\nCSM Operator Dell CSM Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers and CSM Modules provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. The operator can be installed using OLM (Operator Lifecycle Manager) or manually. …More on installation instructions Supported drivers: PowerScale, PowerStore, PowerFlex, PowerMax, Unity XT Supported modules: Authorization, Replication, Observability The Container Storage Modules and the required CSI Drivers can each be deployed following the links below:\nDell CSI Drivers Installation via Helm Dell CSI Helm installer installs the CSI Driver components using the provided Helm charts. …More on installation instructions Installs PowerStore PowerMax PowerScale PowerFlex Unity XT CSM Installation Wizard CSM Installation Wizard generates manifest files to install Dell CSI Drivers and supported modules. …More on installation instructions Generates manifest file for installation Dell CSI Drivers Installation via offline installer Both Helm and Dell CSI opetor supports offline installation of the Dell CSI Storage Providers via csi-offline-bundle.sh script by creating a usable package. …More on installation instructions Offline installation for all drivers Dell CSI Drivers Installation via operator Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using the OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually. …More on installation instructions Installs PowerStore PowerMax PowerScale PowerFlex Unity XT Dell Container Storage Module for Observability CSM for Observability can be deployed either via Helm or CSM for Observability Installer or CSM for Observability Offline Installer …More on installation instructions Installs Observability Module Dell Container Storage Module for Authorization CSM Authorization can be installed by using the provided Helm v3 charts on Kubernetes platforms. …More on installation instructions Installs Authorization Module Dell Container Storage Module for Resiliency CSI drivers that support Helm chart installation allow CSM for Resiliency to be optionally installed by variables in the chart. It can be updated via podmon block specified in the values.yaml …More on installation instructions Installs Resiliency Module Dell Container Storage Module for Replication Replication module can be installed by installing repctl,Container Storage Modules (CSM) for Replication Controller,CSI driver after enabling replication. …More on installation instructions Installs Replication Module Dell Container Storage Module for Application Mobility Application mobility module can be installed via helm charts. This is a tech preview release and it requires a license for installation. …More on installation instructions Installs Application Mobility Module Dell Container Storage Module for Encryption Encryption can be optionally installed via the PowerScale CSI driver Helm chart. …More on installation instructions Installs Encryption Module ","categories":"","description":"Deployment of CSM for Replication","excerpt":"Deployment of CSM for Replication","ref":"/csm-docs/v1/deployment/","tags":"","title":"Deployment"},{"body":"","categories":"","description":"Installation for Dell Container Storage Module (CSM) for Replication\n","excerpt":"Installation for Dell Container Storage Module (CSM) for Replication\n","ref":"/csm-docs/v1/replication/deployment/","tags":"","title":"Deployment"},{"body":"Encryption for Dell Container Storage Modules is enabled via the Dell CSI driver installation. The drivers can be installed either by a Helm chart or by the Dell CSI Operator. In the tech preview release, Encryption can only be enabled via Helm chart installation.\nExcept for additional Encryption related configuration outlined on this page, the rest of the deployment process is described in the correspondent CSI driver documentation.\nVault Server Hashicorp Vault must be pre-configured to support Encryption. The Vault server’s IP address and port must be accessible from the Kubernetes cluster where the CSI driver is to be deployed.\nRekey Controller The Encryption Rekey CRD Controller is an optional component that, if installed, allows encrypted volumes rekeying in a Kubernetes cluster. Please refer to Rekey Configuration for the Rekey Controller installation details.\nHelm Chart Values The drivers that support Encryption via Helm chart have an encryption block in their values.yaml file that looks like this:\nencryption: # enabled: Enable/disable volume encryption feature. enabled: false # pluginName: The name of the provisioner to use for encrypted volumes. pluginName: \"sec-isilon.dellemc.com\" # image: Encryption driver image name. image: \"dellemc/csm-encryption:v0.3.0\" # logLevel: Log level of the encryption driver. # Allowed values: \"error\", \"warning\", \"info\", \"debug\", \"trace\". logLevel: \"error\" # apiPort: TCP port number used by the REST API server. apiPort: 3838 # logLevel: Log level of the encryption driver. # Allowed values: \"error\", \"warning\", \"info\", \"debug\", \"trace\". logLevel: \"debug\" # livenessPort: HTTP liveness probe port number. # Leave empty to disable the liveness probe. # Example: 8080 livenessPort: # ocp: Enable when running on OpenShift Container Platform with CoreOS worker nodes. ocp: false # ocpCoreID: User ID and group ID of user core on CoreOS worker nodes. # Ignored when ocp is set to false. ocpCoreID: \"1000:1000\" # extraArgs: Extra command line parameters to pass to the encryption driver. # Allowed values: # --sharedStorage - may be required by some applications to work properly. # When set, performance is reduced and hard links cannot be created. # See the gocryptfs documentation for more details. extraArgs: [] Parameter Description Required Default enabled Enable/disable volume encryption feature. No false pluginName The name of the provisioner to use for encrypted volumes. No “sec-isilon.dellemc.com” image Encryption driver image name. No “dellemc/csm-encryption:v0.3.0” logLevel Log level of the encryption driver.Allowed values: “error”, “warning”, “info”, “debug”, “trace”. No “error” apiPort TCP Port number used by the REST API Server. No 3838 livenessPort HTTP liveness probe port number. Leave empty to disable the liveness probe. No ocp Enable when running an OCP Platform with CoreOS worker nodes. No false ocpCoreID User ID and group ID of user core on CoreOS worker nodes. Ignored when ocp is set to false. No “1000:1000” extraArgs Extra command line parameters to pass to the encryption driver.Allowed values:\"--sharedStorage\" - may be required by some applications to work properly.When set, performance is reduced and hard links cannot be created.See the gocryptfs documentation for more details. No [] Secrets and Config Maps Apart from any secrets and config maps described in the CSI driver documentation, these resources should be created for Encryption:\nSecret encryption-license Request a trial license following instructions on the License page. You will be provided with a YAML file similar to:\napiVersion: v1 data: license: k1FXzMDZodGNnK4I12Alo4UvuhLd+ithRhuLz2eoIxlcMSfW0xJYWnBiNMvTUl8VdGmR5fsvs2L6KqPfpIJk4wOzCxQ9wfDIJuYqrwV0wi2F2lzb1Hkk7O7/4r8cblPdCRJWfbg8QFc2BVtl4PZ/pFkHZoZVCbhGDD1MsbI1CiKqva9r9TBfswSFnqv7p3QXgbqQov8/q/j2+sHcvFF3j4kx+q1PzXoRNxwuTQaP4VAvipsQNAU5yV2dos2hs4Y/Ltbtreu/vrRGUaxvPbass1vUtIOJnvKkfbp53j8PFJGGISMYvYylUiD7TpoamxT/1I6mkjgRds+tEciMvutqDpmKEtdyp3vBjt4Sgd07ptvsdBJlyRAYb8ZPX9vXr4Ws kind: Secret metadata: name: edit_name namespace: edit_namespace Set name to \"encryption-license\" and namespace to your driver namespace and apply the file:\nkubectl apply -f \u003clicense yaml file name\u003e Secret vault-auth A secret with the AppRole credentials used by Encryption to authenticate to the Vault server.\nSet role_id and secret_id to the values provided by the Vault server administrator.\nIf a self-managed test Vault instance is used, generate role ID and secret ID following these steps.\ncat \u003eauth.json \u003c\u003cEOF { \"role_id\": \"\u003crole ID\u003e\", \"secret_id\": \"\u003csecret ID\u003e\" } EOF kubectl create secret generic vault-auth -n \u003cdriver namespace\u003e --from-file=auth.json -o yaml --dry-run=client | kubectl apply -f - rm -f auth.json In this release, Encryption does not pick up modifications to this secret while the CSI driver is running, unless it needs to re-login which happens at:\nCSI Driver startup an authentication error from the Vault server client token expiration In all other cases, to apply new values in the secret (e.g., to use another role), the CSI driver must be restarted.\nSecret vault-cert A secret with TLS certificates used by Encryption to communicate with the Vault server.\nFiles server-ca.crt, client.crt and client.key should be in PEM format.\nkubectl create secret generic vault-cert -n \u003cdriver namespace\u003e \\ --from-file=server-ca.crt --from-file=client.crt --from-file=client.key \\ -o yaml --dry-run=client | kubectl apply -f - In this release, Encryption does not pick up modifications to this secret while the CSI driver is running. To apply new values in the secret (e.g., to update the client certificate), the CSI driver must be restarted.\nConfigMap vault-client-conf A config map with settings used by Encryption to communicate with the Vault server.\nPopulate client.json with your settings.\ncat \u003eclient.json \u003c\u003cEOF { \"auth_type\": \"approle\", \"auth_conf_file\": \"/etc/dea/vault/auth.json\", \"vault_addr\": \"https://\u003cvault server address\u003e:8400\", \"kv_engine_path\": \"/dea-keys\", \"tls_config\": { \"client_crt\": \"/etc/dea/vault/client.crt\", \"client_key\": \"/etc/dea/vault/client.key\", \"server_ca\": \"/etc/dea/vault/server-ca.crt\" } } EOF kubectl create configmap vault-client-conf -n \u003cdriver namespace\u003e \\ --from-file=client.json -o yaml --dry-run=client | kubectl apply -f - rm -f client.json These fields are available for use in client.json:\nclient.json field Description Required Default auth_type Authentication type used to authenticate to the Vault server. Currently, the only supported type is “approle”. Yes auth_conf_file Set to “/etc/dea/vault/auth.json” Yes auth_timeout Defines in how many seconds key requests to the Vault server fail if there is no valid authentication token. No 5 lease_duration_margin Defines how many seconds in advance the authentication token lease will be renewed. This value should accommodate network and processing delays. No 15 lease_increase Defines the number of seconds used in the authentication token renew call. This value is advisory and may be disregarded by the server. No 3600 vault_addr URL to use for REST calls to the Vault server. It must start with “https”. Yes kv_engine_path The path to which the Key/Value secret engine is mounted on the Vault server. Yes tls_config.client_crt Set to “/etc/dea/vault/client.crt” Yes tls_config.client_key Set to “/etc/dea/vault/client.key” Yes tls_config.client_ca Set to “/etc/dea/vault/server-ca.crt” Yes ","categories":"","description":"Deployment\n","excerpt":"Deployment\n","ref":"/csm-docs/v1/secure/encryption/deployment/","tags":"","title":"Deployment"},{"body":"Pre-requisites Request a License for Application Mobility Object store bucket accessible by both the source and target clusters Installation Repeat the following steps on both clusters:\nCreate a namespace where Application Mobility will be installed. kubectl create ns application-mobility Edit the license Secret file (see Pre-requisites above) and set the correct namespace (ex: namespace: application-mobility) Create the Secret containing a license file kubectl apply -f license.yml Add the Dell Helm Charts repository helm repo add dell https://dell.github.io/helm-charts Either create a values.yml file or provide the --set options to the helm install to override default values from the Configuration section. Install the helm chart helm install application-mobility -n application-mobility dell/csm-application-mobility Configuration This table lists the configurable parameters of the Application Mobility Helm chart and their default values.\nParameter Description Required Default replicaCount Number of replicas for the Application Mobility controllers Yes 1 image.pullPolicy Image pull policy for the Application Mobility controller images Yes IfNotPresent controller.image Location of the Application Mobility Docker image Yes dellemc/csm-application-mobility-controller:v0.3.0 cert-manager.enabled If set to true, cert-manager will be installed during Application Mobility installation Yes false veleroNamespace If Velero is already installed, set to the namespace where Velero is installed No velero licenseName Name of the Secret that contains the License for Application Mobility Yes license objectstore.secretName If velero is already installed on the cluster, specify the name of the secret in velero namespace that has credentials to access object store No velero.enabled If set to true, Velero will be installed during Application Mobility installation Yes true velero.use-volume-snapshots If set to true, Velero will use volume snapshots Yes false velero.deployRestic If set to true, Velero will also deploy Restic Yes true velero.cleanUpCRDs If set to true, Velero CRDs will be cleaned up Yes true velero.credentials.existingSecret Optionally, specify the name of the pre-created secret in the release namespace that holds the object store credentials. Either this or secretContents should be specified No velero.credentials.name Optionally, specify the name to be used for secret that will be created to hold object store credentials. Used in conjunction with secretContents. No velero.credentials.secretContents Optionally, specify the object store access credentials to be stored in a secret with key “cloud”. Either this or existingSecret should be provided. No velero.configuration.provider Provider to use for Velero. Yes aws velero.configuration.backupStorageLocation.name Name of the backup storage location for Velero. Yes default velero.configuration.backupStorageLocation.bucket Name of the object store bucket to use for backups. Yes velero-bucket velero.configuration.backupStorageLocation.config Additional provider-specific configuration. See https://velero.io/docs/v1.9/api-types/backupstoragelocation/ for specific details. Yes velero.initContainers List of plugins used by Velero. Dell Velero plugin is required and plugins for other providers can be added. Yes velero.initContainers[0].name Name of the Dell Velero plugin. Yes dell-custom-velero-plugin velero.initContainers[0].image Location of the Dell Velero plugin image. Yes dellemc/csm-application-mobility-velero-plugin:v0.3.0 velero.initContainers[0].volumeMounts[0].mountPath Mount path of the volume mount. Yes /target velero.initContainers[0].volumeMounts[0].name Name of the volume mount. Yes plugins velero.restic.privileged If set to true, Restic Pods will be run in privileged mode. Note: Set to true when using Red Hat OpenShift No false ","categories":"","description":"Deployment\n","excerpt":"Deployment\n","ref":"/csm-docs/v2/applicationmobility/deployment/","tags":"","title":"Deployment"},{"body":" CSM 1.7.1 is applicable to helm based installations of PowerFlex driver.\nThe Container Storage Modules along with the required CSI Drivers can each be deployed using CSM operator.\nCSM Operator Dell CSM Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers and CSM Modules provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. The operator can be installed using OLM (Operator Lifecycle Manager) or manually. …More on installation instructions Supported drivers: PowerScale, PowerStore, PowerFlex, PowerMax, Unity XT Supported modules: Authorization, Replication, Observability The Container Storage Modules and the required CSI Drivers can each be deployed following the links below:\nDell CSI Drivers Installation via Helm Dell CSI Helm installer installs the CSI Driver components using the provided Helm charts. …More on installation instructions Installs PowerStore PowerMax PowerScale PowerFlex Unity XT CSM Installation Wizard CSM Installation Wizard generates manifest files to install Dell CSI Drivers and supported modules. …More on installation instructions Generates manifest file for installation Dell CSI Drivers Installation via offline installer Both Helm and Dell CSI opetor supports offline installation of the Dell CSI Storage Providers via csi-offline-bundle.sh script by creating a usable package. …More on installation instructions Offline installation for all drivers Dell CSI Drivers Installation via operator Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using the OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually. …More on installation instructions Installs PowerStore PowerMax PowerScale PowerFlex Unity XT Dell Container Storage Module for Observability CSM for Observability can be deployed either via Helm or CSM for Observability Installer or CSM for Observability Offline Installer …More on installation instructions Installs Observability Module Dell Container Storage Module for Authorization CSM Authorization can be installed by using the provided Helm v3 charts on Kubernetes platforms. …More on installation instructions Installs Authorization Module Dell Container Storage Module for Resiliency CSI drivers that support Helm chart installation allow CSM for Resiliency to be optionally installed by variables in the chart. It can be updated via podmon block specified in the values.yaml …More on installation instructions Installs Resiliency Module Dell Container Storage Module for Replication Replication module can be installed by installing repctl,Container Storage Modules (CSM) for Replication Controller,CSI driver after enabling replication. …More on installation instructions Installs Replication Module Dell Container Storage Module for Application Mobility Application mobility module can be installed via helm charts. This is a tech preview release and it requires a license for installation. …More on installation instructions Installs Application Mobility Module Dell Container Storage Module for Encryption Encryption can be optionally installed via the PowerScale CSI driver Helm chart. …More on installation instructions Installs Encryption Module ","categories":"","description":"Deployment of CSM for Replication","excerpt":"Deployment of CSM for Replication","ref":"/csm-docs/v2/deployment/","tags":"","title":"Deployment"},{"body":"","categories":"","description":"Installation for Dell Container Storage Module (CSM) for Replication\n","excerpt":"Installation for Dell Container Storage Module (CSM) for Replication\n","ref":"/csm-docs/v2/replication/deployment/","tags":"","title":"Deployment"},{"body":"Encryption for Dell Container Storage Modules is enabled via the Dell CSI driver installation. The drivers can be installed either by a Helm chart or by the Dell CSI Operator. In the tech preview release, Encryption can only be enabled via Helm chart installation.\nExcept for additional Encryption related configuration outlined on this page, the rest of the deployment process is described in the correspondent CSI driver documentation.\nVault Server Hashicorp Vault must be pre-configured to support Encryption. The Vault server’s IP address and port must be accessible from the Kubernetes cluster where the CSI driver is to be deployed.\nRekey Controller The Encryption Rekey CRD Controller is an optional component that, if installed, allows encrypted volumes rekeying in a Kubernetes cluster. Please refer to Rekey Configuration for the Rekey Controller installation details.\nHelm Chart Values The drivers that support Encryption via Helm chart have an encryption block in their values.yaml file that looks like this:\nencryption: # enabled: Enable/disable volume encryption feature. enabled: false # pluginName: The name of the provisioner to use for encrypted volumes. pluginName: \"sec-isilon.dellemc.com\" # image: Encryption driver image name. image: \"dellemc/csm-encryption:v0.3.0\" # logLevel: Log level of the encryption driver. # Allowed values: \"error\", \"warning\", \"info\", \"debug\", \"trace\". logLevel: \"error\" # apiPort: TCP port number used by the REST API server. apiPort: 3838 # logLevel: Log level of the encryption driver. # Allowed values: \"error\", \"warning\", \"info\", \"debug\", \"trace\". logLevel: \"debug\" # livenessPort: HTTP liveness probe port number. # Leave empty to disable the liveness probe. # Example: 8080 livenessPort: # ocp: Enable when running on OpenShift Container Platform with CoreOS worker nodes. ocp: false # ocpCoreID: User ID and group ID of user core on CoreOS worker nodes. # Ignored when ocp is set to false. ocpCoreID: \"1000:1000\" # extraArgs: Extra command line parameters to pass to the encryption driver. # Allowed values: # --sharedStorage - may be required by some applications to work properly. # When set, performance is reduced and hard links cannot be created. # See the gocryptfs documentation for more details. extraArgs: [] Parameter Description Required Default enabled Enable/disable volume encryption feature. No false pluginName The name of the provisioner to use for encrypted volumes. No “sec-isilon.dellemc.com” image Encryption driver image name. No “dellemc/csm-encryption:v0.3.0” logLevel Log level of the encryption driver.Allowed values: “error”, “warning”, “info”, “debug”, “trace”. No “error” apiPort TCP Port number used by the REST API Server. No 3838 livenessPort HTTP liveness probe port number. Leave empty to disable the liveness probe. No ocp Enable when running an OCP Platform with CoreOS worker nodes. No false ocpCoreID User ID and group ID of user core on CoreOS worker nodes. Ignored when ocp is set to false. No “1000:1000” extraArgs Extra command line parameters to pass to the encryption driver.Allowed values:\"--sharedStorage\" - may be required by some applications to work properly.When set, performance is reduced and hard links cannot be created.See the gocryptfs documentation for more details. No [] Secrets and Config Maps Apart from any secrets and config maps described in the CSI driver documentation, these resources should be created for Encryption:\nSecret encryption-license Request a trial license following instructions on the License page. You will be provided with a YAML file similar to:\napiVersion: v1 data: license: k1FXzMDZodGNnK4I12Alo4UvuhLd+ithRhuLz2eoIxlcMSfW0xJYWnBiNMvTUl8VdGmR5fsvs2L6KqPfpIJk4wOzCxQ9wfDIJuYqrwV0wi2F2lzb1Hkk7O7/4r8cblPdCRJWfbg8QFc2BVtl4PZ/pFkHZoZVCbhGDD1MsbI1CiKqva9r9TBfswSFnqv7p3QXgbqQov8/q/j2+sHcvFF3j4kx+q1PzXoRNxwuTQaP4VAvipsQNAU5yV2dos2hs4Y/Ltbtreu/vrRGUaxvPbass1vUtIOJnvKkfbp53j8PFJGGISMYvYylUiD7TpoamxT/1I6mkjgRds+tEciMvutqDpmKEtdyp3vBjt4Sgd07ptvsdBJlyRAYb8ZPX9vXr4Ws kind: Secret metadata: name: edit_name namespace: edit_namespace Set name to \"encryption-license\" and namespace to your driver namespace and apply the file:\nkubectl apply -f \u003clicense yaml file name\u003e Secret vault-auth A secret with the AppRole credentials used by Encryption to authenticate to the Vault server.\nSet role_id and secret_id to the values provided by the Vault server administrator.\nIf a self-managed test Vault instance is used, generate role ID and secret ID following these steps.\ncat \u003eauth.json \u003c\u003cEOF { \"role_id\": \"\u003crole ID\u003e\", \"secret_id\": \"\u003csecret ID\u003e\" } EOF kubectl create secret generic vault-auth -n \u003cdriver namespace\u003e --from-file=auth.json -o yaml --dry-run=client | kubectl apply -f - rm -f auth.json In this release, Encryption does not pick up modifications to this secret while the CSI driver is running, unless it needs to re-login which happens at:\nCSI Driver startup an authentication error from the Vault server client token expiration In all other cases, to apply new values in the secret (e.g., to use another role), the CSI driver must be restarted.\nSecret vault-cert A secret with TLS certificates used by Encryption to communicate with the Vault server.\nFiles server-ca.crt, client.crt and client.key should be in PEM format.\nkubectl create secret generic vault-cert -n \u003cdriver namespace\u003e \\ --from-file=server-ca.crt --from-file=client.crt --from-file=client.key \\ -o yaml --dry-run=client | kubectl apply -f - In this release, Encryption does not pick up modifications to this secret while the CSI driver is running. To apply new values in the secret (e.g., to update the client certificate), the CSI driver must be restarted.\nConfigMap vault-client-conf A config map with settings used by Encryption to communicate with the Vault server.\nPopulate client.json with your settings.\ncat \u003eclient.json \u003c\u003cEOF { \"auth_type\": \"approle\", \"auth_conf_file\": \"/etc/dea/vault/auth.json\", \"vault_addr\": \"https://\u003cvault server address\u003e:8400\", \"kv_engine_path\": \"/dea-keys\", \"tls_config\": { \"client_crt\": \"/etc/dea/vault/client.crt\", \"client_key\": \"/etc/dea/vault/client.key\", \"server_ca\": \"/etc/dea/vault/server-ca.crt\" } } EOF kubectl create configmap vault-client-conf -n \u003cdriver namespace\u003e \\ --from-file=client.json -o yaml --dry-run=client | kubectl apply -f - rm -f client.json These fields are available for use in client.json:\nclient.json field Description Required Default auth_type Authentication type used to authenticate to the Vault server. Currently, the only supported type is “approle”. Yes auth_conf_file Set to “/etc/dea/vault/auth.json” Yes auth_timeout Defines in how many seconds key requests to the Vault server fail if there is no valid authentication token. No 5 lease_duration_margin Defines how many seconds in advance the authentication token lease will be renewed. This value should accommodate network and processing delays. No 15 lease_increase Defines the number of seconds used in the authentication token renew call. This value is advisory and may be disregarded by the server. No 3600 vault_addr URL to use for REST calls to the Vault server. It must start with “https”. Yes kv_engine_path The path to which the Key/Value secret engine is mounted on the Vault server. Yes tls_config.client_crt Set to “/etc/dea/vault/client.crt” Yes tls_config.client_key Set to “/etc/dea/vault/client.key” Yes tls_config.client_ca Set to “/etc/dea/vault/server-ca.crt” Yes ","categories":"","description":"Deployment\n","excerpt":"Deployment\n","ref":"/csm-docs/v2/secure/encryption/deployment/","tags":"","title":"Deployment"},{"body":"Pre-requisites Request a License for Application Mobility Object store bucket accessible by both the source and target clusters Installation Create a namespace where Application Mobility will be installed. kubectl create ns application-mobility Edit the license Secret file (see Pre-requisites above) and set the correct namespace (ex: namespace: application-mobility) Create the Secret containing a license file kubectl apply -f license.yml Add the Dell Helm Charts repository helm repo add dell https://dell.github.io/helm-charts Either create a values.yml file or provide the --set options to the helm install to override default values from the Configuration section. Install the helm chart helm install application-mobility -n application-mobility dell/csm-application-mobility Configuration This table lists the configurable parameters of the Application Mobility Helm chart and their default values.\nParameter Description Required Default replicaCount Number of replicas for the Application Mobility controllers Yes 1 image.pullPolicy Image pull policy for the Application Mobility controller images Yes IfNotPresent controller.image Location of the Application Mobility Docker image Yes dellemc/csm-application-mobility-controller:v0.3.0 cert-manager.enabled If set to true, cert-manager will be installed during Application Mobility installation Yes false veleroNamespace If Velero is already installed, set to the namespace where Velero is installed No velero licenseName Name of the Secret that contains the License for Application Mobility Yes license objectstore.secretName If velero is already installed on the cluster, specify the name of the secret in velero namespace that has credentials to access object store No velero.enabled If set to true, Velero will be installed during Application Mobility installation Yes true velero.use-volume-snapshots If set to true, Velero will use volume snapshots Yes false velero.deployRestic If set to true, Velero will also deploy Restic Yes true velero.cleanUpCRDs If set to true, Velero CRDs will be cleaned up Yes true velero.credentials.existingSecret Optionally, specify the name of the pre-created secret in the release namespace that holds the object store credentials. Either this or secretContents should be specified No velero.credentials.name Optionally, specify the name to be used for secret that will be created to hold object store credentials. Used in conjunction with secretContents. No velero.credentials.secretContents Optionally, specify the object store access credentials to be stored in a secret with key “cloud”. Either this or existingSecret should be provided. No velero.configuration.provider Provider to use for Velero. Yes aws velero.configuration.backupStorageLocation.name Name of the backup storage location for Velero. Yes default velero.configuration.backupStorageLocation.bucket Name of the object store bucket to use for backups. Yes velero-bucket velero.configuration.backupStorageLocation.config Additional provider-specific configuration. See https://velero.io/docs/v1.9/api-types/backupstoragelocation/ for specific details. Yes velero.initContainers List of plugins used by Velero. Dell Velero plugin is required and plugins for other providers can be added. Yes velero.initContainers[0].name Name of the Dell Velero plugin. Yes dell-custom-velero-plugin velero.initContainers[0].image Location of the Dell Velero plugin image. Yes dellemc/csm-application-mobility-velero-plugin:v0.3.0 velero.initContainers[0].volumeMounts[0].mountPath Mount path of the volume mount. Yes /target velero.initContainers[0].volumeMounts[0].name Name of the volume mount. Yes plugins velero.restic.privileged If set to true, Restic Pods will be run in privileged mode. Note: Set to true when using Red Hat OpenShift No false ","categories":"","description":"Deployment\n","excerpt":"Deployment\n","ref":"/csm-docs/v3/applicationmobility/deployment/","tags":"","title":"Deployment"},{"body":"The Container Storage Modules along with the required CSI Drivers can each be deployed using CSM operator.\nCSM Operator Dell CSM Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers and CSM Modules provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. The operator can be installed using OLM (Operator Lifecycle Manager) or manually. …More on installation instructions Supported drivers: PowerScale, PowerStore, PowerFlex Supported modules: Authorization, Replication, Observability The Container Storage Modules and the required CSI Drivers can each be deployed following the links below:\nDell CSI Drivers Installation via Helm Dell CSI Helm installer installs the CSI Driver components using the provided Helm charts. …More on installation instructions Installs PowerStore PowerMax PowerScale PowerFlex Unity CSM Installation Wizard CSM Installation Wizard generates manifest files to install Dell CSI Drivers and supported modules. …More on installation instructions Generates manifest file for installation Dell CSI Drivers Installation via offline installer Both Helm and Dell CSI opetor supports offline installation of the Dell CSI Storage Providers via csi-offline-bundle.sh script by creating a usable package. …More on installation instructions Offline installation for all drivers Dell CSI Drivers Installation via operator Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using the OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually. …More on installation instructions Installs PowerStore PowerMax PowerScale PowerFlex Unity Dell Container Storage Module for Observability CSM for Observability can be deployed either via Helm or CSM for Observability Installer or CSM for Observability Offline Installer …More on installation instructions Installs Observability Module Dell Container Storage Module for Authorization CSM Authorization can be installed by using the provided Helm v3 charts on Kubernetes platforms. …More on installation instructions Installs Authorization Module Dell Container Storage Module for Resiliency CSI drivers that support Helm chart installation allow CSM for Resiliency to be optionally installed by variables in the chart. It can be updated via podmon block specified in the values.yaml …More on installation instructions Installs Resiliency Module Dell Container Storage Module for Replication Replication module can be installed by installing repctl,Container Storage Modules (CSM) for Replication Controller,CSI driver after enabling replication. …More on installation instructions Installs Replication Module Dell Container Storage Module for Application Mobility Application mobility module can be installed via helm charts. This is a tech preview release and it requires a license for installation. …More on installation instructions Installs Application Mobility Module Dell Container Storage Module for Encryption Encryption can be optionally installed via the PowerScale CSI driver Helm chart. …More on installation instructions Installs Encryption Module ","categories":"","description":"Deployment of CSM for Replication","excerpt":"Deployment of CSM for Replication","ref":"/csm-docs/v3/deployment/","tags":"","title":"Deployment"},{"body":"","categories":"","description":"Installation for Dell Container Storage Module (CSM) for Replication\n","excerpt":"Installation for Dell Container Storage Module (CSM) for Replication\n","ref":"/csm-docs/v3/replication/deployment/","tags":"","title":"Deployment"},{"body":"Encryption for Dell Container Storage Modules is enabled via the Dell CSI driver installation. The drivers can be installed either by a Helm chart or by the Dell CSI Operator. In the tech preview release, Encryption can only be enabled via Helm chart installation.\nExcept for additional Encryption related configuration outlined on this page, the rest of the deployment process is described in the correspondent CSI driver documentation.\nVault Server Hashicorp Vault must be pre-configured to support Encryption. The Vault server’s IP address and port must be accessible from the Kubernetes cluster where the CSI driver is to be deployed.\nRekey Controller The Encryption Rekey CRD Controller is an optional component that, if installed, allows encrypted volumes rekeying in a Kubernetes cluster. Please refer to Rekey Configuration for the Rekey Controller installation details.\nHelm Chart Values The drivers that support Encryption via Helm chart have an encryption block in their values.yaml file that looks like this:\nencryption: # enabled: Enable/disable volume encryption feature. enabled: false # pluginName: The name of the provisioner to use for encrypted volumes. pluginName: \"sec-isilon.dellemc.com\" # image: Encryption driver image name. image: \"dellemc/csm-encryption:v0.3.0\" # logLevel: Log level of the encryption driver. # Allowed values: \"error\", \"warning\", \"info\", \"debug\", \"trace\". logLevel: \"error\" # apiPort: TCP port number used by the REST API server. apiPort: 3838 # logLevel: Log level of the encryption driver. # Allowed values: \"error\", \"warning\", \"info\", \"debug\", \"trace\". logLevel: \"debug\" # livenessPort: HTTP liveness probe port number. # Leave empty to disable the liveness probe. # Example: 8080 livenessPort: # ocp: Enable when running on OpenShift Container Platform with CoreOS worker nodes. ocp: false # ocpCoreID: User ID and group ID of user core on CoreOS worker nodes. # Ignored when ocp is set to false. ocpCoreID: \"1000:1000\" # extraArgs: Extra command line parameters to pass to the encryption driver. # Allowed values: # --sharedStorage - may be required by some applications to work properly. # When set, performance is reduced and hard links cannot be created. # See the gocryptfs documentation for more details. extraArgs: [] Parameter Description Required Default enabled Enable/disable volume encryption feature. No false pluginName The name of the provisioner to use for encrypted volumes. No “sec-isilon.dellemc.com” image Encryption driver image name. No “dellemc/csm-encryption:v0.3.0” logLevel Log level of the encryption driver.Allowed values: “error”, “warning”, “info”, “debug”, “trace”. No “error” apiPort TCP Port number used by the REST API Server. No 3838 livenessPort HTTP liveness probe port number. Leave empty to disable the liveness probe. No ocp Enable when running an OCP Platform with CoreOS worker nodes. No false ocpCoreID User ID and group ID of user core on CoreOS worker nodes. Ignored when ocp is set to false. No “1000:1000” extraArgs Extra command line parameters to pass to the encryption driver.Allowed values:\"--sharedStorage\" - may be required by some applications to work properly.When set, performance is reduced and hard links cannot be created.See the gocryptfs documentation for more details. No [] Secrets and Config Maps Apart from any secrets and config maps described in the CSI driver documentation, these resources should be created for Encryption:\nSecret encryption-license Request a trial license following instructions on the License page. You will be provided with a YAML file similar to:\napiVersion: v1 data: license: k1FXzMDZodGNnK4I12Alo4UvuhLd+ithRhuLz2eoIxlcMSfW0xJYWnBiNMvTUl8VdGmR5fsvs2L6KqPfpIJk4wOzCxQ9wfDIJuYqrwV0wi2F2lzb1Hkk7O7/4r8cblPdCRJWfbg8QFc2BVtl4PZ/pFkHZoZVCbhGDD1MsbI1CiKqva9r9TBfswSFnqv7p3QXgbqQov8/q/j2+sHcvFF3j4kx+q1PzXoRNxwuTQaP4VAvipsQNAU5yV2dos2hs4Y/Ltbtreu/vrRGUaxvPbass1vUtIOJnvKkfbp53j8PFJGGISMYvYylUiD7TpoamxT/1I6mkjgRds+tEciMvutqDpmKEtdyp3vBjt4Sgd07ptvsdBJlyRAYb8ZPX9vXr4Ws kind: Secret metadata: name: edit_name namespace: edit_namespace Set name to \"encryption-license\" and namespace to your driver namespace and apply the file:\nkubectl apply -f \u003clicense yaml file name\u003e Secret vault-auth A secret with the AppRole credentials used by Encryption to authenticate to the Vault server.\nSet role_id and secret_id to the values provided by the Vault server administrator.\nIf a self-managed test Vault instance is used, generate role ID and secret ID following these steps.\ncat \u003eauth.json \u003c\u003cEOF { \"role_id\": \"\u003crole ID\u003e\", \"secret_id\": \"\u003csecret ID\u003e\" } EOF kubectl create secret generic vault-auth -n \u003cdriver namespace\u003e --from-file=auth.json -o yaml --dry-run=client | kubectl apply -f - rm -f auth.json In this release, Encryption does not pick up modifications to this secret while the CSI driver is running, unless it needs to re-login which happens at:\nCSI Driver startup an authentication error from the Vault server client token expiration In all other cases, to apply new values in the secret (e.g., to use another role), the CSI driver must be restarted.\nSecret vault-cert A secret with TLS certificates used by Encryption to communicate with the Vault server.\nFiles server-ca.crt, client.crt and client.key should be in PEM format.\nkubectl create secret generic vault-cert -n \u003cdriver namespace\u003e \\ --from-file=server-ca.crt --from-file=client.crt --from-file=client.key \\ -o yaml --dry-run=client | kubectl apply -f - In this release, Encryption does not pick up modifications to this secret while the CSI driver is running. To apply new values in the secret (e.g., to update the client certificate), the CSI driver must be restarted.\nConfigMap vault-client-conf A config map with settings used by Encryption to communicate with the Vault server.\nPopulate client.json with your settings.\ncat \u003eclient.json \u003c\u003cEOF { \"auth_type\": \"approle\", \"auth_conf_file\": \"/etc/dea/vault/auth.json\", \"vault_addr\": \"https://\u003cvault server address\u003e:8400\", \"kv_engine_path\": \"/dea-keys\", \"tls_config\": { \"client_crt\": \"/etc/dea/vault/client.crt\", \"client_key\": \"/etc/dea/vault/client.key\", \"server_ca\": \"/etc/dea/vault/server-ca.crt\" } } EOF kubectl create configmap vault-client-conf -n \u003cdriver namespace\u003e \\ --from-file=client.json -o yaml --dry-run=client | kubectl apply -f - rm -f client.json These fields are available for use in client.json:\nclient.json field Description Required Default auth_type Authentication type used to authenticate to the Vault server. Currently, the only supported type is “approle”. Yes auth_conf_file Set to “/etc/dea/vault/auth.json” Yes auth_timeout Defines in how many seconds key requests to the Vault server fail if there is no valid authentication token. No 5 lease_duration_margin Defines how many seconds in advance the authentication token lease will be renewed. This value should accommodate network and processing delays. No 15 lease_increase Defines the number of seconds used in the authentication token renew call. This value is advisory and may be disregarded by the server. No 3600 vault_addr URL to use for REST calls to the Vault server. It must start with “https”. Yes kv_engine_path The path to which the Key/Value secret engine is mounted on the Vault server. Yes tls_config.client_crt Set to “/etc/dea/vault/client.crt” Yes tls_config.client_key Set to “/etc/dea/vault/client.key” Yes tls_config.client_ca Set to “/etc/dea/vault/server-ca.crt” Yes ","categories":"","description":"Deployment\n","excerpt":"Deployment\n","ref":"/csm-docs/v3/secure/encryption/deployment/","tags":"","title":"Deployment"},{"body":"The Deprecation policy for Dell Container Storage Modules (CSM) is in place to help users prevent any disruptive incidents from occurring. We aim to provide appropriate notice when CLI elements, APIs, features, or behaviors are slated to be removed.\nDeprecating a CLI Element This captures situations when a flag or command is removed from a CLI.\nCLI elements must function after their announced deprecation for no less than two releases. This includes when the releases become Generally Available (GA), including both major or minor release versions.\nWhen deprecating a CLI command, a warning message must be displayed each time the command is used. This warning message should capture the deprecation details along with the release in which the command that is being deprecated will be removed.\nDeprecating an API, Feature, or Behavior CSM features must function after their announced deprecation for no less than two releases. This includes when the releases become Generally Available (GA), including both major or minor release versions.\nTech Previews Features released as tech preview are not supported and therefore are not intended for production. No deprecation notice will be required before removing any features/behaviors that are released as tech previews.\nRequired Deprecation Notice CSM documentation for the release in which the deprecation is being announced must include deprecation details along with the release in which the item(s) being deprecated will be removed.\nIn addition, the changelog and release notes for the release in which the deprecation is being announced must contain a section titled “Important Deprecation Information”. In this section, the deprecation details must be provided along with the release in which the item(s) being deprecated will be removed.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) Deprecation Policy\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) Deprecation …","ref":"/csm-docs/docs/references/policies/deprecationpolicy/","tags":"","title":"Deprecation Policy"},{"body":"The Deprecation policy for Dell Container Storage Modules (CSM) is in place to help users prevent any disruptive incidents from occurring. We aim to provide appropriate notice when CLI elements, APIs, features, or behaviors are slated to be removed.\nDeprecating a CLI Element This captures situations when a flag or command is removed from a CLI.\nCLI elements must function after their announced deprecation for no less than two releases. This includes when the releases become Generally Available (GA), including both major or minor release versions.\nWhen deprecating a CLI command, a warning message must be displayed each time the command is used. This warning message should capture the deprecation details along with the release in which the command that is being deprecated will be removed.\nDeprecating an API, Feature, or Behavior CSM features must function after their announced deprecation for no less than two releases. This includes when the releases become Generally Available (GA), including both major or minor release versions.\nTech Previews Features released as tech preview are not supported and therefore are not intended for production. No deprecation notice will be required before removing any features/behaviors that are released as tech previews.\nRequired Deprecation Notice CSM documentation for the release in which the deprecation is being announced must include deprecation details along with the release in which the item(s) being deprecated will be removed.\nIn addition, the changelog and release notes for the release in which the deprecation is being announced must contain a section titled “Important Deprecation Information”. In this section, the deprecation details must be provided along with the release in which the item(s) being deprecated will be removed.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) Deprecation Policy\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) Deprecation …","ref":"/csm-docs/v1/references/policies/deprecationpolicy/","tags":"","title":"Deprecation Policy"},{"body":"The Deprecation policy for Dell Container Storage Modules (CSM) is in place to help users prevent any disruptive incidents from occurring. We aim to provide appropriate notice when CLI elements, APIs, features, or behaviors are slated to be removed.\nDeprecating a CLI Element This captures situations when a flag or command is removed from a CLI.\nCLI elements must function after their announced deprecation for no less than two releases. This includes when the releases become Generally Available (GA), including both major or minor release versions.\nWhen deprecating a CLI command, a warning message must be displayed each time the command is used. This warning message should capture the deprecation details along with the release in which the command that is being deprecated will be removed.\nDeprecating an API, Feature, or Behavior CSM features must function after their announced deprecation for no less than two releases. This includes when the releases become Generally Available (GA), including both major or minor release versions.\nTech Previews Features released as tech preview are not supported and therefore are not intended for production. No deprecation notice will be required before removing any features/behaviors that are released as tech previews.\nRequired Deprecation Notice CSM documentation for the release in which the deprecation is being announced must include deprecation details along with the release in which the item(s) being deprecated will be removed.\nIn addition, the changelog and release notes for the release in which the deprecation is being announced must contain a section titled “Important Deprecation Information”. In this section, the deprecation details must be provided along with the release in which the item(s) being deprecated will be removed.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) Deprecation Policy\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) Deprecation …","ref":"/csm-docs/v2/references/policies/deprecationpolicy/","tags":"","title":"Deprecation Policy"},{"body":"The Deprecation policy for Dell Container Storage Modules (CSM) is in place to help users prevent any disruptive incidents from occurring. We aim to provide appropriate notice when CLI elements, APIs, features, or behaviors are slated to be removed.\nDeprecating a CLI Element This captures situations when a flag or command is removed from a CLI.\nCLI elements must function after their announced deprecation for no less than two releases. This includes when the releases become Generally Available (GA), including both major or minor release versions.\nWhen deprecating a CLI command, a warning message must be displayed each time the command is used. This warning message should capture the deprecation details along with the release in which the command that is being deprecated will be removed.\nDeprecating an API, Feature, or Behavior CSM features must function after their announced deprecation for no less than two releases. This includes when the releases become Generally Available (GA), including both major or minor release versions.\nTech Previews Features released as tech preview are not supported and therefore are not intended for production. No deprecation notice will be required before removing any features/behaviors that are released as tech previews.\nRequired Deprecation Notice CSM documentation for the release in which the deprecation is being announced must include deprecation details along with the release in which the item(s) being deprecated will be removed.\nIn addition, the changelog and release notes for the release in which the deprecation is being announced must contain a section titled “Important Deprecation Information”. In this section, the deprecation details must be provided along with the release in which the item(s) being deprecated will be removed.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) Deprecation Policy\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) Deprecation …","ref":"/csm-docs/v3/references/policies/deprecationpolicy/","tags":"","title":"Deprecation Policy"},{"body":"Container Storage Modules (CSM) for Authorization is designed as a service mesh solution and consists of many internal components that work together in concert to achieve its overall functionality.\nThis document provides an overview of the major components, including how they fit together and pointers to implementation details.\nIf you are a developer who is new to CSM for Authorization and want to build a mental map of how it works, you’re in the right place.\nTerminology Service Mesh - An infrastructure layer consisting of proxies that intercept and route requests between existing services. CSI - Acronym for the Container Storage Interface. Proxy (L7) - A gateway between networked services that inspects request traffic. Sidecar Proxy - A service mesh proxy that runs alongside existing services, rather than within them. Pod - A Kubernetes abstraction for a set of related containers that are to be considered as one unit. Tenant - A named persona who owns a Kubernetes cluster and is considered the “client-side” user. Storage Administrator - A named persona who owns a storage array and is considered the admin user. Bird’s Eye View +-----------------------------------+ | Kubernetes | | | | +---------+ +---------+ | +---------------+ | | CSI | | Sidecar | | | CSM | +---------+ | | Driver |---------\u003e Proxy |---------------\u003e Authorization |--------------\u003e Storage | | +---------+ +---------+ | | Server | | Array | | | +---------------+ +---------+ +-----------------------------------+ ^ | | | +------------+ | karavictl | | CLI | +------------+ NOTE: Arrows indicate request or connection initiation, not necessarily data flow direction.\nThe sections below explain each component in the diagram.\nKubernetes The architecture assumes a Kubernetes cluster that intends to offer external storage to applications hosted therein. The mechanism for managing this storage would utilize a CSI Driver.\nArchitecture Invariant: We assume there may be many Kubernetes clusters, potentially containing multiple CSI Drivers each with their own Sidecar Proxy.\nCSI Driver A CSI Driver supports the Container Service Interface (CSI) specification. Dell provides customers with CSI Drivers for its various storage arrays. CSM for Authorization intends to support a majority, if not all, of these drivers.\nA CSI Driver will typically be configured to communicate directly to its intended storage array and as such will be limited in using only the authentication methods supported by the Storage Array itself, e.g. Basic authentication over TLS.\nArchitecture Invariant: We try to avoid having to make any code changes to the CSI Driver when adding support for it. Any CSI Driver should ideally not be aware that it is communicating to the Sidecar Proxy.\nSidecar Proxy The CSM for Authorization Sidecar Proxy is deployed as a sidecar in the CSI Driver’s Pod. It acts as a proxy and forwards all requests to a CSM Authorization Server.\nThe CSI Driver section noted the limitation of a CSI Driver using Storage Array supported authentication methods only. By nature of being a proxy, the CSM for Authorization Sidecar Proxy is able to override the Authorization HTTP header for outbound requests to use Bearer tokens. Such tokens are managed by CSM for Authorization as will be described later in this document.\nCSM for Authorization Server The CSM for Authorization Server is, at its core, a Layer 7 proxy for intercepting traffic between a CSI Driver and a Storage Array.\nInbound requests are expected to originate from the CSM for Authorization Sidecar Proxy, for the following reasons:\nProcessing a set of agreed upon HTTP headers (added by the CSM for Authorization Sidecar Proxy) to assist in routing traffic to the intended Storage Array. Inspection of CSM-specific Authorization Bearer tokens. CSM for Authorization CLI The karavictl CLI (Command Line Interface) application allows Storage Admins to manage and interact with a running CSM for Authorization Server.\nStorage Array A Storage Array is typically considered to be one of the various Dell storage offerings, e.g. Dell PowerFlex which is supported by CSM for Authorization today. Support for more Storage Arrays will come in the future.\nHow it Works CSM for Authorization intends to override the existing authorization methods between a CSI Driver and its Storage Array. This may be desirable for several reasons, if:\nThe CSI Driver requires privileged login credentials (e.g. “root”) in order to function. The Storage Array does not natively support the concept of RBAC and/or multi-tenancy. This section of of the document describes how CSM for Authorization provides a solution to these problems.\nBearer Tokens CSM for Authorization overrides any existing authorization mechanism between a CSI Driver and its corresponding Storage Array with the use of JSON Web Tokens (JWTs). The CSI Driver and Storage Array will not be aware of this taking place.\nIn the context of RFC-6749 there are two such JWTs that are used:\nAccess token: a single token valid for a short period of time. Refresh token: a single token used to obtain access tokens. Typically valid for a longer period of time. Both tokens are opaque to the client, yet provide meaningful information to the server, specifically:\nThe Tenant for whom the token is associated with. The Roles that are bound to the Tenant. Tokens encode the following set of claims:\n{ \"aud\": \"karavi\", \"exp\": 1915585883, \"iss\": \"com.dell.karavi\", \"sub\": \"karavi-tenant\", \"roles\": \"role-a,role-b,role-c\", \"group\": \"Tenant-1\" } Both tokens are signed using a server-side secret preventing the risk of tampering by any client. For example, a bad-actor is unable to modify a token to give themselves a role that they should not have, at least without knowing the server-side secret.\nThe refresh approach is beneficial for the following reasons:\nAccidental exposure of an access token poses a lesser security concern, given the set expiration time is short (e.g. 30 seconds). The CSM for Authorization Server can fully trust the access token without having to perform a database check on each request (doing so would nullify the benefits of using tokens in the first place). The CSM for Authorization Server can defer Tenant checks at refresh time only, e.g. do not allow refresh if the Tenant’s access has been revoked by a Storage Admin. There may be a short time window in between revocation and enforcement, depending on the access token’s expiration time. The following diagram shows the access and refresh tokens in play and how a valid access token is required for a request to be proxied to the intended Storage Array.\n+---------+ +---------------+ | | | | | | | | +----------+ | |--(A)------------ Access Token -----------\u003e| |------\u003e| | | | | CSM | | | | |\u003c-(B)---------- Protected Resource --------| Authorization |\u003c------| Storage | | Sidecar | | Server | | Array | | Proxy |--(C)------------ Access Token -----------\u003e| | | | | | | | | | | |\u003c-(D)------ Invalid Token Error -----------| | | | | | | | +----------+ | | | | | |--(E)----------- Refresh Token -----------\u003e| | | | \u0026 Expired Access Token | | | |\u003c-(F)----------- Access Token -------------| | +---------+ +---------------+ A) CSI Driver makes a request to the Storage Array: request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token valid. The CSM for Authorization Server permits the request to be proxied to the intended Storage Array. B) Storage Array response is sent back as expected. C) CSI Driver makes a request to the Storage Array: request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token is invalid; it has since expired. D) The CSM for Authorization Server responds with HTTP 401 Unauthorized. E) Sidecar Proxy requests a new access token by passing both refresh token and expired token. F) The CSM for Authorization Server processes the request: is the refresh token valid? is the access token expired? has the Tenant had access revoked? a new access token is sent in response if the checks pass. Roles So we know a token encodes both the identification of a Tenant and their Roles, but what’s in a Role?\nA role can be defined as follows:\nIt has a name, e.g. “role-a”. It can be bound to a Tenant It can be unbound from a Tenant. It determines access to zero or more storage pools and assigns a storage quota for each. Quota represents the upper-limit of the total aggregation of used storage capacity for a Tenant’s resources in a storage pool. It prevents ambiguity by identifying each storage pool in the form of system-type:system-id:pool-name. Below is an example of how roles are represented internally in JSON:\n{ \"Developer\": { \"system_types\": { \"powerflex\": { \"system_ids\": { \"542a2d5f5122210f\": { \"pool_quotas\": { \"bronze\": 99000000 } } } } } } } This role says Allow Tenants with the Developer role access to the bronze pool on PowerFlex system 542a2d5f5122210f, and cap their total capacity usage at 99000000Kb (99Gb).\nPolicy CSM for Authorization leverages the Open Policy Agent to use a policy-as-code approach to policy management. It stores a collection of policy files written in Rego language. Each policy file defines a set of policy rules that form the basis of a policy decision. A policy decision is made by processing the inputs provided. For CSM for Authorization, the inputs are:\nThe set of roles defined by the Storage Admin. The claims section of a validated JWT. The JSON payload of the storage request. Given these inputs, many decisions can be made to answer questions like “Can Tenant X, with these roles provision this volume of size Y?”. The result of the policy decision will determine whether or not the request is proxied.\n+----------------+ | Open Policy | | Agent | | | JWT | +--------+ | Claims ------\\ | | Policy | ----------\u003e Allow/Deny -----\u003e | (Rego) | | Storage -----\u003e +--------+ | Request -----/ +-------^--------+ | | | Role Data Quota \u0026 Volume Ownership Policy decisions based on the current request and set of roles alone are not enough. CSM for Authorization must maintain a cache of volumes approved for creation and deletion in order to know if a Tenant has already consumed their quota on a given storage pool.\nA Redis database is used to store this volume data and their relationship with a Tenant, Storage Array and Pool. The use of composite keys provide fast, constant time look up of volumes, e.g. quota:powerflex:542a2d5f5122210f:bronze:Tenant-1:data is a Redis hash with volume data as its values.\nCross-Cutting Concerns This section documents the pieces of code that are general in nature and shared across multiple packages.\nLogging CSM for Authorization uses the Logrus package when logging messages.\nObservability Both the CSM for Authorization Server and Sidecar Proxy are long-running processes, so it’s important to understand what’s going on inside. We use OpenTelemetry (otel) to help with that.\nThe following otel exporters are used:\ngo.opentelemetry.io/otel/exporters/metric/prometheus go.opentelemetry.io/otel/exporters/trace/zipkin go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization design\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/authorization/design/","tags":"","title":"Design"},{"body":"The solution takes the approach that each storage system that Container Storage Modules (CSM) for Observability supports will have their own metrics deployments in the Kubernetes cluster.\nMetrics Deployment: Queries the Kubernetes API to gather information about storage resources and then queries the storage system’s REST API to gather specific metrics. These metrics are then exported to the OTEL collector. Each supported storage system will have their own Deployment for metrics. They will each follow a similar pattern of querying the Kubernetes and StorageSystem APIs to gather information about storage resources (ex: volumes, storage pools, etc) and their metrics. Metrics will be exported directly to the OTEL collector. A single topology deployment will query the Kubernetes API to gather mapping information between Persistent Volumes and storage resources located on multiple storage systems. This information is queried directly from Grafana and displayed in a custom dashboard.\nRequired Components The following prerequisites must be deployed into the namespace where CSM for Observability is located to support the storage system metrics and topology deployments:\nPrometheus for scraping the metrics from the OTEL collector. Grafana for visualizing the metrics from Prometheus and Topology services using custom dashboards. CSM for Observability will use secrets to get details about the storage systems used by the CSI drivers. These secrets should be copied from the namespaces where the drivers are deployed. CSI PowerFlex driver uses the ‘vxflexos-config’ secret. CSI PowerStore driver uses the ‘powerstore-config’ secret. CSI PowerScale driver uses the ‘isilon-creds’ secret. CSI PowerMax driver uses the secrets in configmap ‘powermax-reverseproxy-config’. Deployment Architectures CSM for Observability can be deployed to either direct storage system requests directly to the storage system or through the CSM for Authorization proxy. The CSI driver must be configured to route storage system requests through the CSM for Authorization proxy in order for CSM for Observability to do the same.\nDefault Deployment of CSM for Observability Deployment of CSM for Observability with CSM for Authorization ","categories":"","description":"CSM for Observability Design\n","excerpt":"CSM for Observability Design\n","ref":"/csm-docs/docs/observability/design/","tags":"","title":"Design"},{"body":"This section covers CSM for Resiliency’s design. The detail is sufficient that you should be able to understand what CSM for Resiliency is designed to do in various situations and how it works. CSM for Resiliency is deployed as a sidecar named podmon with a CSI driver in both the controller pods and node pods. These are referred to as controller-podmon and node-podmon respectively.\nGenerally controller-podmon and the driver controller pods are deployed using a Deployment. The Deployments support one or multiple replicas for High Availability and use a standard K8S leader election protocol so that only one controller is active at a time (as does the driver and all the controller sidecars.) The controller deployment also supports a Node Selector that allows the controllers to be placed on K8S Manager (non Worker) nodes.\nNode-podmon and the driver node pods are deployed in a DaemonSet, with a Pod deployed on every K8S Worker Node.\nController-Podmon Controller-podmon is responsible for:\nSetting up a Watch for CSM for Resiliency labeled pods, and if a Pod is Initialized but Not Ready and resident on a Node with a NoSchedule or NoExecute taint, calling controllerCleanupPod to cleanup the pod so that a replacement pod can be scheduled.\nPeriodically polling the arrays to see if it has connectivity to the nodes that are hosting CSM for Resiliency labeled pods (if enabled.) If an array has lost connectivity to a node hosting CSM for Resiliency labeled pods using that array, controllerCleanupPod is invoked to cleanup the pods that have lost I/O connectivity.\nTainting nodes that have failed so that a) no further pods will get scheduled to them until they are returned to service, and b) podmon-node upon seeing the taint will invoke the cleanup operations to make sure any zombie pods (pods that have been replaced) cannot write to the volumes they were using.\nIf a CSM for Resiliency labeled pod enters a CrashLoopBackOff state, deleting that pod so it can be replaced.\nControllerCleanupPod cleans up the pod by taking the following actions:\nThe VolumeAttachments (VAs) are loaded, and all VAs belonging to the pod being cleaned up are identified. The PVs for each VolumeAttachment are identified and used to get the Volume Handle (array identifier for the volume.) If enabled, the array is queried if any of the volumes to the pod are still doing I/O. If so, cleanup is aborted. The pod’s volumes are “fenced” from the node the pod resides on to prevent any potential I/O from a zombie pod. This is done by calling the CSI ControllerUnpublishVolume call for each of the volumes. A taint is applied to the node to keep any new pods from being scheduled to the node. If the replacement pod were to get scheduled to the same node as a zombie pod, they might both gain access to the volume concurrently causing corruption. The VolumeAttachments for the pod is deleted. This is necessary so the replacement pod to be created can attach the volumes. The pod is forcibly deleted so that a StatefulSet controller which created the pod is free to create a replacement pod. Node-Podmon Node-podmon has the following responsibilities:\nEstablishing a pod watch which is used to maintain a list of pods executing on this node that may need to be cleaned up. The list includes information about each Mount volume or Block volume used by the pod including the volume handle, volume name, private mount path, and mount path in the pod. Periodically (every 30 seconds) polling to see if controller-podmon has applied a taint to the node. If so, node-podmon calls nodeModeCleanupPod for each pod to clean up any remnants of the pod (which is potentially a zombie pod.) If all pods have been successfully cleaned up, and there are no labeled pods on this node still existing, only then will node-podmon remove the taint placed on the node by controller-podmon. NodeModeCleanupPod cleans up the pod remnants by taking the following actions for each volume used by the pod:\nCalling NodeUnpublishVolume to unpublish the volume from the pod. Unmounting and deleting the target path for the volume. Calling NodeUnstageVolume to unpublish the volume from the node. Unmounting and deleting the staging path for the volume. Design Limitations There are some limitations with the current design. Some might be able to be addressed in the future- others are inherent in the approach.\nThe design relies on the array’s ability to revoke access to a volume for a particular node for the fencing operation. The granularity of access control for a volume is per node. Consequently, it isn’t possible to revoke access from one pod on a node while retaining access to another pod on the same node if we cannot communicate with the node. The implications of this are that if more than one pod on a node is sharing the same volume(s), they all must be protected by CSM for Resiliency, and they all must be cleaned up by controller-podmon if the node fails. If only some of the pods are cleaned up, the other pods will lose access to the volumes shared with pods that have been cleaned, so those pods should also fail. The node-podmon cleanup algorithm purposefully will not remove the node taint until all the protected volumes have been cleaned up from the node. This works well if the node fault lasts long enough that controller-podmon can evacuate all the protected pods from the node. However, if the failure is short-lived, and controller-podmon does not clean up all the protected pods on the node, or if for some reason node-podmon cannot clean a pod completely, the taint is left on the node, and manual intervention is required. The required intervention is for the operator to reboot the node, which will ensure that no zombie pods survive. Upon seeing the reboot, node-podmon will then remove the taint. If the node failure is short-lived and controller-podmon has not evacuated some of the protected pods on the node, they may try and restart on the same pod. This has been observed to cause such pods to go into CrashLoopBackoff. We are currently considering solutions to this problem. ","categories":"","description":"CSM for Resiliency Design\n","excerpt":"CSM for Resiliency Design\n","ref":"/csm-docs/docs/resiliency/design/","tags":"","title":"Design"},{"body":"Container Storage Modules (CSM) for Authorization is designed as a service mesh solution and consists of many internal components that work together in concert to achieve its overall functionality.\nThis document provides an overview of the major components, including how they fit together and pointers to implementation details.\nIf you are a developer who is new to CSM for Authorization and want to build a mental map of how it works, you’re in the right place.\nTerminology Service Mesh - An infrastructure layer consisting of proxies that intercept and route requests between existing services. CSI - Acronym for the Container Storage Interface. Proxy (L7) - A gateway between networked services that inspects request traffic. Sidecar Proxy - A service mesh proxy that runs alongside existing services, rather than within them. Pod - A Kubernetes abstraction for a set of related containers that are to be considered as one unit. Tenant - A named persona who owns a Kubernetes cluster and is considered the “client-side” user. Storage Administrator - A named persona who owns a storage array and is considered the admin user. Bird’s Eye View +-----------------------------------+ | Kubernetes | | | | +---------+ +---------+ | +---------------+ | | CSI | | Sidecar | | | CSM | +---------+ | | Driver |---------\u003e Proxy |---------------\u003e Authorization |--------------\u003e Storage | | +---------+ +---------+ | | Server | | Array | | | +---------------+ +---------+ +-----------------------------------+ ^ | | | +------------+ | karavictl | | CLI | +------------+ NOTE: Arrows indicate request or connection initiation, not necessarily data flow direction.\nThe sections below explain each component in the diagram.\nKubernetes The architecture assumes a Kubernetes cluster that intends to offer external storage to applications hosted therein. The mechanism for managing this storage would utilize a CSI Driver.\nArchitecture Invariant: We assume there may be many Kubernetes clusters, potentially containing multiple CSI Drivers each with their own Sidecar Proxy.\nCSI Driver A CSI Driver supports the Container Service Interface (CSI) specification. Dell provides customers with CSI Drivers for its various storage arrays. CSM for Authorization intends to support a majority, if not all, of these drivers.\nA CSI Driver will typically be configured to communicate directly to its intended storage array and as such will be limited in using only the authentication methods supported by the Storage Array itself, e.g. Basic authentication over TLS.\nArchitecture Invariant: We try to avoid having to make any code changes to the CSI Driver when adding support for it. Any CSI Driver should ideally not be aware that it is communicating to the Sidecar Proxy.\nSidecar Proxy The CSM for Authorization Sidecar Proxy is deployed as a sidecar in the CSI Driver’s Pod. It acts as a proxy and forwards all requests to a CSM Authorization Server.\nThe CSI Driver section noted the limitation of a CSI Driver using Storage Array supported authentication methods only. By nature of being a proxy, the CSM for Authorization Sidecar Proxy is able to override the Authorization HTTP header for outbound requests to use Bearer tokens. Such tokens are managed by CSM for Authorization as will be described later in this document.\nCSM for Authorization Server The CSM for Authorization Server is, at its core, a Layer 7 proxy for intercepting traffic between a CSI Driver and a Storage Array.\nInbound requests are expected to originate from the CSM for Authorization Sidecar Proxy, for the following reasons:\nProcessing a set of agreed upon HTTP headers (added by the CSM for Authorization Sidecar Proxy) to assist in routing traffic to the intended Storage Array. Inspection of CSM-specific Authorization Bearer tokens. CSM for Authorization CLI The karavictl CLI (Command Line Interface) application allows Storage Admins to manage and interact with a running CSM for Authorization Server.\nStorage Array A Storage Array is typically considered to be one of the various Dell storage offerings, e.g. Dell PowerFlex which is supported by CSM for Authorization today. Support for more Storage Arrays will come in the future.\nHow it Works CSM for Authorization intends to override the existing authorization methods between a CSI Driver and its Storage Array. This may be desirable for several reasons, if:\nThe CSI Driver requires privileged login credentials (e.g. “root”) in order to function. The Storage Array does not natively support the concept of RBAC and/or multi-tenancy. This section of of the document describes how CSM for Authorization provides a solution to these problems.\nBearer Tokens CSM for Authorization overrides any existing authorization mechanism between a CSI Driver and its corresponding Storage Array with the use of JSON Web Tokens (JWTs). The CSI Driver and Storage Array will not be aware of this taking place.\nIn the context of RFC-6749 there are two such JWTs that are used:\nAccess token: a single token valid for a short period of time. Refresh token: a single token used to obtain access tokens. Typically valid for a longer period of time. Both tokens are opaque to the client, yet provide meaningful information to the server, specifically:\nThe Tenant for whom the token is associated with. The Roles that are bound to the Tenant. Tokens encode the following set of claims:\n{ \"aud\": \"karavi\", \"exp\": 1915585883, \"iss\": \"com.dell.karavi\", \"sub\": \"karavi-tenant\", \"roles\": \"role-a,role-b,role-c\", \"group\": \"Tenant-1\" } Both tokens are signed using a server-side secret preventing the risk of tampering by any client. For example, a bad-actor is unable to modify a token to give themselves a role that they should not have, at least without knowing the server-side secret.\nThe refresh approach is beneficial for the following reasons:\nAccidental exposure of an access token poses a lesser security concern, given the set expiration time is short (e.g. 30 seconds). The CSM for Authorization Server can fully trust the access token without having to perform a database check on each request (doing so would nullify the benefits of using tokens in the first place). The CSM for Authorization Server can defer Tenant checks at refresh time only, e.g. do not allow refresh if the Tenant’s access has been revoked by a Storage Admin. There may be a short time window in between revocation and enforcement, depending on the access token’s expiration time. The following diagram shows the access and refresh tokens in play and how a valid access token is required for a request to be proxied to the intended Storage Array.\n+---------+ +---------------+ | | | | | | | | +----------+ | |--(A)------------ Access Token -----------\u003e| |------\u003e| | | | | CSM | | | | |\u003c-(B)---------- Protected Resource --------| Authorization |\u003c------| Storage | | Sidecar | | Server | | Array | | Proxy |--(C)------------ Access Token -----------\u003e| | | | | | | | | | | |\u003c-(D)------ Invalid Token Error -----------| | | | | | | | +----------+ | | | | | |--(E)----------- Refresh Token -----------\u003e| | | | \u0026 Expired Access Token | | | |\u003c-(F)----------- Access Token -------------| | +---------+ +---------------+ A) CSI Driver makes a request to the Storage Array: request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token valid. The CSM for Authorization Server permits the request to be proxied to the intended Storage Array. B) Storage Array response is sent back as expected. C) CSI Driver makes a request to the Storage Array: request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token is invalid; it has since expired. D) The CSM for Authorization Server responds with HTTP 401 Unauthorized. E) Sidecar Proxy requests a new access token by passing both refresh token and expired token. F) The CSM for Authorization Server processes the request: is the refresh token valid? is the access token expired? has the Tenant had access revoked? a new access token is sent in response if the checks pass. Roles So we know a token encodes both the identification of a Tenant and their Roles, but what’s in a Role?\nA role can be defined as follows:\nIt has a name, e.g. “role-a”. It can be bound to a Tenant It can be unbound from a Tenant. It determines access to zero or more storage pools and assigns a storage quota for each. Quota represents the upper-limit of the total aggregation of used storage capacity for a Tenant’s resources in a storage pool. It prevents ambiguity by identifying each storage pool in the form of system-type:system-id:pool-name. Below is an example of how roles are represented internally in JSON:\n{ \"Developer\": { \"system_types\": { \"powerflex\": { \"system_ids\": { \"542a2d5f5122210f\": { \"pool_quotas\": { \"bronze\": 99000000 } } } } } } } This role says Allow Tenants with the Developer role access to the bronze pool on PowerFlex system 542a2d5f5122210f, and cap their total capacity usage at 99000000Kb (99Gb).\nPolicy CSM for Authorization leverages the Open Policy Agent to use a policy-as-code approach to policy management. It stores a collection of policy files written in Rego language. Each policy file defines a set of policy rules that form the basis of a policy decision. A policy decision is made by processing the inputs provided. For CSM for Authorization, the inputs are:\nThe set of roles defined by the Storage Admin. The claims section of a validated JWT. The JSON payload of the storage request. Given these inputs, many decisions can be made to answer questions like “Can Tenant X, with these roles provision this volume of size Y?”. The result of the policy decision will determine whether or not the request is proxied.\n+----------------+ | Open Policy | | Agent | | | JWT | +--------+ | Claims ------\\ | | Policy | ----------\u003e Allow/Deny -----\u003e | (Rego) | | Storage -----\u003e +--------+ | Request -----/ +-------^--------+ | | | Role Data Quota \u0026 Volume Ownership Policy decisions based on the current request and set of roles alone are not enough. CSM for Authorization must maintain a cache of volumes approved for creation and deletion in order to know if a Tenant has already consumed their quota on a given storage pool.\nA Redis database is used to store this volume data and their relationship with a Tenant, Storage Array and Pool. The use of composite keys provide fast, constant time look up of volumes, e.g. quota:powerflex:542a2d5f5122210f:bronze:Tenant-1:data is a Redis hash with volume data as its values.\nCross-Cutting Concerns This section documents the pieces of code that are general in nature and shared across multiple packages.\nLogging CSM for Authorization uses the Logrus package when logging messages.\nObservability Both the CSM for Authorization Server and Sidecar Proxy are long-running processes, so it’s important to understand what’s going on inside. We use OpenTelemetry (otel) to help with that.\nThe following otel exporters are used:\ngo.opentelemetry.io/otel/exporters/metric/prometheus go.opentelemetry.io/otel/exporters/trace/zipkin go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization design\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v1/authorization/design/","tags":"","title":"Design"},{"body":"The solution takes the approach that each storage system that Container Storage Modules (CSM) for Observability supports will have their own metrics deployments in the Kubernetes cluster.\nMetrics Deployment: Queries the Kubernetes API to gather information about storage resources and then queries the storage system’s REST API to gather specific metrics. These metrics are then exported to the OTEL collector. Each supported storage system will have their own Deployment for metrics. They will each follow a similar pattern of querying the Kubernetes and StorageSystem APIs to gather information about storage resources (ex: volumes, storage pools, etc) and their metrics. Metrics will be exported directly to the OTEL collector. A single topology deployment will query the Kubernetes API to gather mapping information between Persistent Volumes and storage resources located on multiple storage systems. This information is queried directly from Grafana and displayed in a custom dashboard.\nRequired Components The following prerequisites must be deployed into the namespace where CSM for Observability is located to support the storage system metrics and topology deployments:\nPrometheus for scraping the metrics from the OTEL collector. Grafana for visualizing the metrics from Prometheus and Topology services using custom dashboards. CSM for Observability will use secrets to get details about the storage systems used by the CSI drivers. These secrets should be copied from the namespaces where the drivers are deployed. CSI PowerFlex driver uses the ‘vxflexos-config’ secret. CSI PowerStore driver uses the ‘powerstore-config’ secret. CSI PowerScale driver uses the ‘isilon-creds’ secret. CSI PowerMax driver uses the secrets in configmap ‘powermax-reverseproxy-config’. Deployment Architectures CSM for Observability can be deployed to either direct storage system requests directly to the storage system or through the CSM for Authorization proxy. The CSI driver must be configured to route storage system requests through the CSM for Authorization proxy in order for CSM for Observability to do the same.\nDefault Deployment of CSM for Observability Deployment of CSM for Observability with CSM for Authorization ","categories":"","description":"CSM for Observability Design\n","excerpt":"CSM for Observability Design\n","ref":"/csm-docs/v1/observability/design/","tags":"","title":"Design"},{"body":"This section covers CSM for Resiliency’s design. The detail is sufficient that you should be able to understand what CSM for Resiliency is designed to do in various situations and how it works. CSM for Resiliency is deployed as a sidecar named podmon with a CSI driver in both the controller pods and node pods. These are referred to as controller-podmon and node-podmon respectively.\nGenerally controller-podmon and the driver controller pods are deployed using a Deployment. The Deployments support one or multiple replicas for High Availability and use a standard K8S leader election protocol so that only one controller is active at a time (as does the driver and all the controller sidecars.) The controller deployment also supports a Node Selector that allows the controllers to be placed on K8S Manager (non Worker) nodes.\nNode-podmon and the driver node pods are deployed in a DaemonSet, with a Pod deployed on every K8S Worker Node.\nController-Podmon Controller-podmon is responsible for:\nSetting up a Watch for CSM for Resiliency labeled pods, and if a Pod is Initialized but Not Ready and resident on a Node with a NoSchedule or NoExecute taint, calling controllerCleanupPod to cleanup the pod so that a replacement pod can be scheduled.\nPeriodically polling the arrays to see if it has connectivity to the nodes that are hosting CSM for Resiliency labeled pods (if enabled.) If an array has lost connectivity to a node hosting CSM for Resiliency labeled pods using that array, controllerCleanupPod is invoked to cleanup the pods that have lost I/O connectivity.\nTainting nodes that have failed so that a) no further pods will get scheduled to them until they are returned to service, and b) podmon-node upon seeing the taint will invoke the cleanup operations to make sure any zombie pods (pods that have been replaced) cannot write to the volumes they were using.\nIf a CSM for Resiliency labeled pod enters a CrashLoopBackOff state, deleting that pod so it can be replaced.\nControllerCleanupPod cleans up the pod by taking the following actions:\nThe VolumeAttachments (VAs) are loaded, and all VAs belonging to the pod being cleaned up are identified. The PVs for each VolumeAttachment are identified and used to get the Volume Handle (array identifier for the volume.) If enabled, the array is queried if any of the volumes to the pod are still doing I/O. If so, cleanup is aborted. The pod’s volumes are “fenced” from the node the pod resides on to prevent any potential I/O from a zombie pod. This is done by calling the CSI ControllerUnpublishVolume call for each of the volumes. A taint is applied to the node to keep any new pods from being scheduled to the node. If the replacement pod were to get scheduled to the same node as a zombie pod, they might both gain access to the volume concurrently causing corruption. The VolumeAttachments for the pod is deleted. This is necessary so the replacement pod to be created can attach the volumes. The pod is forcibly deleted so that a StatefulSet controller which created the pod is free to create a replacement pod. Node-Podmon Node-podmon has the following responsibilities:\nEstablishing a pod watch which is used to maintain a list of pods executing on this node that may need to be cleaned up. The list includes information about each Mount volume or Block volume used by the pod including the volume handle, volume name, private mount path, and mount path in the pod. Periodically (every 30 seconds) polling to see if controller-podmon has applied a taint to the node. If so, node-podmon calls nodeModeCleanupPod for each pod to clean up any remnants of the pod (which is potentially a zombie pod.) If all pods have been successfully cleaned up, and there are no labeled pods on this node still existing, only then will node-podmon remove the taint placed on the node by controller-podmon. NodeModeCleanupPod cleans up the pod remnants by taking the following actions for each volume used by the pod:\nCalling NodeUnpublishVolume to unpublish the volume from the pod. Unmounting and deleting the target path for the volume. Calling NodeUnstageVolume to unpublish the volume from the node. Unmounting and deleting the staging path for the volume. Design Limitations There are some limitations with the current design. Some might be able to be addressed in the future- others are inherent in the approach.\nThe design relies on the array’s ability to revoke access to a volume for a particular node for the fencing operation. The granularity of access control for a volume is per node. Consequently, it isn’t possible to revoke access from one pod on a node while retaining access to another pod on the same node if we cannot communicate with the node. The implications of this are that if more than one pod on a node is sharing the same volume(s), they all must be protected by CSM for Resiliency, and they all must be cleaned up by controller-podmon if the node fails. If only some of the pods are cleaned up, the other pods will lose access to the volumes shared with pods that have been cleaned, so those pods should also fail. The node-podmon cleanup algorithm purposefully will not remove the node taint until all the protected volumes have been cleaned up from the node. This works well if the node fault lasts long enough that controller-podmon can evacuate all the protected pods from the node. However, if the failure is short-lived, and controller-podmon does not clean up all the protected pods on the node, or if for some reason node-podmon cannot clean a pod completely, the taint is left on the node, and manual intervention is required. The required intervention is for the operator to reboot the node, which will ensure that no zombie pods survive. Upon seeing the reboot, node-podmon will then remove the taint. If the node failure is short-lived and controller-podmon has not evacuated some of the protected pods on the node, they may try and restart on the same pod. This has been observed to cause such pods to go into CrashLoopBackoff. We are currently considering solutions to this problem. ","categories":"","description":"CSM for Resiliency Design\n","excerpt":"CSM for Resiliency Design\n","ref":"/csm-docs/v1/resiliency/design/","tags":"","title":"Design"},{"body":"Container Storage Modules (CSM) for Authorization is designed as a service mesh solution and consists of many internal components that work together in concert to achieve its overall functionality.\nThis document provides an overview of the major components, including how they fit together and pointers to implementation details.\nIf you are a developer who is new to CSM for Authorization and want to build a mental map of how it works, you’re in the right place.\nTerminology Service Mesh - An infrastructure layer consisting of proxies that intercept and route requests between existing services. CSI - Acronym for the Container Storage Interface. Proxy (L7) - A gateway between networked services that inspects request traffic. Sidecar Proxy - A service mesh proxy that runs alongside existing services, rather than within them. Pod - A Kubernetes abstraction for a set of related containers that are to be considered as one unit. Tenant - A named persona who owns a Kubernetes cluster and is considered the “client-side” user. Storage Administrator - A named persona who owns a storage array and is considered the admin user. Bird’s Eye View +-----------------------------------+ | Kubernetes | | | | +---------+ +---------+ | +---------------+ | | CSI | | Sidecar | | | CSM | +---------+ | | Driver |---------\u003e Proxy |---------------\u003e Authorization |--------------\u003e Storage | | +---------+ +---------+ | | Server | | Array | | | +---------------+ +---------+ +-----------------------------------+ ^ | | | +------------+ | karavictl | | CLI | +------------+ NOTE: Arrows indicate request or connection initiation, not necessarily data flow direction.\nThe sections below explain each component in the diagram.\nKubernetes The architecture assumes a Kubernetes cluster that intends to offer external storage to applications hosted therein. The mechanism for managing this storage would utilize a CSI Driver.\nArchitecture Invariant: We assume there may be many Kubernetes clusters, potentially containing multiple CSI Drivers each with their own Sidecar Proxy.\nCSI Driver A CSI Driver supports the Container Service Interface (CSI) specification. Dell provides customers with CSI Drivers for its various storage arrays. CSM for Authorization intends to support a majority, if not all, of these drivers.\nA CSI Driver will typically be configured to communicate directly to its intended storage array and as such will be limited in using only the authentication methods supported by the Storage Array itself, e.g. Basic authentication over TLS.\nArchitecture Invariant: We try to avoid having to make any code changes to the CSI Driver when adding support for it. Any CSI Driver should ideally not be aware that it is communicating to the Sidecar Proxy.\nSidecar Proxy The CSM for Authorization Sidecar Proxy is deployed as a sidecar in the CSI Driver’s Pod. It acts as a proxy and forwards all requests to a CSM Authorization Server.\nThe CSI Driver section noted the limitation of a CSI Driver using Storage Array supported authentication methods only. By nature of being a proxy, the CSM for Authorization Sidecar Proxy is able to override the Authorization HTTP header for outbound requests to use Bearer tokens. Such tokens are managed by CSM for Authorization as will be described later in this document.\nCSM for Authorization Server The CSM for Authorization Server is, at its core, a Layer 7 proxy for intercepting traffic between a CSI Driver and a Storage Array.\nInbound requests are expected to originate from the CSM for Authorization Sidecar Proxy, for the following reasons:\nProcessing a set of agreed upon HTTP headers (added by the CSM for Authorization Sidecar Proxy) to assist in routing traffic to the intended Storage Array. Inspection of CSM-specific Authorization Bearer tokens. CSM for Authorization CLI The karavictl CLI (Command Line Interface) application allows Storage Admins to manage and interact with a running CSM for Authorization Server.\nStorage Array A Storage Array is typically considered to be one of the various Dell storage offerings, e.g. Dell PowerFlex which is supported by CSM for Authorization today. Support for more Storage Arrays will come in the future.\nHow it Works CSM for Authorization intends to override the existing authorization methods between a CSI Driver and its Storage Array. This may be desirable for several reasons, if:\nThe CSI Driver requires privileged login credentials (e.g. “root”) in order to function. The Storage Array does not natively support the concept of RBAC and/or multi-tenancy. This section of of the document describes how CSM for Authorization provides a solution to these problems.\nBearer Tokens CSM for Authorization overrides any existing authorization mechanism between a CSI Driver and its corresponding Storage Array with the use of JSON Web Tokens (JWTs). The CSI Driver and Storage Array will not be aware of this taking place.\nIn the context of RFC-6749 there are two such JWTs that are used:\nAccess token: a single token valid for a short period of time. Refresh token: a single token used to obtain access tokens. Typically valid for a longer period of time. Both tokens are opaque to the client, yet provide meaningful information to the server, specifically:\nThe Tenant for whom the token is associated with. The Roles that are bound to the Tenant. Tokens encode the following set of claims:\n{ \"aud\": \"karavi\", \"exp\": 1915585883, \"iss\": \"com.dell.karavi\", \"sub\": \"karavi-tenant\", \"roles\": \"role-a,role-b,role-c\", \"group\": \"Tenant-1\" } Both tokens are signed using a server-side secret preventing the risk of tampering by any client. For example, a bad-actor is unable to modify a token to give themselves a role that they should not have, at least without knowing the server-side secret.\nThe refresh approach is beneficial for the following reasons:\nAccidental exposure of an access token poses a lesser security concern, given the set expiration time is short (e.g. 30 seconds). The CSM for Authorization Server can fully trust the access token without having to perform a database check on each request (doing so would nullify the benefits of using tokens in the first place). The CSM for Authorization Server can defer Tenant checks at refresh time only, e.g. do not allow refresh if the Tenant’s access has been revoked by a Storage Admin. There may be a short time window in between revocation and enforcement, depending on the access token’s expiration time. The following diagram shows the access and refresh tokens in play and how a valid access token is required for a request to be proxied to the intended Storage Array.\n+---------+ +---------------+ | | | | | | | | +----------+ | |--(A)------------ Access Token -----------\u003e| |------\u003e| | | | | CSM | | | | |\u003c-(B)---------- Protected Resource --------| Authorization |\u003c------| Storage | | Sidecar | | Server | | Array | | Proxy |--(C)------------ Access Token -----------\u003e| | | | | | | | | | | |\u003c-(D)------ Invalid Token Error -----------| | | | | | | | +----------+ | | | | | |--(E)----------- Refresh Token -----------\u003e| | | | \u0026 Expired Access Token | | | |\u003c-(F)----------- Access Token -------------| | +---------+ +---------------+ A) CSI Driver makes a request to the Storage Array: request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token valid. The CSM for Authorization Server permits the request to be proxied to the intended Storage Array. B) Storage Array response is sent back as expected. C) CSI Driver makes a request to the Storage Array: request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token is invalid; it has since expired. D) The CSM for Authorization Server responds with HTTP 401 Unauthorized. E) Sidecar Proxy requests a new access token by passing both refresh token and expired token. F) The CSM for Authorization Server processes the request: is the refresh token valid? is the access token expired? has the Tenant had access revoked? a new access token is sent in response if the checks pass. Roles So we know a token encodes both the identification of a Tenant and their Roles, but what’s in a Role?\nA role can be defined as follows:\nIt has a name, e.g. “role-a”. It can be bound to a Tenant It can be unbound from a Tenant. It determines access to zero or more storage pools and assigns a storage quota for each. Quota represents the upper-limit of the total aggregation of used storage capacity for a Tenant’s resources in a storage pool. It prevents ambiguity by identifying each storage pool in the form of system-type:system-id:pool-name. Below is an example of how roles are represented internally in JSON:\n{ \"Developer\": { \"system_types\": { \"powerflex\": { \"system_ids\": { \"542a2d5f5122210f\": { \"pool_quotas\": { \"bronze\": 99000000 } } } } } } } This role says Allow Tenants with the Developer role access to the bronze pool on PowerFlex system 542a2d5f5122210f, and cap their total capacity usage at 99000000Kb (99Gb).\nPolicy CSM for Authorization leverages the Open Policy Agent to use a policy-as-code approach to policy management. It stores a collection of policy files written in Rego language. Each policy file defines a set of policy rules that form the basis of a policy decision. A policy decision is made by processing the inputs provided. For CSM for Authorization, the inputs are:\nThe set of roles defined by the Storage Admin. The claims section of a validated JWT. The JSON payload of the storage request. Given these inputs, many decisions can be made to answer questions like “Can Tenant X, with these roles provision this volume of size Y?”. The result of the policy decision will determine whether or not the request is proxied.\n+----------------+ | Open Policy | | Agent | | | JWT | +--------+ | Claims ------\\ | | Policy | ----------\u003e Allow/Deny -----\u003e | (Rego) | | Storage -----\u003e +--------+ | Request -----/ +-------^--------+ | | | Role Data Quota \u0026 Volume Ownership Policy decisions based on the current request and set of roles alone are not enough. CSM for Authorization must maintain a cache of volumes approved for creation and deletion in order to know if a Tenant has already consumed their quota on a given storage pool.\nA Redis database is used to store this volume data and their relationship with a Tenant, Storage Array and Pool. The use of composite keys provide fast, constant time look up of volumes, e.g. quota:powerflex:542a2d5f5122210f:bronze:Tenant-1:data is a Redis hash with volume data as its values.\nCross-Cutting Concerns This section documents the pieces of code that are general in nature and shared across multiple packages.\nLogging CSM for Authorization uses the Logrus package when logging messages.\nObservability Both the CSM for Authorization Server and Sidecar Proxy are long-running processes, so it’s important to understand what’s going on inside. We use OpenTelemetry (otel) to help with that.\nThe following otel exporters are used:\ngo.opentelemetry.io/otel/exporters/metric/prometheus go.opentelemetry.io/otel/exporters/trace/zipkin go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization design\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v2/authorization/design/","tags":"","title":"Design"},{"body":"The solution takes the approach that each storage system that Container Storage Modules (CSM) for Observability supports will have their own metrics deployments in the Kubernetes cluster.\nMetrics Deployment: Queries the Kubernetes API to gather information about storage resources and then queries the storage system’s REST API to gather specific metrics. These metrics are then exported to the OTEL collector. Each supported storage system will have their own Deployment for metrics. They will each follow a similar pattern of querying the Kubernetes and StorageSystem APIs to gather information about storage resources (ex: volumes, storage pools, etc) and their metrics. Metrics will be exported directly to the OTEL collector. A single topology deployment will query the Kubernetes API to gather mapping information between Persistent Volumes and storage resources located on multiple storage systems. This information is queried directly from Grafana and displayed in a custom dashboard.\nRequired Components The following prerequisites must be deployed into the namespace where CSM for Observability is located to support the storage system metrics and topology deployments:\nPrometheus for scraping the metrics from the OTEL collector. Grafana for visualizing the metrics from Prometheus and Topology services using custom dashboards. CSM for Observability will use secrets to get details about the storage systems used by the CSI drivers. These secrets should be copied from the namespaces where the drivers are deployed. CSI PowerFlex driver uses the ‘vxflexos-config’ secret. CSI PowerStore driver uses the ‘powerstore-config’ secret. CSI PowerScale driver uses the ‘isilon-creds’ secret. CSI PowerMax driver uses the secrets in configmap ‘powermax-reverseproxy-config’. Deployment Architectures CSM for Observability can be deployed to either direct storage system requests directly to the storage system or through the CSM for Authorization proxy. The CSI driver must be configured to route storage system requests through the CSM for Authorization proxy in order for CSM for Observability to do the same.\nDefault Deployment of CSM for Observability Deployment of CSM for Observability with CSM for Authorization ","categories":"","description":"CSM for Observability Design\n","excerpt":"CSM for Observability Design\n","ref":"/csm-docs/v2/observability/design/","tags":"","title":"Design"},{"body":"This section covers CSM for Resiliency’s design. The detail is sufficient that you should be able to understand what CSM for Resiliency is designed to do in various situations and how it works. CSM for Resiliency is deployed as a sidecar named podmon with a CSI driver in both the controller pods and node pods. These are referred to as controller-podmon and node-podmon respectively.\nGenerally controller-podmon and the driver controller pods are deployed using a Deployment. The Deployments support one or multiple replicas for High Availability and use a standard K8S leader election protocol so that only one controller is active at a time (as does the driver and all the controller sidecars.) The controller deployment also supports a Node Selector that allows the controllers to be placed on K8S Manager (non Worker) nodes.\nNode-podmon and the driver node pods are deployed in a DaemonSet, with a Pod deployed on every K8S Worker Node.\nController-Podmon Controller-podmon is responsible for:\nSetting up a Watch for CSM for Resiliency labeled pods, and if a Pod is Initialized but Not Ready and resident on a Node with a NoSchedule or NoExecute taint, calling controllerCleanupPod to cleanup the pod so that a replacement pod can be scheduled.\nPeriodically polling the arrays to see if it has connectivity to the nodes that are hosting CSM for Resiliency labeled pods (if enabled.) If an array has lost connectivity to a node hosting CSM for Resiliency labeled pods using that array, controllerCleanupPod is invoked to cleanup the pods that have lost I/O connectivity.\nTainting nodes that have failed so that a) no further pods will get scheduled to them until they are returned to service, and b) podmon-node upon seeing the taint will invoke the cleanup operations to make sure any zombie pods (pods that have been replaced) cannot write to the volumes they were using.\nIf a CSM for Resiliency labeled pod enters a CrashLoopBackOff state, deleting that pod so it can be replaced.\nControllerCleanupPod cleans up the pod by taking the following actions:\nThe VolumeAttachments (VAs) are loaded, and all VAs belonging to the pod being cleaned up are identified. The PVs for each VolumeAttachment are identified and used to get the Volume Handle (array identifier for the volume.) If enabled, the array is queried if any of the volumes to the pod are still doing I/O. If so, cleanup is aborted. The pod’s volumes are “fenced” from the node the pod resides on to prevent any potential I/O from a zombie pod. This is done by calling the CSI ControllerUnpublishVolume call for each of the volumes. A taint is applied to the node to keep any new pods from being scheduled to the node. If the replacement pod were to get scheduled to the same node as a zombie pod, they might both gain access to the volume concurrently causing corruption. The VolumeAttachments for the pod is deleted. This is necessary so the replacement pod to be created can attach the volumes. The pod is forcibly deleted so that a StatefulSet controller which created the pod is free to create a replacement pod. Node-Podmon Node-podmon has the following responsibilities:\nEstablishing a pod watch which is used to maintain a list of pods executing on this node that may need to be cleaned up. The list includes information about each Mount volume or Block volume used by the pod including the volume handle, volume name, private mount path, and mount path in the pod. Periodically (every 30 seconds) polling to see if controller-podmon has applied a taint to the node. If so, node-podmon calls nodeModeCleanupPod for each pod to clean up any remnants of the pod (which is potentially a zombie pod.) If all pods have been successfully cleaned up, and there are no labeled pods on this node still existing, only then will node-podmon remove the taint placed on the node by controller-podmon. NodeModeCleanupPod cleans up the pod remnants by taking the following actions for each volume used by the pod:\nCalling NodeUnpublishVolume to unpublish the volume from the pod. Unmounting and deleting the target path for the volume. Calling NodeUnstageVolume to unpublish the volume from the node. Unmounting and deleting the staging path for the volume. Design Limitations There are some limitations with the current design. Some might be able to be addressed in the future- others are inherent in the approach.\nThe design relies on the array’s ability to revoke access to a volume for a particular node for the fencing operation. The granularity of access control for a volume is per node. Consequently, it isn’t possible to revoke access from one pod on a node while retaining access to another pod on the same node if we cannot communicate with the node. The implications of this are that if more than one pod on a node is sharing the same volume(s), they all must be protected by CSM for Resiliency, and they all must be cleaned up by controller-podmon if the node fails. If only some of the pods are cleaned up, the other pods will lose access to the volumes shared with pods that have been cleaned, so those pods should also fail. The node-podmon cleanup algorithm purposefully will not remove the node taint until all the protected volumes have been cleaned up from the node. This works well if the node fault lasts long enough that controller-podmon can evacuate all the protected pods from the node. However, if the failure is short-lived, and controller-podmon does not clean up all the protected pods on the node, or if for some reason node-podmon cannot clean a pod completely, the taint is left on the node, and manual intervention is required. The required intervention is for the operator to reboot the node, which will ensure that no zombie pods survive. Upon seeing the reboot, node-podmon will then remove the taint. If the node failure is short-lived and controller-podmon has not evacuated some of the protected pods on the node, they may try and restart on the same pod. This has been observed to cause such pods to go into CrashLoopBackoff. We are currently considering solutions to this problem. ","categories":"","description":"CSM for Resiliency Design\n","excerpt":"CSM for Resiliency Design\n","ref":"/csm-docs/v2/resiliency/design/","tags":"","title":"Design"},{"body":" The CSM Authorization karavictl CLI is no longer actively maintained or supported. It will be deprecated in CSM 2.0.\nContainer Storage Modules (CSM) for Authorization is designed as a service mesh solution and consists of many internal components that work together in concert to achieve its overall functionality.\nThis document provides an overview of the major components, including how they fit together and pointers to implementation details.\nIf you are a developer who is new to CSM for Authorization and want to build a mental map of how it works, you’re in the right place.\nTerminology Service Mesh - An infrastructure layer consisting of proxies that intercept and route requests between existing services. CSI - Acronym for the Container Storage Interface. Proxy (L7) - A gateway between networked services that inspects request traffic. Sidecar Proxy - A service mesh proxy that runs alongside existing services, rather than within them. Pod - A Kubernetes abstraction for a set of related containers that are to be considered as one unit. Tenant - A named persona who owns a Kubernetes cluster and is considered the “client-side” user. Storage Administrator - A named persona who owns a storage array and is considered the admin user. Bird’s Eye View +-----------------------------------+ | Kubernetes | | | | +---------+ +---------+ | +---------------+ | | CSI | | Sidecar | | | CSM | +---------+ | | Driver |---------\u003e Proxy |---------------\u003e Authorization |--------------\u003e Storage | | +---------+ +---------+ | | Server | | Array | | | +---------------+ +---------+ +-----------------------------------+ ^ | | | +------------+ | karavictl | | CLI | +------------+ NOTE: Arrows indicate request or connection initiation, not necessarily data flow direction.\nThe sections below explain each component in the diagram.\nKubernetes The architecture assumes a Kubernetes cluster that intends to offer external storage to applications hosted therein. The mechanism for managing this storage would utilize a CSI Driver.\nArchitecture Invariant: We assume there may be many Kubernetes clusters, potentially containing multiple CSI Drivers each with their own Sidecar Proxy.\nCSI Driver A CSI Driver supports the Container Service Interface (CSI) specification. Dell provides customers with CSI Drivers for its various storage arrays. CSM for Authorization intends to support a majority, if not all, of these drivers.\nA CSI Driver will typically be configured to communicate directly to its intended storage array and as such will be limited in using only the authentication methods supported by the Storage Array itself, e.g. Basic authentication over TLS.\nArchitecture Invariant: We try to avoid having to make any code changes to the CSI Driver when adding support for it. Any CSI Driver should ideally not be aware that it is communicating to the Sidecar Proxy.\nSidecar Proxy The CSM for Authorization Sidecar Proxy is deployed as a sidecar in the CSI Driver’s Pod. It acts as a proxy and forwards all requests to a CSM Authorization Server.\nThe CSI Driver section noted the limitation of a CSI Driver using Storage Array supported authentication methods only. By nature of being a proxy, the CSM for Authorization Sidecar Proxy is able to override the Authorization HTTP header for outbound requests to use Bearer tokens. Such tokens are managed by CSM for Authorization as will be described later in this document.\nCSM for Authorization Server The CSM for Authorization Server is, at its core, a Layer 7 proxy for intercepting traffic between a CSI Driver and a Storage Array.\nInbound requests are expected to originate from the CSM for Authorization Sidecar Proxy, for the following reasons:\nProcessing a set of agreed upon HTTP headers (added by the CSM for Authorization Sidecar Proxy) to assist in routing traffic to the intended Storage Array. Inspection of CSM-specific Authorization Bearer tokens. CSM for Authorization CLI The karavictl CLI (Command Line Interface) application allows Storage Admins to manage and interact with a running CSM for Authorization Server.\nStorage Array A Storage Array is typically considered to be one of the various Dell storage offerings, e.g. Dell PowerFlex which is supported by CSM for Authorization today. Support for more Storage Arrays will come in the future.\nHow it Works CSM for Authorization intends to override the existing authorization methods between a CSI Driver and its Storage Array. This may be desirable for several reasons, if:\nThe CSI Driver requires privileged login credentials (e.g. “root”) in order to function. The Storage Array does not natively support the concept of RBAC and/or multi-tenancy. This section of of the document describes how CSM for Authorization provides a solution to these problems.\nBearer Tokens CSM for Authorization overrides any existing authorization mechanism between a CSI Driver and its corresponding Storage Array with the use of JSON Web Tokens (JWTs). The CSI Driver and Storage Array will not be aware of this taking place.\nIn the context of RFC-6749 there are two such JWTs that are used:\nAccess token: a single token valid for a short period of time. Refresh token: a single token used to obtain access tokens. Typically valid for a longer period of time. Both tokens are opaque to the client, yet provide meaningful information to the server, specifically:\nThe Tenant for whom the token is associated with. The Roles that are bound to the Tenant. Tokens encode the following set of claims:\n{ \"aud\": \"karavi\", \"exp\": 1915585883, \"iss\": \"com.dell.karavi\", \"sub\": \"karavi-tenant\", \"roles\": \"role-a,role-b,role-c\", \"group\": \"Tenant-1\" } Both tokens are signed using a server-side secret preventing the risk of tampering by any client. For example, a bad-actor is unable to modify a token to give themselves a role that they should not have, at least without knowing the server-side secret.\nThe refresh approach is beneficial for the following reasons:\nAccidental exposure of an access token poses a lesser security concern, given the set expiration time is short (e.g. 30 seconds). The CSM for Authorization Server can fully trust the access token without having to perform a database check on each request (doing so would nullify the benefits of using tokens in the first place). The CSM for Authorization Server can defer Tenant checks at refresh time only, e.g. do not allow refresh if the Tenant’s access has been revoked by a Storage Admin. There may be a short time window in between revocation and enforcement, depending on the access token’s expiration time. The following diagram shows the access and refresh tokens in play and how a valid access token is required for a request to be proxied to the intended Storage Array.\n+---------+ +---------------+ | | | | | | | | +----------+ | |--(A)------------ Access Token -----------\u003e| |------\u003e| | | | | CSM | | | | |\u003c-(B)---------- Protected Resource --------| Authorization |\u003c------| Storage | | Sidecar | | Server | | Array | | Proxy |--(C)------------ Access Token -----------\u003e| | | | | | | | | | | |\u003c-(D)------ Invalid Token Error -----------| | | | | | | | +----------+ | | | | | |--(E)----------- Refresh Token -----------\u003e| | | | \u0026 Expired Access Token | | | |\u003c-(F)----------- Access Token -------------| | +---------+ +---------------+ A) CSI Driver makes a request to the Storage Array: request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token valid. The CSM for Authorization Server permits the request to be proxied to the intended Storage Array. B) Storage Array response is sent back as expected. C) CSI Driver makes a request to the Storage Array: request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token is invalid; it has since expired. D) The CSM for Authorization Server responds with HTTP 401 Unauthorized. E) Sidecar Proxy requests a new access token by passing both refresh token and expired token. F) The CSM for Authorization Server processes the request: is the refresh token valid? is the access token expired? has the Tenant had access revoked? a new access token is sent in response if the checks pass. Roles So we know a token encodes both the identification of a Tenant and their Roles, but what’s in a Role?\nA role can be defined as follows:\nIt has a name, e.g. “role-a”. It can be bound to a Tenant It can be unbound from a Tenant. It determines access to zero or more storage pools and assigns a storage quota for each. Quota represents the upper-limit of the total aggregation of used storage capacity for a Tenant’s resources in a storage pool. It prevents ambiguity by identifying each storage pool in the form of system-type:system-id:pool-name. Below is an example of how roles are represented internally in JSON:\n{ \"Developer\": { \"system_types\": { \"powerflex\": { \"system_ids\": { \"542a2d5f5122210f\": { \"pool_quotas\": { \"bronze\": 99000000 } } } } } } } This role says Allow Tenants with the Developer role access to the bronze pool on PowerFlex system 542a2d5f5122210f, and cap their total capacity usage at 99000000Kb (99Gb).\nPolicy CSM for Authorization leverages the Open Policy Agent to use a policy-as-code approach to policy management. It stores a collection of policy files written in Rego language. Each policy file defines a set of policy rules that form the basis of a policy decision. A policy decision is made by processing the inputs provided. For CSM for Authorization, the inputs are:\nThe set of roles defined by the Storage Admin. The claims section of a validated JWT. The JSON payload of the storage request. Given these inputs, many decisions can be made to answer questions like “Can Tenant X, with these roles provision this volume of size Y?”. The result of the policy decision will determine whether or not the request is proxied.\n+----------------+ | Open Policy | | Agent | | | JWT | +--------+ | Claims ------\\ | | Policy | ----------\u003e Allow/Deny -----\u003e | (Rego) | | Storage -----\u003e +--------+ | Request -----/ +-------^--------+ | | | Role Data Quota \u0026 Volume Ownership Policy decisions based on the current request and set of roles alone are not enough. CSM for Authorization must maintain a cache of volumes approved for creation and deletion in order to know if a Tenant has already consumed their quota on a given storage pool.\nA Redis database is used to store this volume data and their relationship with a Tenant, Storage Array and Pool. The use of composite keys provide fast, constant time look up of volumes, e.g. quota:powerflex:542a2d5f5122210f:bronze:Tenant-1:data is a Redis hash with volume data as its values.\nCross-Cutting Concerns This section documents the pieces of code that are general in nature and shared across multiple packages.\nLogging CSM for Authorization uses the Logrus package when logging messages.\nObservability Both the CSM for Authorization Server and Sidecar Proxy are long-running processes, so it’s important to understand what’s going on inside. We use OpenTelemetry (otel) to help with that.\nThe following otel exporters are used:\ngo.opentelemetry.io/otel/exporters/metric/prometheus go.opentelemetry.io/otel/exporters/trace/zipkin go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization design\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v3/authorization/design/","tags":"","title":"Design"},{"body":"The solution takes the approach that each storage system that Container Storage Modules (CSM) for Observability supports will have their own metrics deployments in the Kubernetes cluster.\nMetrics Deployment: Queries the Kubernetes API to gather information about storage resources and then queries the storage system’s REST API to gather specific metrics. These metrics are then exported to the OTEL collector. Each supported storage system will have their own Deployment for metrics. They will each follow a similar pattern of querying the Kubernetes and StorageSystem APIs to gather information about storage resources (ex: volumes, storage pools, etc) and their metrics. Metrics will be exported directly to the OTEL collector. A single topology deployment will query the Kubernetes API to gather mapping information between Persistent Volumes and storage resources located on multiple storage systems. This information is queried directly from Grafana and displayed in a custom dashboard.\nRequired Components The following prerequisites must be deployed into the namespace where CSM for Observability is located to support the storage system metrics and topology deployments:\nPrometheus for scraping the metrics from the OTEL collector. Grafana for visualizing the metrics from Prometheus and Topology services using custom dashboards. CSM for Observability will use secrets to get details about the storage systems used by the CSI drivers. These secrets should be copied from the namespaces where the drivers are deployed. CSI PowerFlex driver uses the ‘vxflexos-config’ secret. CSI PowerStore driver uses the ‘powerstore-config’ secret. CSI PowerScale driver uses the ‘isilon-creds’ secret. CSI PowerMax driver uses the secrets in configmap ‘powermax-reverseproxy-config’. Deployment Architectures CSM for Observability can be deployed to either direct storage system requests directly to the storage system or through the CSM for Authorization proxy. The CSI driver must be configured to route storage system requests through the CSM for Authorization proxy in order for CSM for Observability to do the same.\nDefault Deployment of CSM for Observability Deployment of CSM for Observability with CSM for Authorization ","categories":"","description":"CSM for Observability Design\n","excerpt":"CSM for Observability Design\n","ref":"/csm-docs/v3/observability/design/","tags":"","title":"Design"},{"body":"This section covers CSM for Resiliency’s design. The detail is sufficient that you should be able to understand what CSM for Resiliency is designed to do in various situations and how it works. CSM for Resiliency is deployed as a sidecar named podmon with a CSI driver in both the controller pods and node pods. These are referred to as controller-podmon and node-podmon respectively.\nGenerally controller-podmon and the driver controller pods are deployed using a Deployment. The Deployments support one or multiple replicas for High Availability and use a standard K8S leader election protocol so that only one controller is active at a time (as does the driver and all the controller sidecars.) The controller deployment also supports a Node Selector that allows the controllers to be placed on K8S Manager (non Worker) nodes.\nNode-podmon and the driver node pods are deployed in a DaemonSet, with a Pod deployed on every K8S Worker Node.\nController-Podmon Controller-podmon is responsible for:\nSetting up a Watch for CSM for Resiliency labeled pods, and if a Pod is Initialized but Not Ready and resident on a Node with a NoSchedule or NoExecute taint, calling controllerCleanupPod to cleanup the pod so that a replacement pod can be scheduled.\nPeriodically polling the arrays to see if it has connectivity to the nodes that are hosting CSM for Resiliency labeled pods (if enabled.) If an array has lost connectivity to a node hosting CSM for Resiliency labeled pods using that array, controllerCleanupPod is invoked to cleanup the pods that have lost I/O connectivity.\nTainting nodes that have failed so that a) no further pods will get scheduled to them until they are returned to service, and b) podmon-node upon seeing the taint will invoke the cleanup operations to make sure any zombie pods (pods that have been replaced) cannot write to the volumes they were using.\nIf a CSM for Resiliency labeled pod enters a CrashLoopBackOff state, deleting that pod so it can be replaced.\nControllerCleanupPod cleans up the pod by taking the following actions:\nThe VolumeAttachments (VAs) are loaded, and all VAs belonging to the pod being cleaned up are identified. The PVs for each VolumeAttachment are identified and used to get the Volume Handle (array identifier for the volume.) If enabled, the array is queried if any of the volumes to the pod are still doing I/O. If so, cleanup is aborted. The pod’s volumes are “fenced” from the node the pod resides on to prevent any potential I/O from a zombie pod. This is done by calling the CSI ControllerUnpublishVolume call for each of the volumes. A taint is applied to the node to keep any new pods from being scheduled to the node. If the replacement pod were to get scheduled to the same node as a zombie pod, they might both gain access to the volume concurrently causing corruption. The VolumeAttachments for the pod is deleted. This is necessary so the replacement pod to be created can attach the volumes. The pod is forcibly deleted so that a StatefulSet controller which created the pod is free to create a replacement pod. Node-Podmon Node-podmon has the following responsibilities:\nEstablishing a pod watch which is used to maintain a list of pods executing on this node that may need to be cleaned up. The list includes information about each Mount volume or Block volume used by the pod including the volume handle, volume name, private mount path, and mount path in the pod. Periodically (every 30 seconds) polling to see if controller-podmon has applied a taint to the node. If so, node-podmon calls nodeModeCleanupPod for each pod to clean up any remnants of the pod (which is potentially a zombie pod.) If all pods have been successfully cleaned up, and there are no labeled pods on this node still existing, only then will node-podmon remove the taint placed on the node by controller-podmon. NodeModeCleanupPod cleans up the pod remnants by taking the following actions for each volume used by the pod:\nCalling NodeUnpublishVolume to unpublish the volume from the pod. Unmounting and deleting the target path for the volume. Calling NodeUnstageVolume to unpublish the volume from the node. Unmounting and deleting the staging path for the volume. Design Limitations There are some limitations with the current design. Some might be able to be addressed in the future- others are inherent in the approach.\nThe design relies on the array’s ability to revoke access to a volume for a particular node for the fencing operation. The granularity of access control for a volume is per node. Consequently, it isn’t possible to revoke access from one pod on a node while retaining access to another pod on the same node if we cannot communicate with the node. The implications of this are that if more than one pod on a node is sharing the same volume(s), they all must be protected by CSM for Resiliency, and they all must be cleaned up by controller-podmon if the node fails. If only some of the pods are cleaned up, the other pods will lose access to the volumes shared with pods that have been cleaned, so those pods should also fail. The node-podmon cleanup algorithm purposefully will not remove the node taint until all the protected volumes have been cleaned up from the node. This works well if the node fault lasts long enough that controller-podmon can evacuate all the protected pods from the node. However, if the failure is short-lived, and controller-podmon does not clean up all the protected pods on the node, or if for some reason node-podmon cannot clean a pod completely, the taint is left on the node, and manual intervention is required. The required intervention is for the operator to reboot the node, which will ensure that no zombie pods survive. Upon seeing the reboot, node-podmon will then remove the taint. If the node failure is short-lived and controller-podmon has not evacuated some of the protected pods on the node, they may try and restart on the same pod. This has been observed to cause such pods to go into CrashLoopBackoff. We are currently considering solutions to this problem. ","categories":"","description":"CSM for Resiliency Design\n","excerpt":"CSM for Resiliency Design\n","ref":"/csm-docs/v3/resiliency/design/","tags":"","title":"Design"},{"body":"Encryption provides the capability to encrypt user data residing on volumes created by Dell CSI Drivers.\nNOTE: This tech-preview release is not intended for use in production environment.\nNOTE: Encryption requires a time-based license to create new encrypted volumes. Request a trial license prior to deployment.\nAfter the license expiration, existing encrypted volume can still be unlocked and used, but no new encrypted volumes can be created.\nThe volume data is encrypted on the Kubernetes worker host running the application workload, transparently for the application.\nUnder the hood, gocryptfs, an open-source FUSE based encryptor, is used to encrypt both files content and the names of files and directories.\nFiles content is encrypted using AES-256-GCM and names are encrypted using AES-256-EME.\ngocryptfs needs a password to initialize and to unlock the encrypted file system. Encryption generates 32 random bytes for the password and stores them in Hashicorp Vault.\nFor detailed information on the cryptography behind gocryptfs, see gocryptfs Cryptography.\nWhen a CSI Driver is installed with the Encryption feature enabled, two provisioners are registered in the cluster:\nProvisioner for unencrypted volumes\nThis provisioner belongs to the storage driver and does not depend on the Encryption feature. Use a storage class with this provisioner to create regular unencrypted volumes.\nProvisioner for encrypted volumes\nThis provisioner belongs to Encryption and registers with the name encryption.pluginName when Encryption is enabled. Use a storage class with this provisioner to create encrypted volumes.\nCapabilities Feature PowerScale Dynamic provisionings of new volumes Yes Static provisioning of new volumes Yes Volume snapshot creation Yes Volume creation from snapshot Yes Volume cloning Yes Volume expansion Yes Encrypted volume unlocking in a different cluster Yes User file and directory names encryption Yes Limitations Only file system volumes are supported. Existing volumes with data cannot be encrypted. Workaround: create a new encrypted volume of the same size and copy/move the data from the original unencrypted volume to the new encrypted volume. Encryption cannot be disabled in-place. Workaround: create a new unencrypted volume of the same size and copy/move the data from the original encrypted volume to the new unencrypted volume. Encrypted volume content can be seen in clear text through root access to the worker node or by obtaining shell access into the Encryption driver container. When deployed with PowerScale CSI driver, controllerCount has to be set to 1. No other CSM component can be enabled simultaneously with Encryption. The only supported authentication method for Vault is AppRole. Encryption secrets, config maps and encryption related values cannot be updated while the CSI driver is running: the CSI driver must be restarted to pick up the change. Supported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.26, 1.27, 1.28 Red Hat OpenShift 4.13, 4.14 Supported Storage Platforms PowerScale Storage Array OneFS 9.3, 9.4, 9.5.0.5, 9.5.0.6 Supported CSI Drivers Encryption supports these CSI drivers and versions: Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerScale csi-powerscale v2.8 + PowerScale When enabling Encryption for PowerScale CSI Driver, make sure these requirements are met:\nPowerScale CSI Driver uses root credentials for the storage array where encrypted volumes will be placed OneFS NFS export configuration does not have root user mapping enabled All other CSM features like Authorization, Replication, Resiliency are disabled Health Monitor feature is disabled CSI driver controllerCount is set to 1 Hashicorp Vault Support Supported Vault version is 1.9.3 and newer.\nVault server (or cluster) is typically deployed in a dedicated Kubernetes cluster, but for the purpose of Encryption, it can be located anywhere. Even the simplest standalone single instance server with in-memory storage will suffice for testing.\nNOTE: Properly deployed and configured Vault is crucial for security of the volumes encrypted with Encryption. Please refer to the Hashicorp Vault documentation regarding recommended deployment options.\nCAUTION: Compromised Vault server or Vault storage back-end may lead to unauthorized access to the volumes encrypted with Encryption.\nCAUTION: Destroyed Vault storage back-end or the encryption key stored in it, will make it impossible to unlock the volume encrypted with Encryption. Access to the data will be lost for ever.\nRefer to Vault Configuration section for minimal configuration steps required to support Encryption and other configuration considerations.\nKey Rotation (rekey) This preview of Encryption includes the ability to change the KEK (Key Encryption Key) of an encrypted volume, an operation commonly known as Shallow Rekey, or Shallow Key Rotation. The KEK is the 256-bit key that encrypts the Data Encryption Key which encrypts the data on the volume.\nKubernetes Worker Hosts Requirements Each Kubernetes worker host should have SSH server running. SSH server should have SSH public key authentication enabled for user root. SSH server should remain running all the time whenever an application with an encrypted volume is running on the host. NOTE: Stopping the SSH server on the worker host makes any encrypted volume attached to this host inaccessible.\nEach Kubernetes worker host should have commands fusermount and mount.fuse. They are pre-installed in most Linux distros. To install package fuse in Ubuntu/Debian run command similar to apt install fuse. To install package fuse in SUSE run command similar to zypper install fuse. ","categories":"","description":"CSI Volumes Encryption\n","excerpt":"CSI Volumes Encryption\n","ref":"/csm-docs/docs/secure/encryption/","tags":"","title":"Encryption"},{"body":"Encryption provides the capability to encrypt user data residing on volumes created by Dell CSI Drivers.\nNOTE: This tech-preview release is not intended for use in production environment.\nNOTE: Encryption requires a time-based license to create new encrypted volumes. Request a trial license prior to deployment.\nAfter the license expiration, existing encrypted volume can still be unlocked and used, but no new encrypted volumes can be created.\nThe volume data is encrypted on the Kubernetes worker host running the application workload, transparently for the application.\nUnder the hood, gocryptfs, an open-source FUSE based encryptor, is used to encrypt both files content and the names of files and directories.\nFiles content is encrypted using AES-256-GCM and names are encrypted using AES-256-EME.\ngocryptfs needs a password to initialize and to unlock the encrypted file system. Encryption generates 32 random bytes for the password and stores them in Hashicorp Vault.\nFor detailed information on the cryptography behind gocryptfs, see gocryptfs Cryptography.\nWhen a CSI Driver is installed with the Encryption feature enabled, two provisioners are registered in the cluster:\nProvisioner for unencrypted volumes\nThis provisioner belongs to the storage driver and does not depend on the Encryption feature. Use a storage class with this provisioner to create regular unencrypted volumes.\nProvisioner for encrypted volumes\nThis provisioner belongs to Encryption and registers with the name encryption.pluginName when Encryption is enabled. Use a storage class with this provisioner to create encrypted volumes.\nCapabilities Feature PowerScale Dynamic provisionings of new volumes Yes Static provisioning of new volumes Yes Volume snapshot creation Yes Volume creation from snapshot Yes Volume cloning Yes Volume expansion Yes Encrypted volume unlocking in a different cluster Yes User file and directory names encryption Yes Limitations Only file system volumes are supported. Existing volumes with data cannot be encrypted. Workaround: create a new encrypted volume of the same size and copy/move the data from the original unencrypted volume to the new encrypted volume. Encryption cannot be disabled in-place. Workaround: create a new unencrypted volume of the same size and copy/move the data from the original encrypted volume to the new unencrypted volume. Encrypted volume content can be seen in clear text through root access to the worker node or by obtaining shell access into the Encryption driver container. When deployed with PowerScale CSI driver, controllerCount has to be set to 1. No other CSM component can be enabled simultaneously with Encryption. The only supported authentication method for Vault is AppRole. Encryption secrets, config maps and encryption related values cannot be updated while the CSI driver is running: the CSI driver must be restarted to pick up the change. Supported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.24, 1.25, 1.26 Red Hat OpenShift 4.10, 4.11 RHEL 7.9, 8.4 Ubuntu 18.04, 20.04 SLES 15SP2 Supported Storage Platforms PowerScale Storage Array OneFS 9.0 Supported CSI Drivers Encryption supports these CSI drivers and versions: Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerScale csi-powerscale v2.4 + PowerScale When enabling Encryption for PowerScale CSI Driver, make sure these requirements are met:\nPowerScale CSI Driver uses root credentials for the storage array where encrypted volumes will be placed OneFS NFS export configuration does not have root user mapping enabled All other CSM features like Authorization, Replication, Resiliency are disabled Health Monitor feature is disabled CSI driver controllerCount is set to 1 Hashicorp Vault Support Supported Vault version is 1.9.3 and newer.\nVault server (or cluster) is typically deployed in a dedicated Kubernetes cluster, but for the purpose of Encryption, it can be located anywhere. Even the simplest standalone single instance server with in-memory storage will suffice for testing.\nNOTE: Properly deployed and configured Vault is crucial for security of the volumes encrypted with Encryption. Please refer to the Hashicorp Vault documentation regarding recommended deployment options.\nCAUTION: Compromised Vault server or Vault storage back-end may lead to unauthorized access to the volumes encrypted with Encryption.\nCAUTION: Destroyed Vault storage back-end or the encryption key stored in it, will make it impossible to unlock the volume encrypted with Encryption. Access to the data will be lost for ever.\nRefer to Vault Configuration section for minimal configuration steps required to support Encryption and other configuration considerations.\nKey Rotation (rekey) This preview of Encryption includes the ability to change the KEK (Key Encryption Key) of an encrypted volume, an operation commonly known as Shallow Rekey, or Shallow Key Rotation. The KEK is the 256-bit key that encrypts the Data Encryption Key which encrypts the data on the volume.\nKubernetes Worker Hosts Requirements Each Kubernetes worker host should have SSH server running. SSH server should have SSH public key authentication enabled for user root. SSH server should remain running all the time whenever an application with an encrypted volume is running on the host. NOTE: Stopping the SSH server on the worker host makes any encrypted volume attached to this host inaccessible.\nEach Kubernetes worker host should have commands fusermount and mount.fuse. They are pre-installed in most Linux distros. To install package fuse in Ubuntu/Debian run command similar to apt install fuse. To install package fuse in SUSE run command similar to zypper install fuse. ","categories":"","description":"CSI Volumes Encryption\n","excerpt":"CSI Volumes Encryption\n","ref":"/csm-docs/v1/secure/encryption/","tags":"","title":"Encryption"},{"body":"Encryption provides the capability to encrypt user data residing on volumes created by Dell CSI Drivers.\nNOTE: This tech-preview release is not intended for use in production environment.\nNOTE: Encryption requires a time-based license to create new encrypted volumes. Request a trial license prior to deployment.\nAfter the license expiration, existing encrypted volume can still be unlocked and used, but no new encrypted volumes can be created.\nThe volume data is encrypted on the Kubernetes worker host running the application workload, transparently for the application.\nUnder the hood, gocryptfs, an open-source FUSE based encryptor, is used to encrypt both files content and the names of files and directories.\nFiles content is encrypted using AES-256-GCM and names are encrypted using AES-256-EME.\ngocryptfs needs a password to initialize and to unlock the encrypted file system. Encryption generates 32 random bytes for the password and stores them in Hashicorp Vault.\nFor detailed information on the cryptography behind gocryptfs, see gocryptfs Cryptography.\nWhen a CSI Driver is installed with the Encryption feature enabled, two provisioners are registered in the cluster:\nProvisioner for unencrypted volumes\nThis provisioner belongs to the storage driver and does not depend on the Encryption feature. Use a storage class with this provisioner to create regular unencrypted volumes.\nProvisioner for encrypted volumes\nThis provisioner belongs to Encryption and registers with the name encryption.pluginName when Encryption is enabled. Use a storage class with this provisioner to create encrypted volumes.\nCapabilities Feature PowerScale Dynamic provisionings of new volumes Yes Static provisioning of new volumes Yes Volume snapshot creation Yes Volume creation from snapshot Yes Volume cloning Yes Volume expansion Yes Encrypted volume unlocking in a different cluster Yes User file and directory names encryption Yes Limitations Only file system volumes are supported. Existing volumes with data cannot be encrypted. Workaround: create a new encrypted volume of the same size and copy/move the data from the original unencrypted volume to the new encrypted volume. Encryption cannot be disabled in-place. Workaround: create a new unencrypted volume of the same size and copy/move the data from the original encrypted volume to the new unencrypted volume. Encrypted volume content can be seen in clear text through root access to the worker node or by obtaining shell access into the Encryption driver container. When deployed with PowerScale CSI driver, controllerCount has to be set to 1. No other CSM component can be enabled simultaneously with Encryption. The only supported authentication method for Vault is AppRole. Encryption secrets, config maps and encryption related values cannot be updated while the CSI driver is running: the CSI driver must be restarted to pick up the change. Supported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.24, 1.25, 1.26 Red Hat OpenShift 4.10, 4.11 RHEL 7.9, 8.4 Ubuntu 18.04, 20.04 SLES 15SP2 Supported Storage Platforms PowerScale Storage Array OneFS 9.0 Supported CSI Drivers Encryption supports these CSI drivers and versions: Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerScale csi-powerscale v2.4 + PowerScale When enabling Encryption for PowerScale CSI Driver, make sure these requirements are met:\nPowerScale CSI Driver uses root credentials for the storage array where encrypted volumes will be placed OneFS NFS export configuration does not have root user mapping enabled All other CSM features like Authorization, Replication, Resiliency are disabled Health Monitor feature is disabled CSI driver controllerCount is set to 1 Hashicorp Vault Support Supported Vault version is 1.9.3 and newer.\nVault server (or cluster) is typically deployed in a dedicated Kubernetes cluster, but for the purpose of Encryption, it can be located anywhere. Even the simplest standalone single instance server with in-memory storage will suffice for testing.\nNOTE: Properly deployed and configured Vault is crucial for security of the volumes encrypted with Encryption. Please refer to the Hashicorp Vault documentation regarding recommended deployment options.\nCAUTION: Compromised Vault server or Vault storage back-end may lead to unauthorized access to the volumes encrypted with Encryption.\nCAUTION: Destroyed Vault storage back-end or the encryption key stored in it, will make it impossible to unlock the volume encrypted with Encryption. Access to the data will be lost for ever.\nRefer to Vault Configuration section for minimal configuration steps required to support Encryption and other configuration considerations.\nKey Rotation (rekey) This preview of Encryption includes the ability to change the KEK (Key Encryption Key) of an encrypted volume, an operation commonly known as Shallow Rekey, or Shallow Key Rotation. The KEK is the 256-bit key that encrypts the Data Encryption Key which encrypts the data on the volume.\nKubernetes Worker Hosts Requirements Each Kubernetes worker host should have SSH server running. SSH server should have SSH public key authentication enabled for user root. SSH server should remain running all the time whenever an application with an encrypted volume is running on the host. NOTE: Stopping the SSH server on the worker host makes any encrypted volume attached to this host inaccessible.\nEach Kubernetes worker host should have commands fusermount and mount.fuse. They are pre-installed in most Linux distros. To install package fuse in Ubuntu/Debian run command similar to apt install fuse. To install package fuse in SUSE run command similar to zypper install fuse. ","categories":"","description":"CSI Volumes Encryption\n","excerpt":"CSI Volumes Encryption\n","ref":"/csm-docs/v2/secure/encryption/","tags":"","title":"Encryption"},{"body":"Encryption provides the capability to encrypt user data residing on volumes created by Dell CSI Drivers.\nNOTE: This tech-preview release is not intended for use in production environment.\nNOTE: Encryption requires a time-based license to create new encrypted volumes. Request a trial license prior to deployment.\nAfter the license expiration, existing encrypted volume can still be unlocked and used, but no new encrypted volumes can be created.\nThe volume data is encrypted on the Kubernetes worker host running the application workload, transparently for the application.\nUnder the hood, gocryptfs, an open-source FUSE based encryptor, is used to encrypt both files content and the names of files and directories.\nFiles content is encrypted using AES-256-GCM and names are encrypted using AES-256-EME.\ngocryptfs needs a password to initialize and to unlock the encrypted file system. Encryption generates 32 random bytes for the password and stores them in Hashicorp Vault.\nFor detailed information on the cryptography behind gocryptfs, see gocryptfs Cryptography.\nWhen a CSI Driver is installed with the Encryption feature enabled, two provisioners are registered in the cluster:\nProvisioner for unencrypted volumes\nThis provisioner belongs to the storage driver and does not depend on the Encryption feature. Use a storage class with this provisioner to create regular unencrypted volumes.\nProvisioner for encrypted volumes\nThis provisioner belongs to Encryption and registers with the name encryption.pluginName when Encryption is enabled. Use a storage class with this provisioner to create encrypted volumes.\nCapabilities Feature PowerScale Dynamic provisionings of new volumes Yes Static provisioning of new volumes Yes Volume snapshot creation Yes Volume creation from snapshot Yes Volume cloning Yes Volume expansion Yes Encrypted volume unlocking in a different cluster Yes User file and directory names encryption Yes Limitations Only file system volumes are supported. Existing volumes with data cannot be encrypted. Workaround: create a new encrypted volume of the same size and copy/move the data from the original unencrypted volume to the new encrypted volume. Encryption cannot be disabled in-place. Workaround: create a new unencrypted volume of the same size and copy/move the data from the original encrypted volume to the new unencrypted volume. Encrypted volume content can be seen in clear text through root access to the worker node or by obtaining shell access into the Encryption driver container. When deployed with PowerScale CSI driver, controllerCount has to be set to 1. No other CSM component can be enabled simultaneously with Encryption. The only supported authentication method for Vault is AppRole. Encryption secrets, config maps and encryption related values cannot be updated while the CSI driver is running: the CSI driver must be restarted to pick up the change. Supported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.24, 1.25, 1.26 Red Hat OpenShift 4.10, 4.11 RHEL 7.9, 8.4 Ubuntu 18.04, 20.04 SLES 15SP2 Supported Storage Platforms PowerScale Storage Array OneFS 9.0 Supported CSI Drivers Encryption supports these CSI drivers and versions: Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerScale csi-powerscale v2.4 + PowerScale When enabling Encryption for PowerScale CSI Driver, make sure these requirements are met:\nPowerScale CSI Driver uses root credentials for the storage array where encrypted volumes will be placed OneFS NFS export configuration does not have root user mapping enabled All other CSM features like Authorization, Replication, Resiliency are disabled Health Monitor feature is disabled CSI driver controllerCount is set to 1 Hashicorp Vault Support Supported Vault version is 1.9.3 and newer.\nVault server (or cluster) is typically deployed in a dedicated Kubernetes cluster, but for the purpose of Encryption, it can be located anywhere. Even the simplest standalone single instance server with in-memory storage will suffice for testing.\nNOTE: Properly deployed and configured Vault is crucial for security of the volumes encrypted with Encryption. Please refer to the Hashicorp Vault documentation regarding recommended deployment options.\nCAUTION: Compromised Vault server or Vault storage back-end may lead to unauthorized access to the volumes encrypted with Encryption.\nCAUTION: Destroyed Vault storage back-end or the encryption key stored in it, will make it impossible to unlock the volume encrypted with Encryption. Access to the data will be lost for ever.\nRefer to Vault Configuration section for minimal configuration steps required to support Encryption and other configuration considerations.\nKey Rotation (rekey) This preview of Encryption includes the ability to change the KEK (Key Encryption Key) of an encrypted volume, an operation commonly known as Shallow Rekey, or Shallow Key Rotation. The KEK is the 256-bit key that encrypts the Data Encryption Key which encrypts the data on the volume.\nKubernetes Worker Hosts Requirements Each Kubernetes worker host should have SSH server running. SSH server should have SSH public key authentication enabled for user root. SSH server should remain running all the time whenever an application with an encrypted volume is running on the host. NOTE: Stopping the SSH server on the worker host makes any encrypted volume attached to this host inaccessible.\nEach Kubernetes worker host should have commands fusermount and mount.fuse. They are pre-installed in most Linux distros. To install package fuse in Ubuntu/Debian run command similar to apt install fuse. To install package fuse in SUSE run command similar to zypper install fuse. ","categories":"","description":"CSI Volumes Encryption\n","excerpt":"CSI Volumes Encryption\n","ref":"/csm-docs/v3/secure/encryption/","tags":"","title":"Encryption"},{"body":" What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Where do I start with Dell Container Storage Modules (CSM)? What are the prerequisites for deploying Container Storage Modules? How do I uninstall or disable a module? How do I troubleshoot Container Storage Modules? Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? Should I install the module in the same namespace as the driver or another? Which Kubernetes distributions are supported? How do I get a list of Container Storage Modules deployed in my cluster with their versions? Do all Container Storage Modules need to be the same version, or can I mix and match? Can I run Container Storage Modules in a production environment? Is Dell Container Storage Modules (CSM) supported by Dell Technologies? Can I modify a module or contribute to the project? What is coming next? What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Dell Container Storage Modules are a set of modules that aim to extend features beyond what is available in the CSI specification.\nThe main goal with CSM modules is to expose storage array enterprise features directly within Kubernetes so developers are empowered to leverage them for their deployment in a seamless way.\nWhere do I start with Dell Container Storage Modules (CSM)? The umbrella repository for every Dell Container Storage Module is: https://github.com/dell/csm.\nWhat are the prerequisites for deploying Container Storage Modules? Prerequisites can be found on the respective module deployment pages:\nDell Container Storage Module for Observability Deployment Dell Container Storage Module for Authorization Deployment Dell Container Storage Module for Resiliency Deployment Dell Container Storage Module for Replication Deployment Dell Container Storage Module for Application Mobility Deployment Dell Container Storage Module for Encryption Deployment Prerequisites for deploying the Dell CSI drivers can be found here:\nDell CSI Drivers Deployment How do I uninstall or disable a module? Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Resiliency Dell Container Storage Module for Replication Dell Container Storage Module for Application Mobility Dell Container Storage Module for Encryption How do I troubleshoot Container Storage Modules? Dell CSI Drivers Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency Dell Container Storage Module for Application Mobility Dell Container Storage Module for Encryption Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? No, all the modules have been designed to work inside Kubernetes with Dell CSI drivers.\nShould I install the module in the same namespace as the driver or another? It is recommended to install CSM for Observability in a namespace separate from the Dell CSI drivers because it works across multiple drivers. All other modules either run as standalone or with the Dell CSI driver as a sidecar.\nWhich Kubernetes distributions are supported? The supported Kubernetes distributions for Container Storage Modules are documented:\nDell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency Dell Container Storage Module for Application Mobility Dell Container Storage Module for Encryption The supported distros for the Dell CSI Drivers are located here.\nHow do I get a list of Container Storage Modules deployed in my cluster with their versions? The easiest way to find the module version is to check the image tag for the module. For all the namespaces you can execute the following:\nkubectl get pods -A -o jsonpath=\"{..image}\" | tr -s '[[:space:]]' '\\n' | grep 'csm\\|karavi' | sort | uniq -c Or if you know the namespace:\nkubectl get deployment,daemonset -o wide -n {{namespace}} Do all Container Storage Modules need to be the same version, or can I mix and match? It is advised to comply with the support matrices (links below) and not deviate from it with mixed versions.\nDell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency Dell Container Storage Module for Application Mobility Dell Container Storage Module for Encryption Dell CSI Drivers. Can I run Container Storage Modules in a production environment? Currently, the Container Storage Modules Authorization, Observability, Replication, and Resiliency are GA and ready for production systems. The modules Encryption and Application Mobility are launched for Tech Preview Release and it is not intended to use in the Production systems.\nIs Dell Container Storage Modules (CSM) supported by Dell Technologies? Yes!\nIf you find an issue, please follow our support process\nCan I modify a module or contribute to the project? Yes!\nAll Container Storage Modules are released as open-source projects under Apache-2.0 License. You are free to contribute directly following the contribution guidelines, fork the projects, modify them, and of course share feedback or open tickets ;-)\nWhat is coming next? This is just the beginning of the journey for Dell Container Storage Modules, and there is a full roadmap with more to come, which you can check under the GitHub Milestones page.\n","categories":"","description":"Frequently asked questions of Dell Technologies (Dell) Container Storage Modules","excerpt":"Frequently asked questions of Dell Technologies (Dell) Container …","ref":"/csm-docs/docs/references/faq/","tags":"","title":"CSM FAQ"},{"body":" What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Where do I start with Dell Container Storage Modules (CSM)? What are the prerequisites for deploying Container Storage Modules? How do I uninstall or disable a module? How do I troubleshoot Container Storage Modules? Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? Should I install the module in the same namespace as the driver or another? Which Kubernetes distributions are supported? How do I get a list of Container Storage Modules deployed in my cluster with their versions? Do all Container Storage Modules need to be the same version, or can I mix and match? Can I run Container Storage Modules in a production environment? Is Dell Container Storage Modules (CSM) supported by Dell Technologies? Can I modify a module or contribute to the project? What is coming next? What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Dell Container Storage Modules are a set of modules that aim to extend features beyond what is available in the CSI specification.\nThe main goal with CSM modules is to expose storage array enterprise features directly within Kubernetes so developers are empowered to leverage them for their deployment in a seamless way.\nWhere do I start with Dell Container Storage Modules (CSM)? The umbrella repository for every Dell Container Storage Module is: https://github.com/dell/csm.\nWhat are the prerequisites for deploying Container Storage Modules? Prerequisites can be found on the respective module deployment pages:\nDell Container Storage Module for Observability Deployment Dell Container Storage Module for Authorization Deployment Dell Container Storage Module for Resiliency Deployment Dell Container Storage Module for Replication Deployment Dell Container Storage Module for Application Mobility Deployment Dell Container Storage Module for Encryption Deployment Prerequisites for deploying the Dell CSI drivers can be found here:\nDell CSI Drivers Deployment How do I uninstall or disable a module? Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Resiliency Dell Container Storage Module for Replication Dell Container Storage Module for Application Mobility Dell Container Storage Module for Encryption How do I troubleshoot Container Storage Modules? Dell CSI Drivers Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency Dell Container Storage Module for Application Mobility Dell Container Storage Module for Encryption Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? No, all the modules have been designed to work inside Kubernetes with Dell CSI drivers.\nShould I install the module in the same namespace as the driver or another? It is recommended to install CSM for Observability in a namespace separate from the Dell CSI drivers because it works across multiple drivers. All other modules either run as standalone or with the Dell CSI driver as a sidecar.\nWhich Kubernetes distributions are supported? The supported Kubernetes distributions for Container Storage Modules are documented:\nDell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency Dell Container Storage Module for Application Mobility Dell Container Storage Module for Encryption The supported distros for the Dell CSI Drivers are located here.\nHow do I get a list of Container Storage Modules deployed in my cluster with their versions? The easiest way to find the module version is to check the image tag for the module. For all the namespaces you can execute the following:\nkubectl get pods -A -o jsonpath=\"{..image}\" | tr -s '[[:space:]]' '\\n' | grep 'csm\\|karavi' | sort | uniq -c Or if you know the namespace:\nkubectl get deployment,daemonset -o wide -n {{namespace}} Do all Container Storage Modules need to be the same version, or can I mix and match? It is advised to comply with the support matrices (links below) and not deviate from it with mixed versions.\nDell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency Dell Container Storage Module for Application Mobility Dell Container Storage Module for Encryption Dell CSI Drivers. Can I run Container Storage Modules in a production environment? Currently, the Container Storage Modules Authorization, Observability, Replication, and Resiliency are GA and ready for production systems. The modules Encryption and Application Mobility are launched for Tech Preview Release and it is not intended to use in the Production systems.\nIs Dell Container Storage Modules (CSM) supported by Dell Technologies? Yes!\nIf you find an issue, please follow our support process\nCan I modify a module or contribute to the project? Yes!\nAll Container Storage Modules are released as open-source projects under Apache-2.0 License. You are free to contribute directly following the contribution guidelines, fork the projects, modify them, and of course share feedback or open tickets ;-)\nWhat is coming next? This is just the beginning of the journey for Dell Container Storage Modules, and there is a full roadmap with more to come, which you can check under the GitHub Milestones page.\n","categories":"","description":"Frequently asked questions of Dell Technologies (Dell) Container Storage Modules","excerpt":"Frequently asked questions of Dell Technologies (Dell) Container …","ref":"/csm-docs/v1/references/faq/","tags":"","title":"CSM FAQ"},{"body":" What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Where do I start with Dell Container Storage Modules (CSM)? What are the prerequisites for deploying Container Storage Modules? How do I uninstall or disable a module? How do I troubleshoot Container Storage Modules? Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? Should I install the module in the same namespace as the driver or another? Which Kubernetes distributions are supported? How do I get a list of Container Storage Modules deployed in my cluster with their versions? Do all Container Storage Modules need to be the same version, or can I mix and match? Can I run Container Storage Modules in a production environment? Is Dell Container Storage Modules (CSM) supported by Dell Technologies? Can I modify a module or contribute to the project? What is coming next? What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Dell Container Storage Modules are a set of modules that aim to extend features beyond what is available in the CSI specification.\nThe main goal with CSM modules is to expose storage array enterprise features directly within Kubernetes so developers are empowered to leverage them for their deployment in a seamless way.\nWhere do I start with Dell Container Storage Modules (CSM)? The umbrella repository for every Dell Container Storage Module is: https://github.com/dell/csm.\nWhat are the prerequisites for deploying Container Storage Modules? Prerequisites can be found on the respective module deployment pages:\nDell Container Storage Module for Observability Deployment Dell Container Storage Module for Authorization Deployment Dell Container Storage Module for Resiliency Deployment Dell Container Storage Module for Replication Deployment Dell Container Storage Module for Application Mobility Deployment Dell Container Storage Module for Encryption Deployment Prerequisites for deploying the Dell CSI drivers can be found here:\nDell CSI Drivers Deployment How do I uninstall or disable a module? Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Resiliency Dell Container Storage Module for Replication Dell Container Storage Module for Application Mobility Dell Container Storage Module for Encryption How do I troubleshoot Container Storage Modules? Dell CSI Drivers Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency Dell Container Storage Module for Application Mobility Dell Container Storage Module for Encryption Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? No, all the modules have been designed to work inside Kubernetes with Dell CSI drivers.\nShould I install the module in the same namespace as the driver or another? It is recommended to install CSM for Observability in a namespace separate from the Dell CSI drivers because it works across multiple drivers. All other modules either run as standalone or with the Dell CSI driver as a sidecar.\nWhich Kubernetes distributions are supported? The supported Kubernetes distributions for Container Storage Modules are documented:\nDell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency Dell Container Storage Module for Application Mobility Dell Container Storage Module for Encryption The supported distros for the Dell CSI Drivers are located here.\nHow do I get a list of Container Storage Modules deployed in my cluster with their versions? The easiest way to find the module version is to check the image tag for the module. For all the namespaces you can execute the following:\nkubectl get pods -A -o jsonpath=\"{..image}\" | tr -s '[[:space:]]' '\\n' | grep 'csm\\|karavi' | sort | uniq -c Or if you know the namespace:\nkubectl get deployment,daemonset -o wide -n {{namespace}} Do all Container Storage Modules need to be the same version, or can I mix and match? It is advised to comply with the support matrices (links below) and not deviate from it with mixed versions.\nDell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency Dell Container Storage Module for Application Mobility Dell Container Storage Module for Encryption Dell CSI Drivers. Can I run Container Storage Modules in a production environment? Currently, the Container Storage Modules Authorization, Observability, Replication, and Resiliency are GA and ready for production systems. The modules Encryption and Application Mobility are launched for Tech Preview Release and it is not intended to use in the Production systems.\nIs Dell Container Storage Modules (CSM) supported by Dell Technologies? Yes!\nIf you find an issue, please follow our support process\nCan I modify a module or contribute to the project? Yes!\nAll Container Storage Modules are released as open-source projects under Apache-2.0 License. You are free to contribute directly following the contribution guidelines, fork the projects, modify them, and of course share feedback or open tickets ;-)\nWhat is coming next? This is just the beginning of the journey for Dell Container Storage Modules, and there is a full roadmap with more to come, which you can check under the GitHub Milestones page.\n","categories":"","description":"Frequently asked questions of Dell Technologies (Dell) Container Storage Modules","excerpt":"Frequently asked questions of Dell Technologies (Dell) Container …","ref":"/csm-docs/v2/references/faq/","tags":"","title":"CSM FAQ"},{"body":" What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Where do I start with Dell Container Storage Modules (CSM)? What are the prerequisites for deploying Container Storage Modules? How do I uninstall or disable a module? How do I troubleshoot Container Storage Modules? Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? Should I install the module in the same namespace as the driver or another? Which Kubernetes distributions are supported? How do I get a list of Container Storage Modules deployed in my cluster with their versions? Do all Container Storage Modules need to be the same version, or can I mix and match? Can I run Container Storage Modules in a production environment? Is Dell Container Storage Modules (CSM) supported by Dell Technologies? Can I modify a module or contribute to the project? What is coming next? What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Dell Container Storage Modules are a set of modules that aim to extend features beyond what is available in the CSI specification.\nThe main goal with CSM modules is to expose storage array enterprise features directly within Kubernetes so developers are empowered to leverage them for their deployment in a seamless way.\nWhere do I start with Dell Container Storage Modules (CSM)? The umbrella repository for every Dell Container Storage Module is: https://github.com/dell/csm.\nWhat are the prerequisites for deploying Container Storage Modules? Prerequisites can be found on the respective module deployment pages:\nDell Container Storage Module for Observability Deployment Dell Container Storage Module for Authorization Deployment Dell Container Storage Module for Resiliency Deployment Dell Container Storage Module for Replication Deployment Dell Container Storage Module for Application Mobility Deployment Dell Container Storage Module for Encryption Deployment Prerequisites for deploying the Dell CSI drivers can be found here:\nDell CSI Drivers Deployment How do I uninstall or disable a module? Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Resiliency Dell Container Storage Module for Replication Dell Container Storage Module for Application Mobility Dell Container Storage Module for Encryption How do I troubleshoot Container Storage Modules? Dell CSI Drivers Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency Dell Container Storage Module for Application Mobility Dell Container Storage Module for Encryption Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? No, all the modules have been designed to work inside Kubernetes with Dell CSI drivers.\nShould I install the module in the same namespace as the driver or another? It is recommended to install CSM for Observability in a namespace separate from the Dell CSI drivers because it works across multiple drivers. All other modules either run as standalone or with the Dell CSI driver as a sidecar.\nWhich Kubernetes distributions are supported? The supported Kubernetes distributions for Container Storage Modules are documented:\nDell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency Dell Container Storage Module for Application Mobility Dell Container Storage Module for Encryption The supported distros for the Dell CSI Drivers are located here.\nHow do I get a list of Container Storage Modules deployed in my cluster with their versions? The easiest way to find the module version is to check the image tag for the module. For all the namespaces you can execute the following:\nkubectl get pods -A -o jsonpath=\"{..image}\" | tr -s '[[:space:]]' '\\n' | grep 'csm\\|karavi' | sort | uniq -c Or if you know the namespace:\nkubectl get deployment,daemonset -o wide -n {{namespace}} Do all Container Storage Modules need to be the same version, or can I mix and match? It is advised to comply with the support matrices (links below) and not deviate from it with mixed versions.\nDell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency Dell Container Storage Module for Resiliency Dell Container Storage Module for Application Mobility Dell Container Storage Module for Encryption Dell CSI Drivers. Dell CSI Drivers Can I run Container Storage Modules in a production environment? Currently, the Container Storage Modules Authorization, Observability, Replication, and Resiliency are GA and ready for production systems. The modules Encryption and Application Mobility are launched for Tech Preview Release and it is not intended to use in the Production systems.\nIs Dell Container Storage Modules (CSM) supported by Dell Technologies? Yes!\nIf you find an issue, please follow our support process\nCan I modify a module or contribute to the project? Yes!\nAll Container Storage Modules are released as open-source projects under Apache-2.0 License. You are free to contribute directly following the contribution guidelines, fork the projects, modify them, and of course share feedback or open tickets ;-)\nWhat is coming next? This is just the beginning of the journey for Dell Container Storage Modules, and there is a full roadmap with more to come, which you can check under the GitHub Milestones page.\n","categories":"","description":"Frequently asked questions of Dell Technologies (Dell) Container Storage Modules","excerpt":"Frequently asked questions of Dell Technologies (Dell) Container …","ref":"/csm-docs/v3/references/faq/","tags":"","title":"CSM FAQ"},{"body":"Fields are specified by their path. Consider the following examples:\nField specified by the following path spec.authenticationType=IAM is reflected in their resources YAML as the following: spec: authenticationType: IAM field specified by path spec.protocols=[Azure,GCS] is reflected in their resources YAML as the following: spec: protocols: - Azure - GCS Prerequisites In order to use COSI Driver on ObjectScale platform, the following components MUST be deployed to your cluster:\nKubernetes Container Object Storage Interface CRDs Container Object Storage Interface Controller ℹ️ NOTE: use the official COSI guide to deploy the required components.\nKubernetes Objects Bucket Bucket represents a Bucket or its equivalent in the storage backend. Generally, it should be created only in the brownfield provisioning scenario. The following is a sample manifest of Bucket resource:\napiVersion: objectstorage.k8s.io/v1alpha1 kind: Bucket metadata: name: my-bucket spec: driverName: cosi.dellemc.com bucketClassName: my-bucket-class bucketClaim: my-bucket-claim deletionPolicy: Delete protocols: - S3 parameters: id: \"my.objectscale\" spec.existingBucketID existingBucketID is an optional field that contains the unique id of the bucket in the ObjectScale. This field should be used to specify a bucket that has been created outside of COSI. Due to the fact that the driver supports multiple arrays and multiple ObjectStores from one instance, the existingBucketID needs to have a format of: \u003cConfiguration ID\u003e-\u003cExisting Bucket ID\u003e, e.g. my.objectscale-existing-bucket.\nBucket Claim BucketClaim represents a claim to provision a Bucket. The following is a sample manifest for creating a BucketClaim resource:\napiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketClaim metadata: name: my-bucketclaim namespace: my-namespace spec: bucketClassName: my-bucketclass protocols: [ 'S3' ] Unsupported options spec.protocols=[Azure,GCS] - Protocols are the set of data API this bucket is required to support. From protocols specified by COSI (v1alpha1), Dell ObjectScale platform only supports the S3 protocol. Protocols Azure and GCS MUST NOT be used. Bucket Class Installation of ObjectScale COSI driver does not create BucketClass resource. BucketClass represents a class of Bucket resources with similar characteristics. Dell COSI Driver is a multi-backend driver, meaning that for every platform the specific BucketClass should be created. The BucketClass resource should contain the name of multi-backend driver and parameters.id for specific Object Storage Platform.\nThe default sample is shown below:\napiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketClass metadata: name: my-bucketclass driverName: cosi.dellemc.com deletionPolicy: Delete parameters: id: \"my.objectscale\" deletionPolicy ⚠ WARNING: this field is case sensitive, and the bucket deletion will fail if policy is not set exactly to Delete or Retain.\ndeletionPolicy in BucketClass resource is used to specify how COSI should handle deletion of the bucket. There are two possible values:\nRetain: Indicates that the bucket should not be deleted from the object store. The underlying bucket is not cleaned up when the Bucket object is deleted. With this option, the bucket is unreachable from Kubernetes level. Delete: Indicates that the bucket should be permanently deleted from the object store once all the workloads accessing this bucket are done. The underlying bucket is cleaned up when the Bucket object is deleted. emptyBucket emptyBucket field is set in config YAML file passed to the chart during COSI driver installation. If it is set to true, then the bucket will be emptied before deletion. If it is set to false, then ObjectScale cannot delete the bucket since it is not empty, and it will return an error.\nemptyBucket has no effect when Deletion Policy is set to Retain.\nBucket Access Class Installation of ObjectScale COSI driver does not create BucketAccessClass resource. BucketAccessClass represents a class of BucketAccess resources with similar characteristics. Dell COSI Driver is a multi-backend driver, meaning that for every platform the specific BucketAccessClass should be created. The BucketClass resource should contain the name of multi-backend driver and parameters.id for specific Object Storage Platform. The default sample is shown below:\napiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketAccessClass metadata: name: my-bucketaccessclass driverName: cosi.dellemc.com authenticationType: Key parameters: id: \"my.objectscale\" authenticationType ⚠ WARNING: this field is case sensitive, and the granting access will fail if it is not set exactly to Key or IAM.\nauthenticationType denotes the style of authentication. The only supported option for COSI Driver is Key.\nUnsupported options authenticationType=IAM - denotes the style of authentication. The IAM value MUST NOT be used, because IAM style authentication is not supported. Bucket Access BucketAccess resource represents a access request to generate a Secret, that will allow you to access ObjectStorage . The following is a sample manifest for creating a BucketClaim resource:\napiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketAccess metadata: name: my-bucketaccess namespace: my-namespace spec: bucketClaimName: my-bucketclaim protocol: S3 bucketAccessClassName: my-bucketaccessclass credentialsSecretName: my-s3-secret spec.protocol ⚠ WARNING: this field is case sensitive, and the provisioning will fail if protocol is not set exactly to S3.\nspec.protocol is the name of the Protocol that this access credential is supposed to support.\nUnsupported options spec.serviceAccountName=... - is the name of the serviceAccount that COSI will map to the object storage provider service account when IAM styled authentication is specified. As the IAM style authentication is not supported, this field is also unsupported. spec.protocol=... - Protocols are the set of data API this bucket is required to support. From protocols specified by COSI (v1alpha1), Dell ObjectScale platform only supports the S3 protocol. Protocols Azure and GCS MUST NOT be used. Provisioning Buckets Each bucket is provisioned using default options:\nCategory Parameter Description Default Policy No policy applied on the bucket. Controls No additional controls have been setup for the bucket. Controls Versioning 1. Enabling versioning allows maintaining multiple versions of same object in same bucket. 2. Bucket Versioning can’t be disabled when Object Lock is enabled. Off Controls Object Lock Enabling object lock allows objects to be locked or protected from deletion or overwrite, for a fixed amount of time or indefinitely, depending on the configuration. Off Controls Quotas 1. Block writes at Quota: Represents a hard quota that prevents bucket writes when total object count/size is reached. 2. Notification at Quota: Represents a soft quota value which triggers a notification when total object count/size is reached. Off Controls Encryption If encryption is turned on bucket data will be saved in encrypted form. Off Event Rules The notifications will be sent to the destination when the selected type of events occur on the bucket. No event rules configured for the bucket. Events All Off Events s3:ObjectCreated:Put Off Events s3:ObjectCreated:Copy Off Events s3:ObjectCreated:CompleteMultipartUpload Off Events s3:ObjectCreated:* Off Events s3:ObjectRemoved:Delete Off Events s3:ObjectRemoved:DeleteMarkerCreated Off Events s3:ObjectRemoved:* Off Events s3:Replication:OperationFailedReplication Off Event Rules Prefix Event rule are applied for object names with the given prefix. Off Event Rules Suffix Event rule are applied for object names with the given suffix. Off Event Rules Send To The notification destination used to send notification for selected events. Off Kubernetes Administrator Steps The first step before you can start provisioning object storage, is to create a BucketClass. The BucketClass is an object that defines the provisioning and management characteristics of Bucket resources. It acts as an abstraction layer between users (such as applications or pods) and the underlying object storage infrastructure. BucketClass allows you to dynamically provision and manage Buckets in a consistent and automated manner.\nThe following example shows how to create a BucketClass:\ncat \u003c\u003cEOF | kubectl create --filename - apiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketClass metadata: name: my-bucketclass driverName: cosi.dellemc.com deletionPolicy: Delete parameters: id: \"my.objectscale\" EOF End-user Steps Greenfield Provisioning Greenfield Provisioning means creating a new bucket from scratch, without any existing data.\nThe following example shows how to create a BucketClaim for greenfield provisioning.\ncat \u003c\u003cEOF | kubectl create --namespace=my-namespace --filename - apiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketClaim metadata: name: my-bucketclaim spec: bucketClassName: my-bucketclass protocols: [ 'S3' ] EOF ℹ️ NOTE: remember to replace my-namespace, my-bucketclass and my-bucketclaim with actual values.\nBrownfield Provisioning Brownfield Provisioning means using an existing bucket, that can already contain the data. This differs slightly from Greenfield Provisioning, as we need to create both Bucket and BucketClaim manually.\nThe following example shows how to create Bucket and BucketClaim for brownfield provisioning.\ncat \u003c\u003cEOF | kubectl create --namespace=my-namespace --filename - apiVersion: objectstorage.k8s.io/v1alpha1 kind: Bucket metadata: name: my-brownfield-bucket spec: bucketClaim: {} bucketClassName: my-bucketclass deletionPolicy: Retain driverName: cosi.dellemc.com existingBucketID: my.objectscale-my-existing-bucket parameters: id: my.objectscale protocols: [ 'S3' ] --- apiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketClaim metadata: name: my-brownfield-bucketclaim spec: existingBucketName: my-brownfield-bucket protocols: [ 'S3' ] EOF ℹ️ NOTE: remember to replace my-namespace, existing-bucket-name and my-bucketclaim with actual values.\nDeleting Buckets There are a few crucial details regarding bucket deletion. The first one is deletionPolicy which is used to specify how COSI should handle deletion of a bucket. It is found in BucketClass resource and can be set to Delete and Retain. The second crucial detail is emptyBucket field in the Helm Chart configuration.\nThe following example shows how to delete a BucketClaim.\nkubectl --namespace=my-namespace delete bucketclaim my-bucketclaim ℹ️ NOTE: remember to replace my-namespace and my-bucketclaim with actual values.\nGranting Access Kubernetes Administrator Steps The first step before you start granting access to the object storage for your application, is to create a BucketAccessClass. The BucketAccessClass is an object that defines the access management characteristics of Bucket resources. It acts as an abstraction layer between users (such as applications or pods) and the underlying object storage infrastructure. BucketAccessClass allows you to dynamically grant access to Buckets in a consistent and automated manner.\nThe following example shows how to create a BucketAccessClass:\ncat \u003c\u003cEOF | kubectl create --filename - apiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketAccessClass metadata: name: my-bucketaccessclass driverName: cosi.dellemc.com authenticationType: Key parameters: id: \"my.objectscale\" EOF End-user Steps ⚠ WARNING: only full access granting is supported.\nThe underlying workflow for granting access to the object storage primitive is:\nuser is added to particular account in the ObjectScale; bucket policy is modified to reflect that user has gained permissions for a bucket; access key for the user is added to ObjectScale. The following example shows how to grant an access using BucketAccess resource:\ncat \u003c\u003cEOF | kubectl create --namespace=my-namespace --filename - apiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketAccess metadata: name: my-bucketaccess spec: bucketAccessClassName: my-bucketaccessclass bucketClaimName: my-bucketclaim credentialsSecretName: my-s3-secret protocol: S3 EOF ℹ️ NOTE: remember to replace my-namespace, my-bucketaccessclass, my-bucketclaim, my-s3-secret and my-bucketaccess with actual values.\nRevoking Access This feature revokes a user’s previously granted access to a particular bucket. When resource of BucketAccess kind is removed from Kubernetes it triggers the process:\naccess key is removed from ObjectScale; bucket policy is modified to reflect that user has lost permissions for a bucket; user is removed from ObjectScale. The following example shows how to revoke a BucketAccess:\nkubectl --namespace=my-namespace delete bucketaccess my-bucketaccess ℹ️ NOTE: remember to replace my-namespace and my-bucketaccess with actual values.\n","categories":"","description":"Code features for ObjectScale COSI Driver","excerpt":"Code features for ObjectScale COSI Driver","ref":"/csm-docs/docs/cosidriver/features/objectscale/","tags":"","title":"ObjectScale"},{"body":" Notational Conventions\nThe keywords “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD NOT”, “RECOMMENDED”, “NOT RECOMMENDED”, “MAY”, and “OPTIONAL” are to be interpreted as described in RFC 2119 (Bradner, S., “Key words for use in RFCs to Indicate Requirement Levels”, BCP 14, RFC 2119, March 1997).\nFields are specified by their path. Consider the following examples:\nField specified by the following path spec.authenticationType=IAM is reflected in their resources YAML as the following: spec: authenticationType: IAM field specified by path spec.protocols=[Azure,GCS] is reflected in their resources YAML as the following: spec: protocols: - Azure - GCS Prerequisites In order to use COSI Driver on ObjectScale platform, the following components MUST be deployed to your cluster:\nKubernetes Container Object Storage Interface CRDs Container Object Storage Interface Controller ℹ️ NOTE: use the official COSI guide to deploy the required components.\nKubernetes Objects Bucket Bucket represents a Bucket or its equivalent in the storage backend. Generally, it should be created only in the brownfield provisioning scenario. The following is a sample manifest of Bucket resource:\napiVersion: objectstorage.k8s.io/v1alpha1 kind: Bucket metadata: name: my-bucket spec: driverName: cosi.dellemc.com bucketClassName: my-bucket-class bucketClaim: my-bucket-claim deletionPolicy: Delete protocols: - S3 parameters: id: \"my.objectscale\" spec.existingBucketID existingBucketID is an optional field that contains the unique id of the bucket in the ObjectScale. This field should be used to specify a bucket that has been created outside of COSI. Due to the fact that the driver supports multiple arrays and multiple ObjectStores from one instance, the existingBucketID needs to have a format of: \u003cConfiguration ID\u003e-\u003cExisting Bucket ID\u003e, e.g. my.objectscale-existing-bucket.\nBucket Claim BucketClaim represents a claim to provision a Bucket. The following is a sample manifest for creating a BucketClaim resource:\napiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketClaim metadata: name: my-bucketclaim namespace: my-namespace spec: bucketClassName: my-bucketclass protocols: [ 'S3' ] Unsupported options spec.protocols=[Azure,GCS] - Protocols are the set of data API this bucket is required to support. From protocols specified by COSI (v1alpha1), Dell ObjectScale platform only supports the S3 protocol. Protocols Azure and GCS MUST NOT be used. Bucket Class Installation of ObjectScale COSI driver does not create BucketClass resource. BucketClass represents a class of Bucket resources with similar characteristics. Dell COSI Driver is a multi-backend driver, meaning that for every platform the specific BucketClass should be created. The BucketClass resource should contain the name of multi-backend driver and parameters.id for specific Object Storage Platform.\nThe default sample is shown below:\napiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketClass metadata: name: my-bucketclass driverName: cosi.dellemc.com deletionPolicy: Delete parameters: id: \"my.objectscale\" deletionPolicy ⚠ WARNING: this field is case sensitive, and the bucket deletion will fail if policy is not set exactly to Delete or Retain.\ndeletionPolicy in BucketClass resource is used to specify how COSI should handle deletion of the bucket. There are two possible values:\nRetain: Indicates that the bucket should not be deleted from the object store. The underlying bucket is not cleaned up when the Bucket object is deleted. With this option, the bucket is unreachable from Kubernetes level. Delete: Indicates that the bucket should be permanently deleted from the object store once all the workloads accessing this bucket are done. The underlying bucket is cleaned up when the Bucket object is deleted. emptyBucket emptyBucket field is set in config YAML file passed to the chart during COSI driver installation. If it is set to true, then the bucket will be emptied before deletion. If it is set to false, then ObjectScale cannot delete the bucket since it is not empty, and it will return an error.\nemptyBucket has no effect when Deletion Policy is set to Retain.\nBucket Access Class Installation of ObjectScale COSI driver does not create BucketAccessClass resource. BucketAccessClass represents a class of BucketAccess resources with similar characteristics. Dell COSI Driver is a multi-backend driver, meaning that for every platform the specific BucketAccessClass should be created. The BucketClass resource should contain the name of multi-backend driver and parameters.id for specific Object Storage Platform. The default sample is shown below:\napiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketAccessClass metadata: name: my-bucketaccessclass driverName: cosi.dellemc.com authenticationType: Key parameters: id: \"my.objectscale\" authenticationType ⚠ WARNING: this field is case sensitive, and the granting access will fail if it is not set exactly to Key or IAM.\nauthenticationType denotes the style of authentication. The only supported option for COSI Driver is Key.\nUnsupported options authenticationType=IAM - denotes the style of authentication. The IAM value MUST NOT be used, because IAM style authentication is not supported. Bucket Access BucketAccess resource represents a access request to generate a Secret, that will allow you to access ObjectStorage . The following is a sample manifest for creating a BucketClaim resource:\napiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketAccess metadata: name: my-bucketaccess namespace: my-namespace spec: bucketClaimName: my-bucketclaim protocol: S3 bucketAccessClassName: my-bucketaccessclass credentialsSecretName: my-s3-secret spec.protocol ⚠ WARNING: this field is case sensitive, and the provisioning will fail if protocol is not set exactly to S3.\nspec.protocol is the name of the Protocol that this access credential is supposed to support.\nUnsupported options spec.serviceAccountName=... - is the name of the serviceAccount that COSI will map to the object storage provider service account when IAM styled authentication is specified. As the IAM style authentication is not supported, this field is also unsupported. spec.protocol=... - Protocols are the set of data API this bucket is required to support. From protocols specified by COSI (v1alpha1), Dell ObjectScale platform only supports the S3 protocol. Protocols Azure and GCS MUST NOT be used. Provisioning Buckets Each bucket is provisioned using default options:\nCategory Parameter Description Default Policy No policy applied on the bucket. Controls No additional controls have been setup for the bucket. Controls Versioning 1. Enabling versioning allows maintaining multiple versions of same object in same bucket. 2. Bucket Versioning can’t be disabled when Object Lock is enabled. Off Controls Object Lock Enabling object lock allows objects to be locked or protected from deletion or overwrite, for a fixed amount of time or indefinitely, depending on the configuration. Off Controls Quotas 1. Block writes at Quota: Represents a hard quota that prevents bucket writes when total object count/size is reached. 2. Notification at Quota: Represents a soft quota value which triggers a notification when total object count/size is reached. Off Controls Encryption If encryption is turned on bucket data will be saved in encrypted form. Off Event Rules The notifications will be sent to the destination when the selected type of events occur on the bucket. No event rules configured for the bucket. Events All Off Events s3:ObjectCreated:Put Off Events s3:ObjectCreated:Copy Off Events s3:ObjectCreated:CompleteMultipartUpload Off Events s3:ObjectCreated:* Off Events s3:ObjectRemoved:Delete Off Events s3:ObjectRemoved:DeleteMarkerCreated Off Events s3:ObjectRemoved:* Off Events s3:Replication:OperationFailedReplication Off Event Rules Prefix Event rule are applied for object names with the given prefix. Off Event Rules Suffix Event rule are applied for object names with the given suffix. Off Event Rules Send To The notification destination used to send notification for selected events. Off Kubernetes Administrator Steps The first step before you can start provisioning object storage, is to create a BucketClass. The BucketClass is an object that defines the provisioning and management characteristics of Bucket resources. It acts as an abstraction layer between users (such as applications or pods) and the underlying object storage infrastructure. BucketClass allows you to dynamically provision and manage Buckets in a consistent and automated manner.\nThe following example shows how to create a BucketClass:\ncat \u003c\u003cEOF | kubectl create --filename - apiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketClass metadata: name: my-bucketclass driverName: cosi.dellemc.com deletionPolicy: Delete parameters: id: \"my.objectscale\" EOF End-user Steps Greenfield Provisioning Greenfield Provisioning means creating a new bucket from scratch, without any existing data.\nThe following example shows how to create a BucketClaim for greenfield provisioning.\ncat \u003c\u003cEOF | kubectl create --namespace=my-namespace --filename - apiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketClaim metadata: name: my-bucketclaim spec: bucketClassName: my-bucketclass protocols: [ 'S3' ] EOF ℹ️ NOTE: remember to replace my-namespace, my-bucketclass and my-bucketclaim with actual values.\nBrownfield Provisioning Brownfield Provisioning means using an existing bucket, that can already contain the data. This differs slightly from Greenfield Provisioning, as we need to create both Bucket and BucketClaim manually.\nThe following example shows how to create Bucket and BucketClaim for brownfield provisioning.\ncat \u003c\u003cEOF | kubectl create --namespace=my-namespace --filename - apiVersion: objectstorage.k8s.io/v1alpha1 kind: Bucket metadata: name: my-brownfield-bucket spec: bucketClaim: {} bucketClassName: my-bucketclass deletionPolicy: Retain driverName: cosi.dellemc.com existingBucketID: my.objectscale-my-existing-bucket parameters: id: my.objectscale protocols: [ 'S3' ] --- apiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketClaim metadata: name: my-brownfield-bucketclaim spec: existingBucketName: my-brownfield-bucket protocols: [ 'S3' ] EOF ℹ️ NOTE: remember to replace my-namespace, existing-bucket-name and my-bucketclaim with actual values.\nDeleting Buckets There are a few crucial details regarding bucket deletion. The first one is deletionPolicy which is used to specify how COSI should handle deletion of a bucket. It is found in BucketClass resource and can be set to Delete and Retain. The second crucial detail is emptyBucket field in the Helm Chart configuration.\nThe following example shows how to delete a BucketClaim.\nkubectl --namespace=my-namespace delete bucketclaim my-bucketclaim ℹ️ NOTE: remember to replace my-namespace and my-bucketclaim with actual values.\nGranting Access Kubernetes Administrator Steps The first step before you start granting access to the object storage for your application, is to create a BucketAccessClass. The BucketAccessClass is an object that defines the access management characteristics of Bucket resources. It acts as an abstraction layer between users (such as applications or pods) and the underlying object storage infrastructure. BucketAccessClass allows you to dynamically grant access to Buckets in a consistent and automated manner.\nThe following example shows how to create a BucketAccessClass:\ncat \u003c\u003cEOF | kubectl create --filename - apiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketAccessClass metadata: name: my-bucketaccessclass driverName: cosi.dellemc.com authenticationType: Key parameters: id: \"my.objectscale\" EOF End-user Steps ⚠ WARNING: only full access granting is supported.\nThe underlying workflow for granting access to the object storage primitive is:\nuser is added to particular account in the ObjectScale; bucket policy is modified to reflect that user has gained permissions for a bucket; access key for the user is added to ObjectScale. The following example shows how to grant an access using BucketAccess resource:\ncat \u003c\u003cEOF | kubectl create --namespace=my-namespace --filename - apiVersion: objectstorage.k8s.io/v1alpha1 kind: BucketAccess metadata: name: my-bucketaccess spec: bucketAccessClassName: my-bucketaccessclass bucketClaimName: my-bucketclaim credentialsSecretName: my-s3-secret protocol: S3 EOF ℹ️ NOTE: remember to replace my-namespace, my-bucketaccessclass, my-bucketclaim, my-s3-secret and my-bucketaccess with actual values.\nRevoking Access This feature revokes a user’s previously granted access to a particular bucket. When resource of BucketAccess kind is removed from Kubernetes it triggers the process:\naccess key is removed from ObjectScale; bucket policy is modified to reflect that user has lost permissions for a bucket; user is removed from ObjectScale. The following example shows how to revoke a BucketAccess:\nkubectl --namespace=my-namespace delete bucketaccess my-bucketaccess ℹ️ NOTE: remember to replace my-namespace and my-bucketaccess with actual values.\n","categories":"","description":"Code features for ObjectScale COSI Driver","excerpt":"Code features for ObjectScale COSI Driver","ref":"/csm-docs/v1/cosidriver/features/objectscale/","tags":"","title":"ObjectScale"},{"body":"Volume Snapshot Feature The CSI PowerFlex driver versions 2.0 and higher support v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class Installation of PowerFlex driver v1.5 and later does not create VolumeSnapshotClass. You can find a sample of a default v1 VolumeSnapshotClass instance in samples/volumesnapshotclass directory. If needed, you can install the default sample. Following is the default sample for v1:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com # Configure what happens to a VolumeSnapshotContent when the VolumeSnapshot object # it is bound to is to be deleted # Allowed values: # Delete: the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. # Retain: both the underlying snapshot and VolumeSnapshotContent remain. deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Create Consistent Snapshot of Group of Volumes This feature extends CSI specification to add the capability to create crash-consistent snapshots of a group of volumes. This feature is available as a technical preview. To use this feature, users have to deploy the csi-volumegroupsnapshotter side-car as part of the PowerFlex driver. Once the sidecar has been deployed, users can make snapshots by using yaml files, More information can be found here: Volume Group Snapshotter.\nVolume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, which is when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true.\nFollowing is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the next highest multiple of 8.\nVolume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nCustom File System Format Options The CSI PowerFlex driver version 1.5 and later support additional mkfs format options. A user is able to specify additional format options as needed for the driver. Format options are specified in storageclass yaml under mkfsFormatOption as in the following example:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \u003cSTORAGE_POOL\u003e # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID mkfsFormatOption: \"\u003cmkfs_format_option\u003e\" # Insert file system format option volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e # Insert System ID values: - csi-vxflexos.dellemc.com WARNING: Before utilizing format options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option. Topology Support The CSI PowerFlex driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\nThe PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed. This Topology support does not include customer-defined topology, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that the pod schedule takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2020-05-27T13:24:55Z\" labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \"170198\" selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\nNOTE: In the manifest file of the Dell CSM operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\nController HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\nNOTE: If the controller count is greater than the number of available nodes, excess controller pods will be stuck in a pending state.\nIf you are using the Dell CSM Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file at csi-vxflexos/helm/csi-vxflexos/values.yaml:\nTo assign controller pods to worker nodes only (Default):\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: # - key: \"node-role.kubernetes.io/master\" # operator: \"Exists\" # effect: \"NoSchedule\" To assign controller pods to master and worker nodes:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" To assign controller pods to master nodes only:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" NOTE: Tolerations/selectors work the same way for node pods.\nFor configuring Controller HA on the Dell CSM Operator, please refer to the Dell CSM Operator documentation.\nSDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Kubernetes nodes which run the node portion of the CSI driver. The deployment of the SDC kernel module occurs on these nodes with OS platforms which support automatic SDC deployment: currently Red Hat CoreOS (RHCOS), RHEL8.x,RHEL 7.9 are the only supported OS platforms. On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\nOn Kubernetes nodes which run the node portion of the CSI driver, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the nodes with OS version which supports automatic SDC deployment. If there is an SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On nodes that do not support automatic SDC deployment by SDC init container, manual installation steps must be followed. The SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions. There is no automated uninstallation of the SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from the node. Multiarray Support The CSI PowerFlex driver version 1.4 added support for managing multiple PowerFlex arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration There is a sample yaml file in the samples folder called secret.yaml with the following content:\n# Username for accessing PowerFlex system. # If authorization is enabled, username will be ignored. - username: \"admin\" # Password for accessing PowerFlex system. # If authorization is enabled, password will be ignored. password: \"password\" # PowerFlex system name or ID.\t# Required: true systemID: \"1a99aa999999aa9a\" # Required: false # Previous names used in secret of PowerFlex system. Only needed if PowerFlex System Name has been changed by user # and old resources are still based on the old name. allSystemNames: \"pflex-1,pflex-2\" # REST API gateway HTTPS endpoint for PowerFlex system. # If authorization is enabled, endpoint should be the HTTPS localhost endpoint that # the authorization sidecar will listen on endpoint: \"https://127.0.0.1\" # Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. # Allowed values: true or false # Default value: true skipCertificateValidation: true # indicates if this array is the default array # needed for backwards compatibility # only one array is allowed to have this set to true # Default value: false isDefault: true # defines the MDM(s) that SDC should register with on start. # Allowed values: a list of IP addresses or hostnames separated by comma. # Default value: none mdm: \"10.0.0.1,10.0.0.2\" # Defines all system names used to create powerflex volumes # Required: false # Default value: none AllSystemNames: \"name1,name2\" - username: \"admin\" password: \"Password123\" systemID: \"2b11bb111111bb1b\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true mdm: \"10.0.0.3,10.0.0.4\" AllSystemNames: \"name1,name2\" The systemID can be found by displaying system level information, which is outlined here\nHere we specify that we want the CSI driver to manage two arrays: one with an IP 127.0.0.1 and the other with an IP 127.0.0.2.\nTo use this config we need to create a Kubernetes secret from it. To do so, run the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=secret.yaml Dynamic Array Configuration To update or change any array configuration property, edit the secret. The driver will detect the change automatically and use the new values based on the Kubernetes watcher file change detection time. You can use kubectl command to delete the current secret and create a new secret with changes. For example, refer yaml above and change only the password.\n- username: \"admin\" password: \"Password123\" to\n- username: \"admin\" password: \"Password456\" Below are sample command lines to delete a secret and create modified properties from file secret.yaml.\nkubectl delete secret vxflexos-config -n vxflexos kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=./secret.yaml Dynamic array configuration change detection is only used for properties of an existing array, like username or password. To add a new array to the secret, or to alter an array’s mdm field, you must run csi-install.sh with --upgrade option to update the MDM key in secret and restart the node pods.\ncd \u003cDRIVER-HOME\u003e/dell-csi-helm-installer ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade kubectl delete pods --all -n vxflexos Creating storage classes To be able to provision Kubernetes volumes using a specific array, we need to create corresponding storage classes.\nFind the sample yaml files under samples/storageclass. Edit storageclass.yaml if you want ext4 filesystem, and use storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have, and replace \u003cSYSTEM_ID\u003e with the system ID or system name for the array you’d like to use.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl apply -f storageclass.yaml After that, you can use the storage class for the corresponding array.\nEphemeral Inline Volume Starting from version 1.4, CSI PowerFlex driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest (found in csi-vxflexos/test/helm/ephemeral) for creating ephemeral volume in pod manifest with CSI PowerFlex driver.\nkind: Pod apiVersion: v1 metadata: name: my-csi-app-inline-volumes spec: containers: - name: my-frontend image: busybox command: [ \"sleep\", \"100000\" ] volumeMounts: - mountPath: \"/data0\" name: my-csi-volume - mountPath: \"/data1\" name: my-csi-volume-xfs volumes: - name: my-csi-volume csi: driver: csi-vxflexos.dellemc.com fsType: \"ext4\" volumeAttributes: volumeName: \"my-csi-volume\" size: \"8Gi\" storagepool: sample systemID: sample - name: my-csi-volume-xfs csi: driver: csi-vxflexos.dellemc.com fsType: \"xfs\" volumeAttributes: volumeName: \"my-csi-volume-xfs\" size: \"10Gi\" storagepool: sample systemID: sample This manifest creates a pod and attach two newly created ephemeral inline csi volumes to it, one ext4 and the other xfs.\nTo run the corresponding helm test, go to csi-vxflexos/test/helm/ephemeral and fill in the values for storagepool and systemID in sample.yaml.\nThen run:\n./testEphemeral.sh this test deploys the pod with two ephemeral volumes, and write some data to them before deleting the pod.\nWhen creating ephemeral volumes, it is important to specify the following within the volumeAttributes section: volumeName, size, storagepool, and if you want to use a non-default array, systemID.\nConsuming Existing Volumes with Static Provisioning To use existing volumes from PowerFlex array as Peristent volumes in your Kubernetes environment, perform these steps:\nLog into one of the MDMs of the PowerFlex cluster. Execute these commands to retrieve the systemID and volumeID. scli --mdm_ip \u003cIPs, comma separated\u003e --login --username \u003cusername\u003e --password \u003cpassword\u003e Output: Logged in. User role is SuperUser. System ID is \u003csystemID\u003e scli --query_volume --volume_name \u003cvolume name\u003e Output: Volume ID: \u003cvolumeID\u003e Name: \u003cvolume name\u003e Create PersistentVolume and use this volume ID in the volumeHandle with the format systemID-volumeID in the manifest. Modify other parameters according to your needs. apiVersion: v1 kind: PersistentVolume metadata: name: existingVol spec: capacity: storage: 8Gi csi: driver: csi-vxflexos.dellemc.com volumeHandle: \u003csystemID\u003e-\u003cvolumeID\u003e volumeMode: Filesystem accessModes: - ReadWriteOnce storageClassName: vxflexos Create PersistentVolumeClaim to use this PersistentVolume. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: vxflexos Then use this PVC as a volume in a pod. apiVersion: v1 kind: Pod metadata: name: static-prov-pod spec: containers: - name: test image: busybox command: [ \"sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data0\" name: pvol volumes: - name: pvol persistentVolumeClaim: claimName: pvol After the pod is Ready and Running, you can start to use this pod and volume. Note: Retrieval of the volume ID is possible through the UI. You must select the volume, navigate to Details section and click the volume in the graph. This selection will set the filter to the desired volume. At this point the volume ID can be found in the URL.\nDynamic Logging Configuration The dynamic logging configuration that was introduced in v1.5 of the driver was revamped for v2.0; v1.5 logging configuration is not compatible with v2.0.\nTwo fields in values.yaml (located at helm/csi-vxflexos/values.yaml) are used to configure the dynamic logging: logLevel and logFormat.\n# CSI driver log level # Allowed values: \"error\", \"warn\"/\"warning\", \"info\", \"debug\" # Default value: \"debug\" logLevel: \"debug\" # CSI driver log format # Allowed values: \"TEXT\" or \"JSON\" # Default value: \"TEXT\" logFormat: \"TEXT\" To change the logging fields after the driver is deployed, you can use this command to edit the configmap:\nkubectl edit configmap -n vxflexos vxflexos-config-params and then make the necessary adjustments for CSI_LOG_LEVEL and CSI_LOG_FORMAT.\nIf either option is set to a value outside of what is supported, the driver will use the default values of “debug” and “text” .\nVolume Health Monitoring NOTE: This feature requires the alpha feature gate, CSIVolumeHealth to be set to true. If the feature gate is on, and you want to use this feature, ensure the proper values are enabled in your values file. See the values table in the installation doc for more details.\nStarting in version 2.1, CSI Driver for PowerFlex now supports volume health monitoring. This allows Kubernetes to report on the condition of the underlying volumes via events when a volume condition is abnormal. For example, if a volume were to be deleted from the array, or unmounted outside of Kubernetes, Kubernetes will now report these abnormal conditions as events.\nTo accomplish this, the driver utilizes the external-health-monitor sidecar. When driver detects a volume condition is abnormal, the sidecar will report an event to the corresponding PVC. For example, in this event from kubectl describe pvc -n \u003cns\u003e we can see that the underlying volume was deleted from the PowerFlex array:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------ Warning VolumeConditionAbnormal 32s csi-pv-monitor-controller-csi-vxflexos.dellemc.com Volume is not found at 2021-11-03 20:31:04 Events will also be reported to pods that have abnormal volumes. In these two events from kubectl describe pods -n \u003cns\u003e, we can see that this pod has two abnormal volumes: one volume was unmounted outside of Kubernetes, while another was deleted from PowerFlex array.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------ Warning VolumeConditionAbnormal 35s (x9 over 12m) kubelet Volume vol4: volPath: /var/.../rhel-705f0dcbf1/mount is not mounted: \u003cnil\u003e Warning VolumeConditionAbnormal 5s kubelet Volume vol2: Volume is not found by node driver at 2021-11-11 02:04:49 Set QoS Limits Starting in version 2.5, CSI Driver for PowerFlex now supports setting the limits for the bandwidth and IOPS that one SDC generates for the specified volume. This enables the CSI driver to control the quality of service (QoS). In this release this is supported at the StorageClass level, so once a volume is created QoS Settings can’t be adjusted later. To accomplish this, two new parameters are introduced in the storage class: bandwidthLimitInKbps and iopsLimit.\nEnsure that the proper values are enabled in your storage class yaml files. Refer to the sample storage class yamls for more details.\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \"pool2\" # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID bandwidthLimitInKbps: \"10240\" # Insert bandwidth limit in Kbps iopsLimit: \"11\" # Insert iops limit csi.storage.k8s.io/fstype: ext4 volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e # Insert System ID values: - csi-vxflexos.dellemc.com Once the volume gets created, the ControllerPublishVolume will set the QoS limits for the volumes mapped to SDC.\nRename SDC Starting with version 2.6, the CSI driver for PowerFlex will support renaming of SDCs. To use this feature, the node section of values.yaml should have renameSDC keys enabled with a prefix value.\nTo enable renaming of SDC, make the following edits to values.yaml file:\n# \"node\" allows to configure node specific parameters node: ... ... # \"renameSDC\" defines the rename operation for SDC # Default value: None renameSDC: # enabled: Enable/Disable rename of SDC # Allowed values: # true: enable renaming # false: disable renaming # Default value: \"false\" enabled: false # \"prefix\" defines a string for the new name of the SDC. # \"prefix\" + \"worker_node_hostname\" should not exceed 31 chars. # Default value: none # Examples: \"rhel-sdc\", \"sdc-test\" prefix: \"sdc-test\" The renameSDC section is going to be used by the Node Service, it has two keys enabled and prefix:\nenabled: Boolean variable that specifies if the renaming for SDC is to be carried out or not. If true then the driver will perform the rename operation. By default, its value will be false. prefix: string variable that is used to set the prefix for SDC name. Based on these two keys, there are certain scenarios on which the driver is going to perform the rename SDC operation:\nIf enabled and prefix given then set the prefix+worker_node_name for SDC name. If enabled and prefix not given then set worker_node_name for SDC name. NOTE: name of the SDC cannot be more than 31 characters, hence the prefix given and the worker node hostname name taken should be such that the total length does not exceed 31 character limit.\nPre-approving SDC by GUID Starting with version 2.6, the CSI Driver for PowerFlex will support pre-approving SDC by GUID. CSI PowerFlex driver will detect the SDC mode set on the PowerFlex array and will request SDC approval from the array prior to publishing a volume. This is specific to each SDC.\nTo request SDC approval for GUID, make the following edits to values.yaml file:\n# \"node\" allows to configure node specific parameters node: ... ... # \"approveSDC\" defines the approve operation for SDC # Default value: None approveSDC: # enabled: Enable/Disable SDC approval #Allowed values: # true: Driver will attempt to approve restricted SDC by GUID during setup # false: Driver will not attempt to approve restricted SDC by GUID during setup # Default value: false enabled: false NOTE: Currently, the CSI-PowerFlex driver only supports GUID for the restricted SDC mode.\nIf SDC approval is denied, then provisioning of the volume will not be attempted and an appropriate error message is reported in the logs/events so the user is informed.\nVolume Limit The CSI Driver for Dell PowerFlex allows users to specify the maximum number of PowerFlex volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-vxflexos-volumes-per-node and specifying the volume limit for that node. kubectl label node \u003cnode_name\u003e max-vxflexos-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxVxflexosVolumesPerNode attribute in values.yaml file.\nNOTE: To reflect the changes after setting the value either via node label or in values.yaml file, user has to bounce the driver controller and node pods using the command kubectl get pods -n vxflexos --no-headers=true | awk '/vxflexos-/{print $1}'| xargs kubectl delete -n vxflexos pod. If the value is set both by node label and values.yaml file then node label value will get the precedence and user has to remove the node label in order to reflect the values.yaml value. The default value of maxVxflexosVolumesPerNode is 0. If maxVxflexosVolumesPerNode is set to zero, then Container Orchestration decides how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxVxflexosVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-vxflexos-volumes-per-node is not set.\nNFS volume support Starting with version 2.8, the CSI driver for PowerFlex will support NFS volumes for PowerFlex storage systems version 4.0.x.\nCSI driver will support following operations for NFS volumes:\nCreation and deletion of a NFS volume with RWO/RWX/ROX access modes. Support of tree quotas while volume creation. Expand the size of a NFS volume. Creation and deletion of snapshot of a NFS volume while retaining file permissions. Create NFS volume from the snapshot. To enable the support of NFS volumes operations from CSI driver, there are a few new keys introduced which needs to be set before performing the operations for NFS volumes.\nnasName: defines the NAS server name that should be used for NFS volumes. enableQuota: when enabled will set quota limit for a newly provisioned NFS volume. NOTE:\nnasName nasName is a mandatory parameter and has to be provided in secret yaml, else it will be an error state and will be captured in driver logs. nasName can be given at storage class level as well. If specified in both, secret and storage class, then precedence is given to storage class value. If nasName not given in secret, irrespective of it specified in SC, then it’s an error state and will be captured in driver logs. If the PowerFlex storage system v4.0.x is configured with only block capabilities, then the user is required to give the default value for nasName as “none”. The user has to update the secret.yaml, values.yaml and storageclass-nfs.yaml with the above keys as like below:\nsamples/secret.yaml\n- username: \"admin\" password: \"Password123\" systemID: \"2b11bb111111bb1b\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true isDefault: true mdm: \"10.0.0.3,10.0.0.4\" nasName: \"nas-server\" samples/storageclass/storageclass-nfs.yaml\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-nfs provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \"pool2\" # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID csi.storage.k8s.io/fstype: nfs nasName: \"nas-server\" # path: /csi # softLimit: \"80\" # gracePeriod: \"86400\" volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e-nfs # Insert System ID values: - \"true\" helm/csi-vxflexos/values.yaml\n... enableQuota: false ... Usage of Quotas to Limit Storage Consumption for NFS volumes Starting with version 2.8, the CSI driver for PowerFlex will support enabling tree quotas for limiting capacity for NFS volumes. To use the quota feature user can specify the boolean value enableQuota in values.yaml.\nTo enable quota for NFS volumes, make the following edits to values.yaml file:\n... ... # enableQuota: a boolean that, when enabled, will set quota limit for a newly provisioned NFS volume. # Allowed values: # true: set quota for volume # false: do not set quota for volume # Optional: true # Default value: none enableQuota: true ... ... For example, if the user creates a PVC with 3 Gi of storage and quotas have already been enabled in PowerFlex system for the specified volume.\nWhen enableQuota is set to true\nThe driver sets the hard limit of the PVC to 3Gi. The user adds data of 2Gi to the PVC (by logging into POD). It works as expected. The user tries to add 2Gi more data. Driver doesn’t allow the user to enter more data as total data to be added is 4Gi and PVC limit is 3Gi. The user can expand the volume from 3Gi to 6Gi. The driver allows it and sets the hard limit of PVC to 6Gi. User retries adding 2Gi more data (which has been errored out previously). The driver accepts the data. When enableQuota is set to false\nDriver doesn’t set any hard limit against the PVC created. The user adds 2Gi data to the PVC, which has a limit of 3Gi. It works as expected. The user tries to add 2Gi more data. Now the total size of data is 4Gi. Driver allows the user to enter more data irrespective of the initial PVC size (since no quota is set against this PVC) The user can expand the volume from an initial size of 3Gi to 4Gi or more. The driver allows it. If enableQuota feature is set, user can also set other tree quota parameters such as soft limit, soft grace period and path using storage class yaml file.\npath: relative path to the root of the associated NFS volume. softLimit: soft limit set to quota. Specified as a percentage w.r.t. PVC size. gracePeriod: grace period of quota, must be mentioned along with softLimit, in seconds. Soft Limit can be exceeded until the grace period. NOTE:\nhardLimit is set to same size as that of PVC size. When a volume with quota enabled is expanded then the hardLimit and softLimit are also recalculated by driver w.r.t. to the new PVC size. sofLimit cannot be set to unlimited value (0), otherwise it will become greater than hardLimit (PVC size). softLimit should be lesser than 100%, since hardLimit will be set to 100% (PVC size) internally by the driver. Storage Class Example with Quota Limit Parameters samples/storageclass/storageclass-nfs.yaml\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-nfs provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \"pool2\" # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID csi.storage.k8s.io/fstype: nfs nasName: \"nas-server\" path: /csi softLimit: \"80\" gracePeriod: \"86400\" volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e-nfs # Insert System ID values: - \"true\" Configuring custom access to NFS exports CSI PowerFlex driver Version 2.9.0 and later supports the ability to configure NFS access to nodes that use dedicated storage networks.\nTo enable this feature you need to specify externalAccess parameter in your helm values.yaml file or X_CSI_POWERFLEX_EXTERNAL_ACCESS variable when creating CustomResource using an operator.\nThe value of that parameter is added as an additional entry to NFS Export host access.\nFor example the following notation:\nexternalAccess: \"10.0.0.0/24\" This means that we allow for NFS Export created by driver to be consumed by address range 10.0.0.0-10.0.0.255.\nStorage Capacity Tracking CSI-PowerFlex driver version 2.8.0 and above supports Storage Capacity Tracking.\nThis feature helps the scheduler to make more informed choices about where to schedule pods which depend on unbound volumes with late binding (aka “wait for first consumer”). Pods will be scheduled on a node (satisfying the topology constraints) only if the requested capacity is available on the storage array. If such a node is not available, the pods stay in Pending state. This means pods are not scheduled.\nWithout storage capacity tracking, pods get scheduled on a node satisfying the topology constraints. If the required capacity is not available, volume attachment to the pods fails, and pods remain in ContainerCreating state. Storage capacity tracking eliminates unnecessary scheduling of pods when there is insufficient capacity.\nThe attribute storageCapacity.enabled in values.yaml can be used to enable/disable the feature during driver installation using helm. This is by default set to true. To configure how often the driver checks for changed capacity set storageCapacity.pollInterval attribute. In case of driver installed via operator, this interval can be configured in the sample file provided here by editing the --capacity-poll-interval argument present in the provisioner sidecar.\n","categories":"","description":"Code features for PowerFlex Driver","excerpt":"Code features for PowerFlex Driver","ref":"/csm-docs/docs/csidriver/features/powerflex/","tags":"","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell PowerFlex using Helm or Dell CSM Operator.\nUpdate Driver from v2.8.0 to v2.9.2 using Helm Steps\nRun git clone -b v2.9.2 https://github.com/dell/csi-powerflex.git to clone the git repository and get the v2.9.2 driver. You need to create secret.yaml with the configuration of your system. Check this section in installation documentation: Install the Driver Update myvalues file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade NOTE:\nIf you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver.\nTo update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option --upgrade, for example:\n./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade The logging configuration from v1.5 will not work in v2.1, since the log configuration parameters are now set in the myvalues.yaml file located at dell-csi-helm-installer/myvalues.yaml. Please set the logging configuration parameters in the myvalues.yaml file.\nYou cannot upgrade between drivers with different fsGroupPolicies. To check the current driver’s fsGroupPolicy, use this command:\nkubectl describe csidriver csi-vxflexos.dellemc.com and check the “Spec” section:\n... Spec: Attach Required: true Fs Group Policy: ReadWriteOnceWithFSType Pod Info On Mount: true Requires Republish: false Storage Capacity: false ... Upgrade using Dell CSM Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\nUpgrade the Dell CSM Operator by following here Once the operator is upgraded, to upgrade the driver, refer here ","categories":"","description":"Upgrade PowerFlex CSI driver","excerpt":"Upgrade PowerFlex CSI driver","ref":"/csm-docs/docs/csidriver/upgradation/drivers/powerflex/","tags":["upgrade","csi-driver"],"title":"PowerFlex"},{"body":"Volume Snapshot Feature The CSI PowerFlex driver versions 2.0 and higher support v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class Installation of PowerFlex driver v1.5 and later does not create VolumeSnapshotClass. You can find a sample of a default v1 VolumeSnapshotClass instance in samples/volumesnapshotclass directory. If needed, you can install the default sample. Following is the default sample for v1:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com # Configure what happens to a VolumeSnapshotContent when the VolumeSnapshot object # it is bound to is to be deleted # Allowed values: # Delete: the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. # Retain: both the underlying snapshot and VolumeSnapshotContent remain. deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Create Consistent Snapshot of Group of Volumes This feature extends CSI specification to add the capability to create crash-consistent snapshots of a group of volumes. This feature is available as a technical preview. To use this feature, users have to deploy the csi-volumegroupsnapshotter side-car as part of the PowerFlex driver. Once the sidecar has been deployed, users can make snapshots by using yaml files, More information can be found here: Volume Group Snapshotter.\nVolume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, which is when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true.\nFollowing is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the next highest multiple of 8.\nVolume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nCustom File System Format Options The CSI PowerFlex driver version 1.5 and later support additional mkfs format options. A user is able to specify additional format options as needed for the driver. Format options are specified in storageclass yaml under mkfsFormatOption as in the following example:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \u003cSTORAGE_POOL\u003e # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID mkfsFormatOption: \"\u003cmkfs_format_option\u003e\" # Insert file system format option volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e # Insert System ID values: - csi-vxflexos.dellemc.com WARNING: Before utilizing format options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option. Topology Support The CSI PowerFlex driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\nThe PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed. This Topology support does not include customer-defined topology, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that the pod schedule takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2020-05-27T13:24:55Z\" labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \"170198\" selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\nNOTE: In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\nController HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\nNOTE: If the controller count is greater than the number of available nodes, excess controller pods will be stuck in a pending state.\nIf you are using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file at csi-vxflexos/helm/csi-vxflexos/values.yaml:\nTo assign controller pods to worker nodes only (Default):\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: # - key: \"node-role.kubernetes.io/master\" # operator: \"Exists\" # effect: \"NoSchedule\" To assign controller pods to master and worker nodes:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" To assign controller pods to master nodes only:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" NOTE: Tolerations/selectors work the same way for node pods.\nFor configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nSDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Kubernetes nodes which run the node portion of the CSI driver. The deployment of the SDC kernel module occurs on these nodes with OS platforms which support automatic SDC deployment: currently Red Hat CoreOS (RHCOS), RHEL8.x,RHEL 7.9 are the only supported OS platforms. On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\nOn Kubernetes nodes which run the node portion of the CSI driver, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the nodes with OS version which supports automatic SDC deployment. If there is an SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On nodes that do not support automatic SDC deployment by SDC init container, manual installation steps must be followed. The SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions. There is no automated uninstallation of the SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from the node. Multiarray Support The CSI PowerFlex driver version 1.4 added support for managing multiple PowerFlex arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration There is a sample yaml file in the samples folder called secret.yaml with the following content:\n# Username for accessing PowerFlex system. # If authorization is enabled, username will be ignored. - username: \"admin\" # Password for accessing PowerFlex system. # If authorization is enabled, password will be ignored. password: \"password\" # PowerFlex system name or ID.\t# Required: true systemID: \"1a99aa999999aa9a\" # Required: false # Previous names used in secret of PowerFlex system. Only needed if PowerFlex System Name has been changed by user # and old resources are still based on the old name. allSystemNames: \"pflex-1,pflex-2\" # REST API gateway HTTPS endpoint for PowerFlex system. # If authorization is enabled, endpoint should be the HTTPS localhost endpoint that # the authorization sidecar will listen on endpoint: \"https://127.0.0.1\" # Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. # Allowed values: true or false # Default value: true skipCertificateValidation: true # indicates if this array is the default array # needed for backwards compatibility # only one array is allowed to have this set to true # Default value: false isDefault: true # defines the MDM(s) that SDC should register with on start. # Allowed values: a list of IP addresses or hostnames separated by comma. # Default value: none mdm: \"10.0.0.1,10.0.0.2\" # Defines all system names used to create powerflex volumes # Required: false # Default value: none AllSystemNames: \"name1,name2\" - username: \"admin\" password: \"Password123\" systemID: \"2b11bb111111bb1b\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true mdm: \"10.0.0.3,10.0.0.4\" AllSystemNames: \"name1,name2\" The systemID can be found by displaying system level information, which is outlined here\nHere we specify that we want the CSI driver to manage two arrays: one with an IP 127.0.0.1 and the other with an IP 127.0.0.2.\nTo use this config we need to create a Kubernetes secret from it. To do so, run the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=secret.yaml Dynamic Array Configuration To update or change any array configuration property, edit the secret. The driver will detect the change automatically and use the new values based on the Kubernetes watcher file change detection time. You can use kubectl command to delete the current secret and create a new secret with changes. For example, refer yaml above and change only the password.\n- username: \"admin\" password: \"Password123\" to\n- username: \"admin\" password: \"Password456\" Below are sample command lines to delete a secret and create modified properties from file secret.yaml.\nkubectl delete secret vxflexos-config -n vxflexos kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=./secret.yaml Dynamic array configuration change detection is only used for properties of an existing array, like username or password. To add a new array to the secret, or to alter an array’s mdm field, you must run csi-install.sh with --upgrade option to update the MDM key in secret and restart the node pods.\ncd \u003cDRIVER-HOME\u003e/dell-csi-helm-installer ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade kubectl delete pods --all -n vxflexos Creating storage classes To be able to provision Kubernetes volumes using a specific array, we need to create corresponding storage classes.\nFind the sample yaml files under samples/storageclass. Edit storageclass.yaml if you want ext4 filesystem, and use storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have, and replace \u003cSYSTEM_ID\u003e with the system ID or system name for the array you’d like to use.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl apply -f storageclass.yaml After that, you can use the storage class for the corresponding array.\nEphemeral Inline Volume Starting from version 1.4, CSI PowerFlex driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest (found in csi-vxflexos/test/helm/ephemeral) for creating ephemeral volume in pod manifest with CSI PowerFlex driver.\nkind: Pod apiVersion: v1 metadata: name: my-csi-app-inline-volumes spec: containers: - name: my-frontend image: busybox command: [ \"sleep\", \"100000\" ] volumeMounts: - mountPath: \"/data0\" name: my-csi-volume - mountPath: \"/data1\" name: my-csi-volume-xfs volumes: - name: my-csi-volume csi: driver: csi-vxflexos.dellemc.com fsType: \"ext4\" volumeAttributes: volumeName: \"my-csi-volume\" size: \"8Gi\" storagepool: sample systemID: sample - name: my-csi-volume-xfs csi: driver: csi-vxflexos.dellemc.com fsType: \"xfs\" volumeAttributes: volumeName: \"my-csi-volume-xfs\" size: \"10Gi\" storagepool: sample systemID: sample This manifest creates a pod and attach two newly created ephemeral inline csi volumes to it, one ext4 and the other xfs.\nTo run the corresponding helm test, go to csi-vxflexos/test/helm/ephemeral and fill in the values for storagepool and systemID in sample.yaml.\nThen run:\n./testEphemeral.sh this test deploys the pod with two ephemeral volumes, and write some data to them before deleting the pod.\nWhen creating ephemeral volumes, it is important to specify the following within the volumeAttributes section: volumeName, size, storagepool, and if you want to use a non-default array, systemID.\nConsuming Existing Volumes with Static Provisioning To use existing volumes from PowerFlex array as Peristent volumes in your Kubernetes environment, perform these steps:\nLog into one of the MDMs of the PowerFlex cluster. Execute these commands to retrieve the systemID and volumeID. scli --mdm_ip \u003cIPs, comma separated\u003e --login --username \u003cusername\u003e --password \u003cpassword\u003e Output: Logged in. User role is SuperUser. System ID is \u003csystemID\u003e scli --query_volume --volume_name \u003cvolume name\u003e Output: Volume ID: \u003cvolumeID\u003e Name: \u003cvolume name\u003e Create PersistentVolume and use this volume ID in the volumeHandle with the format systemID-volumeID in the manifest. Modify other parameters according to your needs. apiVersion: v1 kind: PersistentVolume metadata: name: existingVol spec: capacity: storage: 8Gi csi: driver: csi-vxflexos.dellemc.com volumeHandle: \u003csystemID\u003e-\u003cvolumeID\u003e volumeMode: Filesystem accessModes: - ReadWriteOnce storageClassName: vxflexos Create PersistentVolumeClaim to use this PersistentVolume. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: vxflexos Then use this PVC as a volume in a pod. apiVersion: v1 kind: Pod metadata: name: static-prov-pod spec: containers: - name: test image: busybox command: [ \"sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data0\" name: pvol volumes: - name: pvol persistentVolumeClaim: claimName: pvol After the pod is Ready and Running, you can start to use this pod and volume. Note: Retrieval of the volume ID is possible through the UI. You must select the volume, navigate to Details section and click the volume in the graph. This selection will set the filter to the desired volume. At this point the volume ID can be found in the URL.\nDynamic Logging Configuration The dynamic logging configuration that was introduced in v1.5 of the driver was revamped for v2.0; v1.5 logging configuration is not compatible with v2.0.\nTwo fields in values.yaml (located at helm/csi-vxflexos/values.yaml) are used to configure the dynamic logging: logLevel and logFormat.\n# CSI driver log level # Allowed values: \"error\", \"warn\"/\"warning\", \"info\", \"debug\" # Default value: \"debug\" logLevel: \"debug\" # CSI driver log format # Allowed values: \"TEXT\" or \"JSON\" # Default value: \"TEXT\" logFormat: \"TEXT\" To change the logging fields after the driver is deployed, you can use this command to edit the configmap:\nkubectl edit configmap -n vxflexos vxflexos-config-params and then make the necessary adjustments for CSI_LOG_LEVEL and CSI_LOG_FORMAT.\nIf either option is set to a value outside of what is supported, the driver will use the default values of “debug” and “text” .\nVolume Health Monitoring NOTE: This feature requires the alpha feature gate, CSIVolumeHealth to be set to true. If the feature gate is on, and you want to use this feature, ensure the proper values are enabled in your values file. See the values table in the installation doc for more details.\nStarting in version 2.1, CSI Driver for PowerFlex now supports volume health monitoring. This allows Kubernetes to report on the condition of the underlying volumes via events when a volume condition is abnormal. For example, if a volume were to be deleted from the array, or unmounted outside of Kubernetes, Kubernetes will now report these abnormal conditions as events.\nTo accomplish this, the driver utilizes the external-health-monitor sidecar. When driver detects a volume condition is abnormal, the sidecar will report an event to the corresponding PVC. For example, in this event from kubectl describe pvc -n \u003cns\u003e we can see that the underlying volume was deleted from the PowerFlex array:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------ Warning VolumeConditionAbnormal 32s csi-pv-monitor-controller-csi-vxflexos.dellemc.com Volume is not found at 2021-11-03 20:31:04 Events will also be reported to pods that have abnormal volumes. In these two events from kubectl describe pods -n \u003cns\u003e, we can see that this pod has two abnormal volumes: one volume was unmounted outside of Kubernetes, while another was deleted from PowerFlex array.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------ Warning VolumeConditionAbnormal 35s (x9 over 12m) kubelet Volume vol4: volPath: /var/.../rhel-705f0dcbf1/mount is not mounted: \u003cnil\u003e Warning VolumeConditionAbnormal 5s kubelet Volume vol2: Volume is not found by node driver at 2021-11-11 02:04:49 Set QoS Limits Starting in version 2.5, CSI Driver for PowerFlex now supports setting the limits for the bandwidth and IOPS that one SDC generates for the specified volume. This enables the CSI driver to control the quality of service (QoS). In this release this is supported at the StorageClass level, so once a volume is created QoS Settings can’t be adjusted later. To accomplish this, two new parameters are introduced in the storage class: bandwidthLimitInKbps and iopsLimit.\nEnsure that the proper values are enabled in your storage class yaml files. Refer to the sample storage class yamls for more details.\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \"pool2\" # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID bandwidthLimitInKbps: \"10240\" # Insert bandwidth limit in Kbps iopsLimit: \"11\" # Insert iops limit csi.storage.k8s.io/fstype: ext4 volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e # Insert System ID values: - csi-vxflexos.dellemc.com Once the volume gets created, the ControllerPublishVolume will set the QoS limits for the volumes mapped to SDC.\nRename SDC Starting with version 2.6, the CSI driver for PowerFlex will support renaming of SDCs. To use this feature, the node section of values.yaml should have renameSDC keys enabled with a prefix value.\nTo enable renaming of SDC, make the following edits to values.yaml file:\n# \"node\" allows to configure node specific parameters node: ... ... # \"renameSDC\" defines the rename operation for SDC # Default value: None renameSDC: # enabled: Enable/Disable rename of SDC # Allowed values: # true: enable renaming # false: disable renaming # Default value: \"false\" enabled: false # \"prefix\" defines a string for the new name of the SDC. # \"prefix\" + \"worker_node_hostname\" should not exceed 31 chars. # Default value: none # Examples: \"rhel-sdc\", \"sdc-test\" prefix: \"sdc-test\" The renameSDC section is going to be used by the Node Service, it has two keys enabled and prefix:\nenabled: Boolean variable that specifies if the renaming for SDC is to be carried out or not. If true then the driver will perform the rename operation. By default, its value will be false. prefix: string variable that is used to set the prefix for SDC name. Based on these two keys, there are certain scenarios on which the driver is going to perform the rename SDC operation:\nIf enabled and prefix given then set the prefix+worker_node_name for SDC name. If enabled and prefix not given then set worker_node_name for SDC name. NOTE: name of the SDC cannot be more than 31 characters, hence the prefix given and the worker node hostname name taken should be such that the total length does not exceed 31 character limit.\nPre-approving SDC by GUID Starting with version 2.6, the CSI Driver for PowerFlex will support pre-approving SDC by GUID. CSI PowerFlex driver will detect the SDC mode set on the PowerFlex array and will request SDC approval from the array prior to publishing a volume. This is specific to each SDC.\nTo request SDC approval for GUID, make the following edits to values.yaml file:\n# \"node\" allows to configure node specific parameters node: ... ... # \"approveSDC\" defines the approve operation for SDC # Default value: None approveSDC: # enabled: Enable/Disable SDC approval #Allowed values: # true: Driver will attempt to approve restricted SDC by GUID during setup # false: Driver will not attempt to approve restricted SDC by GUID during setup # Default value: false enabled: false NOTE: Currently, the CSI-PowerFlex driver only supports GUID for the restricted SDC mode.\nIf SDC approval is denied, then provisioning of the volume will not be attempted and an appropriate error message is reported in the logs/events so the user is informed.\nVolume Limit The CSI Driver for Dell PowerFlex allows users to specify the maximum number of PowerFlex volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-vxflexos-volumes-per-node and specifying the volume limit for that node. kubectl label node \u003cnode_name\u003e max-vxflexos-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxVxflexosVolumesPerNode attribute in values.yaml file.\nNOTE: To reflect the changes after setting the value either via node label or in values.yaml file, user has to bounce the driver controller and node pods using the command kubectl get pods -n vxflexos --no-headers=true | awk '/vxflexos-/{print $1}'| xargs kubectl delete -n vxflexos pod. If the value is set both by node label and values.yaml file then node label value will get the precedence and user has to remove the node label in order to reflect the values.yaml value. The default value of maxVxflexosVolumesPerNode is 0. If maxVxflexosVolumesPerNode is set to zero, then Container Orchestration decides how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxVxflexosVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-vxflexos-volumes-per-node is not set.\nNFS volume support Starting with version 2.8, the CSI driver for PowerFlex will support NFS volumes for PowerFlex storage systems version 4.0.x.\nCSI driver will support following operations for NFS volumes:\nCreation and deletion of a NFS volume with RWO/RWX/ROX access modes. Support of tree quotas while volume creation. Expand the size of a NFS volume. Creation and deletion of snapshot of a NFS volume while retaining file permissions. Create NFS volume from the snapshot. To enable the support of NFS volumes operations from CSI driver, there are a few new keys introduced which needs to be set before performing the operations for NFS volumes.\nnasName: defines the NAS server name that should be used for NFS volumes. enableQuota: when enabled will set quota limit for a newly provisioned NFS volume. NOTE:\nnasName nasName is a mandatory parameter and has to be provided in secret yaml, else it will be an error state and will be captured in driver logs. nasName can be given at storage class level as well. If specified in both, secret and storage class, then precedence is given to storage class value. If nasName not given in secret, irrespective of it specified in SC, then it’s an error state and will be captured in driver logs. If the PowerFlex storage system v4.0.x is configured with only block capabilities, then the user is required to give the default value for nasName as “none”. The user has to update the secret.yaml, values.yaml and storageclass-nfs.yaml with the above keys as like below:\nsamples/secret.yaml\n- username: \"admin\" password: \"Password123\" systemID: \"2b11bb111111bb1b\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true isDefault: true mdm: \"10.0.0.3,10.0.0.4\" nasName: \"nas-server\" samples/storageclass/storageclass-nfs.yaml\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-nfs provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \"pool2\" # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID csi.storage.k8s.io/fstype: nfs nasName: \"nas-server\" # path: /csi # softLimit: \"80\" # gracePeriod: \"86400\" volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e-nfs # Insert System ID values: - \"true\" helm/csi-vxflexos/values.yaml\n... enableQuota: false ... Usage of Quotas to Limit Storage Consumption for NFS volumes Starting with version 2.8, the CSI driver for PowerFlex will support enabling tree quotas for limiting capacity for NFS volumes. To use the quota feature user can specify the boolean value enableQuota in values.yaml.\nTo enable quota for NFS volumes, make the following edits to values.yaml file:\n... ... # enableQuota: a boolean that, when enabled, will set quota limit for a newly provisioned NFS volume. # Allowed values: # true: set quota for volume # false: do not set quota for volume # Optional: true # Default value: none enableQuota: true ... ... For example, if the user creates a PVC with 3 Gi of storage and quotas have already been enabled in PowerFlex system for the specified volume.\nWhen enableQuota is set to true\nThe driver sets the hard limit of the PVC to 3Gi. The user adds data of 2Gi to the PVC (by logging into POD). It works as expected. The user tries to add 2Gi more data. Driver doesn’t allow the user to enter more data as total data to be added is 4Gi and PVC limit is 3Gi. The user can expand the volume from 3Gi to 6Gi. The driver allows it and sets the hard limit of PVC to 6Gi. User retries adding 2Gi more data (which has been errored out previously). The driver accepts the data. When enableQuota is set to false\nDriver doesn’t set any hard limit against the PVC created. The user adds 2Gi data to the PVC, which has a limit of 3Gi. It works as expected. The user tries to add 2Gi more data. Now the total size of data is 4Gi. Driver allows the user to enter more data irrespective of the initial PVC size (since no quota is set against this PVC) The user can expand the volume from an initial size of 3Gi to 4Gi or more. The driver allows it. If enableQuota feature is set, user can also set other tree quota parameters such as soft limit, soft grace period and path using storage class yaml file.\npath: relative path to the root of the associated NFS volume. softLimit: soft limit set to quota. Specified as a percentage w.r.t. PVC size. gracePeriod: grace period of quota, must be mentioned along with softLimit, in seconds. Soft Limit can be exceeded until the grace period. NOTE:\nhardLimit is set to same size as that of PVC size. When a volume with quota enabled is expanded then the hardLimit and softLimit are also recalculated by driver w.r.t. to the new PVC size. sofLimit cannot be set to unlimited value (0), otherwise it will become greater than hardLimit (PVC size). softLimit should be lesser than 100%, since hardLimit will be set to 100% (PVC size) internally by the driver. Storage Class Example with Quota Limit Parameters samples/storageclass/storageclass-nfs.yaml\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-nfs provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \"pool2\" # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID csi.storage.k8s.io/fstype: nfs nasName: \"nas-server\" path: /csi softLimit: \"80\" gracePeriod: \"86400\" volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e-nfs # Insert System ID values: - \"true\" Storage Capacity Tracking CSI-PowerFlex driver version 2.8.0 and above supports Storage Capacity Tracking.\nThis feature helps the scheduler to make more informed choices about where to schedule pods which depend on unbound volumes with late binding (aka “wait for first consumer”). Pods will be scheduled on a node (satisfying the topology constraints) only if the requested capacity is available on the storage array. If such a node is not available, the pods stay in Pending state. This means pods are not scheduled.\nWithout storage capacity tracking, pods get scheduled on a node satisfying the topology constraints. If the required capacity is not available, volume attachment to the pods fails, and pods remain in ContainerCreating state. Storage capacity tracking eliminates unnecessary scheduling of pods when there is insufficient capacity.\nThe attribute storageCapacity.enabled in values.yaml can be used to enable/disable the feature during driver installation using helm. This is by default set to true. To configure how often the driver checks for changed capacity set storageCapacity.pollInterval attribute. In case of driver installed via operator, this interval can be configured in the sample file provided here by editing the --capacity-poll-interval argument present in the provisioner sidecar.\n","categories":"","description":"Code features for PowerFlex Driver","excerpt":"Code features for PowerFlex Driver","ref":"/csm-docs/v1/csidriver/features/powerflex/","tags":"","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v2.7.1 to v2.8 using Helm Steps\nRun git clone -b v2.8.0 https://github.com/dell/csi-powerflex.git to clone the git repository and get the v2.8.0 driver. You need to create secret.yaml with the configuration of your system. Check this section in installation documentation: Install the Driver Update myvalues file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade NOTE:\nIf you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver.\nTo update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option --upgrade, for example:\n./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade The logging configuration from v1.5 will not work in v2.1, since the log configuration parameters are now set in the myvalues.yaml file located at dell-csi-helm-installer/myvalues.yaml. Please set the logging configuration parameters in the myvalues.yaml file.\nYou cannot upgrade between drivers with different fsGroupPolicies. To check the current driver’s fsGroupPolicy, use this command:\nkubectl describe csidriver csi-vxflexos.dellemc.com and check the “Spec” section:\n... Spec: Attach Required: true Fs Group Policy: ReadWriteOnceWithFSType Pod Info On Mount: true Requires Republish: false Storage Capacity: false ... Upgrade using Dell CSM Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\nUpgrade the Dell CSM Operator by following here Once the operator is upgraded, to upgrade the driver, refer here ","categories":"","description":"Upgrade PowerFlex CSI driver","excerpt":"Upgrade PowerFlex CSI driver","ref":"/csm-docs/v1/csidriver/upgradation/drivers/powerflex/","tags":["upgrade","csi-driver"],"title":"PowerFlex"},{"body":"Volume Snapshot Feature The CSI PowerFlex driver versions 2.0 and higher support v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class Installation of PowerFlex driver v1.5 and later does not create VolumeSnapshotClass. You can find a sample of a default v1 VolumeSnapshotClass instance in samples/volumesnapshotclass directory. If needed, you can install the default sample. Following is the default sample for v1:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com # Configure what happens to a VolumeSnapshotContent when the VolumeSnapshot object # it is bound to is to be deleted # Allowed values: # Delete: the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. # Retain: both the underlying snapshot and VolumeSnapshotContent remain. deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Create Consistent Snapshot of Group of Volumes This feature extends CSI specification to add the capability to create crash-consistent snapshots of a group of volumes. This feature is available as a technical preview. To use this feature, users have to deploy the csi-volumegroupsnapshotter side-car as part of the PowerFlex driver. Once the sidecar has been deployed, users can make snapshots by using yaml files, More information can be found here: Volume Group Snapshotter.\nVolume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, which is when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true.\nFollowing is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the next highest multiple of 8.\nVolume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nCustom File System Format Options The CSI PowerFlex driver version 1.5 and later support additional mkfs format options. A user is able to specify additional format options as needed for the driver. Format options are specified in storageclass yaml under mkfsFormatOption as in the following example:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \u003cSTORAGE_POOL\u003e # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID mkfsFormatOption: \"\u003cmkfs_format_option\u003e\" # Insert file system format option volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e # Insert System ID values: - csi-vxflexos.dellemc.com WARNING: Before utilizing format options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option. Topology Support The CSI PowerFlex driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\nThe PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed. This Topology support does not include customer-defined topology, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that the pod schedule takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2020-05-27T13:24:55Z\" labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \"170198\" selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\nNOTE: In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\nController HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\nNOTE: If the controller count is greater than the number of available nodes, excess controller pods will be stuck in a pending state.\nIf you are using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file at csi-vxflexos/helm/csi-vxflexos/values.yaml:\nTo assign controller pods to worker nodes only (Default):\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: # - key: \"node-role.kubernetes.io/master\" # operator: \"Exists\" # effect: \"NoSchedule\" To assign controller pods to master and worker nodes:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" To assign controller pods to master nodes only:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" NOTE: Tolerations/selectors work the same way for node pods.\nFor configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nSDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Kubernetes nodes which run the node portion of the CSI driver. The deployment of the SDC kernel module occurs on these nodes with OS platforms which support automatic SDC deployment: currently Red Hat CoreOS (RHCOS), RHEL8.x,RHEL 7.9 are the only supported OS platforms. On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\nOn Kubernetes nodes which run the node portion of the CSI driver, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the nodes with OS version which supports automatic SDC deployment. If there is an SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On nodes that do not support automatic SDC deployment by SDC init container, manual installation steps must be followed. The SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions. There is no automated uninstallation of the SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from the node. Multiarray Support The CSI PowerFlex driver version 1.4 added support for managing multiple PowerFlex arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration There is a sample yaml file in the samples folder called secret.yaml with the following content:\n# Username for accessing PowerFlex system. # If authorization is enabled, username will be ignored. - username: \"admin\" # Password for accessing PowerFlex system. # If authorization is enabled, password will be ignored. password: \"password\" # PowerFlex system name or ID.\t# Required: true systemID: \"1a99aa999999aa9a\" # Required: false # Previous names used in secret of PowerFlex system. Only needed if PowerFlex System Name has been changed by user # and old resources are still based on the old name. allSystemNames: \"pflex-1,pflex-2\" # REST API gateway HTTPS endpoint for PowerFlex system. # If authorization is enabled, endpoint should be the HTTPS localhost endpoint that # the authorization sidecar will listen on endpoint: \"https://127.0.0.1\" # Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. # Allowed values: true or false # Default value: true skipCertificateValidation: true # indicates if this array is the default array # needed for backwards compatibility # only one array is allowed to have this set to true # Default value: false isDefault: true # defines the MDM(s) that SDC should register with on start. # Allowed values: a list of IP addresses or hostnames separated by comma. # Default value: none mdm: \"10.0.0.1,10.0.0.2\" # Defines all system names used to create powerflex volumes # Required: false # Default value: none AllSystemNames: \"name1,name2\" - username: \"admin\" password: \"Password123\" systemID: \"2b11bb111111bb1b\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true mdm: \"10.0.0.3,10.0.0.4\" AllSystemNames: \"name1,name2\" The systemID can be found by displaying system level information, which is outlined here\nHere we specify that we want the CSI driver to manage two arrays: one with an IP 127.0.0.1 and the other with an IP 127.0.0.2.\nTo use this config we need to create a Kubernetes secret from it. To do so, run the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=secret.yaml Dynamic Array Configuration To update or change any array configuration property, edit the secret. The driver will detect the change automatically and use the new values based on the Kubernetes watcher file change detection time. You can use kubectl command to delete the current secret and create a new secret with changes. For example, refer yaml above and change only the password.\n- username: \"admin\" password: \"Password123\" to\n- username: \"admin\" password: \"Password456\" Below are sample command lines to delete a secret and create modified properties from file secret.yaml.\nkubectl delete secret vxflexos-config -n vxflexos kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=./secret.yaml Dynamic array configuration change detection is only used for properties of an existing array, like username or password. To add a new array to the secret, or to alter an array’s mdm field, you must run csi-install.sh with --upgrade option to update the MDM key in secret and restart the node pods.\ncd \u003cDRIVER-HOME\u003e/dell-csi-helm-installer ./csi-install.sh --upgrade --namespace vxflexos --values ../helm/csi-vxflexos/values.yaml kubectl delete pods --all -n vxflexos Creating storage classes To be able to provision Kubernetes volumes using a specific array, we need to create corresponding storage classes.\nFind the sample yaml files under samples/storageclass. Edit storageclass.yaml if you want ext4 filesystem, and use storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have, and replace \u003cSYSTEM_ID\u003e with the system ID or system name for the array you’d like to use.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl apply -f storageclass.yaml After that, you can use the storage class for the corresponding array.\nEphemeral Inline Volume Starting from version 1.4, CSI PowerFlex driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest (found in csi-vxflexos/test/helm/ephemeral) for creating ephemeral volume in pod manifest with CSI PowerFlex driver.\nkind: Pod apiVersion: v1 metadata: name: my-csi-app-inline-volumes spec: containers: - name: my-frontend image: busybox command: [ \"sleep\", \"100000\" ] volumeMounts: - mountPath: \"/data0\" name: my-csi-volume - mountPath: \"/data1\" name: my-csi-volume-xfs volumes: - name: my-csi-volume csi: driver: csi-vxflexos.dellemc.com fsType: \"ext4\" volumeAttributes: volumeName: \"my-csi-volume\" size: \"8Gi\" storagepool: sample systemID: sample - name: my-csi-volume-xfs csi: driver: csi-vxflexos.dellemc.com fsType: \"xfs\" volumeAttributes: volumeName: \"my-csi-volume-xfs\" size: \"10Gi\" storagepool: sample systemID: sample This manifest creates a pod and attach two newly created ephemeral inline csi volumes to it, one ext4 and the other xfs.\nTo run the corresponding helm test, go to csi-vxflexos/test/helm/ephemeral and fill in the values for storagepool and systemID in sample.yaml.\nThen run:\n./testEphemeral.sh this test deploys the pod with two ephemeral volumes, and write some data to them before deleting the pod.\nWhen creating ephemeral volumes, it is important to specify the following within the volumeAttributes section: volumeName, size, storagepool, and if you want to use a non-default array, systemID.\nConsuming Existing Volumes with Static Provisioning To use existing volumes from PowerFlex array as Peristent volumes in your Kubernetes environment, perform these steps:\nLog into one of the MDMs of the PowerFlex cluster. Execute these commands to retrieve the systemID and volumeID. scli --mdm_ip \u003cIPs, comma separated\u003e --login --username \u003cusername\u003e --password \u003cpassword\u003e Output: Logged in. User role is SuperUser. System ID is \u003csystemID\u003e scli --query_volume --volume_name \u003cvolume name\u003e Output: Volume ID: \u003cvolumeID\u003e Name: \u003cvolume name\u003e Create PersistentVolume and use this volume ID in the volumeHandle with the format systemID-volumeID in the manifest. Modify other parameters according to your needs. apiVersion: v1 kind: PersistentVolume metadata: name: existingVol spec: capacity: storage: 8Gi csi: driver: csi-vxflexos.dellemc.com volumeHandle: \u003csystemID\u003e-\u003cvolumeID\u003e volumeMode: Filesystem accessModes: - ReadWriteOnce storageClassName: vxflexos Create PersistentVolumeClaim to use this PersistentVolume. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: vxflexos Then use this PVC as a volume in a pod. apiVersion: v1 kind: Pod metadata: name: static-prov-pod spec: containers: - name: test image: busybox command: [ \"sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data0\" name: pvol volumes: - name: pvol persistentVolumeClaim: claimName: pvol After the pod is Ready and Running, you can start to use this pod and volume. Note: Retrieval of the volume ID is possible through the UI. You must select the volume, navigate to Details section and click the volume in the graph. This selection will set the filter to the desired volume. At this point the volume ID can be found in the URL.\nDynamic Logging Configuration The dynamic logging configuration that was introduced in v1.5 of the driver was revamped for v2.0; v1.5 logging configuration is not compatible with v2.0.\nTwo fields in values.yaml (located at helm/csi-vxflexos/values.yaml) are used to configure the dynamic logging: logLevel and logFormat.\n# CSI driver log level # Allowed values: \"error\", \"warn\"/\"warning\", \"info\", \"debug\" # Default value: \"debug\" logLevel: \"debug\" # CSI driver log format # Allowed values: \"TEXT\" or \"JSON\" # Default value: \"TEXT\" logFormat: \"TEXT\" To change the logging fields after the driver is deployed, you can use this command to edit the configmap:\nkubectl edit configmap -n vxflexos vxflexos-config-params and then make the necessary adjustments for CSI_LOG_LEVEL and CSI_LOG_FORMAT.\nIf either option is set to a value outside of what is supported, the driver will use the default values of “debug” and “text” .\nVolume Health Monitoring NOTE: This feature requires the alpha feature gate, CSIVolumeHealth to be set to true. If the feature gate is on, and you want to use this feature, ensure the proper values are enabled in your values file. See the values table in the installation doc for more details.\nStarting in version 2.1, CSI Driver for PowerFlex now supports volume health monitoring. This allows Kubernetes to report on the condition of the underlying volumes via events when a volume condition is abnormal. For example, if a volume were to be deleted from the array, or unmounted outside of Kubernetes, Kubernetes will now report these abnormal conditions as events.\nTo accomplish this, the driver utilizes the external-health-monitor sidecar. When driver detects a volume condition is abnormal, the sidecar will report an event to the corresponding PVC. For example, in this event from kubectl describe pvc -n \u003cns\u003e we can see that the underlying volume was deleted from the PowerFlex array:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------ Warning VolumeConditionAbnormal 32s csi-pv-monitor-controller-csi-vxflexos.dellemc.com Volume is not found at 2021-11-03 20:31:04 Events will also be reported to pods that have abnormal volumes. In these two events from kubectl describe pods -n \u003cns\u003e, we can see that this pod has two abnormal volumes: one volume was unmounted outside of Kubernetes, while another was deleted from PowerFlex array.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------ Warning VolumeConditionAbnormal 35s (x9 over 12m) kubelet Volume vol4: volPath: /var/.../rhel-705f0dcbf1/mount is not mounted: \u003cnil\u003e Warning VolumeConditionAbnormal 5s kubelet Volume vol2: Volume is not found by node driver at 2021-11-11 02:04:49 Set QoS Limits Starting in version 2.5, CSI Driver for PowerFlex now supports setting the limits for the bandwidth and IOPS that one SDC generates for the specified volume. This enables the CSI driver to control the quality of service (QoS). In this release this is supported at the StorageClass level, so once a volume is created QoS Settings can’t be adjusted later. To accomplish this, two new parameters are introduced in the storage class: bandwidthLimitInKbps and iopsLimit.\nEnsure that the proper values are enabled in your storage class yaml files. Refer to the sample storage class yamls for more details.\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \"pool2\" # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID bandwidthLimitInKbps: \"10240\" # Insert bandwidth limit in Kbps iopsLimit: \"11\" # Insert iops limit csi.storage.k8s.io/fstype: ext4 volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e # Insert System ID values: - csi-vxflexos.dellemc.com Once the volume gets created, the ControllerPublishVolume will set the QoS limits for the volumes mapped to SDC.\nRename SDC Starting with version 2.6, the CSI driver for PowerFlex will support renaming of SDCs. To use this feature, the node section of values.yaml should have renameSDC keys enabled with a prefix value.\nTo enable renaming of SDC, make the following edits to values.yaml file:\n# \"node\" allows to configure node specific parameters node: ... ... # \"renameSDC\" defines the rename operation for SDC # Default value: None renameSDC: # enabled: Enable/Disable rename of SDC # Allowed values: # true: enable renaming # false: disable renaming # Default value: \"false\" enabled: false # \"prefix\" defines a string for the new name of the SDC. # \"prefix\" + \"worker_node_hostname\" should not exceed 31 chars. # Default value: none # Examples: \"rhel-sdc\", \"sdc-test\" prefix: \"sdc-test\" The renameSDC section is going to be used by the Node Service, it has two keys enabled and prefix:\nenabled: Boolean variable that specifies if the renaming for SDC is to be carried out or not. If true then the driver will perform the rename operation. By default, its value will be false. prefix: string variable that is used to set the prefix for SDC name. Based on these two keys, there are certain scenarios on which the driver is going to perform the rename SDC operation:\nIf enabled and prefix given then set the prefix+worker_node_name for SDC name. If enabled and prefix not given then set worker_node_name for SDC name. NOTE: name of the SDC cannot be more than 31 characters, hence the prefix given and the worker node hostname name taken should be such that the total length does not exceed 31 character limit.\nPre-approving SDC by GUID Starting with version 2.6, the CSI Driver for PowerFlex will support pre-approving SDC by GUID. CSI PowerFlex driver will detect the SDC mode set on the PowerFlex array and will request SDC approval from the array prior to publishing a volume. This is specific to each SDC.\nTo request SDC approval for GUID, make the following edits to values.yaml file:\n# \"node\" allows to configure node specific parameters node: ... ... # \"approveSDC\" defines the approve operation for SDC # Default value: None approveSDC: # enabled: Enable/Disable SDC approval #Allowed values: # true: Driver will attempt to approve restricted SDC by GUID during setup # false: Driver will not attempt to approve restricted SDC by GUID during setup # Default value: false enabled: false NOTE: Currently, the CSI-PowerFlex driver only supports GUID for the restricted SDC mode.\nIf SDC approval is denied, then provisioning of the volume will not be attempted and an appropriate error message is reported in the logs/events so the user is informed.\n","categories":"","description":"Code features for PowerFlex Driver","excerpt":"Code features for PowerFlex Driver","ref":"/csm-docs/v2/csidriver/features/powerflex/","tags":"","title":"PowerFlex"},{"body":" CSM 1.7.1 is applicable to helm based installations of PowerFlex driver.\nYou can upgrade the CSI Driver for Dell PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v2.6 to v2.7.1 using Helm Steps\nRun git clone -b v2.7.1 https://github.com/dell/csi-powerflex.git to clone the git repository and get the v2.7.1 driver. You need to create secret.yaml with the configuration of your system. Check this section in installation documentation: Install the Driver Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade NOTE:\nIf you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver.\nTo update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option --upgrade, for example:\n./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade The logging configuration from v1.5 will not work in v2.1, since the log configuration parameters are now set in the values.yaml file located at helm/csi-vxflexos/values.yaml. Please set the logging configuration parameters in the values.yaml file.\nYou cannot upgrade between drivers with different fsGroupPolicies. To check the current driver’s fsGroupPolicy, use this command:\nkubectl describe csidriver csi-vxflexos.dellemc.com and check the “Spec” section:\n... Spec: Attach Required: true Fs Group Policy: ReadWriteOnceWithFSType Pod Info On Mount: true Requires Republish: false Storage Capacity: false ... Upgrade using Dell CSI Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\nPlease upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here. Upgrade using Dell CSM Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\nPlease upgrade the Dell CSM Operator by following here Once the operator is upgraded, to upgrade the driver, refer here ","categories":"","description":"Upgrade PowerFlex CSI driver","excerpt":"Upgrade PowerFlex CSI driver","ref":"/csm-docs/v2/csidriver/upgradation/drivers/powerflex/","tags":["upgrade","csi-driver"],"title":"PowerFlex"},{"body":"Volume Snapshot Feature The CSI PowerFlex driver versions 2.0 and higher support v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class Installation of PowerFlex driver v1.5 and later does not create VolumeSnapshotClass. You can find a sample of a default v1 VolumeSnapshotClass instance in samples/volumesnapshotclass directory. If needed, you can install the default sample. Following is the default sample for v1:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com # Configure what happens to a VolumeSnapshotContent when the VolumeSnapshot object # it is bound to is to be deleted # Allowed values: # Delete: the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. # Retain: both the underlying snapshot and VolumeSnapshotContent remain. deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Create Consistent Snapshot of Group of Volumes This feature extends CSI specification to add the capability to create crash-consistent snapshots of a group of volumes. This feature is available as a technical preview. To use this feature, users have to deploy the csi-volumegroupsnapshotter side-car as part of the PowerFlex driver. Once the sidecar has been deployed, users can make snapshots by using yaml files, More information can be found here: Volume Group Snapshotter.\nVolume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, which is when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true.\nFollowing is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the next highest multiple of 8.\nVolume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nCustom File System Format Options The CSI PowerFlex driver version 1.5 and later support additional mkfs format options. A user is able to specify additional format options as needed for the driver. Format options are specified in storageclass yaml under mkfsFormatOption as in the following example:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \u003cSTORAGE_POOL\u003e # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID mkfsFormatOption: \"\u003cmkfs_format_option\u003e\" # Insert file system format option volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e # Insert System ID values: - csi-vxflexos.dellemc.com WARNING: Before utilizing format options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option. Topology Support The CSI PowerFlex driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\nThe PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed. This Topology support does not include customer-defined topology, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that the pod schedule takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2020-05-27T13:24:55Z\" labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \"170198\" selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\nNOTE: In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\nController HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\nNOTE: If the controller count is greater than the number of available nodes, excess controller pods will be stuck in a pending state.\nIf you are using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file at csi-vxflexos/helm/csi-vxflexos/values.yaml:\nTo assign controller pods to worker nodes only (Default):\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: # - key: \"node-role.kubernetes.io/master\" # operator: \"Exists\" # effect: \"NoSchedule\" To assign controller pods to master and worker nodes:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" To assign controller pods to master nodes only:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" NOTE: Tolerations/selectors work the same way for node pods.\nFor configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nSDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Kubernetes nodes which run the node portion of the CSI driver. The deployment of the SDC kernel module occurs on these nodes with OS platforms which support automatic SDC deployment: currently Red Hat CoreOS (RHCOS), RHEL8.x,RHEL 7.9 are the only supported OS platforms. On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\nOn Kubernetes nodes which run the node portion of the CSI driver, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the nodes with OS version which supports automatic SDC deployment. If there is an SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On nodes that do not support automatic SDC deployment by SDC init container, manual installation steps must be followed. The SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions. There is no automated uninstallation of the SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from the node. Multiarray Support The CSI PowerFlex driver version 1.4 added support for managing multiple PowerFlex arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration There is a sample yaml file in the samples folder called secret.yaml with the following content:\n# Username for accessing PowerFlex system. # If authorization is enabled, username will be ignored. - username: \"admin\" # Password for accessing PowerFlex system. # If authorization is enabled, password will be ignored. password: \"password\" # PowerFlex system name or ID.\t# Required: true systemID: \"1a99aa999999aa9a\" # Required: false # Previous names used in secret of PowerFlex system. Only needed if PowerFlex System Name has been changed by user # and old resources are still based on the old name. allSystemNames: \"pflex-1,pflex-2\" # REST API gateway HTTPS endpoint for PowerFlex system. # If authorization is enabled, endpoint should be the HTTPS localhost endpoint that # the authorization sidecar will listen on endpoint: \"https://127.0.0.1\" # Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. # Allowed values: true or false # Default value: true skipCertificateValidation: true # indicates if this array is the default array # needed for backwards compatibility # only one array is allowed to have this set to true # Default value: false isDefault: true # defines the MDM(s) that SDC should register with on start. # Allowed values: a list of IP addresses or hostnames separated by comma. # Default value: none mdm: \"10.0.0.1,10.0.0.2\" # Defines all system names used to create powerflex volumes # Required: false # Default value: none AllSystemNames: \"name1,name2\" - username: \"admin\" password: \"Password123\" systemID: \"2b11bb111111bb1b\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true mdm: \"10.0.0.3,10.0.0.4\" AllSystemNames: \"name1,name2\" The systemID can be found by displaying system level information, which is outlined here\nHere we specify that we want the CSI driver to manage two arrays: one with an IP 127.0.0.1 and the other with an IP 127.0.0.2.\nTo use this config we need to create a Kubernetes secret from it. To do so, run the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=secret.yaml\nDynamic Array Configuration To update or change any array configuration property, edit the secret. The driver will detect the change automatically and use the new values based on the Kubernetes watcher file change detection time. You can use kubectl command to delete the current secret and create a new secret with changes. For example, refer yaml above and change only the password.\n- username: \"admin\" password: \"Password123\" to\n- username: \"admin\" password: \"Password456\" Below are sample command lines to delete a secret and create modified properties from file secret.yaml.\nkubectl delete secret vxflexos-config -n vxflexos kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=./secret.yaml Dynamic array configuration change detection is only used for properties of an existing array, like username or password. To add a new array to the secret, or to alter an array’s mdm field, you must run csi-install.sh with --upgrade option to update the MDM key in secret and restart the node pods.\ncd \u003cDRIVER-HOME\u003e/dell-csi-helm-installer ./csi-install.sh --upgrade --namespace vxflexos --values ../helm/csi-vxflexos/values.yaml kubectl delete pods --all -n vxflexos Creating storage classes To be able to provision Kubernetes volumes using a specific array, we need to create corresponding storage classes.\nFind the sample yaml files under samples/storageclass. Edit storageclass.yaml if you want ext4 filesystem, and use storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have, and replace \u003cSYSTEM_ID\u003e with the system ID or system name for the array you’d like to use.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl apply -f storageclass.yaml After that, you can use the storage class for the corresponding array.\nEphemeral Inline Volume Starting from version 1.4, CSI PowerFlex driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest (found in csi-vxflexos/test/helm/ephemeral) for creating ephemeral volume in pod manifest with CSI PowerFlex driver.\nkind: Pod apiVersion: v1 metadata: name: my-csi-app-inline-volumes spec: containers: - name: my-frontend image: busybox command: [ \"sleep\", \"100000\" ] volumeMounts: - mountPath: \"/data0\" name: my-csi-volume - mountPath: \"/data1\" name: my-csi-volume-xfs volumes: - name: my-csi-volume csi: driver: csi-vxflexos.dellemc.com fsType: \"ext4\" volumeAttributes: volumeName: \"my-csi-volume\" size: \"8Gi\" storagepool: sample systemID: sample - name: my-csi-volume-xfs csi: driver: csi-vxflexos.dellemc.com fsType: \"xfs\" volumeAttributes: volumeName: \"my-csi-volume-xfs\" size: \"10Gi\" storagepool: sample systemID: sample This manifest creates a pod and attach two newly created ephemeral inline csi volumes to it, one ext4 and the other xfs.\nTo run the corresponding helm test, go to csi-vxflexos/test/helm/ephemeral and fill in the values for storagepool and systemID in sample.yaml.\nThen run:\n./testEphemeral.sh this test deploys the pod with two ephemeral volumes, and write some data to them before deleting the pod.\nWhen creating ephemeral volumes, it is important to specify the following within the volumeAttributes section: volumeName, size, storagepool, and if you want to use a non-default array, systemID.\nConsuming Existing Volumes with Static Provisioning To use existing volumes from PowerFlex array as Peristent volumes in your Kubernetes environment, perform these steps:\nLog into one of the MDMs of the PowerFlex cluster. Execute these commands to retrieve the systemID and volumeID. scli --mdm_ip \u003cIPs, comma separated\u003e --login --username \u003cusername\u003e --password \u003cpassword\u003e Output: Logged in. User role is SuperUser. System ID is \u003csystemID\u003e scli --query_volume --volume_name \u003cvolume name\u003e Output: Volume ID: \u003cvolumeID\u003e Name: \u003cvolume name\u003e Create PersistentVolume and use this volume ID in the volumeHandle with the format systemID-volumeID in the manifest. Modify other parameters according to your needs. apiVersion: v1 kind: PersistentVolume metadata: name: existingVol spec: capacity: storage: 8Gi csi: driver: csi-vxflexos.dellemc.com volumeHandle: \u003csystemID\u003e-\u003cvolumeID\u003e volumeMode: Filesystem accessModes: - ReadWriteOnce storageClassName: vxflexos Create PersistentVolumeClaim to use this PersistentVolume. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: vxflexos Then use this PVC as a volume in a pod. apiVersion: v1 kind: Pod metadata: name: static-prov-pod spec: containers: - name: test image: busybox command: [ \"sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data0\" name: pvol volumes: - name: pvol persistentVolumeClaim: claimName: pvol After the pod is Ready and Running, you can start to use this pod and volume. Note: Retrieval of the volume ID is possible through the UI. You must select the volume, navigate to Details section and click the volume in the graph. This selection will set the filter to the desired volume. At this point the volume ID can be found in the URL.\nDynamic Logging Configuration The dynamic logging configuration that was introduced in v1.5 of the driver was revamped for v2.0; v1.5 logging configuration is not compatible with v2.0.\nTwo fields in values.yaml (located at helm/csi-vxflexos/values.yaml) are used to configure the dynamic logging: logLevel and logFormat.\n# CSI driver log level # Allowed values: \"error\", \"warn\"/\"warning\", \"info\", \"debug\" # Default value: \"debug\" logLevel: \"debug\" # CSI driver log format # Allowed values: \"TEXT\" or \"JSON\" # Default value: \"TEXT\" logFormat: \"TEXT\" To change the logging fields after the driver is deployed, you can use this command to edit the configmap:\nkubectl edit configmap -n vxflexos vxflexos-config-params\nand then make the necessary adjustments for CSI_LOG_LEVEL and CSI_LOG_FORMAT.\nIf either option is set to a value outside of what is supported, the driver will use the default values of “debug” and “text” .\nVolume Health Monitoring NOTE: This feature requires the alpha feature gate, CSIVolumeHealth to be set to true. If the feature gate is on, and you want to use this feature, ensure the proper values are enabled in your values file. See the values table in the installation doc for more details.\nStarting in version 2.1, CSI Driver for PowerFlex now supports volume health monitoring. This allows Kubernetes to report on the condition of the underlying volumes via events when a volume condition is abnormal. For example, if a volume were to be deleted from the array, or unmounted outside of Kubernetes, Kubernetes will now report these abnormal conditions as events.\nTo accomplish this, the driver utilizes the external-health-monitor sidecar. When driver detects a volume condition is abnormal, the sidecar will report an event to the corresponding PVC. For example, in this event from kubectl describe pvc -n \u003cns\u003e we can see that the underlying volume was deleted from the PowerFlex array:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------ Warning VolumeConditionAbnormal 32s csi-pv-monitor-controller-csi-vxflexos.dellemc.com Volume is not found at 2021-11-03 20:31:04 Events will also be reported to pods that have abnormal volumes. In these two events from kubectl describe pods -n \u003cns\u003e, we can see that this pod has two abnormal volumes: one volume was unmounted outside of Kubernetes, while another was deleted from PowerFlex array.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------ Warning VolumeConditionAbnormal 35s (x9 over 12m) kubelet Volume vol4: volPath: /var/.../rhel-705f0dcbf1/mount is not mounted: \u003cnil\u003e Warning VolumeConditionAbnormal 5s kubelet Volume vol2: Volume is not found by node driver at 2021-11-11 02:04:49 Set QoS Limits Starting in version 2.5, CSI Driver for PowerFlex now supports setting the limits for the bandwidth and IOPS that one SDC generates for the specified volume. This enables the CSI driver to control the quality of service (QoS). In this release this is supported at the StorageClass level, so once a volume is created QoS Settings can’t be adjusted later. To accomplish this, two new parameters are introduced in the storage class: bandwidthLimitInKbps and iopsLimit.\nEnsure that the proper values are enabled in your storage class yaml files. Refer to the sample storage class yamls for more details.\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \"pool2\" # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID bandwidthLimitInKbps: \"10240\" # Insert bandwidth limit in Kbps iopsLimit: \"11\" # Insert iops limit csi.storage.k8s.io/fstype: ext4 volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e # Insert System ID values: - csi-vxflexos.dellemc.com Once the volume gets created, the ControllerPublishVolume will set the QoS limits for the volumes mapped to SDC.\nRename SDC Starting with version 2.6, the CSI driver for PowerFlex will support renaming of SDCs. To use this feature, the node section of values.yaml should have renameSDC keys enabled with a prefix value.\nTo enable renaming of SDC, make the following edits to values.yaml file:\n# \"node\" allows to configure node specific parameters node: ... ... # \"renameSDC\" defines the rename operation for SDC # Default value: None renameSDC: # enabled: Enable/Disable rename of SDC # Allowed values: # true: enable renaming # false: disable renaming # Default value: \"false\" enabled: false # \"prefix\" defines a string for the new name of the SDC. # \"prefix\" + \"worker_node_hostname\" should not exceed 31 chars. # Default value: none # Examples: \"rhel-sdc\", \"sdc-test\" prefix: \"sdc-test\" The renameSDC section is going to be used by the Node Service, it has two keys enabled and prefix:\nenabled: Boolean variable that specifies if the renaming for SDC is to be carried out or not. If true then the driver will perform the rename operation. By default, its value will be false. prefix: string variable that is used to set the prefix for SDC name. Based on these two keys, there are certain scenarios on which the driver is going to perform the rename SDC operation:\nIf enabled and prefix given then set the prefix+worker_node_name for SDC name. If enabled and prefix not given then set worker_node_name for SDC name. NOTE: name of the SDC cannot be more than 31 characters, hence the prefix given and the worker node hostname name taken should be such that the total length does not exceed 31 character limit.\nPre-approving SDC by GUID Starting with version 2.6, the CSI Driver for PowerFlex will support pre-approving SDC by GUID. CSI PowerFlex driver will detect the SDC mode set on the PowerFlex array and will request SDC approval from the array prior to publishing a volume. This is specific to each SDC.\nTo request SDC approval for GUID, make the following edits to values.yaml file:\n# \"node\" allows to configure node specific parameters node: ... ... # \"approveSDC\" defines the approve operation for SDC # Default value: None approveSDC: # enabled: Enable/Disable SDC approval #Allowed values: # true: Driver will attempt to approve restricted SDC by GUID during setup # false: Driver will not attempt to approve restricted SDC by GUID during setup # Default value: false enabled: false NOTE: Currently, the CSI-PowerFlex driver only supports GUID for the restricted SDC mode.\nIf SDC approval is denied, then provisioning of the volume will not be attempted and an appropriate error message is reported in the logs/events so the user is informed.\n","categories":"","description":"Code features for PowerFlex Driver","excerpt":"Code features for PowerFlex Driver","ref":"/csm-docs/v3/csidriver/features/powerflex/","tags":"","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v2.5 to v2.6 using Helm Steps\nRun git clone -b v2.6.0 https://github.com/dell/csi-powerflex.git to clone the git repository and get the v2.6.0 driver. You need to create config.yaml with the configuration of your system. Check this section in installation documentation: Install the Driver Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade. NOTE:\nIf you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver.\nTo update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.\nThe logging configuration from v1.5 will not work in v2.1, since the log configuration parameters are now set in the values.yaml file located at helm/csi-vxflexos/values.yaml. Please set the logging configuration parameters in the values.yaml file.\nYou cannot upgrade between drivers with different fsGroupPolicies. To check the current driver’s fsGroupPolicy, use this command:\nkubectl describe csidriver csi-vxflexos.dellemc.com\nand check the “Spec” section:\n... Spec: Attach Required: true Fs Group Policy: ReadWriteOnceWithFSType Pod Info On Mount: true Requires Republish: false Storage Capacity: false ... Upgrade using Dell CSI Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\nPlease upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here. ","categories":"","description":"Upgrade PowerFlex CSI driver","excerpt":"Upgrade PowerFlex CSI driver","ref":"/csm-docs/v3/csidriver/upgradation/drivers/powerflex/","tags":["upgrade","csi-driver"],"title":"PowerFlex"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerFlex. The Grafana reference dashboards for PowerFlex metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the sdc_metrics_enabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerflex_export_node_read_bw_megabytes_per_second The export node read bandwidth (MB/s) within PowerFlex system powerflex_export_node_write_bw_megabytes_per_second The export node write bandwidth (MB/s) powerflex_export_node_read_latency_milliseconds The time (in ms) to complete read operations within PowerFlex system by the export node powerflex_export_node_write_latency_milliseconds The time (in ms) to complete write operations within PowerFlex system by the export host powerflex_export_node_read_iops_per_second The number of read operations performed by an export node (per second) powerflex_export_node_write_iops_per_second The number of write operations performed by an export node (per second) powerflex_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s) powerflex_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s) powerflex_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume powerflex_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume powerflex_volume_read_iops_per_second The number of read operations performed against a volume (per second) powerflex_volume_write_iops_per_second The number of write operations performed against a volume (per second) Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the storage_class_pool_metrics_enabled field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerflex_storage_pool_total_logical_capacity_gigabytes The logical capacity (size) of a storage pool (GB) powerflex_storage_pool_logical_capacity_available_gigabytes The capacity available for use (GB) powerflex_storage_pool_logical_capacity_in_use_gigabytes The logical capacity of a storage pool in use (GB) powerflex_storage_pool_logical_provisioned_gigabytes The total size of volumes (thick and thin) provisioned in a storage pool (GB) ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability PowerFlex Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability PowerFlex …","ref":"/csm-docs/docs/observability/metrics/powerflex/","tags":"","title":"PowerFlex Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerFlex. The Grafana reference dashboards for PowerFlex metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the sdc_metrics_enabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerflex_export_node_read_bw_megabytes_per_second The export node read bandwidth (MB/s) within PowerFlex system powerflex_export_node_write_bw_megabytes_per_second The export node write bandwidth (MB/s) powerflex_export_node_read_latency_milliseconds The time (in ms) to complete read operations within PowerFlex system by the export node powerflex_export_node_write_latency_milliseconds The time (in ms) to complete write operations within PowerFlex system by the export host powerflex_export_node_read_iops_per_second The number of read operations performed by an export node (per second) powerflex_export_node_write_iops_per_second The number of write operations performed by an export node (per second) powerflex_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s) powerflex_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s) powerflex_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume powerflex_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume powerflex_volume_read_iops_per_second The number of read operations performed against a volume (per second) powerflex_volume_write_iops_per_second The number of write operations performed against a volume (per second) Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the storage_class_pool_metrics_enabled field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerflex_storage_pool_total_logical_capacity_gigabytes The logical capacity (size) of a storage pool (GB) powerflex_storage_pool_logical_capacity_available_gigabytes The capacity available for use (GB) powerflex_storage_pool_logical_capacity_in_use_gigabytes The logical capacity of a storage pool in use (GB) powerflex_storage_pool_logical_provisioned_gigabytes The total size of volumes (thick and thin) provisioned in a storage pool (GB) ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability PowerFlex Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability PowerFlex …","ref":"/csm-docs/v1/observability/metrics/powerflex/","tags":"","title":"PowerFlex Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerFlex. The Grafana reference dashboards for PowerFlex metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the sdc_metrics_enabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerflex_export_node_read_bw_megabytes_per_second The export node read bandwidth (MB/s) within PowerFlex system powerflex_export_node_write_bw_megabytes_per_second The export node write bandwidth (MB/s) powerflex_export_node_read_latency_milliseconds The time (in ms) to complete read operations within PowerFlex system by the export node powerflex_export_node_write_latency_milliseconds The time (in ms) to complete write operations within PowerFlex system by the export host powerflex_export_node_read_iops_per_second The number of read operations performed by an export node (per second) powerflex_export_node_write_iops_per_second The number of write operations performed by an export node (per second) powerflex_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s) powerflex_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s) powerflex_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume powerflex_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume powerflex_volume_read_iops_per_second The number of read operations performed against a volume (per second) powerflex_volume_write_iops_per_second The number of write operations performed against a volume (per second) Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the storage_class_pool_metrics_enabled field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerflex_storage_pool_total_logical_capacity_gigabytes The logical capacity (size) of a storage pool (GB) powerflex_storage_pool_logical_capacity_available_gigabytes The capacity available for use (GB) powerflex_storage_pool_logical_capacity_in_use_gigabytes The logical capacity of a storage pool in use (GB) powerflex_storage_pool_logical_provisioned_gigabytes The total size of volumes (thick and thin) provisioned in a storage pool (GB) ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability PowerFlex Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability PowerFlex …","ref":"/csm-docs/v2/observability/metrics/powerflex/","tags":"","title":"PowerFlex Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerFlex. The Grafana reference dashboards for PowerFlex metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the sdc_metrics_enabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerflex_export_node_read_bw_megabytes_per_second The export node read bandwidth (MB/s) within PowerFlex system powerflex_export_node_write_bw_megabytes_per_second The export node write bandwidth (MB/s) powerflex_export_node_read_latency_milliseconds The time (in ms) to complete read operations within PowerFlex system by the export node powerflex_export_node_write_latency_milliseconds The time (in ms) to complete write operations within PowerFlex system by the export host powerflex_export_node_read_iops_per_second The number of read operations performed by an export node (per second) powerflex_export_node_write_iops_per_second The number of write operations performed by an export node (per second) powerflex_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s) powerflex_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s) powerflex_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume powerflex_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume powerflex_volume_read_iops_per_second The number of read operations performed against a volume (per second) powerflex_volume_write_iops_per_second The number of write operations performed against a volume (per second) Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the storage_class_pool_metrics_enabled field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerflex_storage_pool_total_logical_capacity_gigabytes The logical capacity (size) of a storage pool (GB) powerflex_storage_pool_logical_capacity_available_gigabytes The capacity available for use (GB) powerflex_storage_pool_logical_capacity_in_use_gigabytes The logical capacity of a storage pool in use (GB) powerflex_storage_pool_logical_provisioned_gigabytes The total size of volumes (thick and thin) provisioned in a storage pool (GB) ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability PowerFlex Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability PowerFlex …","ref":"/csm-docs/v3/observability/metrics/powerflex/","tags":"","title":"PowerFlex Metrics"},{"body":"Multi Unisphere Support Starting with v1.7, the CSI PowerMax driver can communicate with multiple Unisphere for PowerMax servers to manage multiple PowerMax arrays. In order to use this feature, you must install CSI PowerMax ReverseProxy in StandAlone mode with the driver. For more details on how to configure the driver and ReverseProxy, see the relevant section here\nVolume Snapshot Feature The CSI PowerMax driver version 1.7 and later supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class To use this feature, enable it in values.yaml\nsnapshot: enabled: true Note: From v1.7, the CSI PowerMax driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the csi-powermax/samples/volumesnapshotclass folder\nNote: Snapshots for File in PowerMax is currently not supported.\nCreating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 spec: volumeSnapshotClassName: powermax-snapclass source: persistentVolumeClaimName: pvol0 After the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmax-restore-pvc-demo namespace: test spec: storageClassName: powermax dataSource: name: pmax-snapshot-demo kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Creating PVCs with PVCs as source This is a sample manifest for creating a PVC with another PVC as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmax-clone-pvc-demo namespace: test spec: storageClassName: powermax dataSource: name: pmax-pvc-demo kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi iSCSI CHAP Starting from version 1.3.0, the unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI has been supported. To enable CHAP authentication:\nCreate secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true. The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u003csymid\u003e -iscsi \u003chost iqn\u003e set chap -cred \u003chost IQN\u003e -secret \u003cCHAP secret\u003e Where host IQN is the name of the iSCSI initiator of a host IQN, and CHAP secret is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which is stored in the host initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name Starting from version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this feature, set the following values under customDriverName in my-powermax-settings.yaml.\nValue: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template. If enabled, the driver name is in the following format: \u003cnamespace\u003e.\u003cdriver name\u003e.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax, then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers To install multiple CSI Drivers for Dell PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions that should be strictly adhered to:\nOnly one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations To install multiple CSI drivers, follow these steps:\nCreate (or use) a new namespace. Ensure that all the pre-requisites are met: powermax-creds secret is created in this namespace (Optional) powermax-certs secret is created in this namespace Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver. Volume expansion Starting in v1.4, the CSI PowerMax driver supports the expansion of Persistent Volumes (PVs). This expansion is done online, which is when the PVC is attached to any node.\nTo use this feature, enable in values.yaml\nresizer: enabled: true To use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThis is a sample manifest for a storage class that allows for Volume Expansion.\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-expand-sc annotations: storageclass.kubernetes.io/is-default-class: false provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true #Set this attribute to true if you plan to expand any PVCs created using this storage class parameters: SYMID: \"000000000001\" SRP: \"DEFAULT_SRP\" ServiceLevel: \"Bronze\" To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC.\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pmax-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 10Gi #Updated size from 5Gi to 10Gi storageClassName: powermax-expand-sc NOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of the volume, it cannot be used to shrink a volume.\nRaw block support Starting in v1.4, the CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powermaxtest namespace: {{ .Values.namespace }} spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: powermax resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy CSI PowerMax Reverse Proxy application is deployed along with the driver to get the maximum performance out of the CSI driver for PowerMax and Unisphere for PowerMax REST APIs.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server that acts as a reverse proxy for the Unisphere for PowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy application helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and the performance of the CSI PowerMax driver.\nOptionally, you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation The CSI PowerMax Reverse Proxy can be installed in two ways:\nIt can be installed as a Kubernetes deployment in the same namespace as the driver. It can be installed as a sidecar to the driver’s controller Pod. It is also configured as a Kubernetes “NodePort” service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is an HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be an X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nStarting from v2.7.0 , the secrets for proxy will be created automatically using the below tls.key and tls.cert contents provided in values.yaml file. For this , we need to install cert-manager using below command which manages the certs and secrets .\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml Here is an example showing how to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 Using Helm installer In the my-powermax-settings.yaml file, the csireverseproxy section can be used to configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. The install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSM Operator For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSM Operator documentation for PowerMax.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two ‘%’ characters. String prefixing first ‘%’ and string suffixing second ‘%’ is used as is before and after every node identifier.\nAlso, a new setting, modifyHostName, can be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u003cClusterPrefix\u003e-\u003cHostName\u003e*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1, then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], ‘-’ and ‘_’, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of the controller Pod. Leader election is only applicable for all sidecar containers and driver container will be running in all controller pods . In case of a failure, one of the standby Pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two-controller Pods are ever scheduled on the same node.\nTo increase or decrease the number of controller Pods, edit the following value in values.yaml file:\ncontrollerCount: 2 NOTE: The default value for controllerCount is 2. We recommend not changing this unless it is really necessary. Also, if the controller count is greater than the number of available nodes (where the Pods can be scheduled), some controller Pods will remain in the Pending state\nIf you are using the Dell CSM Operator, the value to adjust is:\nreplicas: 2 For more details about configuring Controller HA using the Dell CSM Operator, see the Dell CSM Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment and driver node Daemonset. There are two new sections in the values file - controller and node - where you can specify these values separately for the controller and node Pods.\ncontroller If you want to apply nodeSelectors and tolerations for the controller Pods, edit the controller section in the values file.\nHere are some examples:\nTo schedule controller Pods to worker nodes only (Default): controller: nodeSelector: tolerations: Set the following values for controller Pods to tolerate the taint NoSchedule on master nodes: controller: nodeSelector: tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" Set the following values for controller Pods to be scheduled only on nodes labelled master (node-role.kubernetes.io/master): controller: nodeSelector: node-role.kubernetes.io/master: \"\" tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" node If you want to apply nodeSelectors and tolerations for the node Pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add and remove tolerations to this list\n# \"node\" allows to configure node specific parameters node: # \"node.nodeSelector\" defines what nodes would be selected for Pods of node daemonset # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"node.tolerations\" defines tolerations that would be applied to node daemonset # Add/Remove tolerations as per requirement # Leave as blank if you wish to not apply any tolerations tolerations: - key: \"node.kubernetes.io/memory-pressure\" operator: \"Exists\" effect: \"NoExecute\" - key: \"node.kubernetes.io/disk-pressure\" operator: \"Exists\" effect: \"NoExecute\" - key: \"node.kubernetes.io/network-unavailable\" operator: \"Exists\" effect: \"NoExecute\" Topology Support Starting from version 1.5, the CSI PowerMax driver supports topology-aware volume provisioning which helps the Kubernetes scheduler place PVCs on worker nodes that have access to the backend storage. When used with nodeSelectors which can be specified for the driver node Pods, it provides an effective way to provision applications on nodes that have access to the PowerMax array.\nAfter a successful installation of the driver, if a node Pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\ncsi-powermax.dellemc.com/\\\u003carray-id\\\u003e If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.iscsi The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\nStarting from version 2.3.0, topology keys have been enhanced to filter out arrays, associated transport protocol available to each node and create topology keys based on any such user input.\nTopology Usage To use the Topology feature, the storage classes must be modified as follows:\nvolumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section For example, a PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-fc parameters: SRP: \"SRP_1\" SYMID: \"000000000001\" ServiceLevel: \u003cService Level\u003e #Insert Service Level Name provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true allowedTopologies: - matchLabelExpressions: - key: csi-powermax.dellemc.com/000000000001 values: - csi-powermax.dellemc.com - key: csi-powermax.dellemc.com/000000000001.fc values: - csi-powermax.dellemc.com In the above example, if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\nA set of sample storage class definitions to enable topology-aware volume provisioning has been provided in the csi-powermax/samples/storageclass folder\nFor additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\nCustom Topology keys To use the enhanced topology keys:\nTo use this feature, set node.topologyControl.enabled to true. Edit the config file topologyConfig.yaml in csi-powermax/samples/configmap folder and provide values for the following parameters. Parameter Description allowedConnections List of node, array and protocol info for user allowed configuration allowedConnections.nodeName Name of the node on which user wants to apply given rules allowedConnections.rules List of StorageArrayID:TransportProtocol pair deniedConnections List of node, array and protocol info for user denied configuration deniedConnections.nodeName Name of the node on which user wants to apply given rules deniedConnections.rules List of StorageArrayID:TransportProtocol pair Sample config file:\n# allowedConnections contains a list of (node, array and protocol) info for user allowed configuration # For any given storage array ID and protocol on a Node, topology keys will be created for just those pair and # every other configuration is ignored # Please refer to the doc website about a detailed explanation of each configuration parameter # and the various possible inputs allowedConnections: # nodeName: Name of the node on which user wants to apply given rules # Allowed values: # nodeName - name of a specific node # * - all the nodes # Examples: \"node1\", \"*\" - nodeName: \"node1\" # rules is a list of 'StorageArrayID:TransportProtocol' pair. ':' is required between both value # Allowed values: # StorageArrayID: # - SymmetrixID : for specific storage array # - \"*\" :- for all the arrays connected to the node # TransportProtocol: # - FC : Fibre Channel protocol # - ISCSI : iSCSI protocol # - \"*\" - for all the possible Transport Protocol # Examples: \"000000000001:FC\", \"000000000002:*\", \"*:FC\", \"*:*\" rules: - \"000000000001:FC\" - \"000000000002:FC\" - nodeName: \"*\" rules: - \"000000000002:FC\" # deniedConnections contains a list of (node, array and protocol) info for denied configurations by user # For any given storage array ID and protocol on a Node, topology keys will be created for every other configuration but # not these input pairs deniedConnections: - nodeName: \"node2\" rules: - \"000000000002:*\" - nodeName: \"node3\" rules: - \"*:*\" Use the below command to create ConfigMap with configmap name as node-topology-config in the namespace powermax, kubectl create configmap node-topology-config --from-file=topologyConfig.yaml -n powermax For example, let there be 3 nodes and 2 arrays, so based on the sample config file above, topology keys will be created as below:\nNew Topology keys N1: csi-driver/000000000001.FC:csi-driver, csi-driver/000000000002.FC:csi-driver N2 and N3: None\nNote: Name of the configmap should always be node-topology-config.\nDynamic Logging Configuration This feature is introduced in CSI Driver for PowerMax version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in my-powermax-settings.yaml during driver installation.\nTo change the log level dynamically to a different value, the user can edit the same my-powermax-settings.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade Note: my-powermax-settings.yaml is a values.yaml file which the user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level the user can set this field during driver installation.\nTo update the log level dynamically, the user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Volume Health Monitoring CSI Driver for Dell PowerMax 2.2.0 and above supports volume health monitoring. To enable Volume Health Monitoring from the node side, the alpha feature gate CSIVolumeHealth needs to be enabled. To use this feature, set controller.healthMonitor.enabled and node.healthMonitor.enabled to true. To change the monitor interval, set controller.healthMonitor.interval parameter.\nSingle Pod Access Mode for PersistentVolumes- ReadWriteOncePod (ALPHA FEATURE) Use ReadWriteOncePod(RWOP) access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it. This is only supported for CSI Driver for PowerMax 2.2.0+ and Kubernetes version 1.22+.\nTo use this feature, enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet, by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\"\nCreating a PersistentVolumeClaim kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-writer-only spec: accessModes: - ReadWriteOncePod # the volume can be mounted as read-write by a single pod across the whole cluster resources: requests: storage: 1Gi When this feature is enabled, the existing ReadWriteOnce(RWO) access mode restricts volume access to a single node and allows multiple pods on the same node to read from and write to the same volume.\nTo migrate existing PersistentVolumes to use ReadWriteOncePod, please follow the instruction from here.\nSupport for auto RDM for vSphere over FC CSI Driver for Dell PowerMax 2.5.0 and above supports auto RDM for vSphere over FC.\nThis feature supports volume provisioning on Kubernetes clusters running on vSphere (VMware hypervisor) via RDM mechanism. This feature enables the users to use PMAX CSI drivers with VMs on vSphere Hypervisor with the same feature and functionality as there with bare metal servers when they have only FC ports in PMAX storage.\nIt will be supported only on new/freshly installed clusters where the cluster is exclusively deployed in a virtualized vSphere environment. Having hybrid topologies like ISCSI or FC (in pass-through) is not supported.\nTo use this feature\nSet vSphere.enabled to true. Create a secret which contains vCenter privileges. Follow the steps here to create it. Update vCenterCredSecret with the secret name created. VMware/vSphere virtualization support # set enable to true, if you to enable VMware virtualized environment support via RDM # Allowed Values: # \"true\" - vSphere volumes are enabled # \"false\" - vSphere volumes are disabled # Default value: \"false\" vSphere: enabled: false # fcPortGroup: an existing portGroup that driver will use for vSphere # recommended format: csi-x-VC-PG, x can be anything of user choice fcPortGroup: \"csi-vsphere-VC-PG\" # fcHostGroup: an existing host(initiator group) that driver will use for vSphere # this hostGroup should contain initiators from all the ESXs/ESXi host # where the cluster is deployed # recommended format: csi-x-VC-HG, x can be anything of user choice fcHostGroup: \"csi-vsphere-VC-HG\" # vCenterHost: URL/endpoint of the vCenter where all the ESX are present vCenterHost: \"00.000.000.01\" # vCenterCredSecret: secret name for the vCenter credentials vCenterCredSecret: vcenter-creds Note: Replication is not supported with this feature. Limitations of RDM can be referred here.\nSupported number of RDM Volumes per VM is 60 as per the limitations. RDMs should not be added/removed manually from vCenter on any of the cluster VMs.\nStorage Capacity Tracking CSI PowerMax driver version 2.8.0 and above supports Storage Capacity Tracking.\nThis feature helps the scheduler to make more informed choices about where to start pods that depend on unbound volumes with late binding (aka “wait for first consumer”). Nodes satisfying the topology constraints, and with the requested capacity that is present on the storage array, will be available for scheduling the pods, Otherwise, the pods stay in pending state. External-provisioner makes one GetCapacity() call per storage class that is present on the cluster to get the AvailableCapacity for the array specified in the storage class that matches with the array mentioned during driver deployment.\nWithout storage capacity tracking, pods get scheduled on a node satisfying the topology constraints. If the required capacity is not available, volume attachment to the pods fails, and pods remain in the ContainerCreating state. Storage capacity tracking eliminates unnecessary scheduling of pods when there is insufficient capacity.\nStorage capacity can be tracked by setting the attribute storageCapacity.enabled to true in values.yaml (set to true by default) during driver installation. To configure how often driver checks for changed capacity, set the storageCapacity.pollInterval attribute (set to 5m by default). In case of driver installed via operator, this interval can be configured in the sample file provided here. by editing the --capacity-poll-interval argument present in the provisioner sidecar.\nVolume Limits The CSI Driver for Dell PowerMax allows users to specify the maximum number of PowerMax volumes that can be created on a node.\nThe user can set the volume limit for a node by creating a node label max-powermax-volumes-per-node and specifying the volume limit for that node. kubectl label node \u003cnode_name\u003e max-powermax-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxPowerMaxVolumesPerNode attribute in values.yaml. In case of driver installed via operator, this attribute can be modified in the sample file provided here by editing the X_CSI_MAX_VOLUMES_PER_NODE parameter.\nThis feature is also supported for limiting the volume provisioning on Kubernetes clusters running on vSphere (VMware hypervisor) via RDM mechanism. User can set vSphere.enabled to true and also set volume limits to positive values less than or equal 60 via labels or in Values.yaml file.\nNOTE: The default value of maxPowerMaxVolumesPerNode is 0. If maxPowerMaxVolumesPerNode is set to zero, then CO shall decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxPowerMaxVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-powermax-volumes-per-node is not set. Supported maximum number of RDM Volumes per VM is 60 as per the limitations. If the value is set both by node label and values.yaml file then node label value will get the precedence and user has to remove the node label in order to reflect the values.yaml value.\n","categories":"","description":"Code features for PowerMax Driver","excerpt":"Code features for PowerMax Driver","ref":"/csm-docs/docs/csidriver/features/powermax/","tags":"","title":"PowerMax"},{"body":"You can upgrade CSI Driver for Dell PowerMax using Helm or Dell CSM Operator.\nNote: CSI Driver for PowerMax v2.4.0 requires 10.0 REST endpoint support of Unisphere.\nUpdating the CSI Driver to use 10.0 Unisphere Upgrade the Unisphere to have 10.0 endpoint support.Please find the instructions here. Update the my-powermax-settings.yaml to have endpoint with 10.0 support. Update Driver from v2.8 to v2.9.1 using Helm Steps\nRun git clone -b v2.9.1 https://github.com/dell/csi-powermax.git to clone the git repository and get the driver. Update the values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade NOTE:\nIf you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver.\nTo update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option --upgrade, for example:\n./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade You cannot upgrade between drivers with different fsGroupPolicies. To check the current driver’s fsGroupPolicy, use this command:\nkubectl describe csidriver csi-powermax and check the “Spec” section:\n... Spec: Attach Required: true Fs Group Policy: ReadWriteOnceWithFSType Pod Info On Mount: false Requires Republish: false Storage Capacity: false ... Upgrade using Dell CSM Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\nUpgrade the Dell CSM Operator by following here Once the operator is upgraded, to upgrade the driver, refer here ","categories":"","description":"Upgrade PowerMax CSI driver","excerpt":"Upgrade PowerMax CSI driver","ref":"/csm-docs/docs/csidriver/upgradation/drivers/powermax/","tags":["upgrade","csi-driver"],"title":"PowerMax"},{"body":" Linked Proxy mode for CSI reverse proxy is no longer actively maintained or supported. It will be deprecated in CSM 1.9 (Driver Version 2.9.0). It is highly recommended that you use stand alone mode going forward. Multi Unisphere Support Starting with v1.7, the CSI PowerMax driver can communicate with multiple Unisphere for PowerMax servers to manage multiple PowerMax arrays. In order to use this feature, you must install CSI PowerMax ReverseProxy in StandAlone mode with the driver. For more details on how to configure the driver and ReverseProxy, see the relevant section here\nVolume Snapshot Feature The CSI PowerMax driver version 1.7 and later supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class To use this feature, enable it in values.yaml\nsnapshot: enabled: true Note: From v1.7, the CSI PowerMax driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the csi-powermax/samples/volumesnapshotclass folder\nNote: Snapshot for FIle in PowerMax is currently not supported.\nCreating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 spec: volumeSnapshotClassName: powermax-snapclass source: persistentVolumeClaimName: pvol0 After the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmax-restore-pvc-demo namespace: test spec: storageClassName: powermax dataSource: name: pmax-snapshot-demo kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Creating PVCs with PVCs as source This is a sample manifest for creating a PVC with another PVC as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmax-clone-pvc-demo namespace: test spec: storageClassName: powermax dataSource: name: pmax-pvc-demo kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi iSCSI CHAP Starting from version 1.3.0, the unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI has been supported. To enable CHAP authentication:\nCreate secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true. The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u003csymid\u003e -iscsi \u003chost iqn\u003e set chap -cred \u003chost IQN\u003e -secret \u003cCHAP secret\u003e Where host IQN is the name of the iSCSI initiator of a host IQN, and CHAP secret is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which is stored in the host initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name Starting from version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this feature, set the following values under customDriverName in my-powermax-settings.yaml.\nValue: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template. If enabled, the driver name is in the following format: \u003cnamespace\u003e.\u003cdriver name\u003e.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax, then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers To install multiple CSI Drivers for Dell PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions that should be strictly adhered to:\nOnly one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations To install multiple CSI drivers, follow these steps:\nCreate (or use) a new namespace. Ensure that all the pre-requisites are met: powermax-creds secret is created in this namespace (Optional) powermax-certs secret is created in this namespace Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver. Volume expansion Starting in v1.4, the CSI PowerMax driver supports the expansion of Persistent Volumes (PVs). This expansion is done online, which is when the PVC is attached to any node.\nTo use this feature, enable in values.yaml\nresizer: enabled: true To use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThis is a sample manifest for a storage class that allows for Volume Expansion.\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-expand-sc annotations: storageclass.kubernetes.io/is-default-class: false provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true #Set this attribute to true if you plan to expand any PVCs created using this storage class parameters: SYMID: \"000000000001\" SRP: \"DEFAULT_SRP\" ServiceLevel: \"Bronze\" To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC.\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pmax-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 10Gi #Updated size from 5Gi to 10Gi storageClassName: powermax-expand-sc NOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of the volume, it cannot be used to shrink a volume.\nRaw block support Starting in v1.4, the CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powermaxtest namespace: {{ .Values.namespace }} spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: powermax resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy CSI PowerMax Reverse Proxy application is deployed along with the driver to get the maximum performance out of the CSI driver for PowerMax and Unisphere for PowerMax REST APIs.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server that acts as a reverse proxy for the Unisphere for PowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy application helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and the performance of the CSI PowerMax driver.\nOptionally, you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation The CSI PowerMax Reverse Proxy can be installed in two ways:\nIt can be installed as a Kubernetes deployment in the same namespace as the driver. It can be installed as a sidecar to the driver’s controller Pod. It is also configured as a Kubernetes “NodePort” service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is an HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be an X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nStarting from v2.7.0 , the secrets for proxy will be created automatically using the below tls.key and tls.cert contents provided in values.yaml file. For this , we need to install cert-manager using below command which manages the certs and secrets .\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml Here is an example showing how to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 Using Helm installer In the my-powermax-settings.yaml file, the csireverseproxy section can be used to configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. The install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name “powermax-reverseproxy”. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation for PowerMax.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two ‘%’ characters. String prefixing first ‘%’ and string suffixing second ‘%’ is used as is before and after every node identifier.\nAlso, a new setting, modifyHostName, can be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u003cClusterPrefix\u003e-\u003cHostName\u003e*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1, then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], ‘-’ and ‘_’, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of the controller Pod. At any time, only one controller Pod is active(leader), and the rest are on standby. In case of a failure, one of the standby Pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two-controller Pods are ever scheduled on the same node.\nTo increase or decrease the number of controller Pods, edit the following value in values.yaml file:\ncontrollerCount: 2 NOTE: The default value for controllerCount is 2. We recommend not changing this unless it is really necessary. Also, if the controller count is greater than the number of available nodes (where the Pods can be scheduled), some controller Pods will remain in the Pending state\nIf you are using dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, see the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment and driver node Daemonset. There are two new sections in the values file - controller and node - where you can specify these values separately for the controller and node Pods.\ncontroller If you want to apply nodeSelectors and tolerations for the controller Pods, edit the controller section in the values file.\nHere are some examples:\nTo schedule controller Pods to worker nodes only (Default): controller: nodeSelector: tolerations: Set the following values for controller Pods to tolerate the taint NoSchedule on master nodes: controller: nodeSelector: tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" Set the following values for controller Pods to be scheduled only on nodes labelled master (node-role.kubernetes.io/master): controller: nodeSelector: node-role.kubernetes.io/master: \"\" tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" node If you want to apply nodeSelectors and tolerations for the node Pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add and remove tolerations to this list\n# \"node\" allows to configure node specific parameters node: # \"node.nodeSelector\" defines what nodes would be selected for Pods of node daemonset # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"node.tolerations\" defines tolerations that would be applied to node daemonset # Add/Remove tolerations as per requirement # Leave as blank if you wish to not apply any tolerations tolerations: - key: \"node.kubernetes.io/memory-pressure\" operator: \"Exists\" effect: \"NoExecute\" - key: \"node.kubernetes.io/disk-pressure\" operator: \"Exists\" effect: \"NoExecute\" - key: \"node.kubernetes.io/network-unavailable\" operator: \"Exists\" effect: \"NoExecute\" Topology Support Starting from version 1.5, the CSI PowerMax driver supports topology-aware volume provisioning which helps the Kubernetes scheduler place PVCs on worker nodes that have access to the backend storage. When used with nodeSelectors which can be specified for the driver node Pods, it provides an effective way to provision applications on nodes that have access to the PowerMax array.\nAfter a successful installation of the driver, if a node Pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\ncsi-powermax.dellemc.com/\\\u003carray-id\\\u003e If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.iscsi The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\nStarting from version 2.3.0, topology keys have been enhanced to filter out arrays, associated transport protocol available to each node and create topology keys based on any such user input.\nTopology Usage To use the Topology feature, the storage classes must be modified as follows:\nvolumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section For example, a PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-fc parameters: SRP: \"SRP_1\" SYMID: \"000000000001\" ServiceLevel: \u003cService Level\u003e #Insert Service Level Name provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true allowedTopologies: - matchLabelExpressions: - key: csi-powermax.dellemc.com/000000000001 values: - csi-powermax.dellemc.com - key: csi-powermax.dellemc.com/000000000001.fc values: - csi-powermax.dellemc.com In the above example, if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\nA set of sample storage class definitions to enable topology-aware volume provisioning has been provided in the csi-powermax/samples/storageclass folder\nFor additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\nCustom Topology keys To use the enhanced topology keys:\nTo use this feature, set node.topologyControl.enabled to true. Edit the config file topologyConfig.yaml in csi-powermax/samples/configmap folder and provide values for the following parameters. Parameter Description allowedConnections List of node, array and protocol info for user allowed configuration allowedConnections.nodeName Name of the node on which user wants to apply given rules allowedConnections.rules List of StorageArrayID:TransportProtocol pair deniedConnections List of node, array and protocol info for user denied configuration deniedConnections.nodeName Name of the node on which user wants to apply given rules deniedConnections.rules List of StorageArrayID:TransportProtocol pair Sample config file:\n# allowedConnections contains a list of (node, array and protocol) info for user allowed configuration # For any given storage array ID and protocol on a Node, topology keys will be created for just those pair and # every other configuration is ignored # Please refer to the doc website about a detailed explanation of each configuration parameter # and the various possible inputs allowedConnections: # nodeName: Name of the node on which user wants to apply given rules # Allowed values: # nodeName - name of a specific node # * - all the nodes # Examples: \"node1\", \"*\" - nodeName: \"node1\" # rules is a list of 'StorageArrayID:TransportProtocol' pair. ':' is required between both value # Allowed values: # StorageArrayID: # - SymmetrixID : for specific storage array # - \"*\" :- for all the arrays connected to the node # TransportProtocol: # - FC : Fibre Channel protocol # - ISCSI : iSCSI protocol # - \"*\" - for all the possible Transport Protocol # Examples: \"000000000001:FC\", \"000000000002:*\", \"*:FC\", \"*:*\" rules: - \"000000000001:FC\" - \"000000000002:FC\" - nodeName: \"*\" rules: - \"000000000002:FC\" # deniedConnections contains a list of (node, array and protocol) info for denied configurations by user # For any given storage array ID and protocol on a Node, topology keys will be created for every other configuration but # not these input pairs deniedConnections: - nodeName: \"node2\" rules: - \"000000000002:*\" - nodeName: \"node3\" rules: - \"*:*\" Use the below command to create ConfigMap with configmap name as node-topology-config in the namespace powermax, kubectl create configmap node-topology-config --from-file=topologyConfig.yaml -n powermax For example, let there be 3 nodes and 2 arrays, so based on the sample config file above, topology keys will be created as below:\nNew Topology keys N1: csi-driver/000000000001.FC:csi-driver, csi-driver/000000000002.FC:csi-driver N2 and N3: None\nNote: Name of the configmap should always be node-topology-config.\nDynamic Logging Configuration This feature is introduced in CSI Driver for PowerMax version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in my-powermax-settings.yaml during driver installation.\nTo change the log level dynamically to a different value, the user can edit the same my-powermax-settings.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade Note: my-powermax-settings.yaml is a values.yaml file which the user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level the user can set this field during driver installation.\nTo update the log level dynamically, the user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Volume Health Monitoring CSI Driver for Dell PowerMax 2.2.0 and above supports volume health monitoring. To enable Volume Health Monitoring from the node side, the alpha feature gate CSIVolumeHealth needs to be enabled. To use this feature, set controller.healthMonitor.enabled and node.healthMonitor.enabled to true. To change the monitor interval, set controller.healthMonitor.interval parameter.\nSingle Pod Access Mode for PersistentVolumes- ReadWriteOncePod (ALPHA FEATURE) Use ReadWriteOncePod(RWOP) access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it. This is only supported for CSI Driver for PowerMax 2.2.0+ and Kubernetes version 1.22+.\nTo use this feature, enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet, by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\"\nCreating a PersistentVolumeClaim kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-writer-only spec: accessModes: - ReadWriteOncePod # the volume can be mounted as read-write by a single pod across the whole cluster resources: requests: storage: 1Gi When this feature is enabled, the existing ReadWriteOnce(RWO) access mode restricts volume access to a single node and allows multiple pods on the same node to read from and write to the same volume.\nTo migrate existing PersistentVolumes to use ReadWriteOncePod, please follow the instruction from here.\nSupport for auto RDM for vSphere over FC CSI Driver for Dell PowerMax 2.5.0 and above supports auto RDM for vSphere over FC.\nThis feature supports volume provisioning on Kubernetes clusters running on vSphere (VMware hypervisor) via RDM mechanism. This feature enables the users to use PMAX CSI drivers with VMs on vSphere Hypervisor with the same feature and functionality as there with bare metal servers when they have only FC ports in PMAX storage.\nIt will be supported only on new/freshly installed clusters where the cluster is exclusively deployed in a virtualized vSphere environment. Having hybrid topologies like ISCSI or FC (in pass-through) is not supported.\nTo use this feature\nSet vSphere.enabled to true. Create a secret which contains vCenter privileges. Follow the steps here to create it. Update vCenterCredSecret with the secret name created. VMware/vSphere virtualization support # set enable to true, if you to enable VMware virtualized environment support via RDM # Allowed Values: # \"true\" - vSphere volumes are enabled # \"false\" - vSphere volumes are disabled # Default value: \"false\" vSphere: enabled: false # fcPortGroup: an existing portGroup that driver will use for vSphere # recommended format: csi-x-VC-PG, x can be anything of user choice fcPortGroup: \"csi-vsphere-VC-PG\" # fcHostGroup: an existing host(initiator group) that driver will use for vSphere # this hostGroup should contain initiators from all the ESXs/ESXi host # where the cluster is deployed # recommended format: csi-x-VC-HG, x can be anything of user choice fcHostGroup: \"csi-vsphere-VC-HG\" # vCenterHost: URL/endpoint of the vCenter where all the ESX are present vCenterHost: \"00.000.000.01\" # vCenterCredSecret: secret name for the vCenter credentials vCenterCredSecret: vcenter-creds Note: Replication is not supported with this feature. Limitations of RDM can be referred here.\nSupported number of RDM Volumes per VM is 60 as per the limitations. RDMs should not be added/removed manually from vCenter on any of the cluster VMs.\nStorage Capacity Tracking CSI PowerMax driver version 2.8.0 and above supports Storage Capacity Tracking.\nThis feature helps the scheduler to make more informed choices about where to start pods that depend on unbound volumes with late binding (aka “wait for first consumer”). Nodes satisfying the topology constraints, and with the requested capacity that is present on the storage array, will be available for scheduling the pods, Otherwise, the pods stay in pending state. External-provisioner makes one GetCapacity() call per storage class that is present on the cluster to get the AvailableCapacity for the array specified in the storage class that matches with the array mentioned during driver deployment.\nWithout storage capacity tracking, pods get scheduled on a node satisfying the topology constraints. If the required capacity is not available, volume attachment to the pods fails, and pods remain in the ContainerCreating state. Storage capacity tracking eliminates unnecessary scheduling of pods when there is insufficient capacity.\nStorage capacity can be tracked by setting the attribute storageCapacity.enabled to true in values.yaml (set to true by default) during driver installation. To configure how often driver checks for changed capacity, set the storageCapacity.pollInterval attribute (set to 5m by default). In case of driver installed via operator, this interval can be configured in the sample file provided here. by editing the --capacity-poll-interval argument present in the provisioner sidecar.\nNote: This feature requires kubernetes v1.24 and above and will be automatically disabled in lower version of kubernetes.\nVolume Limits The CSI Driver for Dell PowerMax allows users to specify the maximum number of PowerMax volumes that can be created on a node.\nThe user can set the volume limit for a node by creating a node label max-powermax-volumes-per-node and specifying the volume limit for that node. kubectl label node \u003cnode_name\u003e max-powermax-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxPowerMaxVolumesPerNode attribute in values.yaml. In case of driver installed via operator, this attribute can be modified in the sample file provided here by editing the X_CSI_MAX_VOLUMES_PER_NODE parameter.\nThis feature is also supported for limiting the volume provisioning on Kubernetes clusters running on vSphere (VMware hypervisor) via RDM mechanism. User can set vSphere.enabled to true and also set volume limits to positive values less than or equal 60 via labels or in Values.yaml file.\nNOTE: The default value of maxPowerMaxVolumesPerNode is 0. If maxPowerMaxVolumesPerNode is set to zero, then CO shall decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxPowerMaxVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-powermax-volumes-per-node is not set. Supported maximum number of RDM Volumes per VM is 60 as per the limitations. If the value is set both by node label and values.yaml file then node label value will get the precedence and user has to remove the node label in order to reflect the values.yaml value.\n","categories":"","description":"Code features for PowerMax Driver","excerpt":"Code features for PowerMax Driver","ref":"/csm-docs/v1/csidriver/features/powermax/","tags":"","title":"PowerMax"},{"body":"You can upgrade CSI Driver for Dell PowerMax using Helm or Dell CSI Operator.\nNote: CSI Driver for PowerMax v2.4.0 requires 10.0 REST endpoint support of Unisphere.\nUpdating the CSI Driver to use 10.0 Unisphere Upgrade the Unisphere to have 10.0 endpoint support.Please find the instructions here. Update the my-powermax-settings.yaml to have endpoint with 10.0 support. Update Driver from v2.7 to v2.8 using Helm Steps\nRun git clone -b v2.8.0 https://github.com/dell/csi-powermax.git to clone the git repository and get the driver. Update the values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade NOTE:\nIf you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver.\nTo update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option --upgrade, for example:\n./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade You cannot upgrade between drivers with different fsGroupPolicies. To check the current driver’s fsGroupPolicy, use this command:\nkubectl describe csidriver csi-powermax and check the “Spec” section:\n... Spec: Attach Required: true Fs Group Policy: ReadWriteOnceWithFSType Pod Info On Mount: false Requires Republish: false Storage Capacity: false ... Upgrade using Dell CSM Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\nUpgrade the Dell CSM Operator by following here Once the operator is upgraded, to upgrade the driver, refer here ","categories":"","description":"Upgrade PowerMax CSI driver","excerpt":"Upgrade PowerMax CSI driver","ref":"/csm-docs/v1/csidriver/upgradation/drivers/powermax/","tags":["upgrade","csi-driver"],"title":"PowerMax"},{"body":" Linked Proxy mode for CSI reverse proxy is no longer actively maintained or supported. It will be deprecated in CSM 1.9 (Driver Version 2.9.0). It is highly recommended that you use stand alone mode going forward. Multi Unisphere Support Starting with v1.7, the CSI PowerMax driver can communicate with multiple Unisphere for PowerMax servers to manage multiple PowerMax arrays. In order to use this feature, you must install CSI PowerMax ReverseProxy in StandAlone mode with the driver. For more details on how to configure the driver and ReverseProxy, see the relevant section here\nVolume Snapshot Feature The CSI PowerMax driver version 1.7 and later supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class To use this feature, enable it in values.yaml\nsnapshot: enabled: true Note: From v1.7, the CSI PowerMax driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the csi-powermax/samples/volumesnapshotclass folder\nCreating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 spec: volumeSnapshotClassName: powermax-snapclass source: persistentVolumeClaimName: pvol0 After the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmax-restore-pvc-demo namespace: test spec: storageClassName: powermax dataSource: name: pmax-snapshot-demo kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Creating PVCs with PVCs as source This is a sample manifest for creating a PVC with another PVC as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmax-clone-pvc-demo namespace: test spec: storageClassName: powermax dataSource: name: pmax-pvc-demo kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi iSCSI CHAP Starting from version 1.3.0, the unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI has been supported. To enable CHAP authentication:\nCreate secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true. The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u003csymid\u003e -iscsi \u003chost iqn\u003e set chap -cred \u003chost IQN\u003e -secret \u003cCHAP secret\u003e Where host IQN is the name of the iSCSI initiator of a host IQN, and CHAP secret is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which is stored in the host initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name Starting from version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this feature, set the following values under customDriverName in my-powermax-settings.yaml.\nValue: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template. If enabled, the driver name is in the following format: \u003cnamespace\u003e.\u003cdriver name\u003e.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax, then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers To install multiple CSI Drivers for Dell PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions that should be strictly adhered to:\nOnly one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations To install multiple CSI drivers, follow these steps:\nCreate (or use) a new namespace. Ensure that all the pre-requisites are met: powermax-creds secret is created in this namespace (Optional) powermax-certs secret is created in this namespace Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver. Volume expansion Starting in v1.4, the CSI PowerMax driver supports the expansion of Persistent Volumes (PVs). This expansion is done online, which is when the PVC is attached to any node.\nTo use this feature, enable in values.yaml\nresizer: enabled: true To use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThis is a sample manifest for a storage class that allows for Volume Expansion.\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-expand-sc annotations: storageclass.kubernetes.io/is-default-class: false provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true #Set this attribute to true if you plan to expand any PVCs created using this storage class parameters: SYMID: \"000000000001\" SRP: \"DEFAULT_SRP\" ServiceLevel: \"Bronze\" To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC.\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pmax-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 10Gi #Updated size from 5Gi to 10Gi storageClassName: powermax-expand-sc NOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of the volume, it cannot be used to shrink a volume.\nRaw block support Starting in v1.4, the CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powermaxtest namespace: {{ .Values.namespace }} spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: powermax resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy CSI PowerMax Reverse Proxy application is deployed along with the driver to get the maximum performance out of the CSI driver for PowerMax and Unisphere for PowerMax REST APIs.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server that acts as a reverse proxy for the Unisphere for PowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy application helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and the performance of the CSI PowerMax driver.\nOptionally, you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation The CSI PowerMax Reverse Proxy can be installed in two ways:\nIt can be installed as a Kubernetes deployment in the same namespace as the driver. It can be installed as a sidecar to the driver’s controller Pod. It is also configured as a Kubernetes “NodePort” service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is an HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be an X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nStarting from v2.7.0 , the secrets for proxy will be created automatically using the below tls.key and tls.cert contents provided in values.yaml file. For this , we need to install cert-manager using below command which manages the certs and secrets .\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml Here is an example showing how to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 Using Helm installer In the my-powermax-settings.yaml file, the csireverseproxy section can be used to configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. The install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name “powermax-reverseproxy”. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation for PowerMax.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two ‘%’ characters. String prefixing first ‘%’ and string suffixing second ‘%’ is used as is before and after every node identifier.\nAlso, a new setting, modifyHostName, can be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u003cClusterPrefix\u003e-\u003cHostName\u003e*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1, then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], ‘-’ and ‘_’, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of the controller Pod. At any time, only one controller Pod is active(leader), and the rest are on standby. In case of a failure, one of the standby Pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two-controller Pods are ever scheduled on the same node.\nTo increase or decrease the number of controller Pods, edit the following value in values.yaml file:\ncontrollerCount: 2 NOTE: The default value for controllerCount is 2. We recommend not changing this unless it is really necessary. Also, if the controller count is greater than the number of available nodes (where the Pods can be scheduled), some controller Pods will remain in the Pending state\nIf you are using dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, see the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment and driver node Daemonset. There are two new sections in the values file - controller and node - where you can specify these values separately for the controller and node Pods.\ncontroller If you want to apply nodeSelectors and tolerations for the controller Pods, edit the controller section in the values file.\nHere are some examples:\nTo schedule controller Pods to worker nodes only (Default): controller: nodeSelector: tolerations: Set the following values for controller Pods to tolerate the taint NoSchedule on master nodes: controller: nodeSelector: tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" Set the following values for controller Pods to be scheduled only on nodes labelled master (node-role.kubernetes.io/master): controller: nodeSelector: node-role.kubernetes.io/master: \"\" tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" node If you want to apply nodeSelectors and tolerations for the node Pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add and remove tolerations to this list\n# \"node\" allows to configure node specific parameters node: # \"node.nodeSelector\" defines what nodes would be selected for Pods of node daemonset # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"node.tolerations\" defines tolerations that would be applied to node daemonset # Add/Remove tolerations as per requirement # Leave as blank if you wish to not apply any tolerations tolerations: - key: \"node.kubernetes.io/memory-pressure\" operator: \"Exists\" effect: \"NoExecute\" - key: \"node.kubernetes.io/disk-pressure\" operator: \"Exists\" effect: \"NoExecute\" - key: \"node.kubernetes.io/network-unavailable\" operator: \"Exists\" effect: \"NoExecute\" Topology Support Starting from version 1.5, the CSI PowerMax driver supports topology-aware volume provisioning which helps the Kubernetes scheduler place PVCs on worker nodes that have access to the backend storage. When used with nodeSelectors which can be specified for the driver node Pods, it provides an effective way to provision applications on nodes that have access to the PowerMax array.\nAfter a successful installation of the driver, if a node Pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\ncsi-powermax.dellemc.com/\\\u003carray-id\\\u003e If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.iscsi The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\nStarting from version 2.3.0, topology keys have been enhanced to filter out arrays, associated transport protocol available to each node and create topology keys based on any such user input.\nTopology Usage To use the Topology feature, the storage classes must be modified as follows:\nvolumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section For example, a PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-fc parameters: SRP: \"SRP_1\" SYMID: \"000000000001\" ServiceLevel: \u003cService Level\u003e #Insert Service Level Name provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true allowedTopologies: - matchLabelExpressions: - key: csi-powermax.dellemc.com/000000000001 values: - csi-powermax.dellemc.com - key: csi-powermax.dellemc.com/000000000001.fc values: - csi-powermax.dellemc.com In the above example, if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\nA set of sample storage class definitions to enable topology-aware volume provisioning has been provided in the csi-powermax/samples/storageclass folder\nFor additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\nCustom Topology keys To use the enhanced topology keys:\nTo use this feature, set node.topologyControl.enabled to true. Edit the config file topologyConfig.yaml in csi-powermax/samples/configmap folder and provide values for the following parameters. Parameter Description allowedConnections List of node, array and protocol info for user allowed configuration allowedConnections.nodeName Name of the node on which user wants to apply given rules allowedConnections.rules List of StorageArrayID:TransportProtocol pair deniedConnections List of node, array and protocol info for user denied configuration deniedConnections.nodeName Name of the node on which user wants to apply given rules deniedConnections.rules List of StorageArrayID:TransportProtocol pair Sample config file:\n# allowedConnections contains a list of (node, array and protocol) info for user allowed configuration # For any given storage array ID and protocol on a Node, topology keys will be created for just those pair and # every other configuration is ignored # Please refer to the doc website about a detailed explanation of each configuration parameter # and the various possible inputs allowedConnections: # nodeName: Name of the node on which user wants to apply given rules # Allowed values: # nodeName - name of a specific node # * - all the nodes # Examples: \"node1\", \"*\" - nodeName: \"node1\" # rules is a list of 'StorageArrayID:TransportProtocol' pair. ':' is required between both value # Allowed values: # StorageArrayID: # - SymmetrixID : for specific storage array # - \"*\" :- for all the arrays connected to the node # TransportProtocol: # - FC : Fibre Channel protocol # - ISCSI : iSCSI protocol # - \"*\" - for all the possible Transport Protocol # Examples: \"000000000001:FC\", \"000000000002:*\", \"*:FC\", \"*:*\" rules: - \"000000000001:FC\" - \"000000000002:FC\" - nodeName: \"*\" rules: - \"000000000002:FC\" # deniedConnections contains a list of (node, array and protocol) info for denied configurations by user # For any given storage array ID and protocol on a Node, topology keys will be created for every other configuration but # not these input pairs deniedConnections: - nodeName: \"node2\" rules: - \"000000000002:*\" - nodeName: \"node3\" rules: - \"*:*\" Use the below command to create ConfigMap with configmap name as node-topology-config in the namespace powermax, kubectl create configmap node-topology-config --from-file=topologyConfig.yaml -n powermax For example, let there be 3 nodes and 2 arrays, so based on the sample config file above, topology keys will be created as below:\nNew Topology keys N1: csi-driver/000000000001.FC:csi-driver, csi-driver/000000000002.FC:csi-driver N2 and N3: None\nNote: Name of the configmap should always be node-topology-config.\nDynamic Logging Configuration This feature is introduced in CSI Driver for PowerMax version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in my-powermax-settings.yaml during driver installation.\nTo change the log level dynamically to a different value, the user can edit the same my-powermax-settings.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade Note: my-powermax-settings.yaml is a values.yaml file which the user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level the user can set this field during driver installation.\nTo update the log level dynamically, the user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Volume Health Monitoring CSI Driver for Dell PowerMax 2.2.0 and above supports volume health monitoring. To enable Volume Health Monitoring from the node side, the alpha feature gate CSIVolumeHealth needs to be enabled. To use this feature, set controller.healthMonitor.enabled and node.healthMonitor.enabled to true. To change the monitor interval, set controller.healthMonitor.interval parameter.\nSingle Pod Access Mode for PersistentVolumes- ReadWriteOncePod (ALPHA FEATURE) Use ReadWriteOncePod(RWOP) access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it. This is only supported for CSI Driver for PowerMax 2.2.0+ and Kubernetes version 1.22+.\nTo use this feature, enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet, by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\"\nCreating a PersistentVolumeClaim kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-writer-only spec: accessModes: - ReadWriteOncePod # the volume can be mounted as read-write by a single pod across the whole cluster resources: requests: storage: 1Gi When this feature is enabled, the existing ReadWriteOnce(RWO) access mode restricts volume access to a single node and allows multiple pods on the same node to read from and write to the same volume.\nTo migrate existing PersistentVolumes to use ReadWriteOncePod, please follow the instruction from here.\nSupport for auto RDM for vSphere over FC CSI Driver for Dell PowerMax 2.5.0 and above supports auto RDM for vSphere over FC.\nThis feature supports volume provisioning on Kubernetes clusters running on vSphere (VMware hypervisor) via RDM mechanism. This feature enables the users to use PMAX CSI drivers with VMs on vSphere Hypervisor with the same feature and functionality as there with bare metal servers when they have only FC ports in PMAX storage.\nIt will be supported only on new/freshly installed clusters where the cluster is exclusively deployed in a virtualized vSphere environment. Having hybrid topologies like ISCSI or FC (in pass-through) is not supported.\nTo use this feature\nSet vSphere.enabled to true. Create a secret which contains vCenter privileges. Follow the steps here to create it. Update vCenterCredSecret with the secret name created. VMware/vSphere virtualization support # set enable to true, if you to enable VMware virtualized environment support via RDM # Allowed Values: # \"true\" - vSphere volumes are enabled # \"false\" - vSphere volumes are disabled # Default value: \"false\" vSphere: enabled: false # fcPortGroup: an existing portGroup that driver will use for vSphere # recommended format: csi-x-VC-PG, x can be anything of user choice fcPortGroup: \"csi-vsphere-VC-PG\" # fcHostGroup: an existing host(initiator group) that driver will use for vSphere # this hostGroup should contain initiators from all the ESXs/ESXi host # where the cluster is deployed # recommended format: csi-x-VC-HG, x can be anything of user choice fcHostGroup: \"csi-vsphere-VC-HG\" # vCenterHost: URL/endpoint of the vCenter where all the ESX are present vCenterHost: \"00.000.000.01\" # vCenterCredSecret: secret name for the vCenter credentials vCenterCredSecret: vcenter-creds Note: Replication is not supported with this feature. Limitations of RDM can be referred here.\nSupported number of RDM Volumes per VM is 60 as per the limitations. RDMs should not be added/removed manually from vCenter on any of the cluster VMs.\n","categories":"","description":"Code features for PowerMax Driver","excerpt":"Code features for PowerMax Driver","ref":"/csm-docs/v2/csidriver/features/powermax/","tags":"","title":"PowerMax"},{"body":"You can upgrade CSI Driver for Dell PowerMax using Helm or Dell CSI Operator.\nNote: CSI Driver for PowerMax v2.4.0 requires 10.0 REST endpoint support of Unisphere.\nUpdating the CSI Driver to use 10.0 Unisphere Upgrade the Unisphere to have 10.0 endpoint support.Please find the instructions here. Update the my-powermax-settings.yaml to have endpoint with 10.0 support. Update Driver from v2.6 to v2.7 using Helm Steps\nRun git clone -b v2.7.0 https://github.com/dell/csi-powermax.git to clone the git repository and get the driver. Update the values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade NOTE:\nIf you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver.\nTo update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option --upgrade, for example:\n./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade You cannot upgrade between drivers with different fsGroupPolicies. To check the current driver’s fsGroupPolicy, use this command:\nkubectl describe csidriver csi-powermax and check the “Spec” section:\n... Spec: Attach Required: true Fs Group Policy: ReadWriteOnceWithFSType Pod Info On Mount: false Requires Republish: false Storage Capacity: false ... Upgrade using Dell CSI Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\nPlease upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here. ","categories":"","description":"Upgrade PowerMax CSI driver","excerpt":"Upgrade PowerMax CSI driver","ref":"/csm-docs/v2/csidriver/upgradation/drivers/powermax/","tags":["upgrade","csi-driver"],"title":"PowerMax"},{"body":"Multi Unisphere Support Starting with v1.7, the CSI PowerMax driver can communicate with multiple Unisphere for PowerMax servers to manage multiple PowerMax arrays. In order to use this feature, you must install CSI PowerMax ReverseProxy in StandAlone mode with the driver. For more details on how to configure the driver and ReverseProxy, see the relevant section here\nVolume Snapshot Feature The CSI PowerMax driver version 1.7 and later supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class To use this feature, enable it in values.yaml\nsnapshot: enabled: true Note: From v1.7, the CSI PowerMax driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the csi-powermax/samples/volumesnapshotclass folder\nCreating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 spec: volumeSnapshotClassName: powermax-snapclass source: persistentVolumeClaimName: pvol0 After the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmax-restore-pvc-demo namespace: test spec: storageClassName: powermax dataSource: name: pmax-snapshot-demo kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Creating PVCs with PVCs as source This is a sample manifest for creating a PVC with another PVC as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmax-clone-pvc-demo namespace: test spec: storageClassName: powermax dataSource: name: pmax-pvc-demo kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi iSCSI CHAP Starting from version 1.3.0, the unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI has been supported. To enable CHAP authentication:\nCreate secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true. The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u003csymid\u003e -iscsi \u003chost iqn\u003e set chap -cred \u003chost IQN\u003e -secret \u003cCHAP secret\u003e\nWhere host IQN is the name of the iSCSI initiator of a host IQN, and CHAP secret is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which is stored in the host initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name Starting from version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this feature, set the following values under customDriverName in my-powermax-settings.yaml.\nValue: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template. If enabled, the driver name is in the following format: \u003cnamespace\u003e.\u003cdriver name\u003e.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax, then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers To install multiple CSI Drivers for Dell PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions that should be strictly adhered to:\nOnly one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations To install multiple CSI drivers, follow these steps:\nCreate (or use) a new namespace. Ensure that all the pre-requisites are met: powermax-creds secret is created in this namespace (Optional) powermax-certs secret is created in this namespace Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver. Volume expansion Starting in v1.4, the CSI PowerMax driver supports the expansion of Persistent Volumes (PVs). This expansion is done online, which is when the PVC is attached to any node.\nTo use this feature, enable in values.yaml\nresizer: enabled: true To use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThis is a sample manifest for a storage class that allows for Volume Expansion.\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-expand-sc annotations: storageclass.kubernetes.io/is-default-class: false provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true #Set this attribute to true if you plan to expand any PVCs created using this storage class parameters: SYMID: \"000000000001\" SRP: \"DEFAULT_SRP\" ServiceLevel: \"Bronze\" To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC.\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pmax-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 10Gi #Updated size from 5Gi to 10Gi storageClassName: powermax-expand-sc NOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of the volume, it cannot be used to shrink a volume.\nRaw block support Starting in v1.4, the CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powermaxtest namespace: {{ .Values.namespace }} spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: powermax resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy CSI PowerMax Reverse Proxy application is deployed along with the driver to get the maximum performance out of the CSI driver for PowerMax and Unisphere for PowerMax REST APIs.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server that acts as a reverse proxy for the Unisphere for PowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy application helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and the performance of the CSI PowerMax driver.\nOptionally, you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation The CSI PowerMax Reverse Proxy can be installed in two ways:\nIt can be installed as a Kubernetes deployment in the same namespace as the driver. It can be installed as a sidecar to the driver’s controller Pod. It is also configured as a Kubernetes “NodePort” service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is an HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be an X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nHere is an example showing how to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Using Helm installer In the my-powermax-settings.yaml file, the csireverseproxy section can be used to configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. The install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name “powermax-reverseproxy”. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation for PowerMax.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two ‘%’ characters. String prefixing first ‘%’ and string suffixing second ‘%’ is used as is before and after every node identifier.\nAlso, a new setting, modifyHostName, can be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u003cClusterPrefix\u003e-\u003cHostName\u003e*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1, then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], ‘-’ and ‘_’, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of the controller Pod. At any time, only one controller Pod is active(leader), and the rest are on standby. In case of a failure, one of the standby Pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two-controller Pods are ever scheduled on the same node.\nTo increase or decrease the number of controller Pods, edit the following value in values.yaml file:\ncontrollerCount: 2 NOTE: The default value for controllerCount is 2. We recommend not changing this unless it is really necessary. Also, if the controller count is greater than the number of available nodes (where the Pods can be scheduled), some controller Pods will remain in the Pending state\nIf you are using dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, see the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment and driver node Daemonset. There are two new sections in the values file - controller and node - where you can specify these values separately for the controller and node Pods.\ncontroller If you want to apply nodeSelectors and tolerations for the controller Pods, edit the controller section in the values file.\nHere are some examples:\nTo schedule controller Pods to worker nodes only (Default): controller: nodeSelector: tolerations: Set the following values for controller Pods to tolerate the taint NoSchedule on master nodes: controller: nodeSelector: tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" Set the following values for controller Pods to be scheduled only on nodes labelled master (node-role.kubernetes.io/master): controller: nodeSelector: node-role.kubernetes.io/master: \"\" tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" node If you want to apply nodeSelectors and tolerations for the node Pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add and remove tolerations to this list\n# \"node\" allows to configure node specific parameters node: # \"node.nodeSelector\" defines what nodes would be selected for Pods of node daemonset # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"node.tolerations\" defines tolerations that would be applied to node daemonset # Add/Remove tolerations as per requirement # Leave as blank if you wish to not apply any tolerations tolerations: - key: \"node.kubernetes.io/memory-pressure\" operator: \"Exists\" effect: \"NoExecute\" - key: \"node.kubernetes.io/disk-pressure\" operator: \"Exists\" effect: \"NoExecute\" - key: \"node.kubernetes.io/network-unavailable\" operator: \"Exists\" effect: \"NoExecute\" Topology Support Starting from version 1.5, the CSI PowerMax driver supports topology-aware volume provisioning which helps the Kubernetes scheduler place PVCs on worker nodes that have access to the backend storage. When used with nodeSelectors which can be specified for the driver node Pods, it provides an effective way to provision applications on nodes that have access to the PowerMax array.\nAfter a successful installation of the driver, if a node Pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\ncsi-powermax.dellemc.com/\\\u003carray-id\\\u003e If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.iscsi The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\nStarting from version 2.3.0, topology keys have been enhanced to filter out arrays, associated transport protocol available to each node and create topology keys based on any such user input.\nTopology Usage To use the Topology feature, the storage classes must be modified as follows:\nvolumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section For example, a PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-fc parameters: SRP: \"SRP_1\" SYMID: \"000000000001\" ServiceLevel: \u003cService Level\u003e #Insert Service Level Name provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true allowedTopologies: - matchLabelExpressions: - key: csi-powermax.dellemc.com/000000000001 values: - csi-powermax.dellemc.com - key: csi-powermax.dellemc.com/000000000001.fc values: - csi-powermax.dellemc.com In the above example, if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\nA set of sample storage class definitions to enable topology-aware volume provisioning has been provided in the csi-powermax/samples/storageclass folder\nFor additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\nCustom Topology keys To use the enhanced topology keys:\nTo use this feature, set node.topologyControl.enabled to true. Edit the config file topologyConfig.yaml in csi-powermax/samples/configmap folder and provide values for the following parameters. Parameter Description allowedConnections List of node, array and protocol info for user allowed configuration allowedConnections.nodeName Name of the node on which user wants to apply given rules allowedConnections.rules List of StorageArrayID:TransportProtocol pair deniedConnections List of node, array and protocol info for user denied configuration deniedConnections.nodeName Name of the node on which user wants to apply given rules deniedConnections.rules List of StorageArrayID:TransportProtocol pair Sample config file:\n# allowedConnections contains a list of (node, array and protocol) info for user allowed configuration # For any given storage array ID and protocol on a Node, topology keys will be created for just those pair and # every other configuration is ignored # Please refer to the doc website about a detailed explanation of each configuration parameter # and the various possible inputs allowedConnections: # nodeName: Name of the node on which user wants to apply given rules # Allowed values: # nodeName - name of a specific node # * - all the nodes # Examples: \"node1\", \"*\" - nodeName: \"node1\" # rules is a list of 'StorageArrayID:TransportProtocol' pair. ':' is required between both value # Allowed values: # StorageArrayID: # - SymmetrixID : for specific storage array # - \"*\" :- for all the arrays connected to the node # TransportProtocol: # - FC : Fibre Channel protocol # - ISCSI : iSCSI protocol # - \"*\" - for all the possible Transport Protocol # Examples: \"000000000001:FC\", \"000000000002:*\", \"*:FC\", \"*:*\" rules: - \"000000000001:FC\" - \"000000000002:FC\" - nodeName: \"*\" rules: - \"000000000002:FC\" # deniedConnections contains a list of (node, array and protocol) info for denied configurations by user # For any given storage array ID and protocol on a Node, topology keys will be created for every other configuration but # not these input pairs deniedConnections: - nodeName: \"node2\" rules: - \"000000000002:*\" - nodeName: \"node3\" rules: - \"*:*\" Use the below command to create ConfigMap with configmap name as node-topology-config in the namespace powermax, kubectl create configmap node-topology-config --from-file=topologyConfig.yaml -n powermax\nFor example, let there be 3 nodes and 2 arrays, so based on the sample config file above, topology keys will be created as below:\nNew Topology keys N1: csi-driver/000000000001.FC:csi-driver, csi-driver/000000000002.FC:csi-driver N2 and N3: None\nNote: Name of the configmap should always be node-topology-config.\nDynamic Logging Configuration This feature is introduced in CSI Driver for PowerMax version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in my-powermax-settings.yaml during driver installation.\nTo change the log level dynamically to a different value, the user can edit the same my-powermax-settings.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade Note: my-powermax-settings.yaml is a values.yaml file which the user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level the user can set this field during driver installation.\nTo update the log level dynamically, the user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Volume Health Monitoring CSI Driver for Dell PowerMax 2.2.0 and above supports volume health monitoring. To enable Volume Health Monitoring from the node side, the alpha feature gate CSIVolumeHealth needs to be enabled. To use this feature, set controller.healthMonitor.enabled and node.healthMonitor.enabled to true. To change the monitor interval, set controller.healthMonitor.interval parameter.\nSingle Pod Access Mode for PersistentVolumes- ReadWriteOncePod (ALPHA FEATURE) Use ReadWriteOncePod(RWOP) access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it. This is only supported for CSI Driver for PowerMax 2.2.0+ and Kubernetes version 1.22+.\nTo use this feature, enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet, by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\"\nCreating a PersistentVolumeClaim kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-writer-only spec: accessModes: - ReadWriteOncePod # the volume can be mounted as read-write by a single pod across the whole cluster resources: requests: storage: 1Gi When this feature is enabled, the existing ReadWriteOnce(RWO) access mode restricts volume access to a single node and allows multiple pods on the same node to read from and write to the same volume.\nTo migrate existing PersistentVolumes to use ReadWriteOncePod, please follow the instruction from here.\nSupport for auto RDM for vSphere over FC CSI Driver for Dell PowerMax 2.5.0 and above supports auto RDM for vSphere over FC.\nThis feature supports volume provisioning on Kubernetes clusters running on vSphere (VMware hypervisor) via RDM mechanism. This feature enables the users to use PMAX CSI drivers with VMs on vSphere Hypervisor with the same feature and functionality as there with bare metal servers when they have only FC ports in PMAX storage.\nIt will be supported only on new/freshly installed clusters where the cluster is exclusively deployed in a virtualized vSphere environment. Having hybrid topologies like ISCSI or FC (in pass-through) is not supported.\nTo use this feature\nSet vSphere.enabled to true. Create a secret which contains vCenter privileges. Follow the steps here to create it. Update vCenterCredSecret with the secret name created. VMware/vSphere virtualization support # set enable to true, if you to enable VMware virtualized environment support via RDM # Allowed Values: # \"true\" - vSphere volumes are enabled # \"false\" - vSphere volumes are disabled # Default value: \"false\" vSphere: enabled: false # fcPortGroup: an existing portGroup that driver will use for vSphere # recommended format: csi-x-VC-PG, x can be anything of user choice fcPortGroup: \"csi-vsphere-VC-PG\" # fcHostGroup: an existing host(initiator group) that driver will use for vSphere # this hostGroup should contain initiators from all the ESXs/ESXi host # where the cluster is deployed # recommended format: csi-x-VC-HG, x can be anything of user choice fcHostGroup: \"csi-vsphere-VC-HG\" # vCenterHost: URL/endpoint of the vCenter where all the ESX are present vCenterHost: \"00.000.000.01\" # vCenterCredSecret: secret name for the vCenter credentials vCenterCredSecret: vcenter-creds Note: Replication is not supported with this feature. Limitations of RDM can be referred here.\nSupported number of RDM Volumes per VM is 60 as per the limitations. RDMs should not be added/removed manually from vCenter on any of the cluster VMs.\n","categories":"","description":"Code features for PowerMax Driver","excerpt":"Code features for PowerMax Driver","ref":"/csm-docs/v3/csidriver/features/powermax/","tags":"","title":"PowerMax"},{"body":"You can upgrade CSI Driver for Dell PowerMax using Helm or Dell CSI Operator.\nNote: CSI Driver for Powermax v2.4.0 requires 10.0 REST endpoint support of Unisphere.\nUpdating the CSI Driver to use 10.0 Unisphere Upgrade the Unisphere to have 10.0 endpoint support.Please find the instructions here. Update the my-powermax-settings.yaml to have endpoint with 10.0 support. Update Driver from v2.5 to v2.6 using Helm Steps\nRun git clone -b v2.6.0 https://github.com/dell/csi-powermax.git to clone the git repository and get the driver. Update the values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade. NOTE:\nIf you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver. To update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade. You cannot upgrade between drivers with different fsGroupPolicies. To check the current driver’s fsGroupPolicy, use this command: kubectl describe csidriver csi-powermax and check the “Spec” section: ... Spec: Attach Required: true Fs Group Policy: ReadWriteOnceWithFSType Pod Info On Mount: false Requires Republish: false Storage Capacity: false ... Upgrade using Dell CSI Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\nPlease upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here. ","categories":"","description":"Upgrade PowerMax CSI driver","excerpt":"Upgrade PowerMax CSI driver","ref":"/csm-docs/v3/csidriver/upgradation/drivers/powermax/","tags":["upgrade","csi-driver"],"title":"PowerMax"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerMax. The Grafana reference dashboards for PowerMax metrics can be uploaded to your Grafana instance.\nPrerequisites Unisphere user credentials must have PERF_MONITOR permissions. Ensure time synchronization for Kubernetes cluster and PowerMax Unisphere by using Network Time Protocol (NTP). I/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by storage group and volume.\nTo disable these metrics, set the performanceMetricsEnabled field under karaviMetricsPowerMax to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powermax_storage_group_read_bw_megabytes_per_second The storage group read bandwidth (MB/s) powermax_storage_group_write_bw_megabytes_per_second The storage group write bandwidth (MB/s) powermax_storage_group_read_latency_milliseconds The time (in ms) to complete read operations within PowerMax system by the storage group powermax_storage_group_write_latency_milliseconds The time (in ms) to complete write operations within PowerMax system by the storage group powermax_storage_group_read_iops_per_second The number of read operations performed by a storage group (per second) powermax_storage_group_write_iops_per_second The number of write operations performed by a storage group (per second) powermax_storage_group_average_io_size_megabytes_per_second The storage group average IO sizes (MB/s) powermax_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s) powermax_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s) powermax_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume powermax_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume powermax_volume_read_iops_per_second The number of read operations performed against a volume (per second) powermax_volume_write_iops_per_second The number of write operations performed against a volume (per second) Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the capacityMetricsEnabled field under karaviMetricsPowerMax to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powermax_storage_class_total_capacity_gigabytes Total capacity for a given storage class (GB) powermax_storage_class_used_capacity_gigabytes Total used capacity for a given storage class (GB) powermax_storage_class_used_capacity_percentage Used capacity of a storage class in percent powermax_array_total_capacity_gigabytes Total capacity on a given array managed by CSI driver (GB) powermax_array_used_capacity_gigabytes Total used capacity on a given array managed by CSI driver (GB) powermax_array_used_capacity_percentage Total used capacity on a given array managed by CSI driver in percent powermax_storage_group_total_capacity_gigabytes Total capacity for a given storage group (GB) powermax_storage_group_used_capacity_gigabytes Total used capacity for a given storage group (GB) powermax_storage_group_used_capacity_percentage Used capacity of a storage group in percent powermax_srp_total_capacity_gigabytes Total capacity of the storage resource pool in GB managed by CSI driver powermax_srp_used_capacity_gigabytes Used capacity of a storage resource pool in GB managed by CSI driver powermax_srp_used_capacity_percentage Used capacity of a storage resource pool in percent powermax_volume_total_capacity_gigabytes Total capacity of the volume in GB powermax_volume_used_capacity_gigabytes Used capacity of a volume in GB powermax_volume_used_capacity_percentage Used capacity of a volume in percent ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability PowerMax Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability PowerMax …","ref":"/csm-docs/docs/observability/metrics/powermax/","tags":"","title":"PowerMax Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerMax. The Grafana reference dashboards for PowerMax metrics can be uploaded to your Grafana instance.\nPrerequisites Unisphere user credentials must have PERF_MONITOR permissions. Ensure time synchronization for Kubernetes cluster and PowerMax Unisphere by using Network Time Protocol (NTP). I/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by storage group and volume.\nTo disable these metrics, set the performanceMetricsEnabled field under karaviMetricsPowerMax to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powermax_storage_group_read_bw_megabytes_per_second The storage group read bandwidth (MB/s) powermax_storage_group_write_bw_megabytes_per_second The storage group write bandwidth (MB/s) powermax_storage_group_read_latency_milliseconds The time (in ms) to complete read operations within PowerMax system by the storage group powermax_storage_group_write_latency_milliseconds The time (in ms) to complete write operations within PowerMax system by the storage group powermax_storage_group_read_iops_per_second The number of read operations performed by a storage group (per second) powermax_storage_group_write_iops_per_second The number of write operations performed by a storage group (per second) powermax_storage_group_average_io_size_megabytes_per_second The storage group average IO sizes (MB/s) powermax_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s) powermax_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s) powermax_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume powermax_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume powermax_volume_read_iops_per_second The number of read operations performed against a volume (per second) powermax_volume_write_iops_per_second The number of write operations performed against a volume (per second) Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the capacityMetricsEnabled field under karaviMetricsPowerMax to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powermax_storage_class_total_capacity_gigabytes Total capacity for a given storage class (GB) powermax_storage_class_used_capacity_gigabytes Total used capacity for a given storage class (GB) powermax_storage_class_used_capacity_percentage Used capacity of a storage class in percent powermax_array_total_capacity_gigabytes Total capacity on a given array managed by CSI driver (GB) powermax_array_used_capacity_gigabytes Total used capacity on a given array managed by CSI driver (GB) powermax_array_used_capacity_percentage Total used capacity on a given array managed by CSI driver in percent powermax_storage_group_total_capacity_gigabytes Total capacity for a given storage group (GB) powermax_storage_group_used_capacity_gigabytes Total used capacity for a given storage group (GB) powermax_storage_group_used_capacity_percentage Used capacity of a storage group in percent powermax_srp_total_capacity_gigabytes Total capacity of the storage resource pool in GB managed by CSI driver powermax_srp_used_capacity_gigabytes Used capacity of a storage resource pool in GB managed by CSI driver powermax_srp_used_capacity_percentage Used capacity of a storage resource pool in percent powermax_volume_total_capacity_gigabytes Total capacity of the volume in GB powermax_volume_used_capacity_gigabytes Used capacity of a volume in GB powermax_volume_used_capacity_percentage Used capacity of a volume in percent ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability PowerMax Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability PowerMax …","ref":"/csm-docs/v1/observability/metrics/powermax/","tags":"","title":"PowerMax Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerMax. The Grafana reference dashboards for PowerMax metrics can be uploaded to your Grafana instance.\nPrerequisites Unisphere user credentials must have PERF_MONITOR permissions. Ensure time synchronization for Kubernetes cluster and PowerMax Unisphere by using Network Time Protocol (NTP). I/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by storage group and volume.\nTo disable these metrics, set the performanceMetricsEnabled field under karaviMetricsPowerMax to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powermax_storage_group_read_bw_megabytes_per_second The storage group read bandwidth (MB/s) powermax_storage_group_write_bw_megabytes_per_second The storage group write bandwidth (MB/s) powermax_storage_group_read_latency_milliseconds The time (in ms) to complete read operations within PowerMax system by the storage group powermax_storage_group_write_latency_milliseconds The time (in ms) to complete write operations within PowerMax system by the storage group powermax_storage_group_read_iops_per_second The number of read operations performed by a storage group (per second) powermax_storage_group_write_iops_per_second The number of write operations performed by a storage group (per second) powermax_storage_group_average_io_size_megabytes_per_second The storage group average IO sizes (MB/s) powermax_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s) powermax_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s) powermax_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume powermax_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume powermax_volume_read_iops_per_second The number of read operations performed against a volume (per second) powermax_volume_write_iops_per_second The number of write operations performed against a volume (per second) Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the capacityMetricsEnabled field under karaviMetricsPowerMax to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powermax_storage_class_total_capacity_gigabytes Total capacity for a given storage class (GB) powermax_storage_class_used_capacity_gigabytes Total used capacity for a given storage class (GB) powermax_storage_class_used_capacity_percentage Used capacity of a storage class in percent powermax_array_total_capacity_gigabytes Total capacity on a given array managed by CSI driver (GB) powermax_array_used_capacity_gigabytes Total used capacity on a given array managed by CSI driver (GB) powermax_array_used_capacity_percentage Total used capacity on a given array managed by CSI driver in percent powermax_storage_group_total_capacity_gigabytes Total capacity for a given storage group (GB) powermax_storage_group_used_capacity_gigabytes Total used capacity for a given storage group (GB) powermax_storage_group_used_capacity_percentage Used capacity of a storage group in percent powermax_srp_total_capacity_gigabytes Total capacity of the storage resource pool in GB managed by CSI driver powermax_srp_used_capacity_gigabytes Used capacity of a storage resource pool in GB managed by CSI driver powermax_srp_used_capacity_percentage Used capacity of a storage resource pool in percent powermax_volume_total_capacity_gigabytes Total capacity of the volume in GB powermax_volume_used_capacity_gigabytes Used capacity of a volume in GB powermax_volume_used_capacity_percentage Used capacity of a volume in percent ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability PowerMax Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability PowerMax …","ref":"/csm-docs/v2/observability/metrics/powermax/","tags":"","title":"PowerMax Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerMax. The Grafana reference dashboards for PowerMax metrics can be uploaded to your Grafana instance.\nPrerequisites Unisphere user credentials must have PERF_MONITOR permissions. Ensure time synchronization for Kubernetes cluster and PowerMax Unisphere by using Network Time Protocol (NTP). I/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by storage group and volume.\nTo disable these metrics, set the performanceMetricsEnabled field under karaviMetricsPowermax to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powermax_storage_group_read_bw_megabytes_per_second The storage group read bandwidth (MB/s) powermax_storage_group_write_bw_megabytes_per_second The storage group write bandwidth (MB/s) powermax_storage_group_read_latency_milliseconds The time (in ms) to complete read operations within PowerMax system by the storage group powermax_storage_group_write_latency_milliseconds The time (in ms) to complete write operations within PowerMax system by the storage group powermax_storage_group_read_iops_per_second The number of read operations performed by a storage group (per second) powermax_storage_group_write_iops_per_second The number of write operations performed by a storage group (per second) powermax_storage_group_average_io_size_megabytes_per_second The storage group average IO sizes (MB/s) powermax_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s) powermax_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s) powermax_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume powermax_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume powermax_volume_read_iops_per_second The number of read operations performed against a volume (per second) powermax_volume_write_iops_per_second The number of write operations performed against a volume (per second) Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the capacityMetricsEnabled field under karaviMetricsPowermax to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powermax_storage_class_total_capacity_gigabytes Total capacity for a given storage class (GB) powermax_storage_class_used_capacity_gigabytes Total used capacity for a given storage class (GB) powermax_storage_class_used_capacity_percentage Used capacity of a storage class in percent powermax_array_total_capacity_gigabytes Total capacity on a given array managed by CSI driver (GB) powermax_array_used_capacity_gigabytes Total used capacity on a given array managed by CSI driver (GB) powermax_array_used_capacity_percentage Total used capacity on a given array managed by CSI driver in percent powermax_storage_group_total_capacity_gigabytes Total capacity for a given storage group (GB) powermax_storage_group_used_capacity_gigabytes Total used capacity for a given storage group (GB) powermax_storage_group_used_capacity_percentage Used capacity of a storage group in percent powermax_srp_total_capacity_gigabytes Total capacity of the storage resource pool in GB managed by CSI driver powermax_srp_used_capacity_gigabytes Used capacity of a storage resource pool in GB managed by CSI driver powermax_srp_used_capacity_percentage Used capacity of a storage resource pool in percent powermax_volume_total_capacity_gigabytes Total capacity of the volume in GB powermax_volume_used_capacity_gigabytes Used capacity of a volume in GB powermax_volume_used_capacity_percentage Used capacity of a volume in percent ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability PowerMax Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability PowerMax …","ref":"/csm-docs/v3/observability/metrics/powermax/","tags":"","title":"PowerMax Metrics"},{"body":"Multicluster support You can connect a single CSI-PowerScale driver with multiple PowerScale clusters.\nPre-Requisites:\nCreation of secret.yaml with credentials related to one or more Clusters. Creation of (at least) one Storage class for each cluster. Creation of custom-volumesnapshot classes with proper isiPath matching corresponding storage classes. Inclusion of cluster name in volume handle, if you want to provision existing static volumes. Consuming existing volumes with static provisioning You can use existing volumes from the PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\nOpen your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, the PowerScale cluster accessZone is assumed as ‘System’, storage class as ‘isilon’, cluster name as ‘pscale-cluster’ and volume’s internal name as ‘isilonvol’. The volume-handle should be in the format of \u003cvolume_name\u003e=_=_=\u003cexport_id\u003e=_=_=\u003caccess_zone_name\u003e=_=_=\u003ccluster_name\u003e If Quotas are enabled in the driver, it is required to add the Quota ID to the description of the NFS export in this format: CSI_QUOTA_ID:sC-kAAEAAAAAAAAAAAAAQEpVAAAAAAAA Quota ID can be identified by querying the PowerScale system. apiVersion: v1 kind: PersistentVolume metadata: name: isilonstaticpv namespace: default spec: capacity: storage: 5Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: isilon csi: driver: csi-isilon.dellemc.com volumeAttributes: Path: \"/ifs/data/csi/isilonvol\" Name: \"isilonvol\" AzServiceIP: 'XX.XX.XX.XX' volumeHandle: isilonvol=_=_=652=_=_=System=_=_=pscale-cluster claimRef: name: isilonstaticpvc namespace: default Create PersistentVolumeClaim to use this PersistentVolume. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: isilonstaticpvc namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi volumeName: isilonstaticpv storageClassName: isilon Then use this PVC as a volume in a pod. apiVersion: v1 kind: Pod metadata: name: static-prov-pod spec: containers: - name: test image: docker.io/centos:latest command: [ \"/bin/sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data0\" name: pvol volumes: - name: pvol persistentVolumeClaim: claimName: isilonstaticpvc After the pod becomes Ready and Running, you can start to use this pod and volume. PVC Creation Feature The following yaml content can be used to create a PVC without referring any PV.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: testvolume namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi storageClassName: isilon Volume Snapshot Feature The CSI PowerScale driver version 2.0 and later supports managing v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\nVolume Snapshot Class During the installation of CSI PowerScale driver version 2.0 and higher, no default Volume Snapshot Class will get created.\nThe following are the manifests for the Volume Snapshot Class:\nVolumeSnapshotClass apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: \"isilon-snapclass\" driver: csi-isilon.dellemc.com #The deletionPolicy of a volume snapshot class can either be Retain or Delete #If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object. #If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remain deletionPolicy: Delete parameters: #IsiPath should match with respective storageClass IsiPath IsiPath: \"/ifs/data/csi\" The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs; The following snippet assumes that the persistent volume claim name is testvolume.\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvcsnap namespace: default spec: volumeSnapshotClassName: isilon-snapclass source: persistentVolumeClaimName: testvolume Once the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-xxxxxxxxxxxxx creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: createfromsnap namespace: default spec: storageClassName: isilon dataSource: name: pvcsnap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteMany resources: requests: storage: 5Gi Starting from CSI PowerScale driver version 2.2, different isi paths can be used to create PersistentVolumeClaim from VolumeSnapshot.This means the isi paths of the new volume and the VolumeSnapshot can be different.\nVolume Expansion CSI PowerScale driver version 1.2 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon-expand-sc annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: \"csi-isilon.dellemc.com\" reclaimPolicy: Delete parameters: ClusterName: \u003cclusterName specified in secret.yaml\u003e AccessZone: System isiPath: \"/ifs/data/csi\" AzServiceIP : 'XX.XX.XX.XX' rootClientEnabled: \"true\" allowVolumeExpansion: true volumeBindingMode: Immediate To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: isilon-pvc-expansion-demo spec: accessModes: - ReadWriteOnce resources: requests: storage: 30Gi # Updated size from 3Gi to 30Gi storageClassName: isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\nVolume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing PVC:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: existing-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi storageClassName: isilon The following is a sample manifest for cloning:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: volume-from-volume namespace: default spec: accessModes: - ReadWriteMany volumeMode: Filesystem resources: requests: storage: 50Gi storageClassName: isilon dataSource: kind: PersistentVolumeClaim name: existing-pvc apiGroup: \"\" Controller HA CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of the controller pod. Leader election is only applicable for all sidecar containers and driver container will be running in all controller pods. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two-controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2 NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if the controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in a Pending state.\nIf you are using the Dell CSM Operator, the value to adjust is:\nreplicas: 2 For more details about configuring Controller HA using the Dell CSM Operator, see the Dell CSM Operator documentation.\nCSI Ephemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves as use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind: Pod apiVersion: v1 metadata: name: my-csi-app-inline-volume spec: containers: - name: my-frontend image: busybox command: [ \"sleep\", \"100000\" ] volumeMounts: - mountPath: \"/data\" name: my-csi-volume volumes: - name: my-csi-volume csi: driver: csi-isilon.dellemc.com volumeAttributes: size: \"2Gi\" ClusterName: \"cluster1\" This manifest creates a pod in a given cluster and attaches a newly created ephemeral inline CSI volume to it.\nNote: Storage class is not supported in CSI ephemeral inline volumes and all parameters are driver specific. CSI ephemeral volumes allow users to provide volumeAttributes directly to the CSI driver as part of the Pod spec. These volumeAttributes are supported: size, ClusterName, AccessZone, IsiPath, IsiVolumePathPermissions, AzServiceIP. For reference, check the description of parameters in the following example: isilon.yaml\nTopology Topology Support CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This results in nodes which have access to PowerScale Array being appropriately labeled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, the CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and uses them to initialize node pod with custom PowerScale FQDN/IP.\nNote: Only a single cluster can be configured as part of secret.yaml for custom topology.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that the Pod schedule takes advantage of the topology and the selected node has access to provisioned volumes.\nNote: Whenever a new storage cluster is being added in secret, even though it is dynamic, the new storage cluster IP address-related label is not added to worker nodes dynamically. The user has to spin off (bounce) driver-related pods (controller and node pods) in order to apply newly added information to be reflected in worker nodes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options. # PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon # Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server # Provide mount options through \"mountOptions\" attribute # to create PVCs with mount options. apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon provisioner: csi-isilon.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: AccessZone: System IsiPath: \"/ifs/data/csi\" # AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP. #AzServiceIP : 192.168.2.1 # When a PVC is being created, it takes the storage class' value of \"storageclass.rootClientEnabled\", # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \"Root clients\" field (when true) or \"Clients\" field (when false) of the NFS export RootClientEnabled: \"false\" # Name of PowerScale cluster where pv will be provisioned # This name should match with name of one of the cluster configs in isilon-creds secret # If this parameter is not specified, then default cluster config in isilon-creds secret will be considered if available #ClusterName: \"\u003ccluster_name\u003e\" # volumeBindingMode controls when volume binding and dynamic provisioning should occur. # Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created # WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume # until a Pod using the PersistentVolumeClaim is created volumeBindingMode: WaitForFirstConsumer # allowedTopologies helps scheduling pod on worker nodes which match all of below expressions # If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologies allowedTopologies: - matchLabelExpressions: - key: csi-isilon.dellemc.com/\u003cISILON_IP\u003e values: - csi-isilon.dellemc.com # specify additional mount options for when a Persistent Volume is being mounted on a node. # To mount volume with NFSv4, specify mount option vers=4. Make sure NFSv4 is enabled on the Isilon Cluster. mountOptions: [\"\u003cmountOption1\u003e\", \"\u003cmountOption2\u003e\", ..., \"\u003cmountOptionN\u003e\"] For additional information, see the Kubernetes Topology documentation.\nSupport custom networks for NFS I/O traffic When allowedNetworks is specified for using custom networks to handle NFS traffic, and a user already has workloads scheduled, there is a possibility that it might lead to backward compatibility issues. For example, ControllerUnPublish might not be able to completely remove clients from the NFS exports of previously created pods.\nAlso, the previous workload will still be using the default network and not custom networks. For previous workloads to use custom networks, the recreation of pods is required.\nWhen csi-powerscale driver creates an NFS export, the traffic flows through the client specified in the export. By default, the client is the network interface for Kubernetes communication (same IP/fqdn as k8s node) by default.\nFor a cluster with multiple network interfaces and if a user wants to segregate k8s traffic from NFS traffic; you can use the allowedNetworks option. allowedNetworks takes CIDR addresses as a parameter to match the IPs to be picked up by the driver to allow and route NFS traffic.\nVolume Limit The CSI Driver for Dell PowerScale allows users to specify the maximum number of PowerScale volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-isilon-volumes-per-node and specifying the volume limit for that node. kubectl label node \u003cnode_name\u003e max-isilon-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxIsilonVolumesPerNode attribute in values.yaml.\nNOTE: The default value of maxIsilonVolumesPerNode is 0. If maxIsilonVolumesPerNode is set to zero, then CO shall decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxIsilonVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-isilon-volumes-per-node is not set.\nStorage Capacity Tracking CSI for PowerScale driver version 2.8.0 and above supports Storage Capacity Tracking.\nThis feature helps the scheduler to make more informed choices about where to schedule pods which depends on unbound volumes with late binding (aka “wait for first consumer”). Pods will be scheduled on a node (satisfying the topology constraints) only if the requested capacity is available on the storage array. If such a node is not available, the pods stay in Pending state. This means pods are not scheduled.\nWithout storage capacity tracking, pods get scheduled on a node satisfying the topology constraints. If the required capacity is not available, volume attachment to the pods fails, and pods remain in ContainerCreating state. Storage capacity tracking eliminates unnecessary scheduling of pods when there is insufficient capacity.\nThe attribute storageCapacity.enabled in values.yaml can be used to enable/disable the feature during driver installation using helm. This is by default set to true. To configure how often driver checks for changed capacity set storageCapacity.pollInterval attribute. In case of driver installed via operator, this interval can be configured in the sample file provided here. by editing the --capacity-poll-interval argument present in the provisioner sidecar.\nNode selector in helm template Now user can define in which worker node, the CSI node pod daemonset can run (just like any other pod in Kubernetes world).For more information, refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector\nSimilarly, users can define the tolerations based on various conditions like memory pressure, disk pressure and network availability. Refer to https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#taints-and-tolerations for more information.\nUsage of SmartQuotas to Limit Storage Consumption CSI driver for Dell Isilon handles capacity limiting using SmartQuotas feature.\nTo use the SmartQuotas feature user can specify the boolean value ’enableQuota’ in myvalues.yaml or my-isilon-settings.yaml.\nLet us assume the user creates a PVC with 3 Gi of storage and ‘SmartQuotas’ have already been enabled in PowerScale Cluster.\nWhen ’enableQuota’ is set to ’true’\nThe driver sets the hard limit of the PVC to 3Gi. The user adds data of 2Gi to the above said PVC (by logging into POD). It works as expected. The user tries to add 2Gi more data. Driver doesn’t allow the user to enter more data as total data to be added is 4Gi and PVC limit is 3Gi. The user can expand the volume from 3Gi to 6Gi. The driver allows it and sets the hard limit of PVC to 6Gi. User retries adding 2Gi more data (which has been errored out previously). The driver accepts the data. When ’enableQuota’ is set to ‘false’\nDriver doesn’t set any hard limit against the PVC created. The user adds data of 2Gi to the above said PVC, which is having the size 3Gi (by logging into POD). It works as expected. The user tries to add 2Gi more data. Now the total size of data is 4Gi. Driver allows the user to enter more data irrespective of the initial PVC size (since no quota is set against this PVC) The user can expand the volume from an initial size of 3Gi to 4Gi or more. The driver allows it. If SmartQuota feature is enabled, user can also set other quota parameters such as Soft Limit , Advisory Limit and soft grace period using storage class yaml file or pvc yaml file.\nStorage Class Example with Quota Limit Parameters:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon provisioner: csi-isilon.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: # The name of the access zone a volume can be created in # Optional: true # Default value: default value specified in values.yaml # Examples: System, zone1 AccessZone: System # The base path for the volumes to be created on PowerScale cluster. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Optional: true # Default value: value specified in values.yaml for isiPath # Examples: /ifs/data/csi, /ifs/engineering IsiPath: /ifs/data/csi #Parameter to set Advisory Limit to quota #Optional: true #Default value: Limit not Set #AdvisoryLimit: \"50\" #Parameter to set soft limit to quota #Optional: true #Default value: Limit not Set #SoftLimit: \"80\" #Parameter which must be mentioned along with Soft Limit #Soft Limit can be exceeded until the grace period #Optional: true #Default value: Limit not Set #SoftGracePrd: \"86400\" # The permissions for isi volume directory path # This value overrides the isiVolumePathPermissions attribute of corresponding cluster config in secret, if present # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" #IsiVolumePathPermissions: \"0777\" # AccessZone groupnet service IP. Update AzServiceIP if different than endpoint. # Optional: true # Default value: endpoint of the cluster ClusterName #AzServiceIP : 192.168.2.1 # When a PVC is being created, this parameter determines, when a node mounts the PVC, # whether to add the k8s node to the \"Root clients\" field or \"Clients\" field of the NFS export # Allowed values: # \"true\": adds k8s node to the \"Root clients\" field of the NFS export # \"false\": adds k8s node to the \"Clients\" field of the NFS export # Optional: true # Default value: \"false\" RootClientEnabled: \"false\" PVC Example with Quota Limit Parameters:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc #Uncomment below 4 lines to set quota limit parameters # labels: # pvcSoftLimit: \"10\" # pvcAdvisoryLimit: \"50\" # pvcSoftGracePrd : \"85400\" spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: isilon Note\nIf quota limit values are specified in both storage class yaml and PVC yaml , then values mentioned in PVC yaml will get precedence. If few parameters are specified in storage class yaml and few in PVC yaml , then both will be combined and applied while quota creation For Example: If advisory limit = 30 is mentioned in storage class yaml and soft limit = 50 and soft grace period = 86400 are mentioned in PVC yaml . Then values set in quota will be advisory limit = 30, soft limit = 50 and soft grace period =86400. Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerScale version 1.6.0 and updated in version 2.0.0\nHelm based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade Note: here my-isilon-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap isilon-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n isilon isilon-config-params Note: Prior to CSI Driver for PowerScale version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object.\nNAT Support CSI Driver for Dell PowerScale is supported in the NAT environment.\nConfigurable permissions for volume directory This feature is introduced in CSI Driver for PowerScale version 2.0.0\nHelm based installation The permissions for volume directory can now be configured in 3 ways:\nThrough values.yaml Through secrets Through storage class # isiVolumePathPermissions: The permissions for isi volume directory path # This value acts as a default value for isiVolumePathPermissions, if not specified for a cluster config in secret # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" isiVolumePathPermissions: \"0777\" The permissions present in values.yaml are the default for all cluster config.\nIf the volume permission is not present in storage class then secrets are considered and if it is not present even in secrets then values.yaml is considered.\nNote: For volume creation from source (volume from snapshot/volume from volume) permissions are inherited from source. Create myvalues.yaml/my-isilon-settings.yaml and storage class accordingly.\nOperator based installation In the case of operator-based installation, default permission for powerscale directory is present in the samples file.\nOther ways of configuring powerscale volume permissions remain the same as helm-based installation.\nPV/PVC Metrics CSI Driver for Dell PowerScale 2.1.0 and above supports volume health monitoring. This allows Kubernetes to report on the condition, status and usage of the underlying volumes. For example, if a volume were to be deleted from the array, or unmounted outside of Kubernetes, Kubernetes will now report these abnormal conditions as events.\nThis feature can be enabled For controller plugin, by setting attribute controller.healthMonitor.enabled to true in values.yaml file. Also health monitoring interval can be changed through attribute controller.healthMonitor.interval in values.yaml file. For node plugin, by setting attribute node.healthMonitor.enabled to true in values.yaml file and by enabling the alpha feature gate CSIVolumeHealth. Single Pod Access Mode for PersistentVolumes- ReadWriteOncePod (ALPHA FEATURE) Use ReadWriteOncePod(RWOP) access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it. This is supported for CSI Driver for PowerScale 2.1.0+ and Kubernetes version 1.22+.\nTo use this feature, enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet, by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\"\nCreating a PersistentVolumeClaim kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-writer-only spec: accessModes: - ReadWriteOncePod # the volume can be mounted as read-write by a single pod across the whole cluster resources: requests: storage: 1Gi When this feature is enabled, the existing ReadWriteOnce(RWO) access mode restricts volume access to a single node and allows multiple pods on the same node to read from and write to the same volume.\nTo migrate existing PersistentVolumes to use ReadWriteOncePod, please follow the instruction from here.\n","categories":"","description":"Code features for PowerScale Driver","excerpt":"Code features for PowerScale Driver","ref":"/csm-docs/docs/csidriver/features/powerscale/","tags":"","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell PowerScale using Helm or Dell CSM Operator.\nUpgrade Driver from version 2.8.0 to 2.9.1 using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps Clone the repository using git clone -b v2.9.1 https://github.com/dell/csi-powerscale.git\nChange to directory dell-csi-helm-installer to install the Dell PowerScale cd dell-csi-helm-installer\nDownload the default values.yaml using following command:\nwget -O my-isilon-settings.yaml https://raw.githubusercontent.com/dell/helm-charts/csi-isilon-2.9.1/charts/csi-isilon/values.yaml Edit the my-isilon-settings.yaml as per the requirements.\nUpgrade the CSI Driver for Dell PowerScale using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade Upgrade using Dell CSM Operator Note: Upgrading the Operator does not upgrade the CSI Driver.\nPlease upgrade the Dell CSM Operator by following here Once the operator is upgraded, to upgrade the driver, refer here ","categories":"","description":"Upgrade PowerScale CSI driver","excerpt":"Upgrade PowerScale CSI driver","ref":"/csm-docs/docs/csidriver/upgradation/drivers/isilon/","tags":["upgrade","csi-driver"],"title":"PowerScale"},{"body":"Multicluster support You can connect a single CSI-PowerScale driver with multiple PowerScale clusters.\nPre-Requisites:\nCreation of secret.yaml with credentials related to one or more Clusters. Creation of (at least) one Storage class for each cluster. Creation of custom-volumesnapshot classes with proper isiPath matching corresponding storage classes. Inclusion of cluster name in volume handle, if you want to provision existing static volumes. Consuming existing volumes with static provisioning You can use existing volumes from the PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\nOpen your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, the PowerScale cluster accessZone is assumed as ‘System’, storage class as ‘isilon’, cluster name as ‘pscale-cluster’ and volume’s internal name as ‘isilonvol’. The volume-handle should be in the format of \u003cvolume_name\u003e=_=_=\u003cexport_id\u003e=_=_=\u003caccess_zone_name\u003e=_=_=\u003ccluster_name\u003e If Quotas are enabled in the driver, it is required to add the Quota ID to the description of the NFS export in this format: CSI_QUOTA_ID:sC-kAAEAAAAAAAAAAAAAQEpVAAAAAAAA Quota ID can be identified by querying the PowerScale system. apiVersion: v1 kind: PersistentVolume metadata: name: isilonstaticpv namespace: default spec: capacity: storage: 5Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: isilon csi: driver: csi-isilon.dellemc.com volumeAttributes: Path: \"/ifs/data/csi/isilonvol\" Name: \"isilonvol\" AzServiceIP: 'XX.XX.XX.XX' volumeHandle: isilonvol=_=_=652=_=_=System=_=_=pscale-cluster claimRef: name: isilonstaticpvc namespace: default Create PersistentVolumeClaim to use this PersistentVolume. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: isilonstaticpvc namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi volumeName: isilonstaticpv storageClassName: isilon Then use this PVC as a volume in a pod. apiVersion: v1 kind: Pod metadata: name: static-prov-pod spec: containers: - name: test image: docker.io/centos:latest command: [ \"/bin/sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data0\" name: pvol volumes: - name: pvol persistentVolumeClaim: claimName: isilonstaticpvc After the pod becomes Ready and Running, you can start to use this pod and volume. PVC Creation Feature The following yaml content can be used to create a PVC without referring any PV.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: testvolume namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi storageClassName: isilon Volume Snapshot Feature The CSI PowerScale driver version 2.0 and later supports managing v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\nVolume Snapshot Class During the installation of CSI PowerScale driver version 2.0 and higher, no default Volume Snapshot Class will get created.\nThe following are the manifests for the Volume Snapshot Class:\nVolumeSnapshotClass apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: \"isilon-snapclass\" driver: csi-isilon.dellemc.com #The deletionPolicy of a volume snapshot class can either be Retain or Delete #If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object. #If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remain deletionPolicy: Delete parameters: #IsiPath should match with respective storageClass IsiPath IsiPath: \"/ifs/data/csi\" The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs; The following snippet assumes that the persistent volume claim name is testvolume.\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvcsnap namespace: default spec: volumeSnapshotClassName: isilon-snapclass source: persistentVolumeClaimName: testvolume Once the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-xxxxxxxxxxxxx creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: createfromsnap namespace: default spec: storageClassName: isilon dataSource: name: pvcsnap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteMany resources: requests: storage: 5Gi Starting from CSI PowerScale driver version 2.2, different isi paths can be used to create PersistentVolumeClaim from VolumeSnapshot.This means the isi paths of the new volume and the VolumeSnapshot can be different.\nVolume Expansion CSI PowerScale driver version 1.2 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon-expand-sc annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: \"csi-isilon.dellemc.com\" reclaimPolicy: Delete parameters: ClusterName: \u003cclusterName specified in secret.yaml\u003e AccessZone: System isiPath: \"/ifs/data/csi\" AzServiceIP : 'XX.XX.XX.XX' rootClientEnabled: \"true\" allowVolumeExpansion: true volumeBindingMode: Immediate To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: isilon-pvc-expansion-demo spec: accessModes: - ReadWriteOnce resources: requests: storage: 30Gi # Updated size from 3Gi to 30Gi storageClassName: isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\nVolume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing PVC:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: existing-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi storageClassName: isilon The following is a sample manifest for cloning:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: volume-from-volume namespace: default spec: accessModes: - ReadWriteMany volumeMode: Filesystem resources: requests: storage: 50Gi storageClassName: isilon dataSource: kind: PersistentVolumeClaim name: existing-pvc apiGroup: \"\" Controller HA CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of the controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two-controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2 NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if the controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in a Pending state.\nIf you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nCSI Ephemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves as use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind: Pod apiVersion: v1 metadata: name: my-csi-app-inline-volume spec: containers: - name: my-frontend image: busybox command: [ \"sleep\", \"100000\" ] volumeMounts: - mountPath: \"/data\" name: my-csi-volume volumes: - name: my-csi-volume csi: driver: csi-isilon.dellemc.com volumeAttributes: size: \"2Gi\" ClusterName: \"cluster1\" This manifest creates a pod in a given cluster and attaches a newly created ephemeral inline CSI volume to it.\nNote: Storage class is not supported in CSI ephemeral inline volumes and all parameters are driver specific. CSI ephemeral volumes allow users to provide volumeAttributes directly to the CSI driver as part of the Pod spec. These volumeAttributes are supported: size, ClusterName, AccessZone, IsiPath, IsiVolumePathPermissions, AzServiceIP. For reference, check the description of parameters in the following example: isilon.yaml\nTopology Topology Support CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This results in nodes which have access to PowerScale Array being appropriately labeled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, the CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and uses them to initialize node pod with custom PowerScale FQDN/IP.\nNote: Only a single cluster can be configured as part of secret.yaml for custom topology.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that the Pod schedule takes advantage of the topology and the selected node has access to provisioned volumes.\nNote: Whenever a new storage cluster is being added in secret, even though it is dynamic, the new storage cluster IP address-related label is not added to worker nodes dynamically. The user has to spin off (bounce) driver-related pods (controller and node pods) in order to apply newly added information to be reflected in worker nodes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options. # PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon # Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server # Provide mount options through \"mountOptions\" attribute # to create PVCs with mount options. apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon provisioner: csi-isilon.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: AccessZone: System IsiPath: \"/ifs/data/csi\" # AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP. #AzServiceIP : 192.168.2.1 # When a PVC is being created, it takes the storage class' value of \"storageclass.rootClientEnabled\", # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \"Root clients\" field (when true) or \"Clients\" field (when false) of the NFS export RootClientEnabled: \"false\" # Name of PowerScale cluster where pv will be provisioned # This name should match with name of one of the cluster configs in isilon-creds secret # If this parameter is not specified, then default cluster config in isilon-creds secret will be considered if available #ClusterName: \"\u003ccluster_name\u003e\" # volumeBindingMode controls when volume binding and dynamic provisioning should occur. # Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created # WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume # until a Pod using the PersistentVolumeClaim is created volumeBindingMode: WaitForFirstConsumer # allowedTopologies helps scheduling pod on worker nodes which match all of below expressions # If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologies allowedTopologies: - matchLabelExpressions: - key: csi-isilon.dellemc.com/\u003cISILON_IP\u003e values: - csi-isilon.dellemc.com # specify additional mount options for when a Persistent Volume is being mounted on a node. # To mount volume with NFSv4, specify mount option vers=4. Make sure NFSv4 is enabled on the Isilon Cluster. mountOptions: [\"\u003cmountOption1\u003e\", \"\u003cmountOption2\u003e\", ..., \"\u003cmountOptionN\u003e\"] For additional information, see the Kubernetes Topology documentation.\nSupport custom networks for NFS I/O traffic When allowedNetworks is specified for using custom networks to handle NFS traffic, and a user already has workloads scheduled, there is a possibility that it might lead to backward compatibility issues. For example, ControllerUnPublish might not be able to completely remove clients from the NFS exports of previously created pods.\nAlso, the previous workload will still be using the default network and not custom networks. For previous workloads to use custom networks, the recreation of pods is required.\nWhen csi-powerscale driver creates an NFS export, the traffic flows through the client specified in the export. By default, the client is the network interface for Kubernetes communication (same IP/fqdn as k8s node) by default.\nFor a cluster with multiple network interfaces and if a user wants to segregate k8s traffic from NFS traffic; you can use the allowedNetworks option. allowedNetworks takes CIDR addresses as a parameter to match the IPs to be picked up by the driver to allow and route NFS traffic.\nVolume Limit The CSI Driver for Dell PowerScale allows users to specify the maximum number of PowerScale volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-isilon-volumes-per-node and specifying the volume limit for that node. kubectl label node \u003cnode_name\u003e max-isilon-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxIsilonVolumesPerNode attribute in values.yaml.\nNOTE: The default value of maxIsilonVolumesPerNode is 0. If maxIsilonVolumesPerNode is set to zero, then CO shall decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxIsilonVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-isilon-volumes-per-node is not set.\nNode selector in helm template Now user can define in which worker node, the CSI node pod daemonset can run (just like any other pod in Kubernetes world).For more information, refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector\nSimilarly, users can define the tolerations based on various conditions like memory pressure, disk pressure and network availability. Refer to https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#taints-and-tolerations for more information.\nUsage of SmartQuotas to Limit Storage Consumption CSI driver for Dell Isilon handles capacity limiting using SmartQuotas feature.\nTo use the SmartQuotas feature user can specify the boolean value ’enableQuota’ in myvalues.yaml or my-isilon-settings.yaml.\nLet us assume the user creates a PVC with 3 Gi of storage and ‘SmartQuotas’ have already been enabled in PowerScale Cluster.\nWhen ’enableQuota’ is set to ’true’\nThe driver sets the hard limit of the PVC to 3Gi. The user adds data of 2Gi to the above said PVC (by logging into POD). It works as expected. The user tries to add 2Gi more data. Driver doesn’t allow the user to enter more data as total data to be added is 4Gi and PVC limit is 3Gi. The user can expand the volume from 3Gi to 6Gi. The driver allows it and sets the hard limit of PVC to 6Gi. User retries adding 2Gi more data (which has been errored out previously). The driver accepts the data. When ’enableQuota’ is set to ‘false’\nDriver doesn’t set any hard limit against the PVC created. The user adds data of 2Gi to the above said PVC, which is having the size 3Gi (by logging into POD). It works as expected. The user tries to add 2Gi more data. Now the total size of data is 4Gi. Driver allows the user to enter more data irrespective of the initial PVC size (since no quota is set against this PVC) The user can expand the volume from an initial size of 3Gi to 4Gi or more. The driver allows it. If SmartQuota feature is enabled, user can also set other quota parameters such as Soft Limit , Advisory Limit and soft grace period using storage class yaml file or pvc yaml file.\nStorage Class Example with Quota Limit Parameters:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon provisioner: csi-isilon.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: # The name of the access zone a volume can be created in # Optional: true # Default value: default value specified in values.yaml # Examples: System, zone1 AccessZone: System # The base path for the volumes to be created on PowerScale cluster. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Optional: true # Default value: value specified in values.yaml for isiPath # Examples: /ifs/data/csi, /ifs/engineering IsiPath: /ifs/data/csi #Parameter to set Advisory Limit to quota #Optional: true #Default value: Limit not Set #AdvisoryLimit: \"50\" #Parameter to set soft limit to quota #Optional: true #Default value: Limit not Set #SoftLimit: \"80\" #Parameter which must be mentioned along with Soft Limit #Soft Limit can be exceeded until the grace period #Optional: true #Default value: Limit not Set #SoftGracePrd: \"86400\" # The permissions for isi volume directory path # This value overrides the isiVolumePathPermissions attribute of corresponding cluster config in secret, if present # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" #IsiVolumePathPermissions: \"0777\" # AccessZone groupnet service IP. Update AzServiceIP if different than endpoint. # Optional: true # Default value: endpoint of the cluster ClusterName #AzServiceIP : 192.168.2.1 # When a PVC is being created, this parameter determines, when a node mounts the PVC, # whether to add the k8s node to the \"Root clients\" field or \"Clients\" field of the NFS export # Allowed values: # \"true\": adds k8s node to the \"Root clients\" field of the NFS export # \"false\": adds k8s node to the \"Clients\" field of the NFS export # Optional: true # Default value: \"false\" RootClientEnabled: \"false\" PVC Example with Quota Limit Parameters:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc #Uncomment below 4 lines to set quota limit parameters # labels: # pvcSoftLimit: \"10\" # pvcAdvisoryLimit: \"50\" # pvcSoftGracePrd : \"85400\" spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: isilon Note\nIf quota limit values are specified in both storage class yaml and PVC yaml , then values mentioned in PVC yaml will get precedence. If few parameters are specified in storage class yaml and few in PVC yaml , then both will be combined and applied while quota creation For Example: If advisory limit = 30 is mentioned in storage class yaml and soft limit = 50 and soft grace period = 86400 are mentioned in PVC yaml . Then values set in quota will be advisory limit = 30, soft limit = 50 and soft grace period =86400. Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerScale version 1.6.0 and updated in version 2.0.0\nHelm based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade Note: here my-isilon-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap isilon-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n isilon isilon-config-params Note: Prior to CSI Driver for PowerScale version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object.\nNAT Support CSI Driver for Dell PowerScale is supported in the NAT environment.\nConfigurable permissions for volume directory This feature is introduced in CSI Driver for PowerScale version 2.0.0\nHelm based installation The permissions for volume directory can now be configured in 3 ways:\nThrough values.yaml Through secrets Through storage class # isiVolumePathPermissions: The permissions for isi volume directory path # This value acts as a default value for isiVolumePathPermissions, if not specified for a cluster config in secret # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" isiVolumePathPermissions: \"0777\" The permissions present in values.yaml are the default for all cluster config.\nIf the volume permission is not present in storage class then secrets are considered and if it is not present even in secrets then values.yaml is considered.\nNote: For volume creation from source (volume from snapshot/volume from volume) permissions are inherited from source. Create myvalues.yaml/my-isilon-settings.yaml and storage class accordingly.\nOperator based installation In the case of operator-based installation, default permission for powerscale directory is present in the samples file.\nOther ways of configuring powerscale volume permissions remain the same as helm-based installation.\nPV/PVC Metrics CSI Driver for Dell PowerScale 2.1.0 and above supports volume health monitoring. This allows Kubernetes to report on the condition, status and usage of the underlying volumes. For example, if a volume were to be deleted from the array, or unmounted outside of Kubernetes, Kubernetes will now report these abnormal conditions as events.\nThis feature can be enabled For controller plugin, by setting attribute controller.healthMonitor.enabled to true in values.yaml file. Also health monitoring interval can be changed through attribute controller.healthMonitor.interval in values.yaml file. For node plugin, by setting attribute node.healthMonitor.enabled to true in values.yaml file and by enabling the alpha feature gate CSIVolumeHealth. Single Pod Access Mode for PersistentVolumes- ReadWriteOncePod (ALPHA FEATURE) Use ReadWriteOncePod(RWOP) access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it. This is supported for CSI Driver for PowerScale 2.1.0+ and Kubernetes version 1.22+.\nTo use this feature, enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet, by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\"\nCreating a PersistentVolumeClaim kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-writer-only spec: accessModes: - ReadWriteOncePod # the volume can be mounted as read-write by a single pod across the whole cluster resources: requests: storage: 1Gi When this feature is enabled, the existing ReadWriteOnce(RWO) access mode restricts volume access to a single node and allows multiple pods on the same node to read from and write to the same volume.\nTo migrate existing PersistentVolumes to use ReadWriteOncePod, please follow the instruction from here.\n","categories":"","description":"Code features for PowerScale Driver","excerpt":"Code features for PowerScale Driver","ref":"/csm-docs/v1/csidriver/features/powerscale/","tags":"","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell PowerScale using Helm or Dell CSI Operator.\nUpgrade Driver from version 2.7.0 to 2.8.0 using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps Clone the repository using git clone -b v2.8.0 https://github.com/dell/csi-powerscale.git\nChange to directory dell-csi-helm-installer to install the Dell PowerScale cd dell-csi-helm-installer\nDownload the default values.yaml using following command:\nwget -O my-isilon-settings.yaml https://raw.githubusercontent.com/dell/helm-charts/csi-isilon-2.8.0/charts/csi-isilon/values.yaml Edit the my-isilon-settings.yaml as per the requirements.\nUpgrade the CSI Driver for Dell PowerScale using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade Upgrade using Dell CSM Operator Note: Upgrading the Operator does not upgrade the CSI Driver.\nPlease upgrade the Dell CSM Operator by following here Once the operator is upgraded, to upgrade the driver, refer here ","categories":"","description":"Upgrade PowerScale CSI driver","excerpt":"Upgrade PowerScale CSI driver","ref":"/csm-docs/v1/csidriver/upgradation/drivers/isilon/","tags":["upgrade","csi-driver"],"title":"PowerScale"},{"body":"Multicluster support You can connect a single CSI-PowerScale driver with multiple PowerScale clusters.\nPre-Requisites:\nCreation of secret.yaml with credentials related to one or more Clusters. Creation of (at least) one Storage class for each cluster. Creation of custom-volumesnapshot classes with proper isiPath matching corresponding storage classes. Inclusion of cluster name in volume handle, if you want to provision existing static volumes. Consuming existing volumes with static provisioning You can use existing volumes from the PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\nOpen your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, the PowerScale cluster accessZone is assumed as ‘System’, storage class as ‘isilon’, cluster name as ‘pscale-cluster’ and volume’s internal name as ‘isilonvol’. The volume-handle should be in the format of \u003cvolume_name\u003e===\u003cexport_id\u003e======\u003ccluster_name\u003e If Quotas are enabled in the driver, it is required to add the Quota ID to the description of the NFS export in this format: CSI_QUOTA_ID:sC-kAAEAAAAAAAAAAAAAQEpVAAAAAAAA Quota ID can be identified by querying the PowerScale system. apiVersion: v1 kind: PersistentVolume metadata: name: isilonstaticpv namespace: default spec: capacity: storage: 5Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: isilon csi: driver: csi-isilon.dellemc.com volumeAttributes: Path: \"/ifs/data/csi/isilonvol\" Name: \"isilonvol\" AzServiceIP: 'XX.XX.XX.XX' volumeHandle: isilonvol=_=_=652=_=_=System=_=_=pscale-cluster claimRef: name: isilonstaticpvc namespace: default Create PersistentVolumeClaim to use this PersistentVolume. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: isilonstaticpvc namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi volumeName: isilonstaticpv storageClassName: isilon Then use this PVC as a volume in a pod. apiVersion: v1 kind: Pod metadata: name: static-prov-pod spec: containers: - name: test image: docker.io/centos:latest command: [ \"/bin/sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data0\" name: pvol volumes: - name: pvol persistentVolumeClaim: claimName: isilonstaticpvc After the pod becomes Ready and Running, you can start to use this pod and volume. PVC Creation Feature The following yaml content can be used to create a PVC without referring any PV.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: testvolume namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi storageClassName: isilon Volume Snapshot Feature The CSI PowerScale driver version 2.0 and later supports managing v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\nVolume Snapshot Class During the installation of CSI PowerScale driver version 2.0 and higher, no default Volume Snapshot Class will get created.\nThe following are the manifests for the Volume Snapshot Class:\nVolumeSnapshotClass apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: \"isilon-snapclass\" driver: csi-isilon.dellemc.com #The deletionPolicy of a volume snapshot class can either be Retain or Delete #If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object. #If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remain deletionPolicy: Delete parameters: #IsiPath should match with respective storageClass IsiPath IsiPath: \"/ifs/data/csi\" The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs; The following snippet assumes that the persistent volume claim name is testvolume.\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvcsnap namespace: default spec: volumeSnapshotClassName: isilon-snapclass source: persistentVolumeClaimName: testvolume Once the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-xxxxxxxxxxxxx creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: createfromsnap namespace: default spec: storageClassName: isilon dataSource: name: pvcsnap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteMany resources: requests: storage: 5Gi Starting from CSI PowerScale driver version 2.2, different isi paths can be used to create PersistentVolumeClaim from VolumeSnapshot.This means the isi paths of the new volume and the VolumeSnapshot can be different.\nVolume Expansion CSI PowerScale driver version 1.2 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon-expand-sc annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: \"csi-isilon.dellemc.com\" reclaimPolicy: Delete parameters: ClusterName: \u003cclusterName specified in secret.yaml\u003e AccessZone: System isiPath: \"/ifs/data/csi\" AzServiceIP : 'XX.XX.XX.XX' rootClientEnabled: \"true\" allowVolumeExpansion: true volumeBindingMode: Immediate To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: isilon-pvc-expansion-demo spec: accessModes: - ReadWriteOnce resources: requests: storage: 30Gi # Updated size from 3Gi to 30Gi storageClassName: isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\nVolume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing PVC:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: existing-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi storageClassName: isilon The following is a sample manifest for cloning:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: volume-from-volume namespace: default spec: accessModes: - ReadWriteMany volumeMode: Filesystem resources: requests: storage: 50Gi storageClassName: isilon dataSource: kind: PersistentVolumeClaim name: existing-pvc apiGroup: \"\" Controller HA CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of the controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two-controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2 NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if the controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in a Pending state.\nIf you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nCSI Ephemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves as use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind: Pod apiVersion: v1 metadata: name: my-csi-app-inline-volume spec: containers: - name: my-frontend image: busybox command: [ \"sleep\", \"100000\" ] volumeMounts: - mountPath: \"/data\" name: my-csi-volume volumes: - name: my-csi-volume csi: driver: csi-isilon.dellemc.com volumeAttributes: size: \"2Gi\" ClusterName: \"cluster1\" This manifest creates a pod in a given cluster and attaches a newly created ephemeral inline CSI volume to it.\nNote: Storage class is not supported in CSI ephemeral inline volumes and all parameters are driver specific. CSI ephemeral volumes allow users to provide volumeAttributes directly to the CSI driver as part of the Pod spec. These volumeAttributes are supported: size, ClusterName, AccessZone, IsiPath, IsiVolumePathPermissions, AzServiceIP. For reference, check the description of parameters in the following example: isilon.yaml\nTopology Topology Support CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This results in nodes which have access to PowerScale Array being appropriately labeled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, the CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and uses them to initialize node pod with custom PowerScale FQDN/IP.\nNote: Only a single cluster can be configured as part of secret.yaml for custom topology.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that the Pod schedule takes advantage of the topology and the selected node has access to provisioned volumes.\nNote: Whenever a new storage cluster is being added in secret, even though it is dynamic, the new storage cluster IP address-related label is not added to worker nodes dynamically. The user has to spin off (bounce) driver-related pods (controller and node pods) in order to apply newly added information to be reflected in worker nodes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options. # PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon # Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server # Provide mount options through \"mountOptions\" attribute # to create PVCs with mount options. apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon provisioner: csi-isilon.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: AccessZone: System IsiPath: \"/ifs/data/csi\" # AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP. #AzServiceIP : 192.168.2.1 # When a PVC is being created, it takes the storage class' value of \"storageclass.rootClientEnabled\", # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \"Root clients\" field (when true) or \"Clients\" field (when false) of the NFS export RootClientEnabled: \"false\" # Name of PowerScale cluster where pv will be provisioned # This name should match with name of one of the cluster configs in isilon-creds secret # If this parameter is not specified, then default cluster config in isilon-creds secret will be considered if available #ClusterName: \"\u003ccluster_name\u003e\" # volumeBindingMode controls when volume binding and dynamic provisioning should occur. # Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created # WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume # until a Pod using the PersistentVolumeClaim is created volumeBindingMode: WaitForFirstConsumer # allowedTopologies helps scheduling pod on worker nodes which match all of below expressions # If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologies allowedTopologies: - matchLabelExpressions: - key: csi-isilon.dellemc.com/\u003cISILON_IP\u003e values: - csi-isilon.dellemc.com # specify additional mount options for when a Persistent Volume is being mounted on a node. # To mount volume with NFSv4, specify mount option vers=4. Make sure NFSv4 is enabled on the Isilon Cluster. mountOptions: [\"\u003cmountOption1\u003e\", \"\u003cmountOption2\u003e\", ..., \"\u003cmountOptionN\u003e\"] For additional information, see the Kubernetes Topology documentation.\nSupport custom networks for NFS I/O traffic When allowedNetworks is specified for using custom networks to handle NFS traffic, and a user already has workloads scheduled, there is a possibility that it might lead to backward compatibility issues. For example, ControllerUnPublish might not be able to completely remove clients from the NFS exports of previously created pods.\nAlso, the previous workload will still be using the default network and not custom networks. For previous workloads to use custom networks, the recreation of pods is required.\nWhen csi-powerscale driver creates an NFS export, the traffic flows through the client specified in the export. By default, the client is the network interface for Kubernetes communication (same IP/fqdn as k8s node) by default.\nFor a cluster with multiple network interfaces and if a user wants to segregate k8s traffic from NFS traffic; you can use the allowedNetworks option. allowedNetworks takes CIDR addresses as a parameter to match the IPs to be picked up by the driver to allow and route NFS traffic.\nVolume Limit The CSI Driver for Dell PowerScale allows users to specify the maximum number of PowerScale volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-isilon-volumes-per-node and specifying the volume limit for that node. kubectl label node \u003cnode_name\u003e max-isilon-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxIsilonVolumesPerNode attribute in values.yaml.\nNOTE: The default value of maxIsilonVolumesPerNode is 0. If maxIsilonVolumesPerNode is set to zero, then CO shall decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxIsilonVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-isilon-volumes-per-node is not set.\nNode selector in helm template Now user can define in which worker node, the CSI node pod daemonset can run (just like any other pod in Kubernetes world).For more information, refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector\nSimilarly, users can define the tolerations based on various conditions like memory pressure, disk pressure and network availability. Refer to https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#taints-and-tolerations for more information.\nUsage of SmartQuotas to Limit Storage Consumption CSI driver for Dell Isilon handles capacity limiting using SmartQuotas feature.\nTo use the SmartQuotas feature user can specify the boolean value ’enableQuota’ in myvalues.yaml or my-isilon-settings.yaml.\nLet us assume the user creates a PVC with 3 Gi of storage and ‘SmartQuotas’ have already been enabled in PowerScale Cluster.\nWhen ’enableQuota’ is set to ’true’\nThe driver sets the hard limit of the PVC to 3Gi. The user adds data of 2Gi to the above said PVC (by logging into POD). It works as expected. The user tries to add 2Gi more data. Driver doesn’t allow the user to enter more data as total data to be added is 4Gi and PVC limit is 3Gi. The user can expand the volume from 3Gi to 6Gi. The driver allows it and sets the hard limit of PVC to 6Gi. User retries adding 2Gi more data (which has been errored out previously). The driver accepts the data. When ’enableQuota’ is set to ‘false’\nDriver doesn’t set any hard limit against the PVC created. The user adds data of 2Gi to the above said PVC, which is having the size 3Gi (by logging into POD). It works as expected. The user tries to add 2Gi more data. Now the total size of data is 4Gi. Driver allows the user to enter more data irrespective of the initial PVC size (since no quota is set against this PVC) The user can expand the volume from an initial size of 3Gi to 4Gi or more. The driver allows it. If SmartQuota feature is enabled, user can also set other quota parameters such as Soft Limit , Advisory Limit and soft grace period using storage class yaml file or pvc yaml file.\nStorage Class Example with Quota Limit Parameters:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon provisioner: csi-isilon.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: # The name of the access zone a volume can be created in # Optional: true # Default value: default value specified in values.yaml # Examples: System, zone1 AccessZone: System # The base path for the volumes to be created on PowerScale cluster. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Optional: true # Default value: value specified in values.yaml for isiPath # Examples: /ifs/data/csi, /ifs/engineering IsiPath: /ifs/data/csi #Parameter to set Advisory Limit to quota #Optional: true #Default value: Limit not Set #AdvisoryLimit: \"50\" #Parameter to set soft limit to quota #Optional: true #Default value: Limit not Set #SoftLimit: \"80\" #Parameter which must be mentioned along with Soft Limit #Soft Limit can be exceeded until the grace period #Optional: true #Default value: Limit not Set #SoftGracePrd: \"86400\" # The permissions for isi volume directory path # This value overrides the isiVolumePathPermissions attribute of corresponding cluster config in secret, if present # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" #IsiVolumePathPermissions: \"0777\" # AccessZone groupnet service IP. Update AzServiceIP if different than endpoint. # Optional: true # Default value: endpoint of the cluster ClusterName #AzServiceIP : 192.168.2.1 # When a PVC is being created, this parameter determines, when a node mounts the PVC, # whether to add the k8s node to the \"Root clients\" field or \"Clients\" field of the NFS export # Allowed values: # \"true\": adds k8s node to the \"Root clients\" field of the NFS export # \"false\": adds k8s node to the \"Clients\" field of the NFS export # Optional: true # Default value: \"false\" RootClientEnabled: \"false\" PVC Example with Quota Limit Parameters:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc #Uncomment below 4 lines to set quota limit parameters # labels: # pvcSoftLimit: \"10\" # pvcAdvisoryLimit: \"50\" # pvcSoftGracePrd : \"85400\" spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: isilon Note\nIf quota limit values are specified in both storage class yaml and PVC yaml , then values mentioned in PVC yaml will get precedence. If few parameters are specified in storage class yaml and few in PVC yaml , then both will be combined and applied while quota creation For Example: If advisory limit = 30 is mentioned in storage class yaml and soft limit = 50 and soft grace period = 86400 are mentioned in PVC yaml . Then values set in quota will be advisory limit = 30, soft limit = 50 and soft grace period =86400. Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerScale version 1.6.0 and updated in version 2.0.0\nHelm based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade Note: here my-isilon-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap isilon-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n isilon isilon-config-params Note: Prior to CSI Driver for PowerScale version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object.\nNAT Support CSI Driver for Dell PowerScale is supported in the NAT environment.\nConfigurable permissions for volume directory This feature is introduced in CSI Driver for PowerScale version 2.0.0\nHelm based installation The permissions for volume directory can now be configured in 3 ways:\nThrough values.yaml Through secrets Through storage class # isiVolumePathPermissions: The permissions for isi volume directory path # This value acts as a default value for isiVolumePathPermissions, if not specified for a cluster config in secret # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" isiVolumePathPermissions: \"0777\" The permissions present in values.yaml are the default for all cluster config.\nIf the volume permission is not present in storage class then secrets are considered and if it is not present even in secrets then values.yaml is considered.\nNote: For volume creation from source (volume from snapshot/volume from volume) permissions are inherited from source. Create myvalues.yaml/my-isilon-settings.yaml and storage class accordingly.\nOperator based installation In the case of operator-based installation, default permission for powerscale directory is present in the samples file.\nOther ways of configuring powerscale volume permissions remain the same as helm-based installation.\nPV/PVC Metrics CSI Driver for Dell PowerScale 2.1.0 and above supports volume health monitoring. This allows Kubernetes to report on the condition, status and usage of the underlying volumes. For example, if a volume were to be deleted from the array, or unmounted outside of Kubernetes, Kubernetes will now report these abnormal conditions as events.\nThis feature can be enabled For controller plugin, by setting attribute controller.healthMonitor.enabled to true in values.yaml file. Also health monitoring interval can be changed through attribute controller.healthMonitor.interval in values.yaml file. For node plugin, by setting attribute node.healthMonitor.enabled to true in values.yaml file and by enabling the alpha feature gate CSIVolumeHealth. Single Pod Access Mode for PersistentVolumes- ReadWriteOncePod (ALPHA FEATURE) Use ReadWriteOncePod(RWOP) access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it. This is supported for CSI Driver for PowerScale 2.1.0+ and Kubernetes version 1.22+.\nTo use this feature, enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet, by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\"\nCreating a PersistentVolumeClaim kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-writer-only spec: accessModes: - ReadWriteOncePod # the volume can be mounted as read-write by a single pod across the whole cluster resources: requests: storage: 1Gi When this feature is enabled, the existing ReadWriteOnce(RWO) access mode restricts volume access to a single node and allows multiple pods on the same node to read from and write to the same volume.\nTo migrate existing PersistentVolumes to use ReadWriteOncePod, please follow the instruction from here.\n","categories":"","description":"Code features for PowerScale Driver","excerpt":"Code features for PowerScale Driver","ref":"/csm-docs/v2/csidriver/features/powerscale/","tags":"","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell PowerScale using Helm or Dell CSI Operator.\nUpgrade Driver from version 2.6.1 to 2.7.0 using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps\nClone the repository using git clone -b v2.7.0 https://github.com/dell/csi-powerscale.git, copy the helm/csi-isilon/values.yaml into a new location with a custom name say my-isilon-settings.yaml, to customize settings for installation. Edit my-isilon-settings.yaml as per the requirements.\nChange to directory dell-csi-helm-installer to install the Dell PowerScale cd dell-csi-helm-installer\nUpgrade the CSI Driver for Dell PowerScale using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade Upgrade using Dell CSI Operator: Notes:\nWhile upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes. Upgrading the Operator does not upgrade the CSI Driver. To upgrade the driver:\nPlease upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here. ","categories":"","description":"Upgrade PowerScale CSI driver","excerpt":"Upgrade PowerScale CSI driver","ref":"/csm-docs/v2/csidriver/upgradation/drivers/isilon/","tags":["upgrade","csi-driver"],"title":"PowerScale"},{"body":"Multicluster support You can connect a single CSI-PowerScale driver with multiple PowerScale clusters.\nPre-Requisites:\nCreation of secret.yaml with credentials related to one or more Clusters. Creation of (at least) one Storage class for each cluster. Creation of custom-volumesnapshot classes with proper isiPath matching corresponding storage classes. Inclusion of cluster name in volume handle, if you want to provision existing static volumes. Consuming existing volumes with static provisioning You can use existent volumes from the PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\nOpen your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, the PowerScale cluster accessZone is assumed as ‘System’, storage class as ‘isilon’, cluster name as ‘pscale-cluster’ and volume’s internal name as ‘isilonvol’. The volume-handle should be in the format of \u003cvolume_name\u003e===\u003cexport_id\u003e======\u003ccluster_name\u003e If Quotas are enabled in the driver, it is required to add the Quota ID to the description of the NFS export in this format: CSI_QUOTA_ID:sC-kAAEAAAAAAAAAAAAAQEpVAAAAAAAA Quota ID can be identified by querying the PowerScale system. apiVersion: v1 kind: PersistentVolume metadata: name: isilonstaticpv namespace: default spec: capacity: storage: 5Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: isilon csi: driver: csi-isilon.dellemc.com volumeAttributes: Path: \"/ifs/data/csi/isilonvol\" Name: \"isilonvol\" AzServiceIP: 'XX.XX.XX.XX' volumeHandle: isilonvol=_=_=652=_=_=System=_=_=pscale-cluster claimRef: name: isilonstaticpvc namespace: default Create PersistentVolumeClaim to use this PersistentVolume. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: isilonstaticpvc namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi volumeName: isilonstaticpv storageClassName: isilon Then use this PVC as a volume in a pod. apiVersion: v1 kind: Pod metadata: name: static-prov-pod spec: containers: - name: test image: docker.io/centos:latest command: [ \"/bin/sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data0\" name: pvol volumes: - name: pvol persistentVolumeClaim: claimName: isilonstaticpvc After the pod becomes Ready and Running, you can start to use this pod and volume. PVC Creation Feature Following yaml content can be used to create a PVC without referring any PV.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: testvolume namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi storageClassName: isilon Volume Snapshot Feature The CSI PowerScale driver version 2.0 and later supports managing v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\nVolume Snapshot Class During the installation of CSI PowerScale driver version 2.0 and higher, no default Volume Snapshot Class will get created.\nFollowing are the manifests for the Volume Snapshot Class:\nVolumeSnapshotClass apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: \"isilon-snapclass\" driver: csi-isilon.dellemc.com #The deletionPolicy of a volume snapshot class can either be Retain or Delete #If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object. #If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remain deletionPolicy: Delete parameters: #IsiPath should match with respective storageClass IsiPath IsiPath: \"/ifs/data/csi\" The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs; The following snippet assumes that the persistent volume claim name is testvolume.\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvcsnap namespace: default spec: volumeSnapshotClassName: isilon-snapclass source: persistentVolumeClaimName: testvolume Once the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-xxxxxxxxxxxxx creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: createfromsnap namespace: default spec: storageClassName: isilon dataSource: name: pvcsnap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteMany resources: requests: storage: 5Gi Starting from CSI PowerScale driver version 2.2, it is allowed to create PersistentVolumeClaim from VolumeSnapshot with different isi paths i.e., isi paths of the new volume and the VolumeSnapshot can be different.\nVolume Expansion The CSI PowerScale driver version 1.2 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon-expand-sc annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: \"csi-isilon.dellemc.com\" reclaimPolicy: Delete parameters: ClusterName: \u003cclusterName specified in secret.yaml\u003e AccessZone: System isiPath: \"/ifs/data/csi\" AzServiceIP : 'XX.XX.XX.XX' rootClientEnabled: \"true\" allowVolumeExpansion: true volumeBindingMode: Immediate To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: isilon-pvc-expansion-demo spec: accessModes: - ReadWriteOnce resources: requests: storage: 30Gi # Updated size from 3Gi to 30Gi storageClassName: isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\nVolume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing PVC:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: existing-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi storageClassName: isilon The following is a sample manifest for cloning:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: volume-from-volume namespace: default spec: accessModes: - ReadWriteMany volumeMode: Filesystem resources: requests: storage: 50Gi storageClassName: isilon dataSource: kind: PersistentVolumeClaim name: existing-pvc apiGroup: \"\" Controller HA The CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two-controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2 NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if the controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in a Pending state.\nIf you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nCSI Ephemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves as use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind: Pod apiVersion: v1 metadata: name: my-csi-app-inline-volume spec: containers: - name: my-frontend image: busybox command: [ \"sleep\", \"100000\" ] volumeMounts: - mountPath: \"/data\" name: my-csi-volume volumes: - name: my-csi-volume csi: driver: csi-isilon.dellemc.com volumeAttributes: size: \"2Gi\" ClusterName: \"cluster1\" This manifest creates a pod in a given cluster and attaches a newly created ephemeral inline CSI volume to it.\nNote: Storage class is not supported in CSI ephemeral inline volumes and all parameters are driver specific. CSI ephemeral volumes allow users to provide volumeAttributes directly to the CSI driver as part of the Pod spec. These volumeAttributes are supported: size, ClusterName, AccessZone, IsiPath, IsiVolumePathPermissions, AzServiceIP. For reference, check the description of parameters in the following example: isilon.yaml\nTopology Topology Support The CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage, as a result of which the nodes which have access to PowerScale Array are appropriately labeled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, the CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and use them to initialize node pod with custom PowerScale FQDN/IP.\nNote: Only a single cluster can be configured as part of secret.yaml for custom topology.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that the Pod schedule takes advantage of the topology and the selected node has access to provisioned volumes.\nNote: Whenever a new storage cluster is being added in secret, even though it is dynamic, the new storage cluster IP address-related label is not added to worker nodes dynamically. The user has to spin off (bounce) driver-related pods (controller and node pods) in order to apply newly added information to be reflected in worker nodes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options. # PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon # Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server # Provide mount options through \"mountOptions\" attribute # to create PVCs with mount options. apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon provisioner: csi-isilon.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: AccessZone: System IsiPath: \"/ifs/data/csi\" # AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP. #AzServiceIP : 192.168.2.1 # When a PVC is being created, it takes the storage class' value of \"storageclass.rootClientEnabled\", # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \"Root clients\" field (when true) or \"Clients\" field (when false) of the NFS export RootClientEnabled: \"false\" # Name of PowerScale cluster where pv will be provisioned # This name should match with name of one of the cluster configs in isilon-creds secret # If this parameter is not specified, then default cluster config in isilon-creds secret will be considered if available #ClusterName: \"\u003ccluster_name\u003e\" # volumeBindingMode controls when volume binding and dynamic provisioning should occur. # Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created # WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume # until a Pod using the PersistentVolumeClaim is created volumeBindingMode: WaitForFirstConsumer # allowedTopologies helps scheduling pod on worker nodes which match all of below expressions # If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologies allowedTopologies: - matchLabelExpressions: - key: csi-isilon.dellemc.com/\u003cISILON_IP\u003e values: - csi-isilon.dellemc.com # specify additional mount options for when a Persistent Volume is being mounted on a node. # To mount volume with NFSv4, specify mount option vers=4. Make sure NFSv4 is enabled on the Isilon Cluster. mountOptions: [\"\u003cmountOption1\u003e\", \"\u003cmountOption2\u003e\", ..., \"\u003cmountOptionN\u003e\"] For additional information, see the Kubernetes Topology documentation.\nSupport custom networks for NFS I/O traffic When allowedNetworks is specified for using custom networks to handle NFS traffic, and a user already has workloads scheduled, there is a possibility that it might lead to backward compatibility issues. For example, ControllerUnPublish might not be able to completely remove clients from the NFS exports of previously created pods.\nAlso, the previous workload will still be using the default network and not custom networks. For previous workloads to use custom networks, the recreation of pods is required.\nWhen csi-powerscale driver creates an NFS export, the traffic flows through the client specified in the export. By default, the client is the network interface for Kubernetes communication (same IP/fqdn as k8s node) by default.\nFor a cluster with multiple network interfaces and if a user wants to segregate k8s traffic from NFS traffic; you can use the allowedNetworks option. allowedNetworks takes CIDR addresses as a parameter to match the IPs to be picked up by the driver to allow and route NFS traffic.\nVolume Limit The CSI Driver for Dell PowerScale allows users to specify the maximum number of PowerScale volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-isilon-volumes-per-node and specifying the volume limit for that node. kubectl label node \u003cnode_name\u003e max-isilon-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxIsilonVolumesPerNode attribute in values.yaml.\nNOTE: The default value of maxIsilonVolumesPerNode is 0. If maxIsilonVolumesPerNode is set to zero, then CO shall decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxIsilonVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-isilon-volumes-per-node is not set.\nNode selector in helm template Now user can define in which worker node, the CSI node pod daemonset can run (just like any other pod in Kubernetes world).For more information, refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector\nSimilarly, users can define the tolerations based on various conditions like memory pressure, disk pressure and network availability. Refer to https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#taints-and-tolerations for more information.\nUsage of SmartQuotas to Limit Storage Consumption CSI driver for Dell Isilon handles capacity limiting using SmartQuotas feature.\nTo use the SmartQuotas feature user can specify the boolean value ’enableQuota’ in myvalues.yaml or my-isilon-settings.yaml.\nLet us assume the user creates a PVC with 3 Gi of storage and ‘SmartQuotas’ have already been enabled in PowerScale Cluster.\nWhen ’enableQuota’ is set to ’true’\nThe driver sets the hard limit of the PVC to 3Gi. The user adds data of 2Gi to the above said PVC (by logging into POD). It works as expected. The user tries to add 2Gi more data. Driver doesn’t allow the user to enter more data as total data to be added is 4Gi and PVC limit is 3Gi. The user can expand the volume from 3Gi to 6Gi. The driver allows it and sets the hard limit of PVC to 6Gi. User retries adding 2Gi more data (which has been errored out previously). The driver accepts the data. When ’enableQuota’ is set to ‘false’\nDriver doesn’t set any hard limit against the PVC created. The user adds data of 2Gi to the above said PVC, which is having the size 3Gi (by logging into POD). It works as expected. The user tries to add 2Gi more data. Now the total size of data is 4Gi. Driver allows the user to enter more data irrespective of the initial PVC size (since no quota is set against this PVC) The user can expand the volume from an initial size of 3Gi to 4Gi or more. The driver allows it. Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerScale version 1.6.0 and updated in version 2.0.0\nHelm based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade Note: here my-isilon-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap isilon-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n isilon isilon-config-params Note: Prior to CSI Driver for PowerScale version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object.\nNAT Support CSI Driver for Dell PowerScale is supported in the NAT environment.\nConfigurable permissions for volume directory This feature is introduced in CSI Driver for PowerScale version 2.0.0\nHelm based installation The permissions for volume directory can now be configured in 3 ways:\nThrough values.yaml Through secrets Through storage class # isiVolumePathPermissions: The permissions for isi volume directory path # This value acts as a default value for isiVolumePathPermissions, if not specified for a cluster config in secret # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" isiVolumePathPermissions: \"0777\" The permissions present in values.yaml are the default for all cluster config.\nIf the volume permission is not present in storage class then secrets are considered and if it is not present even in secrets then values.yaml is considered.\nNote: For volume creation from source (volume from snapshot/volume from volume) permissions are inherited from source. Create myvalues.yaml/my-isilon-settings.yaml and storage class accordingly.\nOperator based installation In the case of operator-based installation, default permission for powerscale directory is present in the samples file.\nOther ways of configuring powerscale volume permissions remain the same as helm-based installation.\nPV/PVC Metrics CSI Driver for Dell PowerScale 2.1.0 and above supports volume health monitoring. This allows Kubernetes to report on the condition, status and usage of the underlying volumes. For example, if a volume were to be deleted from the array, or unmounted outside of Kubernetes, Kubernetes will now report these abnormal conditions as events.\nThis feature can be enabled For controller plugin, by setting attribute controller.healthMonitor.enabled to true in values.yaml file. Also health monitoring interval can be changed through attribute controller.healthMonitor.interval in values.yaml file. For node plugin, by setting attribute node.healthMonitor.enabled to true in values.yaml file and by enabling the alpha feature gate CSIVolumeHealth. Single Pod Access Mode for PersistentVolumes- ReadWriteOncePod (ALPHA FEATURE) Use ReadWriteOncePod(RWOP) access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it. This is supported for CSI Driver for PowerScale 2.1.0+ and Kubernetes version 1.22+.\nTo use this feature, enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet, by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\"\nCreating a PersistentVolumeClaim kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-writer-only spec: accessModes: - ReadWriteOncePod # the volume can be mounted as read-write by a single pod across the whole cluster resources: requests: storage: 1Gi When this feature is enabled, the existing ReadWriteOnce(RWO) access mode restricts volume access to a single node and allows multiple pods on the same node to read from and write to the same volume.\nTo migrate existing PersistentVolumes to use ReadWriteOncePod, please follow the instruction from here.\n","categories":"","description":"Code features for PowerScale Driver","excerpt":"Code features for PowerScale Driver","ref":"/csm-docs/v3/csidriver/features/powerscale/","tags":"","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell PowerScale using Helm or Dell CSI Operator.\nUpgrade Driver from version 2.5.0 to 2.6.1 using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps\nClone the repository using git clone -b v2.6.1 https://github.com/dell/csi-powerscale.git, copy the helm/csi-isilon/values.yaml into a new location with a custom name say my-isilon-settings.yaml, to customize settings for installation. Edit my-isilon-settings.yaml as per the requirements.\nChange to directory dell-csi-helm-installer to install the Dell PowerScale cd dell-csi-helm-installer\nUpgrade the CSI Driver for Dell PowerScale using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade\nUpgrade using Dell CSI Operator: Notes:\nWhile upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes. Upgrading the Operator does not upgrade the CSI Driver. To upgrade the driver:\nPlease upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here. ","categories":"","description":"Upgrade PowerScale CSI driver","excerpt":"Upgrade PowerScale CSI driver","ref":"/csm-docs/v3/csidriver/upgradation/drivers/isilon/","tags":["upgrade","csi-driver"],"title":"PowerScale"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerScale. The Grafana reference dashboards for PowerScale metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth) are available by default and broken down by cluster and quota.\nTo disable these metrics, set the performanceMetricsEnabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerscale_cluster_cpu_use_rate Average CPU usage for all nodes in the monitored cluster powerscale_cluster_disk_read_operation_rate Average rate at which the disks in the cluster servicing data read change requests powerscale_cluster_disk_write_operation_rate Average rate at which the disks in the cluster servicing data write change requests powerscale_cluster_disk_throughput_read_rate_megabytes_per_second Throughput rate of data being read from the disks in the cluster powerscale_cluster_disk_throughput_write_rate_megabytes_per_second Throughput rate of data being written to the disks in the cluster Storage Capacity Metrics Provides visibility into the total, used, and available capacity for PowerScale cluster and quotas.\nTo disable these metrics, set the capacityMetricsEnabled field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerscale_cluster_total_capacity_terabytes Total cluster capacity (TB) powerscale_cluster_remaining_capacity_terabytes Total unused cluster capacity (TB) powerscale_cluster_used_capacity_percentage Percent of total cluster capacity that has been used powerscale_cluster_total_hard_quota_gigabytes Amount of total capacity allocated in all directory hard quotas powerscale_cluster_total_hard_quota_percentage Percent of total capacity allocated in all directory hard quotas powerscale_volume_quota_subscribed_gigabytes Space used of Quota for a directory (GB) powerscale_volume_hard_quota_remaining_gigabytes Unused spaced below the hard limit for a directory (GB) powerscale_volume_quota_subscribed_percentage Percentage of space used in hard limit for a directory powerscale_volume_hard_quota_remaining_percentage Percentage of the remaining space in hard limit for a directory ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability PowerScale Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability PowerScale …","ref":"/csm-docs/docs/observability/metrics/powerscale/","tags":"","title":"PowerScale Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerScale. The Grafana reference dashboards for PowerScale metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth) are available by default and broken down by cluster and quota.\nTo disable these metrics, set the performanceMetricsEnabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerscale_cluster_cpu_use_rate Average CPU usage for all nodes in the monitored cluster powerscale_cluster_disk_read_operation_rate Average rate at which the disks in the cluster servicing data read change requests powerscale_cluster_disk_write_operation_rate Average rate at which the disks in the cluster servicing data write change requests powerscale_cluster_disk_throughput_read_rate_megabytes_per_second Throughput rate of data being read from the disks in the cluster powerscale_cluster_disk_throughput_write_rate_megabytes_per_second Throughput rate of data being written to the disks in the cluster Storage Capacity Metrics Provides visibility into the total, used, and available capacity for PowerScale cluster and quotas.\nTo disable these metrics, set the capacityMetricsEnabled field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerscale_cluster_total_capacity_terabytes Total cluster capacity (TB) powerscale_cluster_remaining_capacity_terabytes Total unused cluster capacity (TB) powerscale_cluster_used_capacity_percentage Percent of total cluster capacity that has been used powerscale_cluster_total_hard_quota_gigabytes Amount of total capacity allocated in all directory hard quotas powerscale_cluster_total_hard_quota_percentage Percent of total capacity allocated in all directory hard quotas powerscale_volume_quota_subscribed_gigabytes Space used of Quota for a directory (GB) powerscale_volume_hard_quota_remaining_gigabytes Unused spaced below the hard limit for a directory (GB) powerscale_volume_quota_subscribed_percentage Percentage of space used in hard limit for a directory powerscale_volume_hard_quota_remaining_percentage Percentage of the remaining space in hard limit for a directory ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability PowerScale Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability PowerScale …","ref":"/csm-docs/v1/observability/metrics/powerscale/","tags":"","title":"PowerScale Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerScale. The Grafana reference dashboards for PowerScale metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth) are available by default and broken down by cluster and quota.\nTo disable these metrics, set the performanceMetricsEnabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerscale_cluster_cpu_use_rate Average CPU usage for all nodes in the monitored cluster powerscale_cluster_disk_read_operation_rate Average rate at which the disks in the cluster servicing data read change requests powerscale_cluster_disk_write_operation_rate Average rate at which the disks in the cluster servicing data write change requests powerscale_cluster_disk_throughput_read_rate_megabytes_per_second Throughput rate of data being read from the disks in the cluster powerscale_cluster_disk_throughput_write_rate_megabytes_per_second Throughput rate of data being written to the disks in the cluster Storage Capacity Metrics Provides visibility into the total, used, and available capacity for PowerScale cluster and quotas.\nTo disable these metrics, set the capacityMetricsEnabled field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerscale_cluster_total_capacity_terabytes Total cluster capacity (TB) powerscale_cluster_remaining_capacity_terabytes Total unused cluster capacity (TB) powerscale_cluster_used_capacity_percentage Percent of total cluster capacity that has been used powerscale_cluster_total_hard_quota_gigabytes Amount of total capacity allocated in all directory hard quotas powerscale_cluster_total_hard_quota_percentage Percent of total capacity allocated in all directory hard quotas powerscale_volume_quota_subscribed_gigabytes Space used of Quota for a directory (GB) powerscale_volume_hard_quota_remaining_gigabytes Unused spaced below the hard limit for a directory (GB) powerscale_volume_quota_subscribed_percentage Percentage of space used in hard limit for a directory powerscale_volume_hard_quota_remaining_percentage Percentage of the remaining space in hard limit for a directory ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability PowerScale Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability PowerScale …","ref":"/csm-docs/v2/observability/metrics/powerscale/","tags":"","title":"PowerScale Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerScale. The Grafana reference dashboards for PowerScale metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth) are available by default and broken down by cluster and quota.\nTo disable these metrics, set the performanceMetricsEnabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerscale_cluster_cpu_use_rate Average CPU usage for all nodes in the monitored cluster powerscale_cluster_disk_read_operation_rate Average rate at which the disks in the cluster servicing data read change requests powerscale_cluster_disk_write_operation_rate Average rate at which the disks in the cluster servicing data write change requests powerscale_cluster_disk_throughput_read_rate_megabytes_per_second Throughput rate of data being read from the disks in the cluster powerscale_cluster_disk_throughput_write_rate_megabytes_per_second Throughput rate of data being written to the disks in the cluster Storage Capacity Metrics Provides visibility into the total, used, and available capacity for PowerScale cluster and quotas.\nTo disable these metrics, set the capacityMetricsEnabled field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerscale_cluster_total_capacity_terabytes Total cluster capacity (TB) powerscale_cluster_remaining_capacity_terabytes Total unused cluster capacity (TB) powerscale_cluster_used_capacity_percentage Percent of total cluster capacity that has been used powerscale_cluster_total_hard_quota_gigabytes Amount of total capacity allocated in all directory hard quotas powerscale_cluster_total_hard_quota_percentage Percent of total capacity allocated in all directory hard quotas powerscale_volume_quota_subscribed_gigabytes Space used of Quota for a directory (GB) powerscale_volume_hard_quota_remaining_gigabytes Unused spaced below the hard limit for a directory (GB) powerscale_volume_quota_subscribed_percentage Percentage of space used in hard limit for a directory powerscale_volume_hard_quota_remaining_percentage Percentage of the remaining space in hard limit for a directory ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability PowerScale Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability PowerScale …","ref":"/csm-docs/v3/observability/metrics/powerscale/","tags":"","title":"PowerScale Metrics"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command creates a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset are created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset’s pods by running kubectl get pods -n testpowerstore\nThe pod must be Ready and Running\nIf Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\nDeleting volumes To delete volumes, pod and statefulset run, use the command:\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\nOpen your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u003cpowerstore.api.ip\u003e/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id in volumeHandle in format \u003cvolume-id/globalID/protocol\u003e in the manifest. Modify other parameters according to your needs. apiVersion: v1 kind: PersistentVolume metadata: name: existingvol spec: accessModes: - ReadWriteOnce capacity: storage: 30Gi csi: driver: csi-powerstore.dellemc.com volumeHandle: 0055558c-5ae1-4ed1-b421-6f5a9475c19f/unique/scsi persistentVolumeReclaimPolicy: Retain storageClassName: powerstore volumeMode: Filesystem Create PersistentVolumeClaim to use this PersistentVolume. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 30Gi storageClassName: powerstore Then use this PVC as a volume in a pod. apiVersion: v1 kind: Pod metadata: name: static-prov-pod spec: containers: - name: test image: quay.io/centos/centos:latest command: [ \"/bin/sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data0\" name: pvol volumes: - name: pvol persistentVolumeClaim: claimName: pvol After the pod is Ready and Running, you can start to use this pod and volume. Volume Snapshot Feature The CSI PowerStore driver version 2.0.0 and higher supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class Note: From v1.4, the CSI PowerStore driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the samples folder\nCreating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 spec: volumeSnapshotClassName: powerstore-snapclass source: persistentVolumeClaimName: pvol0 After the VolumeSnapshot has been successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Snapshot feature is optional for the installation CSI PowerStore driver version 1.4 makes the snapshot feature optional for the installation.\nTo enable or disable this feature, change values.snapshot.enable parameter to true or false, specify the following in values.yaml to enable this feature\nsnapshot: enable: true External Snapshotter and its CRDs are not installed even if the Snapshot feature is enabled. These have to be installed manually before the installation.\nDisabling the Snapshot feature will opt out of the snapshotter sidecar from the installation.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: testpowerstore spec: storageClassName: powerstore dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi iSCSI CHAP The CSI PowerStore driver Version 1.3.0 and later extends Challenge Handshake Authentication Protocol (CHAP) support by adding automatic credentials generation.\nThis means that you no longer need to provide chapsecret/chapuser credentials, they will be automatically generated by the driver for each host.\nTo enable this feature you need to set connection.enableCHAP to true when installing with helm or set X_CSI_POWERSTORE_ENABLE_CHAP to true in your PowerStore CustomResource when installing using operator.\nThe driver uses the generated chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating a new host on powerstore array driver will populate host chap credentials with generated values. When re-using already existing hosts driver must override existing CHAP credentials with newly generated ones.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powerstore-expand-sc annotations: storageclass.kubernetes.io/is-default-class: false provisioner: csi-powerstore.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true # Set this attribute to true if you plan to expand any PVCs created using this storage class parameters: csi.storage.k8s.io/fstype: xfs To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pstore-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 30Gi # Updated size from 3Gi to 30Gi storageClassName: powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\nRaw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion: apps/v1 kind: StatefulSet metadata: name: powerstoretest namespace: {{ .Values.namespace }} spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: powerstore resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: testpowerstore spec: storageClassName: powerstore accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: testpowerstore spec: storageClassName: powerstore dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Ephemeral Inline Volume The CSI PowerStore driver version 1.2 and later supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind: Pod apiVersion: v1 metadata: name: powerstore-inline-volume spec: containers: - name: test-container image: quay.io/centos/centos command: [ \"sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data\" name: volume volumes: - name: volume csi: driver: csi-powerstore.dellemc.com fsType: \"ext4\" volumeAttributes: size: \"20Gi\" arrayID: \"unique\" This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes: - name: volume csi: driver: csi-powerstore.dellemc.com fsType: \"nfs\" volumeAttributes: size: \"20Gi\" nasName: \"csi-nas-name\" nfsAcls: \"0777\" Controller HA The CSI PowerStore driver version 1.2 and later introduces the controller HA feature. Instead of StatefulSet, controller pods are deployed as a Deployment.\nBy default number of replicas is set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver.csiDriverSpec section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in the cluster, each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods must be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameters controller: # \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment nodeSelector: # \"controller.tolerations\" defines tolerations that would be applied to controller deployment tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameters controller: # \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" As mentioned earlier, you can configure where node driver pods would be assigned in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they must use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology features user must create their own storage classes similar to those that can be found in samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powerstore-fc provisioner: csi-powerstore.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-powerstore.dellemc.com/127.0.0.1-fc values: - \"true\" This example matches all nodes where the driver has a connection to PowerStore with an IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS, iSCSI and NVMe.\nYou can check what labels your nodes contain by running\nkubectl get nodes --show-labels Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work.\nFor any additional information about the topology, see the Kubernetes Topology documentation.\nVolume Limits The CSI Driver for Dell PowerStore allows users to specify the maximum number of PowerStore volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-powerstore-volumes-per-node and specifying the volume limit for that node. kubectl label node \u003cnode_name\u003e max-powerstore-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same value for the maxPowerstoreVolumesPerNode attribute in values.yaml during Helm installation. In the case of driver installed via the operator, this attribute can be modified in the sample yaml file for PowerStore, which is located at https://github.com/dell/csm-operator/blob/main/samples/ by editing the X_CSI_POWERSTORE_MAX_VOLUMES_PER_NODE parameter.\nNOTE: The default value of maxPowerstoreVolumesPerNode is 0. If maxPowerstoreVolumesPerNode is set to zero, then CO shall decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified in the maxPowerstoreVolumesPerNode attribute is applicable to all the nodes in the cluster for which the node label max-powerstore-volumes-per-node is not set.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as a Host on the storage array before. It will check if Host initiators and node initiators (FC, iSCSI or NVMe) match. If they do, the driver will not create a new host and will take the existing name of the Host as nodeID.\nMultiarray support The CSI PowerStore driver version 1.3.0 and later support managing multiple PowerStore arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration Create a file called config.yaml and populate it with the following content\narrays: - endpoint: \"https://10.0.0.1/api/rest\" # full URL path to the PowerStore API globalID: \"unique\" # global ID to identify array username: \"user\" # username for connecting to API password: \"password\" # password for connecting to API skipCertificateValidation: true # use insecure connection or not default: true # treat current array as a default (would be used by storage classes without arrayIP parameter) blockProtocol: \"ISCSI\" # what transport protocol use on node side (FC, ISCSI, NVMeTCP, None, or auto) nasName: \"nas-server\" # what NAS must be used for NFS volumes nfsAcls: \"0777\" # (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. # NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. - endpoint: \"https://10.0.0.2/api/rest\" globalID: \"unique\" username: \"user\" password: \"password\" skipCertificateValidation: true blockProtocol: \"FC\" Here we specify that we want to CSI driver to manage two arrays: one with an IP 10.0.0.1 and the other with an IP 10.0.0.2, we want to connect to the first array with iSCSI protocol and with FC to the second array. Also, we want to be able to create NFS-based volume so we provide the name of the NAS to the first array.\nTo use this config we need to create a Kubernetes secret from it, to do so create a file called secret.yaml in the same folder and populate it with the following content:\napiVersion: v1 kind: Secret metadata: name: powerstore-config namespace: \u003cdriver-namespace\u003e type: Opaque data: config: CONFIG_YAML Apply the secret by running following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Creating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nCreate file storageclass.yaml and populate it with the following content:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powerstore-1 provisioner: csi-powerstore.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer parameters: arrayID: \"GlobalUniqueID\" csi.storage.k8s.io/fstype: \"ext4\" --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powerstore-2 provisioner: csi-powerstore.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer parameters: arrayID: \"GlobalUniqueID\" csi.storage.k8s.io/fstype: \"xfs\" Here we specify two storage classes: one of them uses the first array and ext4 filesystem, and the other uses the second array and xfs filesystem.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use powerstore-1 storage class to create volumes on the first array and powerstore-2 storage class to create volumes on the second array.\nDynamic secret change detection CSI PowerStore driver version 1.3.0 and later supports the ability to detect changes to array configuration Kubernetes secret. This essentially means that you can change credentials for your PowerStore arrays in-flight (without restarting the driver).\nTo do so just change your configuration file config.yaml and apply it again using the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - After Kubernetes remounts secret to driver containers (this usually takes around one minute), a driver must detect the change and start using this new configuration information.\nConfiguring custom access to NFS exports CSI PowerStore driver Version 1.3.0 and later supports the ability to configure NFS access to nodes that use dedicated storage networks.\nTo enable this feature you need to specify externalAccess parameter in your helm values.yaml file or X_CSI_POWERSTORE_EXTERNAL_ACCESS variable when creating CustomResource using an operator.\nThe value of that parameter is added as an additional entry to NFS Export host access.\nFor example the following notation:\nexternalAccess: \"10.0.0.0/24\" This means that we allow for NFS Export created by driver to be consumed by address range 10.0.0.0-10.0.0.255.\nArray identification based on GlobalID CSI PowerStore driver version 1.4.0 onwards slightly changes the way arrays are being identified in runtime. In previous versions of the driver, a management IP address was used to identify an array. The address change could lead to an invalid state of PV. From version 1.4.0 a unique GlobalID string is used for an array identification. It has to be specified in config.yaml and in Storage Classes.\nThe change provides backward compatibility with previously created PVs. However, to provision new volumes, make sure to delete old Storage Classes and create new ones with arrayID instead of arrayIP specified.\nNOTE: It is recommended to migrate the PVs to new identifiers before changing management IPs of storage systems. The recommended way to do it is to clone the existing volume and delete the old one. The cloned volume will automatically switch to using globalID instead of management IP.\nRoot squashing CSI PowerStore driver version 1.4.0 and later allows users to enable root squashing for NFS volumes provisioned by the driver.\nRoot squashing rule prevents root users on NFS clients from exercising root privileges on the NFS server.\nTo enable this rule, you need to set parameter allowRoot to false in your NFS storage class.\nYour storage class definition must look similar to this:\napiVersion: storage.k8s.io/v1 kind: StorageClass ... parameters: ... allowRoot: \"false\" # enables or disables root squashing The 1.4 version and later of the driver also enables any container user, to have full access to provisioned NFS volume, in earlier versions only root user had access\nDynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params NAT Support CSI Driver for Dell Powerstore is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nPV/PVC Metrics CSI Driver for Dell Powerstore 2.1.0 and above supports volume health monitoring. To enable Volume Health Monitoring from the node side, the alpha feature gate CSIVolumeHealth needs to be enabled. To use this feature, set controller.healthMonitor.enabled and node.healthMonitor.enabled to true. To change the monitor interval, set controller.healthMonitor.interval parameter.\nSingle Pod Access Mode for PersistentVolumes Starting from version 2.1, CSI Driver for Powerstore now supports a new access mode ReadWriteOncePod for PersistentVolumes and PersistentVolumeClaims. With this feature, CSI Driver for Powerstore allows restricting volume access to a single pod in the cluster and within a worker node.\nPrerequisites\nEnable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet as ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is supported only for CSI volumes. You can enable the feature by setting command-line argument: --feature-gates=\"...,ReadWriteOncePod=true\" Create a PVC with access mode set to ReadWriteOncePod like shown in the sample below kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-node-single-writer spec: accessModes: - ReadWriteOncePod # Allow only a single pod to access single-node-single-writer resources: requests: storage: 5Gi Note: The access mode ReadWriteOnce allows multiple pods to access a single volume within a single worker node and the behavior is consistent across all supported Kubernetes versions.\nPOSIX mode bits and NFSv4 ACLs CSI PowerStore driver version 2.2.0 and later allows users to set user-defined permissions on NFS target mount directory using POSIX mode bits or NFSv4 ACLs.\nNFSv4 ACLs are supported for NFSv4 shares on NFSv4 enabled NAS servers only. Please ensure the order when providing the NFSv4 ACLs.\nTo use this feature, provide permissions in nfsAcls parameter in values.yaml, secrets or NFS storage class.\nFor example:\nPOSIX mode bits nfsAcls: \"0755\" NFSv4 ACLs nfsAcls: \"A::OWNER@:rwatTnNcCy,A::GROUP@:rxtncy,A::EVERYONE@:rxtncy,A::user@domain.com:rxtncy\" Note: If no values are specified, default value of “0777” will be set. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.\nNVMe Support NVMeTCP Support CSI Driver for Dell Powerstore 2.2.0 and above supports NVMe/TCP provisioning. To enable NVMe/TCP provisioning, blockProtocol on secret should be specified as NVMeTCP.\nNote: NVMe/TCP is not supported on RHEL 7.x versions and CoreOS. NVMe/TCP is supported with Powerstore 2.1 and above.\nNVMeFC Support CSI Driver for Dell Powerstore 2.3.0 and above supports NVMe/FC provisioning. To enable NVMe/FC provisioning, blockProtocol on secret should be specified as NVMeFC.\nNVMe/FC is supported with Powerstore 3.0 and above.\nNVMe-FC feature is supported with Helm.\nNote: In case blockProtocol is specified as auto, the driver will be able to find the initiators on the host and choose the protocol accordingly. If the host has multiple protocols enabled, then NVMeFC gets the highest priority followed by NVMeTCP, followed by FC and then iSCSI.\nVolume group snapshot Support CSI Driver for Dell Powerstore 2.3.0 and above supports creating volume groups and take snapshot of them by making use of CRD (Custom Resource Definition). More information can be found here: Volume Group Snapshotter.\nConfigurable Volume Attributes (Optional) The CSI PowerStore driver version 2.3.0 and above supports Configurable volume atttributes.\nPowerStore array provides a set of optional volume creation attributes. These attributes can be configured for the volume (block and NFS) at the time of creation through PowerStore CSI driver. These attributes can be specified as labels in PVC yaml file. The following is a sample manifest for creating volume with some of the configurable volume attributes.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 namespace: default labels: csi.dell.com/description: DB-volume csi.dell.com/appliance_id: A1 csi.dell.com/volume_group_id: f5f9dbbd-d12f-463e-becb-2e6d0a85405e spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: powerstore-ext4 Note: Default description value is pvcName-pvcNamespace.\nThis is the list of all the attributes supported by PowerStore CSI driver:\nBlock Volume NFS Volume csi.dell.com/description csi.dell.com/appliance_id csi.dell.com/volume_group_id csi.dell.com/protection_policy_id csi.dell.com/performance_policy_id csi.dell.com/app_type csi.dell.com/app_type_other csi.dell.com/description csi.dell.com/config_type csi.dell.com/access_policy csi.dell.com/locking_policy csi.dell.com/folder_rename_policy csi.dell.com/is_async_mtime_enabled csi.dell.com/protection_policy_id csi.dell.com/file_events_publishing_mode csi.dell.com/host_io_size csi.dell.com/flr_attributes.flr_create.mode csi.dell.com/flr_attributes.flr_create.default_retention csi.dell.com/flr_attributes.flr_create.maximum_retention csi.dell.com/flr_attributes.flr_create.minimum_retention Note:\nRefer to the PowerStore array specification for the allowed values for each attribute, at https://\u003carray-ip\u003e/swaggerui/. Make sure that the attributes specified are supported by the version of PowerStore array used.\nConfigurable Volume Attributes feature is supported with Helm.\nPrefix csi.dell.com/ has been added to the attributes from CSI PowerStore driver version 2.8.0\nStorage Capacity Tracking CSI PowerStore driver version 2.5.0 and above supports Storage Capacity Tracking.\nThis feature helps the scheduler to make more informed choices about where to start pods which depend on unbound volumes with late binding (aka “wait for first consumer”). Pods will be scheduled on a node (satisfying the topology constraints) only if the requested capacity is available on the storage array. If such a node is not available, the pods stay in Pending state. This means they are not scheduled.\nWithout storage capacity tracking, pods get scheduled on a node satisfying the topology constraints. If the required capacity is not available, volume attachment to the pods fails, and pods remain in ContainerCreating state. Storage capacity tracking eliminates unnecessary scheduling of pods when there is insufficient capacity.\nThe attribute storageCapacity.enabled in my-powerstore-settings.yaml can be used to enabled/disabled the feature during driver installation . To configure how often driver checks for changed capacity set storageCapacity.pollInterval attribute. In case of driver installed via operator, this interval can be configured in the sample files provided here by editing the capacity-poll-interval argument present in the provisioner sidecar.\n","categories":"","description":"Code features for PowerStore Driver","excerpt":"Code features for PowerStore Driver","ref":"/csm-docs/docs/csidriver/features/powerstore/","tags":"","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell PowerStore using Helm.\nUpdate Driver from v2.8.0 to v2.9.1 using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps\nRun git clone -b v2.9.1 https://github.com/dell/csi-powerstore.git to clone the git repository and get the driver.\nEdit samples/secret/secret.yaml file and configure connection information for your PowerStore arrays changing the following parameters:\nendpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what transport protocol we should use (FC, ISCSI, NVMeTCP, None, or auto). nasName: defines what NAS should be used for NFS volumes. nfsAcls: (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. Add more blocks similar to above for each PowerStore array if necessary.\n(optional) create new storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running\nkubectl create -f \u003cpath_to_storageclass_file\u003e Storage classes created by v1.4/v2.0/v2.1/v2.2/v2.3/v2.4/v2.5/v2.6/v2.7 driver will not be deleted, v2.8 driver will use default array to manage volumes provisioned with old storage classes. Thus, if you still have volumes provisioned by v1.4/v2.0/v2.1/v2.2/v2.3/v2.4/v2.5/v2.6/v2.7 in your cluster then be sure to include the same array you have used for the v1.4/v2.0/v2.1/v2.2/v2.3/v2.4/v2.5/v2.6/v2.7 driver and make it default in the secret.yaml file.\nCreate the secret by running\nkubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml Download the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 wget -O my-powerstore-settings.yaml https://github.com/dell/helm-charts/raw/csi-powerstore-2.9.1/charts/csi-powerstore/values.yaml and update parameters as per the requirement.\nRun the csi-install script with the option --upgrade by running:\n./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Upgrade using Dell CSM Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\nPlease upgrade the Dell CSM Operator by following here Once the operator is upgraded, to upgrade the driver, refer here ","categories":"","description":"Upgrade PowerStore CSI driver","excerpt":"Upgrade PowerStore CSI driver","ref":"/csm-docs/docs/csidriver/upgradation/drivers/powerstore/","tags":["upgrade","csi-driver"],"title":"PowerStore"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command creates a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset are created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset’s pods by running kubectl get pods -n testpowerstore\nThe pod must be Ready and Running\nIf Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\nDeleting volumes To delete volumes, pod and statefulset run, use the command:\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\nOpen your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u003cpowerstore.api.ip\u003e/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id in volumeHandle in format \u003cvolume-id/globalID/protocol\u003e in the manifest. Modify other parameters according to your needs. apiVersion: v1 kind: PersistentVolume metadata: name: existingvol spec: accessModes: - ReadWriteOnce capacity: storage: 30Gi csi: driver: csi-powerstore.dellemc.com volumeHandle: 0055558c-5ae1-4ed1-b421-6f5a9475c19f/unique/scsi persistentVolumeReclaimPolicy: Retain storageClassName: powerstore volumeMode: Filesystem Create PersistentVolumeClaim to use this PersistentVolume. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 30Gi storageClassName: powerstore Then use this PVC as a volume in a pod. apiVersion: v1 kind: Pod metadata: name: static-prov-pod spec: containers: - name: test image: quay.io/centos/centos:latest command: [ \"/bin/sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data0\" name: pvol volumes: - name: pvol persistentVolumeClaim: claimName: pvol After the pod is Ready and Running, you can start to use this pod and volume. Volume Snapshot Feature The CSI PowerStore driver version 2.0.0 and higher supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class Note: From v1.4, the CSI PowerStore driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the samples folder\nCreating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 spec: volumeSnapshotClassName: powerstore-snapclass source: persistentVolumeClaimName: pvol0 After the VolumeSnapshot has been successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Snapshot feature is optional for the installation CSI PowerStore driver version 1.4 makes the snapshot feature optional for the installation.\nTo enable or disable this feature, change values.snapshot.enable parameter to true or false, specify the following in values.yaml to enable this feature\nsnapshot: enable: true External Snapshotter and its CRDs are not installed even if the Snapshot feature is enabled. These have to be installed manually before the installation.\nDisabling the Snapshot feature will opt out of the snapshotter sidecar from the installation.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: testpowerstore spec: storageClassName: powerstore dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi iSCSI CHAP The CSI PowerStore driver Version 1.3.0 and later extends Challenge Handshake Authentication Protocol (CHAP) support by adding automatic credentials generation.\nThis means that you no longer need to provide chapsecret/chapuser credentials, they will be automatically generated by the driver for each host.\nTo enable this feature you need to set connection.enableCHAP to true when installing with helm or set X_CSI_POWERSTORE_ENABLE_CHAP to true in your PowerStore CustomResource when installing using operator.\nThe driver uses the generated chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating a new host on powerstore array driver will populate host chap credentials with generated values. When re-using already existing hosts driver must override existing CHAP credentials with newly generated ones.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powerstore-expand-sc annotations: storageclass.kubernetes.io/is-default-class: false provisioner: csi-powerstore.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true # Set this attribute to true if you plan to expand any PVCs created using this storage class parameters: csi.storage.k8s.io/fstype: xfs To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pstore-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 30Gi # Updated size from 3Gi to 30Gi storageClassName: powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\nRaw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion: apps/v1 kind: StatefulSet metadata: name: powerstoretest namespace: {{ .Values.namespace }} spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: powerstore resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: testpowerstore spec: storageClassName: powerstore accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: testpowerstore spec: storageClassName: powerstore dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Ephemeral Inline Volume The CSI PowerStore driver version 1.2 and later supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind: Pod apiVersion: v1 metadata: name: powerstore-inline-volume spec: containers: - name: test-container image: quay.io/centos/centos command: [ \"sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data\" name: volume volumes: - name: volume csi: driver: csi-powerstore.dellemc.com fsType: \"ext4\" volumeAttributes: size: \"20Gi\" arrayID: \"unique\" This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes: - name: volume csi: driver: csi-powerstore.dellemc.com fsType: \"nfs\" volumeAttributes: size: \"20Gi\" nasName: \"csi-nas-name\" nfsAcls: \"0777\" Controller HA The CSI PowerStore driver version 1.2 and later introduces the controller HA feature. Instead of StatefulSet, controller pods are deployed as a Deployment.\nBy default number of replicas is set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in the cluster, each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods must be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameters controller: # \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment nodeSelector: # \"controller.tolerations\" defines tolerations that would be applied to controller deployment tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameters controller: # \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" As mentioned earlier, you can configure where node driver pods would be assigned in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they must use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology features user must create their own storage classes similar to those that can be found in samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powerstore-fc provisioner: csi-powerstore.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-powerstore.dellemc.com/127.0.0.1-fc values: - \"true\" This example matches all nodes where the driver has a connection to PowerStore with an IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS, iSCSI and NVMe.\nYou can check what labels your nodes contain by running\nkubectl get nodes --show-labels Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work.\nFor any additional information about the topology, see the Kubernetes Topology documentation.\nVolume Limits The CSI Driver for Dell PowerStore allows users to specify the maximum number of PowerStore volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-powerstore-volumes-per-node and specifying the volume limit for that node. kubectl label node \u003cnode_name\u003e max-powerstore-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same value for the maxPowerstoreVolumesPerNode attribute in values.yaml during Helm installation. In the case of driver installed via the operator, this attribute can be modified in the sample yaml file for PowerStore, which is located at https://github.com/dell/csm-operator/blob/main/samples/ by editing the X_CSI_POWERSTORE_MAX_VOLUMES_PER_NODE parameter.\nNOTE: The default value of maxPowerstoreVolumesPerNode is 0. If maxPowerstoreVolumesPerNode is set to zero, then CO shall decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified in the maxPowerstoreVolumesPerNode attribute is applicable to all the nodes in the cluster for which the node label max-powerstore-volumes-per-node is not set.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as a Host on the storage array before. It will check if Host initiators and node initiators (FC, iSCSI or NVMe) match. If they do, the driver will not create a new host and will take the existing name of the Host as nodeID.\nMultiarray support The CSI PowerStore driver version 1.3.0 and later support managing multiple PowerStore arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration Create a file called config.yaml and populate it with the following content\narrays: - endpoint: \"https://10.0.0.1/api/rest\" # full URL path to the PowerStore API globalID: \"unique\" # global ID to identify array username: \"user\" # username for connecting to API password: \"password\" # password for connecting to API skipCertificateValidation: true # use insecure connection or not default: true # treat current array as a default (would be used by storage classes without arrayIP parameter) blockProtocol: \"ISCSI\" # what transport protocol use on node side (FC, ISCSI, NVMeTCP, None, or auto) nasName: \"nas-server\" # what NAS must be used for NFS volumes nfsAcls: \"0777\" # (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. # NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. - endpoint: \"https://10.0.0.2/api/rest\" globalID: \"unique\" username: \"user\" password: \"password\" skipCertificateValidation: true blockProtocol: \"FC\" Here we specify that we want to CSI driver to manage two arrays: one with an IP 10.0.0.1 and the other with an IP 10.0.0.2, we want to connect to the first array with iSCSI protocol and with FC to the second array. Also, we want to be able to create NFS-based volume so we provide the name of the NAS to the first array.\nTo use this config we need to create a Kubernetes secret from it, to do so create a file called secret.yaml in the same folder and populate it with the following content:\napiVersion: v1 kind: Secret metadata: name: powerstore-config namespace: \u003cdriver-namespace\u003e type: Opaque data: config: CONFIG_YAML Apply the secret by running following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Creating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nCreate file storageclass.yaml and populate it with the following content:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powerstore-1 provisioner: csi-powerstore.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer parameters: arrayID: \"GlobalUniqueID\" csi.storage.k8s.io/fstype: \"ext4\" --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powerstore-2 provisioner: csi-powerstore.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer parameters: arrayID: \"GlobalUniqueID\" csi.storage.k8s.io/fstype: \"xfs\" Here we specify two storage classes: one of them uses the first array and ext4 filesystem, and the other uses the second array and xfs filesystem.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use powerstore-1 storage class to create volumes on the first array and powerstore-2 storage class to create volumes on the second array.\nDynamic secret change detection CSI PowerStore driver version 1.3.0 and later supports the ability to detect changes to array configuration Kubernetes secret. This essentially means that you can change credentials for your PowerStore arrays in-flight (without restarting the driver).\nTo do so just change your configuration file config.yaml and apply it again using the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - After Kubernetes remounts secret to driver containers (this usually takes around one minute), a driver must detect the change and start using this new configuration information.\nConfiguring custom access to NFS exports CSI PowerStore driver Version 1.3.0 and later supports the ability to configure NFS access to nodes that use dedicated storage networks.\nTo enable this feature you need to specify externalAccess parameter in your helm values.yaml file or X_CSI_POWERSTORE_EXTERNAL_ACCESS variable when creating CustomResource using an operator.\nThe value of that parameter is added as an additional entry to NFS Export host access.\nFor example the following notation:\nexternalAccess: \"10.0.0.0/24\" This means that we allow for NFS Export created by driver to be consumed by address range 10.0.0.0-10.0.0.255.\nArray identification based on GlobalID CSI PowerStore driver version 1.4.0 onwards slightly changes the way arrays are being identified in runtime. In previous versions of the driver, a management IP address was used to identify an array. The address change could lead to an invalid state of PV. From version 1.4.0 a unique GlobalID string is used for an array identification. It has to be specified in config.yaml and in Storage Classes.\nThe change provides backward compatibility with previously created PVs. However, to provision new volumes, make sure to delete old Storage Classes and create new ones with arrayID instead of arrayIP specified.\nNOTE: It is recommended to migrate the PVs to new identifiers before changing management IPs of storage systems. The recommended way to do it is to clone the existing volume and delete the old one. The cloned volume will automatically switch to using globalID instead of management IP.\nRoot squashing CSI PowerStore driver version 1.4.0 and later allows users to enable root squashing for NFS volumes provisioned by the driver.\nRoot squashing rule prevents root users on NFS clients from exercising root privileges on the NFS server.\nTo enable this rule, you need to set parameter allowRoot to false in your NFS storage class.\nYour storage class definition must look similar to this:\napiVersion: storage.k8s.io/v1 kind: StorageClass ... parameters: ... allowRoot: \"false\" # enables or disables root squashing The 1.4 version and later of the driver also enables any container user, to have full access to provisioned NFS volume, in earlier versions only root user had access\nDynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params NAT Support CSI Driver for Dell Powerstore is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nPV/PVC Metrics CSI Driver for Dell Powerstore 2.1.0 and above supports volume health monitoring. To enable Volume Health Monitoring from the node side, the alpha feature gate CSIVolumeHealth needs to be enabled. To use this feature, set controller.healthMonitor.enabled and node.healthMonitor.enabled to true. To change the monitor interval, set controller.healthMonitor.interval parameter.\nSingle Pod Access Mode for PersistentVolumes Starting from version 2.1, CSI Driver for Powerstore now supports a new access mode ReadWriteOncePod for PersistentVolumes and PersistentVolumeClaims. With this feature, CSI Driver for Powerstore allows restricting volume access to a single pod in the cluster and within a worker node.\nPrerequisites\nEnable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet as ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is supported only for CSI volumes. You can enable the feature by setting command-line argument: --feature-gates=\"...,ReadWriteOncePod=true\" Create a PVC with access mode set to ReadWriteOncePod like shown in the sample below kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-node-single-writer spec: accessModes: - ReadWriteOncePod # Allow only a single pod to access single-node-single-writer resources: requests: storage: 5Gi Note: The access mode ReadWriteOnce allows multiple pods to access a single volume within a single worker node and the behavior is consistent across all supported Kubernetes versions.\nPOSIX mode bits and NFSv4 ACLs CSI PowerStore driver version 2.2.0 and later allows users to set user-defined permissions on NFS target mount directory using POSIX mode bits or NFSv4 ACLs.\nNFSv4 ACLs are supported for NFSv4 shares on NFSv4 enabled NAS servers only. Please ensure the order when providing the NFSv4 ACLs.\nTo use this feature, provide permissions in nfsAcls parameter in values.yaml, secrets or NFS storage class.\nFor example:\nPOSIX mode bits nfsAcls: \"0755\" NFSv4 ACLs nfsAcls: \"A::OWNER@:rwatTnNcCy,A::GROUP@:rxtncy,A::EVERYONE@:rxtncy,A::user@domain.com:rxtncy\" Note: If no values are specified, default value of “0777” will be set. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.\nNVMe Support NVMeTCP Support CSI Driver for Dell Powerstore 2.2.0 and above supports NVMe/TCP provisioning. To enable NVMe/TCP provisioning, blockProtocol on secret should be specified as NVMeTCP.\nNote: NVMe/TCP is not supported on RHEL 7.x versions and CoreOS. NVMe/TCP is supported with Powerstore 2.1 and above.\nNVMeFC Support CSI Driver for Dell Powerstore 2.3.0 and above supports NVMe/FC provisioning. To enable NVMe/FC provisioning, blockProtocol on secret should be specified as NVMeFC.\nNVMe/FC is supported with Powerstore 3.0 and above.\nNVMe-FC feature is supported with Helm.\nNote: In case blockProtocol is specified as auto, the driver will be able to find the initiators on the host and choose the protocol accordingly. If the host has multiple protocols enabled, then NVMeFC gets the highest priority followed by NVMeTCP, followed by FC and then iSCSI.\nVolume group snapshot Support CSI Driver for Dell Powerstore 2.3.0 and above supports creating volume groups and take snapshot of them by making use of CRD (Custom Resource Definition). More information can be found here: Volume Group Snapshotter.\nConfigurable Volume Attributes (Optional) The CSI PowerStore driver version 2.3.0 and above supports Configurable volume atttributes.\nPowerStore array provides a set of optional volume creation attributes. These attributes can be configured for the volume (block and NFS) at the time of creation through PowerStore CSI driver. These attributes can be specified as labels in PVC yaml file. The following is a sample manifest for creating volume with some of the configurable volume attributes.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 namespace: default labels: csi.dell.com/description: DB-volume csi.dell.com/appliance_id: A1 csi.dell.com/volume_group_id: f5f9dbbd-d12f-463e-becb-2e6d0a85405e spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: powerstore-ext4 Note: Default description value is pvcName-pvcNamespace.\nThis is the list of all the attributes supported by PowerStore CSI driver:\nBlock Volume NFS Volume csi.dell.com/description csi.dell.com/appliance_id csi.dell.com/volume_group_id csi.dell.com/protection_policy_id csi.dell.com/performance_policy_id csi.dell.com/app_type csi.dell.com/app_type_other csi.dell.com/description csi.dell.com/config_type csi.dell.com/access_policy csi.dell.com/locking_policy csi.dell.com/folder_rename_policy csi.dell.com/is_async_mtime_enabled csi.dell.com/protection_policy_id csi.dell.com/file_events_publishing_mode csi.dell.com/host_io_size csi.dell.com/flr_attributes.flr_create.mode csi.dell.com/flr_attributes.flr_create.default_retention csi.dell.com/flr_attributes.flr_create.maximum_retention csi.dell.com/flr_attributes.flr_create.minimum_retention Note:\nRefer to the PowerStore array specification for the allowed values for each attribute, at https://\u003carray-ip\u003e/swaggerui/. Make sure that the attributes specified are supported by the version of PowerStore array used.\nConfigurable Volume Attributes feature is supported with Helm.\nPrefix csi.dell.com/ has been added to the attributes from CSI PowerStore driver version 2.8.0\nStorage Capacity Tracking CSI PowerStore driver version 2.5.0 and above supports Storage Capacity Tracking.\nThis feature helps the scheduler to make more informed choices about where to start pods which depend on unbound volumes with late binding (aka “wait for first consumer”). Pods will be scheduled on a node (satisfying the topology constraints) only if the requested capacity is available on the storage array. If such a node is not available, the pods stay in Pending state. This means they are not scheduled.\nWithout storage capacity tracking, pods get scheduled on a node satisfying the topology constraints. If the required capacity is not available, volume attachment to the pods fails, and pods remain in ContainerCreating state. Storage capacity tracking eliminates unnecessary scheduling of pods when there is insufficient capacity.\nThe attribute storageCapacity.enabled in my-powerstore-settings.yaml can be used to enabled/disabled the feature during driver installation . To configure how often driver checks for changed capacity set storageCapacity.pollInterval attribute. In case of driver installed via operator, this interval can be configured in the sample files provided here by editing the capacity-poll-interval argument present in the provisioner sidecar.\nNote:\nThis feature requires kubernetes v1.24 and above and will be automatically disabled in lower version of kubernetes.\n","categories":"","description":"Code features for PowerStore Driver","excerpt":"Code features for PowerStore Driver","ref":"/csm-docs/v1/csidriver/features/powerstore/","tags":"","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell PowerStore using Helm.\nUpdate Driver from v2.7 to v2.8 using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps\nRun git clone -b v2.8.0 https://github.com/dell/csi-powerstore.git to clone the git repository and get the driver.\nEdit samples/secret/secret.yaml file and configure connection information for your PowerStore arrays changing the following parameters:\nendpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what transport protocol we should use (FC, ISCSI, NVMeTCP, None, or auto). nasName: defines what NAS should be used for NFS volumes. nfsAcls: (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. Add more blocks similar to above for each PowerStore array if necessary.\n(optional) create new storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running\nkubectl create -f \u003cpath_to_storageclass_file\u003e Storage classes created by v1.4/v2.0/v2.1/v2.2/v2.3/v2.4/v2.5/v2.6/v2.7 driver will not be deleted, v2.8 driver will use default array to manage volumes provisioned with old storage classes. Thus, if you still have volumes provisioned by v1.4/v2.0/v2.1/v2.2/v2.3/v2.4/v2.5/v2.6/v2.7 in your cluster then be sure to include the same array you have used for the v1.4/v2.0/v2.1/v2.2/v2.3/v2.4/v2.5/v2.6/v2.7 driver and make it default in the secret.yaml file.\nCreate the secret by running\nkubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml Download the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 wget -O my-powerstore-settings.yaml https://github.com/dell/helm-charts/raw/csi-powerstore-2.8.0/charts/csi-powerstore/values.yaml and update parameters as per the requirement.\nRun the csi-install script with the option --upgrade by running:\n./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Upgrade using Dell CSM Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\nPlease upgrade the Dell CSM Operator by following here Once the operator is upgraded, to upgrade the driver, refer here ","categories":"","description":"Upgrade PowerStore CSI driver","excerpt":"Upgrade PowerStore CSI driver","ref":"/csm-docs/v1/csidriver/upgradation/drivers/powerstore/","tags":["upgrade","csi-driver"],"title":"PowerStore"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command creates a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset are created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset’s pods by running kubectl get pods -n testpowerstore\nThe pod must be Ready and Running\nIf Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\nDeleting volumes To delete volumes, pod and statefulset run, use the command:\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\nOpen your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u003cpowerstore.api.ip\u003e/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id in volumeHandle in format \u003cvolume-id/globalID/protocol\u003e in the manifest. Modify other parameters according to your needs. apiVersion: v1 kind: PersistentVolume metadata: name: existingvol spec: accessModes: - ReadWriteOnce capacity: storage: 30Gi csi: driver: csi-powerstore.dellemc.com volumeHandle: 0055558c-5ae1-4ed1-b421-6f5a9475c19f/unique/scsi persistentVolumeReclaimPolicy: Retain storageClassName: powerstore volumeMode: Filesystem Create PersistentVolumeClaim to use this PersistentVolume. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 30Gi storageClassName: powerstore Then use this PVC as a volume in a pod. apiVersion: v1 kind: Pod metadata: name: static-prov-pod spec: containers: - name: test image: quay.io/centos/centos:latest command: [ \"/bin/sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data0\" name: pvol volumes: - name: pvol persistentVolumeClaim: claimName: pvol After the pod is Ready and Running, you can start to use this pod and volume. Volume Snapshot Feature The CSI PowerStore driver version 2.0.0 and higher supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class Note: From v1.4, the CSI PowerStore driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the samples folder\nCreating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 spec: volumeSnapshotClassName: powerstore-snapclass source: persistentVolumeClaimName: pvol0 After the VolumeSnapshot has been successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Snapshot feature is optional for the installation CSI PowerStore driver version 1.4 makes the snapshot feature optional for the installation.\nTo enable or disable this feature, change values.snapshot.enable parameter to true or false, specify the following in values.yaml to enable this feature\nsnapshot: enable: true External Snapshotter and its CRDs are not installed even if the Snapshot feature is enabled. These have to be installed manually before the installation.\nDisabling the Snapshot feature will opt out of the snapshotter sidecar from the installation.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: testpowerstore spec: storageClassName: powerstore dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi iSCSI CHAP The CSI PowerStore driver Version 1.3.0 and later extends Challenge Handshake Authentication Protocol (CHAP) support by adding automatic credentials generation.\nThis means that you no longer need to provide chapsecret/chapuser credentials, they will be automatically generated by the driver for each host.\nTo enable this feature you need to set connection.enableCHAP to true when installing with helm or set X_CSI_POWERSTORE_ENABLE_CHAP to true in your PowerStore CustomResource when installing using operator.\nThe driver uses the generated chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating a new host on powerstore array driver will populate host chap credentials with generated values. When re-using already existing hosts driver must override existing CHAP credentials with newly generated ones.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powerstore-expand-sc annotations: storageclass.kubernetes.io/is-default-class: false provisioner: csi-powerstore.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true # Set this attribute to true if you plan to expand any PVCs created using this storage class parameters: csi.storage.k8s.io/fstype: xfs To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pstore-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 30Gi # Updated size from 3Gi to 30Gi storageClassName: powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\nRaw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion: apps/v1 kind: StatefulSet metadata: name: powerstoretest namespace: {{ .Values.namespace }} spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: powerstore resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: testpowerstore spec: storageClassName: powerstore accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: testpowerstore spec: storageClassName: powerstore dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Ephemeral Inline Volume The CSI PowerStore driver version 1.2 and later supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind: Pod apiVersion: v1 metadata: name: powerstore-inline-volume spec: containers: - name: test-container image: quay.io/centos/centos command: [ \"sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data\" name: volume volumes: - name: volume csi: driver: csi-powerstore.dellemc.com fsType: \"ext4\" volumeAttributes: size: \"20Gi\" arrayID: \"unique\" This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes: - name: volume csi: driver: csi-powerstore.dellemc.com fsType: \"nfs\" volumeAttributes: size: \"20Gi\" nasName: \"csi-nas-name\" nfsAcls: \"0777\" Controller HA The CSI PowerStore driver version 1.2 and later introduces the controller HA feature. Instead of StatefulSet, controller pods are deployed as a Deployment.\nBy default number of replicas is set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in the cluster, each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods must be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameters controller: # \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment nodeSelector: # \"controller.tolerations\" defines tolerations that would be applied to controller deployment tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameters controller: # \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" As mentioned earlier, you can configure where node driver pods would be assigned in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they must use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology features user must create their own storage classes similar to those that can be found in samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powerstore-fc provisioner: csi-powerstore.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-powerstore.dellemc.com/127.0.0.1-fc values: - \"true\" This example matches all nodes where the driver has a connection to PowerStore with an IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS, iSCSI and NVMe.\nYou can check what labels your nodes contain by running\nkubectl get nodes --show-labels Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work.\nFor any additional information about the topology, see the Kubernetes Topology documentation.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as a Host on the storage array before. It will check if Host initiators and node initiators (FC, iSCSI or NVMe) match. If they do, the driver will not create a new host and will take the existing name of the Host as nodeID.\nMultiarray support The CSI PowerStore driver version 1.3.0 and later support managing multiple PowerStore arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration Create a file called config.yaml and populate it with the following content\narrays: - endpoint: \"https://10.0.0.1/api/rest\" # full URL path to the PowerStore API globalID: \"unique\" # global ID to identify array username: \"user\" # username for connecting to API password: \"password\" # password for connecting to API skipCertificateValidation: true # use insecure connection or not default: true # treat current array as a default (would be used by storage classes without arrayIP parameter) blockProtocol: \"ISCSI\" # what transport protocol use on node side (FC, ISCSI, NVMeTCP, None, or auto) nasName: \"nas-server\" # what NAS must be used for NFS volumes nfsAcls: \"0777\" # (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. # NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. - endpoint: \"https://10.0.0.2/api/rest\" globalID: \"unique\" username: \"user\" password: \"password\" skipCertificateValidation: true blockProtocol: \"FC\" Here we specify that we want to CSI driver to manage two arrays: one with an IP 10.0.0.1 and the other with an IP 10.0.0.2, we want to connect to the first array with iSCSI protocol and with FC to the second array. Also, we want to be able to create NFS-based volume so we provide the name of the NAS to the first array.\nTo use this config we need to create a Kubernetes secret from it, to do so create a file called secret.yaml in the same folder and populate it with the following content:\napiVersion: v1 kind: Secret metadata: name: powerstore-config namespace: \u003cdriver-namespace\u003e type: Opaque data: config: CONFIG_YAML Apply the secret by running following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Creating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nCreate file storageclass.yaml and populate it with the following content:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powerstore-1 provisioner: csi-powerstore.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer parameters: arrayID: \"GlobalUniqueID\" csi.storage.k8s.io/fstype: \"ext4\" --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powerstore-2 provisioner: csi-powerstore.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer parameters: arrayID: \"GlobalUniqueID\" csi.storage.k8s.io/fstype: \"xfs\" Here we specify two storage classes: one of them uses the first array and ext4 filesystem, and the other uses the second array and xfs filesystem.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use powerstore-1 storage class to create volumes on the first array and powerstore-2 storage class to create volumes on the second array.\nDynamic secret change detection CSI PowerStore driver version 1.3.0 and later supports the ability to detect changes to array configuration Kubernetes secret. This essentially means that you can change credentials for your PowerStore arrays in-flight (without restarting the driver).\nTo do so just change your configuration file config.yaml and apply it again using the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - After Kubernetes remounts secret to driver containers (this usually takes around one minute), a driver must detect the change and start using this new configuration information.\nConfiguring custom access to NFS exports CSI PowerStore driver Version 1.3.0 and later supports the ability to configure NFS access to nodes that use dedicated storage networks.\nTo enable this feature you need to specify externalAccess parameter in your helm values.yaml file or X_CSI_POWERSTORE_EXTERNAL_ACCESS variable when creating CustomResource using an operator.\nThe value of that parameter is added as an additional entry to NFS Export host access.\nFor example the following notation:\nexternalAccess: \"10.0.0.0/24\" This means that we allow for NFS Export created by driver to be consumed by address range 10.0.0.0-10.0.0.255.\nArray identification based on GlobalID CSI PowerStore driver version 1.4.0 onwards slightly changes the way arrays are being identified in runtime. In previous versions of the driver, a management IP address was used to identify an array. The address change could lead to an invalid state of PV. From version 1.4.0 a unique GlobalID string is used for an array identification. It has to be specified in config.yaml and in Storage Classes.\nThe change provides backward compatibility with previously created PVs. However, to provision new volumes, make sure to delete old Storage Classes and create new ones with arrayID instead of arrayIP specified.\nNOTE: It is recommended to migrate the PVs to new identifiers before changing management IPs of storage systems. The recommended way to do it is to clone the existing volume and delete the old one. The cloned volume will automatically switch to using globalID instead of management IP.\nRoot squashing CSI PowerStore driver version 1.4.0 and later allows users to enable root squashing for NFS volumes provisioned by the driver.\nRoot squashing rule prevents root users on NFS clients from exercising root privileges on the NFS server.\nTo enable this rule, you need to set parameter allowRoot to false in your NFS storage class.\nYour storage class definition must look similar to this:\napiVersion: storage.k8s.io/v1 kind: StorageClass ... parameters: ... allowRoot: \"false\" # enables or disables root squashing The 1.4 version and later of the driver also enables any container user, to have full access to provisioned NFS volume, in earlier versions only root user had access\nDynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params NAT Support CSI Driver for Dell Powerstore is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nPV/PVC Metrics CSI Driver for Dell Powerstore 2.1.0 and above supports volume health monitoring. To enable Volume Health Monitoring from the node side, the alpha feature gate CSIVolumeHealth needs to be enabled. To use this feature, set controller.healthMonitor.enabled and node.healthMonitor.enabled to true. To change the monitor interval, set controller.healthMonitor.interval parameter.\nSingle Pod Access Mode for PersistentVolumes Starting from version 2.1, CSI Driver for Powerstore now supports a new access mode ReadWriteOncePod for PersistentVolumes and PersistentVolumeClaims. With this feature, CSI Driver for Powerstore allows restricting volume access to a single pod in the cluster and within a worker node.\nPrerequisites\nEnable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet as ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is supported only for CSI volumes. You can enable the feature by setting command-line argument: --feature-gates=\"...,ReadWriteOncePod=true\" Create a PVC with access mode set to ReadWriteOncePod like shown in the sample below kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-node-single-writer spec: accessModes: - ReadWriteOncePod # Allow only a single pod to access single-node-single-writer resources: requests: storage: 5Gi Note: The access mode ReadWriteOnce allows multiple pods to access a single volume within a single worker node and the behavior is consistent across all supported Kubernetes versions.\nPOSIX mode bits and NFSv4 ACLs CSI PowerStore driver version 2.2.0 and later allows users to set user-defined permissions on NFS target mount directory using POSIX mode bits or NFSv4 ACLs.\nNFSv4 ACLs are supported for NFSv4 shares on NFSv4 enabled NAS servers only. Please ensure the order when providing the NFSv4 ACLs.\nTo use this feature, provide permissions in nfsAcls parameter in values.yaml, secrets or NFS storage class.\nFor example:\nPOSIX mode bits nfsAcls: \"0755\" NFSv4 ACLs nfsAcls: \"A::OWNER@:rwatTnNcCy,A::GROUP@:rxtncy,A::EVERYONE@:rxtncy,A::user@domain.com:rxtncy\" Note: If no values are specified, default value of “0777” will be set. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.\nNVMe Support NVMeTCP Support CSI Driver for Dell Powerstore 2.2.0 and above supports NVMe/TCP provisioning. To enable NVMe/TCP provisioning, blockProtocol on secret should be specified as NVMeTCP.\nNote: NVMe/TCP is not supported on RHEL 7.x versions and CoreOS. NVMe/TCP is supported with Powerstore 2.1 and above.\nNVMeFC Support CSI Driver for Dell Powerstore 2.3.0 and above supports NVMe/FC provisioning. To enable NVMe/FC provisioning, blockProtocol on secret should be specified as NVMeFC.\nNVMe/FC is supported with Powerstore 3.0 and above.\nNVMe-FC feature is supported with Helm.\nNote: In case blockProtocol is specified as auto, the driver will be able to find the initiators on the host and choose the protocol accordingly. If the host has multiple protocols enabled, then NVMeFC gets the highest priority followed by NVMeTCP, followed by FC and then iSCSI.\nVolume group snapshot Support CSI Driver for Dell Powerstore 2.3.0 and above supports creating volume groups and take snapshot of them by making use of CRD (Custom Resource Definition). More information can be found here: Volume Group Snapshotter.\nConfigurable Volume Attributes (Optional) The CSI PowerStore driver version 2.3.0 and above supports Configurable volume atttributes.\nPowerStore array provides a set of optional volume creation attributes. These attributes can be configured for the volume (block and NFS) at the time of creation through PowerStore CSI driver. These attributes can be specified as labels in PVC yaml file. The following is a sample manifest for creating volume with some of the configurable volume attributes.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 namespace: default labels: description: DB-volume appliance_id: A1 volume_group_id: f5f9dbbd-d12f-463e-becb-2e6d0a85405e spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: powerstore-ext4 Note: Default description value is pvcName-pvcNamespace.\nThis is the list of all the attributes supported by PowerStore CSI driver:\nBlock Volume NFS Volume description appliance_id volume_group_id protection_policy_id performance_policy_id app_type app_type_other description config_type access_policy locking_policy folder_rename_policy is_async_mtime_enabled protection_policy_id file_events_publishing_mode host_io_size flr_attributes.flr_create.mode flr_attributes.flr_create.default_retention flr_attributes.flr_create.maximum_retention flr_attributes.flr_create.minimum_retention Note:\nRefer to the PowerStore array specification for the allowed values for each attribute, at https://\u003carray-ip\u003e/swaggerui/. Make sure that the attributes specified are supported by the version of PowerStore array used.\nConfigurable Volume Attributes feature is supported with Helm.\nStorage Capacity Tracking CSI PowerStore driver version 2.5.0 and above supports Storage Capacity Tracking.\nThis feature helps the scheduler to make more informed choices about where to start pods which depend on unbound volumes with late binding (aka “wait for first consumer”). Pods will be scheduled on a node (satisfying the topology constraints) only if the requested capacity is available on the storage array. If such a node is not available, the pods stay in Pending state. This means they are not scheduled.\nWithout storage capacity tracking, pods get scheduled on a node satisfying the topology constraints. If the required capacity is not available, volume attachment to the pods fails, and pods remain in ContainerCreating state. Storage capacity tracking eliminates unnecessary scheduling of pods when there is insufficient capacity.\nThe attribute storageCapacity.enabled in my-powerstore-settings.yaml can be used to enabled/disabled the feature during driver installation . To configure how often driver checks for changed capacity set storageCapacity.pollInterval attribute. In case of driver installed via operator, this interval can be configured in the sample files provided here by editing the capacity-poll-interval argument present in the provisioner sidecar.\nNote:\nThis feature requires kubernetes v1.24 and above and will be automatically disabled in lower version of kubernetes.\n","categories":"","description":"Code features for PowerStore Driver","excerpt":"Code features for PowerStore Driver","ref":"/csm-docs/v2/csidriver/features/powerstore/","tags":"","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell PowerStore using Helm.\nUpdate Driver from v2.6 to v2.7 using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps\nRun git clone -b v2.7.0 https://github.com/dell/csi-powerstore.git to clone the git repository and get the driver.\nEdit samples/secret/secret.yaml file and configure connection information for your PowerStore arrays changing the following parameters:\nendpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what transport protocol we should use (FC, ISCSI, NVMeTCP, None, or auto). nasName: defines what NAS should be used for NFS volumes. nfsAcls: (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. Add more blocks similar to above for each PowerStore array if necessary.\n(optional) create new storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running\nkubectl create -f \u003cpath_to_storageclass_file\u003e Storage classes created by v1.4/v2.0/v2.1/v2.2/v2.3/v2.4/v2.5/v2.6 driver will not be deleted, v2.7 driver will use default array to manage volumes provisioned with old storage classes. Thus, if you still have volumes provisioned by v1.4/v2.0/v2.1/v2.2/v2.3/v2.4/v2.5/v2.6 in your cluster then be sure to include the same array you have used for the v1.4/v2.0/v2.1/v2.2/v2.3/v2.4/v2.5/v2.6 driver and make it default in the secret.yaml file.\nCreate the secret by running\nkubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml Copy the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml and update parameters as per the requirement.\nRun the csi-install script with the option --upgrade by running:\n./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Upgrade using Dell CSI Operator: Notes:\nWhile upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes.\nUpgrading the Operator does not upgrade the CSI Driver.\nPlease upgrade the Dell CSI Operator by following here.\nOnce the operator is upgraded, to upgrade the driver, refer here.\nUpgrade using Dell CSM Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\nPlease upgrade the Dell CSM Operator by following here Once the operator is upgraded, to upgrade the driver, refer here ","categories":"","description":"Upgrade PowerStore CSI driver","excerpt":"Upgrade PowerStore CSI driver","ref":"/csm-docs/v2/csidriver/upgradation/drivers/powerstore/","tags":["upgrade","csi-driver"],"title":"PowerStore"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command creates a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset are created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset’s pods by running kubectl get pods -n testpowerstore\nThe pod must be Ready and Running\nIf Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\nDeleting volumes To delete volumes, pod and statefulset run, use the command:\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\nOpen your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u003cpowerstore.api.ip\u003e/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id in volumeHandle in format \u003cvolume-id/globalID/protocol\u003e in the manifest. Modify other parameters according to your needs. apiVersion: v1 kind: PersistentVolume metadata: name: existingvol spec: accessModes: - ReadWriteOnce capacity: storage: 30Gi csi: driver: csi-powerstore.dellemc.com volumeHandle: 0055558c-5ae1-4ed1-b421-6f5a9475c19f/unique/scsi persistentVolumeReclaimPolicy: Retain storageClassName: powerstore volumeMode: Filesystem Create PersistentVolumeClaim to use this PersistentVolume. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 30Gi storageClassName: powerstore Then use this PVC as a volume in a pod. apiVersion: v1 kind: Pod metadata: name: static-prov-pod spec: containers: - name: test image: quay.io/centos/centos:latest command: [ \"/bin/sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data0\" name: pvol volumes: - name: pvol persistentVolumeClaim: claimName: pvol After the pod is Ready and Running, you can start to use this pod and volume. Volume Snapshot Feature The CSI PowerStore driver version 2.0.0 and higher supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class Note: From v1.4, the CSI PowerStore driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the samples folder\nCreating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 spec: volumeSnapshotClassName: powerstore-snapclass source: persistentVolumeClaimName: pvol0 After the VolumeSnapshot has been successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Snapshot feature is optional for the installation CSI PowerStore driver version 1.4 makes the snapshot feature optional for the installation.\nTo enable or disable this feature, change values.snapshot.enable parameter to true or false, specify the following in values.yaml to enable this feature\nsnapshot: enable: true External Snapshotter and its CRDs are not installed even if the Snapshot feature is enabled. These have to be installed manually before the installation.\nDisabling the Snapshot feature will opt out of the snapshotter sidecar from the installation.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: testpowerstore spec: storageClassName: powerstore dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi iSCSI CHAP The CSI PowerStore driver Version 1.3.0 and later extends Challenge Handshake Authentication Protocol (CHAP) support by adding automatic credentials generation.\nThis means that you no longer need to provide chapsecret/chapuser credentials, they will be automatically generated by the driver for each host.\nTo enable this feature you need to set connection.enableCHAP to true when installing with helm or set X_CSI_POWERSTORE_ENABLE_CHAP to true in your PowerStore CustomResource when installing using operator.\nThe driver uses the generated chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating a new host on powerstore array driver will populate host chap credentials with generated values. When re-using already existing hosts driver must override existing CHAP credentials with newly generated ones.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powerstore-expand-sc annotations: storageclass.kubernetes.io/is-default-class: false provisioner: csi-powerstore.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true # Set this attribute to true if you plan to expand any PVCs created using this storage class parameters: csi.storage.k8s.io/fstype: xfs To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pstore-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 30Gi # Updated size from 3Gi to 30Gi storageClassName: powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\nRaw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion: apps/v1 kind: StatefulSet metadata: name: powerstoretest namespace: {{ .Values.namespace }} spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: powerstore resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: testpowerstore spec: storageClassName: powerstore accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: testpowerstore spec: storageClassName: powerstore dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Ephemeral Inline Volume The CSI PowerStore driver version 1.2 and later supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind: Pod apiVersion: v1 metadata: name: powerstore-inline-volume spec: containers: - name: test-container image: quay.io/centos/centos command: [ \"sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data\" name: volume volumes: - name: volume csi: driver: csi-powerstore.dellemc.com fsType: \"ext4\" volumeAttributes: size: \"20Gi\" arrayID: \"unique\" This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes: - name: volume csi: driver: csi-powerstore.dellemc.com fsType: \"nfs\" volumeAttributes: size: \"20Gi\" nasName: \"csi-nas-name\" nfsAcls: \"0777\" Controller HA The CSI PowerStore driver version 1.2 and later introduces the controller HA feature. Instead of StatefulSet, controller pods are deployed as a Deployment.\nBy default number of replicas is set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in the cluster, each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods must be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameters controller: # \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment nodeSelector: # \"controller.tolerations\" defines tolerations that would be applied to controller deployment tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameters controller: # \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" As mentioned earlier, you can configure where node driver pods would be assigned in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they must use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology features user must create their own storage classes similar to those that can be found in samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powerstore-fc provisioner: csi-powerstore.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-powerstore.dellemc.com/127.0.0.1-fc values: - \"true\" This example matches all nodes where the driver has a connection to PowerStore with an IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS, iSCSI and NVMe.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels\nNotice that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work.\nFor any additional information about the topology, see the Kubernetes Topology documentation.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as a Host on the storage array before. It will check if Host initiators and node initiators (FC, iSCSI or NVMe) match. If they do, the driver will not create a new host and will take the existing name of the Host as nodeID.\nMultiarray support The CSI PowerStore driver version 1.3.0 and later support managing multiple PowerStore arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration Create a file called config.yaml and populate it with the following content\narrays: - endpoint: \"https://10.0.0.1/api/rest\" # full URL path to the PowerStore API globalID: \"unique\" # global ID to identify array username: \"user\" # username for connecting to API password: \"password\" # password for connecting to API skipCertificateValidation: true # use insecure connection or not default: true # treat current array as a default (would be used by storage classes without arrayIP parameter) blockProtocol: \"ISCSI\" # what transport protocol use on node side (FC, ISCSI, NVMeTCP, None, or auto) nasName: \"nas-server\" # what NAS must be used for NFS volumes nfsAcls: \"0777\" # (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. # NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. - endpoint: \"https://10.0.0.2/api/rest\" globalID: \"unique\" username: \"user\" password: \"password\" skipCertificateValidation: true blockProtocol: \"FC\" Here we specify that we want to CSI driver to manage two arrays: one with an IP 10.0.0.1 and the other with an IP 10.0.0.2, we want to connect to the first array with iSCSI protocol and with FC to the second array. Also, we want to be able to create NFS-based volume so we provide the name of the NAS to the first array.\nTo use this config we need to create a Kubernetes secret from it, to do so create a file called secret.yaml in the same folder and populate it with the following content:\napiVersion: v1 kind: Secret metadata: name: powerstore-config namespace: \u003cdriver-namespace\u003e type: Opaque data: config: CONFIG_YAML Apply the secret by running following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Creating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nCreate file storageclass.yaml and populate it with the following content:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powerstore-1 provisioner: csi-powerstore.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer parameters: arrayID: \"GlobalUniqueID\" csi.storage.k8s.io/fstype: \"ext4\" --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powerstore-2 provisioner: csi-powerstore.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer parameters: arrayID: \"GlobalUniqueID\" csi.storage.k8s.io/fstype: \"xfs\" Here we specify two storage classes: one of them uses the first array and ext4 filesystem, and the other uses the second array and xfs filesystem.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use powerstore-1 storage class to create volumes on the first array and powerstore-2 storage class to create volumes on the second array.\nDynamic secret change detection CSI PowerStore driver version 1.3.0 and later supports the ability to detect changes to array configuration Kubernetes secret. This essentially means that you can change credentials for your PowerStore arrays in-flight (without restarting the driver).\nTo do so just change your configuration file config.yaml and apply it again using the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - After Kubernetes remounts secret to driver containers (this usually takes around one minute), a driver must detect the change and start using this new configuration information.\nConfiguring custom access to NFS exports CSI PowerStore driver Version 1.3.0 and later supports the ability to configure NFS access to nodes that use dedicated storage networks.\nTo enable this feature you need to specify externalAccess parameter in your helm values.yaml file or X_CSI_POWERSTORE_EXTERNAL_ACCESS variable when creating CustomResource using an operator.\nThe value of that parameter is added as an additional entry to NFS Export host access.\nFor example the following notation:\nexternalAccess: \"10.0.0.0/24\" This means that we allow for NFS Export created by driver to be consumed by address range 10.0.0.0-10.0.0.255.\nArray identification based on GlobalID CSI PowerStore driver version 1.4.0 onwards slightly changes the way arrays are being identified in runtime. In previous versions of the driver, a management IP address was used to identify an array. The address change could lead to an invalid state of PV. From version 1.4.0 a unique GlobalID string is used for an array identification. It has to be specified in config.yaml and in Storage Classes.\nThe change provides backward compatibility with previously created PVs. However, to provision new volumes, make sure to delete old Storage Classes and create new ones with arrayID instead of arrayIP specified.\nNOTE: It is recommended to migrate the PVs to new identifiers before changing management IPs of storage systems. The recommended way to do it is to clone the existing volume and delete the old one. The cloned volume will automatically switch to using globalID instead of management IP.\nRoot squashing CSI PowerStore driver version 1.4.0 and later allows users to enable root squashing for NFS volumes provisioned by the driver.\nRoot squashing rule prevents root users on NFS clients from exercising root privileges on the NFS server.\nTo enable this rule, you need to set parameter allowRoot to false in your NFS storage class.\nYour storage class definition must look similar to this:\napiVersion: storage.k8s.io/v1 kind: StorageClass ... parameters: ... allowRoot: \"false\" # enables or disables root squashing The 1.4 version and later of the driver also enables any container user, to have full access to provisioned NFS volume, in earlier versions only root user had access\nDynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params NAT Support CSI Driver for Dell Powerstore is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nPV/PVC Metrics CSI Driver for Dell Powerstore 2.1.0 and above supports volume health monitoring. To enable Volume Health Monitoring from the node side, the alpha feature gate CSIVolumeHealth needs to be enabled. To use this feature, set controller.healthMonitor.enabled and node.healthMonitor.enabled to true. To change the monitor interval, set controller.healthMonitor.interval parameter.\nSingle Pod Access Mode for PersistentVolumes Starting from version 2.1, CSI Driver for Powerstore now supports a new access mode ReadWriteOncePod for PersistentVolumes and PersistentVolumeClaims. With this feature, CSI Driver for Powerstore allows restricting volume access to a single pod in the cluster and within a worker node.\nPrerequisites\nEnable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet as ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is supported only for CSI volumes. You can enable the feature by setting command-line argument: --feature-gates=\"...,ReadWriteOncePod=true\"\nCreate a PVC with access mode set to ReadWriteOncePod like shown in the sample below\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-node-single-writer spec: accessModes: - ReadWriteOncePod # Allow only a single pod to access single-node-single-writer resources: requests: storage: 5Gi Note: The access mode ReadWriteOnce allows multiple pods to access a single volume within a single worker node and the behavior is consistent across all supported Kubernetes versions.\nPOSIX mode bits and NFSv4 ACLs CSI PowerStore driver version 2.2.0 and later allows users to set user-defined permissions on NFS target mount directory using POSIX mode bits or NFSv4 ACLs.\nNFSv4 ACLs are supported for NFSv4 shares on NFSv4 enabled NAS servers only. Please ensure the order when providing the NFSv4 ACLs.\nTo use this feature, provide permissions in nfsAcls parameter in values.yaml, secrets or NFS storage class.\nFor example:\nPOSIX mode bits nfsAcls: \"0755\" NFSv4 ACLs nfsAcls: \"A::OWNER@:rwatTnNcCy,A::GROUP@:rxtncy,A::EVERYONE@:rxtncy,A::user@domain.com:rxtncy\" Note: If no values are specified, default value of “0777” will be set. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.\nNVMe Support NVMeTCP Support CSI Driver for Dell Powerstore 2.2.0 and above supports NVMe/TCP provisioning. To enable NVMe/TCP provisioning, blockProtocol on secret should be specified as NVMeTCP.\nNote: NVMe/TCP is not supported on RHEL 7.x versions and CoreOS. NVMe/TCP is supported with Powerstore 2.1 and above.\nNVMeFC Support CSI Driver for Dell Powerstore 2.3.0 and above supports NVMe/FC provisioning. To enable NVMe/FC provisioning, blockProtocol on secret should be specified as NVMeFC.\nNVMe/FC is supported with Powerstore 3.0 and above.\nNVMe-FC feature is supported with Helm.\nNote: In case blockProtocol is specified as auto, the driver will be able to find the initiators on the host and choose the protocol accordingly. If the host has multiple protocols enabled, then NVMeFC gets the highest priority followed by NVMeTCP, followed by FC and then iSCSI.\nVolume group snapshot Support CSI Driver for Dell Powerstore 2.3.0 and above supports creating volume groups and take snapshot of them by making use of CRD (Custom Resource Definition). More information can be found here: Volume Group Snapshotter.\nConfigurable Volume Attributes (Optional) The CSI PowerStore driver version 2.3.0 and above supports Configurable volume atttributes.\nPowerStore array provides a set of optional volume creation attributes. These attributes can be configured for the volume (block and NFS) at the time of creation through PowerStore CSI driver. These attributes can be specified as labels in PVC yaml file. The following is a sample manifest for creating volume with some of the configurable volume attributes.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 namespace: default labels: description: DB-volume appliance_id: A1 volume_group_id: f5f9dbbd-d12f-463e-becb-2e6d0a85405e spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: powerstore-ext4 Note: Default description value is pvcName-pvcNamespace.\nThis is the list of all the attributes supported by PowerStore CSI driver:\nBlock Volume NFS Volume description appliance_id volume_group_id protection_policy_id performance_policy_id app_type app_type_other description config_type access_policy locking_policy folder_rename_policy is_async_mtime_enabled protection_policy_id file_events_publishing_mode host_io_size flr_attributes.flr_create.mode flr_attributes.flr_create.default_retention flr_attributes.flr_create.maximum_retention flr_attributes.flr_create.minimum_retention Note:\nRefer to the PowerStore array specification for the allowed values for each attribute, at https://\u003carray-ip\u003e/swaggerui/. Make sure that the attributes specified are supported by the version of PowerStore array used.\nConfigurable Volume Attributes feature is supported with Helm.\nStorage Capacity Tracking CSI PowerStore driver version 2.5.0 and above supports Storage Capacity Tracking.\nThis feature helps the scheduler to make more informed choices about where to start pods which depend on unbound volumes with late binding (aka “wait for first consumer”). Pods will be scheduled on a node (satisfying the topology constraints) only if the requested capacity is available on the storage array. If such a node is not available, the pods stay in Pending state. This means they are not scheduled.\nWithout storage capacity tracking, pods get scheduled on a node satisfying the topology constraints. If the required capacity is not available, volume attachment to the pods fails, and pods remain in ContainerCreating state. Storage capacity tracking eliminates unnecessary scheduling of pods when there is insufficient capacity.\nThe attribute storageCapacity.enabled in my-powerstore-settings.yaml can be used to enabled/disabled the feature during driver installation . To configure how often driver checks for changed capacity set storageCapacity.pollInterval attribute. In case of driver installed via operator, this interval can be configured in the sample files provided here by editing the capacity-poll-interval argument present in the provisioner sidecar.\nNote:\nThis feature requires kubernetes v1.24 and above and will be automatically disabled in lower version of kubernetes.\n","categories":"","description":"Code features for PowerStore Driver","excerpt":"Code features for PowerStore Driver","ref":"/csm-docs/v3/csidriver/features/powerstore/","tags":"","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell PowerStore using Helm.\nUpdate Driver from v2.5 to v2.6 using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps\nRun git clone -b v2.6.0 https://github.com/dell/csi-powerstore.git to clone the git repository and get the driver.\nEdit samples/secret/secret.yaml file and configure connection information for your PowerStore arrays changing the following parameters:\nendpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what transport protocol we should use (FC, ISCSI, NVMeTCP, None, or auto). nasName: defines what NAS should be used for NFS volumes. nfsAcls: (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. Add more blocks similar to above for each PowerStore array if necessary.\n(optional) create new storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\nStorage classes created by v1.4/v2.0/v2.1/v2.2/v2.3/v2.4/v2.5 driver will not be deleted, v2.6 driver will use default array to manage volumes provisioned with old storage classes. Thus, if you still have volumes provisioned by v1.4/v2.0/v2.1/v2.2/v2.3/v2.4/v2.5 in your cluster then be sure to include the same array you have used for the v1.4/v2.0/v2.1/v2.2/v2.3/v2.4/v2.5 driver and make it default in the secret.yaml file.\nCreate the secret by running kubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml\nCopy the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml and update parameters as per the requirement.\nRun the csi-install script with the option --upgrade by running: ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade.\nUpgrade using Dell CSI Operator: Notes:\nWhile upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes.\nUpgrading the Operator does not upgrade the CSI Driver.\nPlease upgrade the Dell CSI Operator by following here.\nOnce the operator is upgraded, to upgrade the driver, refer here.\n","categories":"","description":"Upgrade PowerStore CSI driver","excerpt":"Upgrade PowerStore CSI driver","ref":"/csm-docs/v3/csidriver/upgradation/drivers/powerstore/","tags":["upgrade","csi-driver"],"title":"PowerStore"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerStore. The Grafana reference dashboards for PowerStore metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the karaviMetricsPowerstore.volumeMetricsEnabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerstore_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s) powerstore_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s) powerstore_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume powerstore_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume powerstore_volume_read_iops_per_second The number of read operations performed against a volume (per second) powerstore_volume_write_iops_per_second The number of write operations performed against a volume (per second) powerstore_filesystem_read_bw_megabytes_per_second The filesystem read bandwidth MB/s powerstore_filesystem_write_bw_megabytes_per_second The filesystem write bandwidth (MB/s) powerstore_filesystem_read_iops_per_second The number of read operations performed against a filesystem (per second) powerstore_filesystem_write_iops_per_second The number of write operations performed against a filesystem (per second) powerstore_filesystem_read_latency_milliseconds The time (in ms) to complete read operations to a filesystem powerstore_filesystem_write_latency_milliseconds The time (in ms) to complete write operations to a filesystem Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the enable_powerstore_metrics field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerstore_array_logical_provisioned_megabytes Total provisioned logical storage on a given array managed by CSI driver powerstore_array_logical_used_megabytes Total used logical storage on a given array powerstore_storage_class_logical_provisioned_megabytes Total provisioned logical storage for a given storage class powerstore_storage_class_logical_used_megabytes Total used logical storage for a given storage class powerstore_volume_logical_provisioned_megabytes Logical provisioned storage for a volume powerstore_volume_logical_used_megabytes Logical used storage for a volume powerstore_filesystem_logical_provisioned_megabytes Logical provisioned storage for a filesystem powerstore_filesystem_logical_used_megabytes Logical used storage for a filesystem ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability PowerStore Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability PowerStore …","ref":"/csm-docs/docs/observability/metrics/powerstore/","tags":"","title":"PowerStore Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerStore. The Grafana reference dashboards for PowerStore metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the karaviMetricsPowerstore.volumeMetricsEnabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerstore_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s) powerstore_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s) powerstore_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume powerstore_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume powerstore_volume_read_iops_per_second The number of read operations performed against a volume (per second) powerstore_volume_write_iops_per_second The number of write operations performed against a volume (per second) powerstore_filesystem_read_bw_megabytes_per_second The filesystem read bandwidth MB/s powerstore_filesystem_write_bw_megabytes_per_second The filesystem write bandwidth (MB/s) powerstore_filesystem_read_iops_per_second The number of read operations performed against a filesystem (per second) powerstore_filesystem_write_iops_per_second The number of write operations performed against a filesystem (per second) powerstore_filesystem_read_latency_milliseconds The time (in ms) to complete read operations to a filesystem powerstore_filesystem_write_latency_milliseconds The time (in ms) to complete write operations to a filesystem Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the enable_powerstore_metrics field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerstore_array_logical_provisioned_megabytes Total provisioned logical storage on a given array managed by CSI driver powerstore_array_logical_used_megabytes Total used logical storage on a given array powerstore_storage_class_logical_provisioned_megabytes Total provisioned logical storage for a given storage class powerstore_storage_class_logical_used_megabytes Total used logical storage for a given storage class powerstore_volume_logical_provisioned_megabytes Logical provisioned storage for a volume powerstore_volume_logical_used_megabytes Logical used storage for a volume powerstore_filesystem_logical_provisioned_megabytes Logical provisioned storage for a filesystem powerstore_filesystem_logical_used_megabytes Logical used storage for a filesystem ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability PowerStore Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability PowerStore …","ref":"/csm-docs/v1/observability/metrics/powerstore/","tags":"","title":"PowerStore Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerStore. The Grafana reference dashboards for PowerStore metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the karaviMetricsPowerstore.volumeMetricsEnabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerstore_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s) powerstore_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s) powerstore_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume powerstore_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume powerstore_volume_read_iops_per_second The number of read operations performed against a volume (per second) powerstore_volume_write_iops_per_second The number of write operations performed against a volume (per second) powerstore_filesystem_read_bw_megabytes_per_second The filesystem read bandwidth MB/s powerstore_filesystem_write_bw_megabytes_per_second The filesystem write bandwidth (MB/s) powerstore_filesystem_read_iops_per_second The number of read operations performed against a filesystem (per second) powerstore_filesystem_write_iops_per_second The number of write operations performed against a filesystem (per second) powerstore_filesystem_read_latency_milliseconds The time (in ms) to complete read operations to a filesystem powerstore_filesystem_write_latency_milliseconds The time (in ms) to complete write operations to a filesystem Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the enable_powerstore_metrics field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerstore_array_logical_provisioned_megabytes Total provisioned logical storage on a given array managed by CSI driver powerstore_array_logical_used_megabytes Total used logical storage on a given array powerstore_storage_class_logical_provisioned_megabytes Total provisioned logical storage for a given storage class powerstore_storage_class_logical_used_megabytes Total used logical storage for a given storage class powerstore_volume_logical_provisioned_megabytes Logical provisioned storage for a volume powerstore_volume_logical_used_megabytes Logical used storage for a volume powerstore_filesystem_logical_provisioned_megabytes Logical provisioned storage for a filesystem powerstore_filesystem_logical_used_megabytes Logical used storage for a filesystem ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability PowerStore Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability PowerStore …","ref":"/csm-docs/v2/observability/metrics/powerstore/","tags":"","title":"PowerStore Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerStore. The Grafana reference dashboards for PowerStore metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the karaviMetricsPowerstore.volumeMetricsEnabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerstore_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s) powerstore_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s) powerstore_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume powerstore_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume powerstore_volume_read_iops_per_second The number of read operations performed against a volume (per second) powerstore_volume_write_iops_per_second The number of write operations performed against a volume (per second) powerstore_filesystem_read_bw_megabytes_per_second The filesystem read bandwidth MB/s powerstore_filesystem_write_bw_megabytes_per_second The filesystem write bandwidth (MB/s) powerstore_filesystem_read_iops_per_second The number of read operations performed against a filesystem (per second) powerstore_filesystem_write_iops_per_second The number of write operations performed against a filesystem (per second) powerstore_filesystem_read_latency_milliseconds The time (in ms) to complete read operations to a filesystem powerstore_filesystem_write_latency_milliseconds The time (in ms) to complete write operations to a filesystem Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the enable_powerstore_metrics field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\nMetric Description powerstore_array_logical_provisioned_megabytes Total provisioned logical storage on a given array managed by CSI driver powerstore_array_logical_used_megabytes Total used logical storage on a given array powerstore_storage_class_logical_provisioned_megabytes Total provisioned logical storage for a given storage class powerstore_storage_class_logical_used_megabytes Total used logical storage for a given storage class powerstore_volume_logical_provisioned_megabytes Logical provisioned storage for a volume powerstore_volume_logical_used_megabytes Logical used storage for a volume powerstore_filesystem_logical_provisioned_megabytes Logical provisioned storage for a filesystem powerstore_filesystem_logical_used_megabytes Logical used storage for a filesystem ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability PowerStore Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability PowerStore …","ref":"/csm-docs/v3/observability/metrics/powerstore/","tags":"","title":"PowerStore Metrics"},{"body":"Release Notes - CSM Resiliency 1.8.1 New Features/Changes #947 - [FEATURE]: Support for Kubernetes 1.28 #1066 - [FEATURE]: Support for Openshift 4.14 #996 - [FEATURE]: Dell CSI to Dell CSM Operator Migration Process #1031 - [FEATURE]: Update to the latest UBI Micro image for CSM Fixed Issues Known Issues There are no known issues in this release.\n","categories":"","description":"Dell Container Storage Modules (CSM) release notes for resiliency\n","excerpt":"Dell Container Storage Modules (CSM) release notes for resiliency\n","ref":"/csm-docs/docs/resiliency/release/","tags":"","title":"Release notes"},{"body":"Release Notes - CSM Resiliency 1.7.0 New Features/Changes #724 - [FEATURE]: CSM support for Openshift 4.13 #922 - [FEATURE]: Use ubi9 micro as base image Fixed Issues #916 - [BUG]: Remove references to deprecated io/ioutil package Known Issues There are no known issues in this release.\n","categories":"","description":"Dell Container Storage Modules (CSM) release notes for resiliency\n","excerpt":"Dell Container Storage Modules (CSM) release notes for resiliency\n","ref":"/csm-docs/v1/resiliency/release/","tags":"","title":"Release notes"},{"body":"Release Notes - CSM Resiliency 1.6.0 New Features/Changes CSM support for Kubernetes 1.27. (#761) CSM 1.7 release specific changes. (#743) CSM Operator: Support install of Resiliency module. (#739) Fixed Issues CSM for Resiliency openshift test required to pass ssh options in scp command. (#737) CSM Resiliency GitHub actions produce sporadic failure. (#733) CSI PODMON is tainting the worker node (#765) ","categories":"","description":"Dell Container Storage Modules (CSM) release notes for resiliency\n","excerpt":"Dell Container Storage Modules (CSM) release notes for resiliency\n","ref":"/csm-docs/v2/resiliency/release/","tags":"","title":"Release notes"},{"body":"Release Notes - CSM Resiliency 1.5.0 New Features/Changes Add CSM Resiliency support for PowerStore. (#587) Update to the latest UBI/UBI Minimal images for CSM. (#612) CSM 1.6 release specific changes. (#583) Fixed Issues Known Issues ","categories":"","description":"Dell Container Storage Modules (CSM) release notes for resiliency\n","excerpt":"Dell Container Storage Modules (CSM) release notes for resiliency\n","ref":"/csm-docs/v3/resiliency/release/","tags":"","title":"Release notes"},{"body":"Creating volumes and consuming them Create a file sample.yaml using sample yaml files located at test/sample.yaml\nThe following command creates a statefulset that consumes three volumes of default storage classes:\nkubectl create -f test/sample.yaml After executing this command 3 PVC and statefulset are created in the unity namespace. You can check created PVCs by running kubectl get pvc -n unity and check statefulset’s pods by running kubectl get pods -n unity command. The pod should be Ready and Running.\nIf Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\nDeleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f test/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity XT array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\nOpen your volume in Unity XT Management UI (Unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. apiVersion: v1 kind: PersistentVolume metadata: name: static-1 annotations: pv.kubernetes.io/provisioned-by: csi-unity.dellemc.com spec: accessModes: - ReadWriteOnce capacity: storage: 5Gi csi: driver: csi-unity.dellemc.com volumeHandle: existingvol-\u003cprotocol\u003e-\u003carray_id\u003e-\u003cvolume-id\u003e persistentVolumeReclaimPolicy: Retain claimRef: namespace: default name: static-pvc1 storageClassName: unity volumeMode: Filesystem Create PersistentVolumeClaim to use this PersistentVolume. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: static-pvc1 spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi volumeName: static-1 storageClassName: unity Then use this PVC as a volume in a pod. apiVersion: v1 kind: Pod metadata: name: static-prov-pod spec: containers: - name: test image: docker.io/centos:latest command: [ \"/bin/sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data0\" name: pvol volumes: - name: pvol persistentVolumeClaim: claimName: static-pvc1 After the pod becomes Ready and Running, you can start to use this pod and volume. Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class Following is the manifest to create Volume Snapshot Class :\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: unity-snapclass driver: csi-unity.dellemc.com deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap namespace: unity spec: volumeSnapshotClassName: unity-snapclass source: persistentVolumeClaimName: pvol Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-xxxxxxxxxxxxx creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Note : A set of annotated volume snapshot class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: unity spec: storageClassName: unity-iscsi dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Volume Expansion The CSI Unity XT driver supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: unity-expand-sc annotations: storageclass.kubernetes.io/is-default-class: false provisioner: csi-unity.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true # Set this attribute to true if you plan to expand any PVCs created using this storage class parameters: csi.storage.k8s.io/fstype: \"xfs\" To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: unity-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 30Gi # Updated size from 3Gi to 30Gi storageClassName: unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\nRaw block support The CSI Unity XT driver supports Raw Block Volumes. Raw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. The following is an example configuration:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: rawblockpvc namespace: default spec: accessModes: - ReadWriteOnce volumeMode: Block resources: requests: storage: 5Gi storageClassName: unity-iscsi apiVersion: v1 kind: Pod metadata: name: rawblockpod namespace: default spec: containers: - name: task-pv-container image: nginx ports: - containerPort: 80 name: \"http-server\" volumeDevices: - devicePath: /usr/share/nginx/html/device name: nov-eleventh-1-pv-storage volumes: - name: nov-eleventh-1-pv-storage persistentVolumeClaim: claimName: rawblockpvc Access modes allowed are ReadWriteOnce and ReadWriteMany. Raw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device.\nRaw Block volumes support online Volume Expansion, but it is up to the application to manage and reconfigure the file system (if any) to the new size. Access mode ReadOnlyMany is not supported with raw block since we cannot restrict volumes to be readonly from Unity XT.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity XT driver supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: vol0 namespace: unity spec: storageClassName: unity-nfs accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: cloned-pvc namespace: unity spec: storageClassName: unity-nfs dataSource: name: vol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Ephemeral Inline Volume The CSI Unity XT driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity XT driver.\nkind: Pod apiVersion: v1 metadata: name: test-unity-ephemeral-volume spec: containers: - name: test-container image: busybox command: [ \"sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data\" name: volume volumes: - name: volume csi: driver: csi-unity.dellemc.com fsType: \"ext4\" volumeAttributes: size: \"10Gi\" arrayId: APM************ protocol: iSCSI thinProvisioned: \"true\" isDataReductionEnabled: \"false\" tieringPolicy: \"1\" storagePool: pool_2 This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes: - name: volume csi: driver: csi-unity.dellemc.com csi.storage.k8s.io/fstype: \"nfs\" volumeAttributes: size: \"20Gi\" nasName: \"csi-nas-name\" Controller HA The CSI Unity XT driver supports controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default, the number of replicas is set to 2. You can set the controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator, you can change the replicas parameter in the spec.driver section in your Unity XT Custom Resource.\nWhen multiple replicas of controller pods are in a cluster each sidecar (Attacher, Provisioner, Resizer, and Snapshotter) tries to get a lease so only one instance of each sidecar is active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameters controller: # \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment nodeSelector: # \"controller.tolerations\" defines tolerations that would be applied to controller deployment tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameters controller: # \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" As said before you can configure where node driver pods would be assigned in a similar way in the node section of myvalues.yaml\nTopology The CSI Unity XT driver supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage User can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: unity-topology-fc provisioner: csi-unity.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-unity.dellemc.com/\u003carray_id\u003e-fc values: - \"true\" This example matches all nodes where the driver has a connection to the Unity XT array with array ID mentioned via Fiber Channel. Similarly, by replacing fc with iscsi in the key checks for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\nNote that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work properly.\nFor any additional information about the topology, see the Kubernetes Topology documentation.\nVolume Limit The CSI Driver for Dell Unity XT allows users to specify the maximum number of Unity XT volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-unity-volumes-per-node and specifying the volume limit for that node. kubectl label node \u003cnode_name\u003e max-unity-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxUnityVolumesPerNode attribute in values.yaml file.\nNOTE: To reflect the changes after setting the value either via node label or in values.yaml file, user has to bounce the driver controller and node pods using the command kubectl get pods -n unity --no-headers=true | awk '/unity-/{print $1}'| xargs kubectl delete -n unity pod. If the value is set both by node label and values.yaml file then node label value will get the precedence and user has to remove the node label in order to reflect the values.yaml value. The default value of maxUnityVolumesPerNode is 0. If maxUnityVolumesPerNode is set to zero, then Container Orchestration decides how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxUnityVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-unity-volumes-per-node is not set.\nNAT Support CSI Driver for Dell Unity XT is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nSingle Pod Access Mode for PersistentVolumes CSI Driver for Unity XT supports a new accessmode ReadWriteOncePod for PersistentVolumes and PersistentVolumeClaims. With this feature, CSI Driver for Unity XT restricts volume access to a single pod in the cluster\nPrerequisites\nEnable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet as the ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is only supported for CSI volumes. You can enable the feature by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\" Create a PVC with access mode set to ReadWriteOncePod like shown in the sample below kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-writer-only spec: accessModes: - ReadWriteOncePod # Allow only a single pod to access single-writer-only. resources: requests: storage: 1Gi Volume Health Monitoring CSI Driver for Unity XT supports volume health monitoring. This is an alpha feature and requires feature gate to be enabled by setting command line arguments\n--feature-gates=\"...,CSIVolumeHealth=true\" This feature:\nReports on the condition of the underlying volumes via events when a volume condition is abnormal. We can watch the events on the describe of pvc kubectl describe pvc \u003cpvc name\u003e -n \u003cnamespace\u003e Collects the volume stats. We can see the volume usage in the node logs kubectl logs \u003cnodepod\u003e -n \u003cnamespacename\u003e -c driver By default this is disabled in CSI Driver for Unity XT. You will have to set the healthMonitor.enable flag for controller, node or for both in values.yaml to get the volume stats and volume condition.\nStorage Capacity Tracking CSI for Unity XT driver version 2.8.0 and above supports Storage Capacity Tracking.\nThis feature helps the scheduler to make more informed choices about where to schedule pods which depends on unbound volumes with late binding (aka “wait for first consumer”). Pods will be scheduled on a node (satisfying the topology constraints) only if the requested capacity is available on the storage array. If such a node is not available, the pods stay in Pending state. This means pods are not scheduled.\nWithout storage capacity tracking, pods get scheduled on a node satisfying the topology constraints. If the required capacity is not available, volume attachment to the pods fails, and pods remain in ContainerCreating state. Storage capacity tracking eliminates unnecessary scheduling of pods when there is insufficient capacity. Moreover, storage capacity tracking returns MaximumVolumeSize parameter, which may be used as an input to the volume creation.\nThe attribute storageCapacity.enabled in values.yaml can be used to enable/disable the feature during driver installation using helm. This is by default set to true. To configure how often driver checks for changed capacity set storageCapacity.pollInterval attribute. In case of driver installed via operator, this interval can be configured in the sample file provided here. by editing the --capacity-poll-interval argument present in the provisioner sidecar.\nDynamic Logging Configuration Helm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params Tenancy support for Unity XT NFS The CSI Unity XT driver supports the Tenancy feature of Unity XT that allows the user to associate specific worker nodes (in the cluster) and NFS storage volumes with Tenant.\nPrerequisites (to be manually created in Unity XT Array) before the driver installation:\nCreate Tenants Create Pools Create NAS Servers with Tenant and Pool mapping The following example describes the usage of Tenant in the NFS pod creation:\nInstall the csi driver using myvalues.yaml with the TenantName as follows: Example myvalues.yaml\nlogLevel: \"info\" certSecretCount: 1 kubeletConfigDir: /var/lib/kubelet controller: controllerCount: 2 volumeNamePrefix : csivol snapshot: snapNamePrefix: csi-snap tenantName: \"tenant3\" Create storageclass with NAS-Server and the Storage-Pool associated with TenantName as follows: Example storageclass.yaml\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: storageclass.kubernetes.io/is-default-class: \"false\" name: unity-nfs parameters: arrayId: \"APM0***XXXXXX\" hostIoSize: \"16384\" isDataReductionEnabled: \"false\" storagePool: pool_7 thinProvisioned: \"true\" tieringPolicy: \"0\" protocol: \"NFS\" nasServer: \"nas_5\" provisioner: csi-unity.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true Create the pod and pvc as follows: Example pvc.yaml\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvcname namespace: nginx spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 2Gi storageClassName: unity-nfs Example pod.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: podname namespace: nginx spec: replicas: 1 selector: matchLabels: app: podname template: metadata: labels: app: podname spec: containers: - args: - \"-c\" - \"while true; do dd if=/dev/urandom of=/data0/foo bs=1M count=1;done\" command: - /bin/bash image: \"docker.io/centos:latest\" name: test volumeMounts: - mountPath: /data0 name: pvcname volumes: - name: pvolx0 persistentVolumeClaim: claimName: pvcname With the usage shown in the example, the user will be able to create an NFS pod with PVC using the NAS and the Pool associated with the added Tenants specified in SC.\nNote: Current feature supports ONLY single Tenant for all the nodes in the cluster. Users may expect an error if PVC is created from the NAS server whose pool is mapped to the different tenants not associated with this SC.\nFor operator based installation, mention the TENANT_NAME in configmap as shown in the following example: Example configmap.yaml\napiVersion: v1 kind: ConfigMap metadata: name: unity-config-params namespace: unity data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"info\" ALLOW_RWO_MULTIPOD_ACCESS: \"false\" MAX_UNITY_VOLUMES_PER_NODE: \"0\" SYNC_NODE_INFO_TIME_INTERVAL: \"15\" TENANT_NAME: \"\" Note: csi-unity supports Tenancy in multi-array setup, provided the TenantName is the same across Unity XT instances.\n","categories":"","description":"Code features for Unity XT Driver","excerpt":"Code features for Unity XT Driver","ref":"/csm-docs/docs/csidriver/features/unity/","tags":"","title":"Unity XT"},{"body":"You can upgrade the CSI Driver for Dell Unity XT using Helm or Dell CSM Operator.\nNote:\nUser has to re-create existing custom-storage classes (if any) according to the latest format. User has to create Volumesnapshotclass after upgrade for taking Snapshots. Secret.yaml files can be updated according to Multiarray normalization parameters only after upgrading the driver. Using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nPreparing myvalues.yaml is the same as explained in the install section.\nTo upgrade the driver from csi-unity v2.8.0 to csi-unity v2.9.1\nGet the latest csi-unity v2.9.1 code from Github using git clone -b v2.9.1 https://github.com/dell/csi-unity.git. Copy the helm/csi-unity/values.yaml to the new location csi-unity/dell-csi-helm-installer and rename it to myvalues.yaml. Customize settings for installation by editing myvalues.yaml as needed. Navigate to csi-unity/dell-csi-hem-installer folder and execute this command: ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Upgrade using Dell CSM Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\nUpgrade the Dell CSM Operator by following here Once the operator is upgraded, to upgrade the driver, refer here ","categories":"","description":"Upgrade Unity XT CSI driver","excerpt":"Upgrade Unity XT CSI driver","ref":"/csm-docs/docs/csidriver/upgradation/drivers/unity/","tags":["upgrade","csi-driver"],"title":"Unity XT"},{"body":"Creating volumes and consuming them Create a file sample.yaml using sample yaml files located at test/sample.yaml\nThe following command creates a statefulset that consumes three volumes of default storage classes:\nkubectl create -f test/sample.yaml After executing this command 3 PVC and statefulset are created in the unity namespace. You can check created PVCs by running kubectl get pvc -n unity and check statefulset’s pods by running kubectl get pods -n unity command. The pod should be Ready and Running.\nIf Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\nDeleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f test/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity XT array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\nOpen your volume in Unity XT Management UI (Unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. apiVersion: v1 kind: PersistentVolume metadata: name: static-1 annotations: pv.kubernetes.io/provisioned-by: csi-unity.dellemc.com spec: accessModes: - ReadWriteOnce capacity: storage: 5Gi csi: driver: csi-unity.dellemc.com volumeHandle: existingvol-\u003cprotocol\u003e-\u003carray_id\u003e-\u003cvolume-id\u003e persistentVolumeReclaimPolicy: Retain claimRef: namespace: default name: static-pvc1 storageClassName: unity volumeMode: Filesystem Create PersistentVolumeClaim to use this PersistentVolume. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: static-pvc1 spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi volumeName: static-1 storageClassName: unity Then use this PVC as a volume in a pod. apiVersion: v1 kind: Pod metadata: name: static-prov-pod spec: containers: - name: test image: docker.io/centos:latest command: [ \"/bin/sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data0\" name: pvol volumes: - name: pvol persistentVolumeClaim: claimName: static-pvc1 After the pod becomes Ready and Running, you can start to use this pod and volume. Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class Following is the manifest to create Volume Snapshot Class :\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: unity-snapclass driver: csi-unity.dellemc.com deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap namespace: unity spec: volumeSnapshotClassName: unity-snapclass source: persistentVolumeClaimName: pvol Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-xxxxxxxxxxxxx creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Note : A set of annotated volume snapshot class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: unity spec: storageClassName: unity-iscsi dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Volume Expansion The CSI Unity XT driver supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: unity-expand-sc annotations: storageclass.kubernetes.io/is-default-class: false provisioner: csi-unity.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true # Set this attribute to true if you plan to expand any PVCs created using this storage class parameters: csi.storage.k8s.io/fstype: \"xfs\" To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: unity-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 30Gi # Updated size from 3Gi to 30Gi storageClassName: unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\nRaw block support The CSI Unity XT driver supports Raw Block Volumes. Raw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. The following is an example configuration:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: rawblockpvc namespace: default spec: accessModes: - ReadWriteOnce volumeMode: Block resources: requests: storage: 5Gi storageClassName: unity-iscsi apiVersion: v1 kind: Pod metadata: name: rawblockpod namespace: default spec: containers: - name: task-pv-container image: nginx ports: - containerPort: 80 name: \"http-server\" volumeDevices: - devicePath: /usr/share/nginx/html/device name: nov-eleventh-1-pv-storage volumes: - name: nov-eleventh-1-pv-storage persistentVolumeClaim: claimName: rawblockpvc Access modes allowed are ReadWriteOnce and ReadWriteMany. Raw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device.\nRaw Block volumes support online Volume Expansion, but it is up to the application to manage and reconfigure the file system (if any) to the new size. Access mode ReadOnlyMany is not supported with raw block since we cannot restrict volumes to be readonly from Unity XT.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity XT driver supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: vol0 namespace: unity spec: storageClassName: unity-nfs accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: cloned-pvc namespace: unity spec: storageClassName: unity-nfs dataSource: name: vol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Ephemeral Inline Volume The CSI Unity XT driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity XT driver.\nkind: Pod apiVersion: v1 metadata: name: test-unity-ephemeral-volume spec: containers: - name: test-container image: busybox command: [ \"sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data\" name: volume volumes: - name: volume csi: driver: csi-unity.dellemc.com fsType: \"ext4\" volumeAttributes: size: \"10Gi\" arrayId: APM************ protocol: iSCSI thinProvisioned: \"true\" isDataReductionEnabled: \"false\" tieringPolicy: \"1\" storagePool: pool_2 This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes: - name: volume csi: driver: csi-unity.dellemc.com csi.storage.k8s.io/fstype: \"nfs\" volumeAttributes: size: \"20Gi\" nasName: \"csi-nas-name\" Controller HA The CSI Unity XT driver supports controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default, the number of replicas is set to 2. You can set the controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator, you can change the replicas parameter in the spec.driver section in your Unity XT Custom Resource.\nWhen multiple replicas of controller pods are in a cluster each sidecar (Attacher, Provisioner, Resizer, and Snapshotter) tries to get a lease so only one instance of each sidecar is active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameters controller: # \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment nodeSelector: # \"controller.tolerations\" defines tolerations that would be applied to controller deployment tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameters controller: # \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" As said before you can configure where node driver pods would be assigned in a similar way in the node section of myvalues.yaml\nTopology The CSI Unity XT driver supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage User can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: unity-topology-fc provisioner: csi-unity.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-unity.dellemc.com/\u003carray_id\u003e-fc values: - \"true\" This example matches all nodes where the driver has a connection to the Unity XT array with array ID mentioned via Fiber Channel. Similarly, by replacing fc with iscsi in the key checks for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\nNote that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work properly.\nFor any additional information about the topology, see the Kubernetes Topology documentation.\nVolume Limit The CSI Driver for Dell Unity XT allows users to specify the maximum number of Unity XT volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-unity-volumes-per-node and specifying the volume limit for that node. kubectl label node \u003cnode_name\u003e max-unity-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxUnityVolumesPerNode attribute in values.yaml file.\nNOTE: To reflect the changes after setting the value either via node label or in values.yaml file, user has to bounce the driver controller and node pods using the command kubectl get pods -n unity --no-headers=true | awk '/unity-/{print $1}'| xargs kubectl delete -n unity pod. If the value is set both by node label and values.yaml file then node label value will get the precedence and user has to remove the node label in order to reflect the values.yaml value. The default value of maxUnityVolumesPerNode is 0. If maxUnityVolumesPerNode is set to zero, then Container Orchestration decides how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxUnityVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-unity-volumes-per-node is not set.\nNAT Support CSI Driver for Dell Unity XT is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nSingle Pod Access Mode for PersistentVolumes CSI Driver for Unity XT supports a new accessmode ReadWriteOncePod for PersistentVolumes and PersistentVolumeClaims. With this feature, CSI Driver for Unity XT restricts volume access to a single pod in the cluster\nPrerequisites\nEnable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet as the ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is only supported for CSI volumes. You can enable the feature by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\" Create a PVC with access mode set to ReadWriteOncePod like shown in the sample below kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-writer-only spec: accessModes: - ReadWriteOncePod # Allow only a single pod to access single-writer-only. resources: requests: storage: 1Gi Volume Health Monitoring CSI Driver for Unity XT supports volume health monitoring. This is an alpha feature and requires feature gate to be enabled by setting command line arguments\n--feature-gates=\"...,CSIVolumeHealth=true\" This feature:\nReports on the condition of the underlying volumes via events when a volume condition is abnormal. We can watch the events on the describe of pvc kubectl describe pvc \u003cpvc name\u003e -n \u003cnamespace\u003e Collects the volume stats. We can see the volume usage in the node logs kubectl logs \u003cnodepod\u003e -n \u003cnamespacename\u003e -c driver By default this is disabled in CSI Driver for Unity XT. You will have to set the healthMonitor.enable flag for controller, node or for both in values.yaml to get the volume stats and volume condition.\nStorage Capacity Tracking CSI for Unity XT driver version 2.8.0 and above supports Storage Capacity Tracking.\nThis feature helps the scheduler to make more informed choices about where to schedule pods which depends on unbound volumes with late binding (aka “wait for first consumer”). Pods will be scheduled on a node (satisfying the topology constraints) only if the requested capacity is available on the storage array. If such a node is not available, the pods stay in Pending state. This means pods are not scheduled.\nWithout storage capacity tracking, pods get scheduled on a node satisfying the topology constraints. If the required capacity is not available, volume attachment to the pods fails, and pods remain in ContainerCreating state. Storage capacity tracking eliminates unnecessary scheduling of pods when there is insufficient capacity.\nThe attribute storageCapacity.enabled in values.yaml can be used to enable/disable the feature during driver installation using helm. This is by default set to true. To configure how often driver checks for changed capacity set storageCapacity.pollInterval attribute. In case of driver installed via operator, this interval can be configured in the sample file provided here. by editing the --capacity-poll-interval argument present in the provisioner sidecar.\nDynamic Logging Configuration Helm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params Tenancy support for Unity XT NFS The CSI Unity XT driver supports the Tenancy feature of Unity XT that allows the user to associate specific worker nodes (in the cluster) and NFS storage volumes with Tenant.\nPrerequisites (to be manually created in Unity XT Array) before the driver installation:\nCreate Tenants Create Pools Create NAS Servers with Tenant and Pool mapping The following example describes the usage of Tenant in the NFS pod creation:\nInstall the csi driver using myvalues.yaml with the TenantName as follows: Example myvalues.yaml\nlogLevel: \"info\" certSecretCount: 1 kubeletConfigDir: /var/lib/kubelet controller: controllerCount: 2 volumeNamePrefix : csivol snapshot: snapNamePrefix: csi-snap tenantName: \"tenant3\" Create storageclass with NAS-Server and the Storage-Pool associated with TenantName as follows: Example storageclass.yaml\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: storageclass.kubernetes.io/is-default-class: \"false\" name: unity-nfs parameters: arrayId: \"APM0***XXXXXX\" hostIoSize: \"16384\" isDataReductionEnabled: \"false\" storagePool: pool_7 thinProvisioned: \"true\" tieringPolicy: \"0\" protocol: \"NFS\" nasServer: \"nas_5\" provisioner: csi-unity.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true Create the pod and pvc as follows: Example pvc.yaml\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvcname namespace: nginx spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 2Gi storageClassName: unity-nfs Example pod.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: podname namespace: nginx spec: replicas: 1 selector: matchLabels: app: podname template: metadata: labels: app: podname spec: containers: - args: - \"-c\" - \"while true; do dd if=/dev/urandom of=/data0/foo bs=1M count=1;done\" command: - /bin/bash image: \"docker.io/centos:latest\" name: test volumeMounts: - mountPath: /data0 name: pvcname volumes: - name: pvolx0 persistentVolumeClaim: claimName: pvcname With the usage shown in the example, the user will be able to create an NFS pod with PVC using the NAS and the Pool associated with the added Tenants specified in SC.\nNote: Current feature supports ONLY single Tenant for all the nodes in the cluster. Users may expect an error if PVC is created from the NAS server whose pool is mapped to the different tenants not associated with this SC.\nFor operator based installation, mention the TENANT_NAME in configmap as shown in the following example: Example configmap.yaml\napiVersion: v1 kind: ConfigMap metadata: name: unity-config-params namespace: unity data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"info\" ALLOW_RWO_MULTIPOD_ACCESS: \"false\" MAX_UNITY_VOLUMES_PER_NODE: \"0\" SYNC_NODE_INFO_TIME_INTERVAL: \"15\" TENANT_NAME: \"\" Note: csi-unity supports Tenancy in multi-array setup, provided the TenantName is the same across Unity XT instances.\n","categories":"","description":"Code features for Unity XT Driver","excerpt":"Code features for Unity XT Driver","ref":"/csm-docs/v1/csidriver/features/unity/","tags":"","title":"Unity XT"},{"body":"You can upgrade the CSI Driver for Dell Unity XT using Helm or Dell CSI Operator.\nNote:\nUser has to re-create existing custom-storage classes (if any) according to the latest format. User has to create Volumesnapshotclass after upgrade for taking Snapshots. Secret.yaml files can be updated according to Multiarray normalization parameters only after upgrading the driver. Using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nPreparing myvalues.yaml is the same as explained in the install section.\nTo upgrade the driver from csi-unity v2.7.0 to csi-unity v2.8.0\nGet the latest csi-unity v2.8.0 code from Github using git clone -b v2.8.0 https://github.com/dell/csi-unity.git. Copy the helm/csi-unity/values.yaml to the new location csi-unity/dell-csi-helm-installer and rename it to myvalues.yaml. Customize settings for installation by editing myvalues.yaml as needed. Navigate to csi-unity/dell-csi-hem-installer folder and execute this command: ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Upgrade using Dell CSM Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\nUpgrade the Dell CSM Operator by following here Once the operator is upgraded, to upgrade the driver, refer here ","categories":"","description":"Upgrade Unity XT CSI driver","excerpt":"Upgrade Unity XT CSI driver","ref":"/csm-docs/v1/csidriver/upgradation/drivers/unity/","tags":["upgrade","csi-driver"],"title":"Unity XT"},{"body":"Creating volumes and consuming them Create a file sample.yaml using sample yaml files located at test/sample.yaml\nThe following command creates a statefulset that consumes three volumes of default storage classes:\nkubectl create -f test/sample.yaml After executing this command 3 PVC and statefulset are created in the unity namespace. You can check created PVCs by running kubectl get pvc -n unity and check statefulset’s pods by running kubectl get pods -n unity command. The pod should be Ready and Running.\nIf Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\nDeleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f test/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity XT array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\nOpen your volume in Unity XT Management UI (Unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. apiVersion: v1 kind: PersistentVolume metadata: name: static-1 annotations: pv.kubernetes.io/provisioned-by: csi-unity.dellemc.com spec: accessModes: - ReadWriteOnce capacity: storage: 5Gi csi: driver: csi-unity.dellemc.com volumeHandle: existingvol-\u003cprotocol\u003e-\u003carray_id\u003e-\u003cvolume-id\u003e persistentVolumeReclaimPolicy: Retain claimRef: namespace: default name: static-pvc1 storageClassName: unity volumeMode: Filesystem Create PersistentVolumeClaim to use this PersistentVolume. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: static-pvc1 spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi volumeName: static-1 storageClassName: unity Then use this PVC as a volume in a pod. apiVersion: v1 kind: Pod metadata: name: static-prov-pod spec: containers: - name: test image: docker.io/centos:latest command: [ \"/bin/sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data0\" name: pvol volumes: - name: pvol persistentVolumeClaim: claimName: static-pvc1 After the pod becomes Ready and Running, you can start to use this pod and volume. Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class Following is the manifest to create Volume Snapshot Class :\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: unity-snapclass driver: csi-unity.dellemc.com deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap namespace: unity spec: volumeSnapshotClassName: unity-snapclass source: persistentVolumeClaimName: pvol Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-xxxxxxxxxxxxx creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Note : A set of annotated volume snapshot class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: unity spec: storageClassName: unity-iscsi dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Volume Expansion The CSI Unity XT driver supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: unity-expand-sc annotations: storageclass.kubernetes.io/is-default-class: false provisioner: csi-unity.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true # Set this attribute to true if you plan to expand any PVCs created using this storage class parameters: csi.storage.k8s.io/fstype: \"xfs\" To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: unity-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 30Gi # Updated size from 3Gi to 30Gi storageClassName: unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\nRaw block support The CSI Unity XT driver supports Raw Block Volumes. Raw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. The following is an example configuration:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: rawblockpvc namespace: default spec: accessModes: - ReadWriteOnce volumeMode: Block resources: requests: storage: 5Gi storageClassName: unity-iscsi apiVersion: v1 kind: Pod metadata: name: rawblockpod namespace: default spec: containers: - name: task-pv-container image: nginx ports: - containerPort: 80 name: \"http-server\" volumeDevices: - devicePath: /usr/share/nginx/html/device name: nov-eleventh-1-pv-storage volumes: - name: nov-eleventh-1-pv-storage persistentVolumeClaim: claimName: rawblockpvc Access modes allowed are ReadWriteOnce and ReadWriteMany. Raw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device.\nRaw Block volumes support online Volume Expansion, but it is up to the application to manage and reconfigure the file system (if any) to the new size. Access mode ReadOnlyMany is not supported with raw block since we cannot restrict volumes to be readonly from Unity XT.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity XT driver supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: vol0 namespace: unity spec: storageClassName: unity-nfs accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: cloned-pvc namespace: unity spec: storageClassName: unity-nfs dataSource: name: vol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Ephemeral Inline Volume The CSI Unity XT driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity XT driver.\nkind: Pod apiVersion: v1 metadata: name: test-unity-ephemeral-volume spec: containers: - name: test-container image: busybox command: [ \"sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data\" name: volume volumes: - name: volume csi: driver: csi-unity.dellemc.com fsType: \"ext4\" volumeAttributes: size: \"10Gi\" arrayId: APM************ protocol: iSCSI thinProvisioned: \"true\" isDataReductionEnabled: \"false\" tieringPolicy: \"1\" storagePool: pool_2 This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes: - name: volume csi: driver: csi-unity.dellemc.com csi.storage.k8s.io/fstype: \"nfs\" volumeAttributes: size: \"20Gi\" nasName: \"csi-nas-name\" Controller HA The CSI Unity XT driver supports controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default, the number of replicas is set to 2. You can set the controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator, you can change the replicas parameter in the spec.driver section in your Unity XT Custom Resource.\nWhen multiple replicas of controller pods are in a cluster each sidecar (Attacher, Provisioner, Resizer, and Snapshotter) tries to get a lease so only one instance of each sidecar is active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameters controller: # \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment nodeSelector: # \"controller.tolerations\" defines tolerations that would be applied to controller deployment tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameters controller: # \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" As said before you can configure where node driver pods would be assigned in a similar way in the node section of myvalues.yaml\nTopology The CSI Unity XT driver supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage User can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: unity-topology-fc provisioner: csi-unity.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-unity.dellemc.com/\u003carray_id\u003e-fc values: - \"true\" This example matches all nodes where the driver has a connection to the Unity XT array with array ID mentioned via Fiber Channel. Similarly, by replacing fc with iscsi in the key checks for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\nNote that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work properly.\nFor any additional information about the topology, see the Kubernetes Topology documentation.\nVolume Limit The CSI Driver for Dell Unity XT allows users to specify the maximum number of Unity XT volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-unity-volumes-per-node and specifying the volume limit for that node. kubectl label node \u003cnode_name\u003e max-unity-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxUnityVolumesPerNode attribute in values.yaml file.\nNOTE: To reflect the changes after setting the value either via node label or in values.yaml file, user has to bounce the driver controller and node pods using the command kubectl get pods -n unity --no-headers=true | awk '/unity-/{print $1}'| xargs kubectl delete -n unity pod. If the value is set both by node label and values.yaml file then node label value will get the precedence and user has to remove the node label in order to reflect the values.yaml value. The default value of maxUnityVolumesPerNode is 0. If maxUnityVolumesPerNode is set to zero, then Container Orchestration decides how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxUnityVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-unity-volumes-per-node is not set.\nNAT Support CSI Driver for Dell Unity XT is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nSingle Pod Access Mode for PersistentVolumes CSI Driver for Unity XT supports a new accessmode ReadWriteOncePod for PersistentVolumes and PersistentVolumeClaims. With this feature, CSI Driver for Unity XT restricts volume access to a single pod in the cluster\nPrerequisites\nEnable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet as the ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is only supported for CSI volumes. You can enable the feature by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\" Create a PVC with access mode set to ReadWriteOncePod like shown in the sample below kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-writer-only spec: accessModes: - ReadWriteOncePod # Allow only a single pod to access single-writer-only. resources: requests: storage: 1Gi Volume Health Monitoring CSI Driver for Unity XT supports volume health monitoring. This is an alpha feature and requires feature gate to be enabled by setting command line arguments\n--feature-gates=\"...,CSIVolumeHealth=true\" This feature:\nReports on the condition of the underlying volumes via events when a volume condition is abnormal. We can watch the events on the describe of pvc kubectl describe pvc \u003cpvc name\u003e -n \u003cnamespace\u003e Collects the volume stats. We can see the volume usage in the node logs kubectl logs \u003cnodepod\u003e -n \u003cnamespacename\u003e -c driver By default this is disabled in CSI Driver for Unity XT. You will have to set the healthMonitor.enable flag for controller, node or for both in values.yaml to get the volume stats and volume condition.\nDynamic Logging Configuration Helm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params Tenancy support for Unity XT NFS The CSI Unity XT driver supports the Tenancy feature of Unity XT that allows the user to associate specific worker nodes (in the cluster) and NFS storage volumes with Tenant.\nPrerequisites (to be manually created in Unity XT Array) before the driver installation:\nCreate Tenants Create Pools Create NAS Servers with Tenant and Pool mapping The following example describes the usage of Tenant in the NFS pod creation:\nInstall the csi driver using myvalues.yaml with the TenantName as follows: Example myvalues.yaml\nlogLevel: \"info\" certSecretCount: 1 kubeletConfigDir: /var/lib/kubelet controller: controllerCount: 2 volumeNamePrefix : csivol snapshot: snapNamePrefix: csi-snap tenantName: \"tenant3\" Create storageclass with NAS-Server and the Storage-Pool associated with TenantName as follows: Example storageclass.yaml\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: storageclass.kubernetes.io/is-default-class: \"false\" name: unity-nfs parameters: arrayId: \"APM0***XXXXXX\" hostIoSize: \"16384\" isDataReductionEnabled: \"false\" storagePool: pool_7 thinProvisioned: \"true\" tieringPolicy: \"0\" protocol: \"NFS\" nasServer: \"nas_5\" provisioner: csi-unity.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true Create the pod and pvc as follows: Example pvc.yaml\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvcname namespace: nginx spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 2Gi storageClassName: unity-nfs Example pod.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: podname namespace: nginx spec: replicas: 1 selector: matchLabels: app: podname template: metadata: labels: app: podname spec: containers: - args: - \"-c\" - \"while true; do dd if=/dev/urandom of=/data0/foo bs=1M count=1;done\" command: - /bin/bash image: \"docker.io/centos:latest\" name: test volumeMounts: - mountPath: /data0 name: pvcname volumes: - name: pvolx0 persistentVolumeClaim: claimName: pvcname With the usage shown in the example, the user will be able to create an NFS pod with PVC using the NAS and the Pool associated with the added Tenants specified in SC.\nNote: Current feature supports ONLY single Tenant for all the nodes in the cluster. Users may expect an error if PVC is created from the NAS server whose pool is mapped to the different tenants not associated with this SC.\nFor operator based installation, mention the TENANT_NAME in configmap as shown in the following example: Example configmap.yaml\napiVersion: v1 kind: ConfigMap metadata: name: unity-config-params namespace: unity data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"info\" ALLOW_RWO_MULTIPOD_ACCESS: \"false\" MAX_UNITY_VOLUMES_PER_NODE: \"0\" SYNC_NODE_INFO_TIME_INTERVAL: \"15\" TENANT_NAME: \"\" Note: csi-unity supports Tenancy in multi-array setup, provided the TenantName is the same across Unity XT instances.\n","categories":"","description":"Code features for Unity XT Driver","excerpt":"Code features for Unity XT Driver","ref":"/csm-docs/v2/csidriver/features/unity/","tags":"","title":"Unity XT"},{"body":"You can upgrade the CSI Driver for Dell Unity XT using Helm or Dell CSI Operator.\nNote:\nUser has to re-create existing custom-storage classes (if any) according to the latest format. User has to create Volumesnapshotclass after upgrade for taking Snapshots. Secret.yaml files can be updated according to Multiarray normalization parameters only after upgrading the driver. Using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nPreparing myvalues.yaml is the same as explained in the install section.\nTo upgrade the driver from csi-unity v2.6.0 to csi-unity v2.7.0\nGet the latest csi-unity v2.7.0 code from Github using git clone -b v2.7.0 https://github.com/dell/csi-unity.git. Copy the helm/csi-unity/values.yaml to the new location csi-unity/dell-csi-helm-installer and rename it to myvalues.yaml. Customize settings for installation by editing myvalues.yaml as needed. Navigate to csi-unity/dell-csi-hem-installer folder and execute this command: ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Using Operator Notes:\nWhile upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes. Upgrading the Operator does not upgrade the CSI Driver. To upgrade the driver:\nPlease upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here. ","categories":"","description":"Upgrade Unity XT CSI driver","excerpt":"Upgrade Unity XT CSI driver","ref":"/csm-docs/v2/csidriver/upgradation/drivers/unity/","tags":["upgrade","csi-driver"],"title":"Unity XT"},{"body":"Creating volumes and consuming them Create a file sample.yaml using sample yaml files located at test/sample.yaml\nThe following command creates a statefulset that consumes three volumes of default storage classes:\nkubectl create -f test/sample.yaml After executing this command 3 PVC and statefulset are created in the test-unity namespace. You can check created PVCs by running kubectl get pvc -n test-unity and check statefulset’s pods by running kubectl get pods -n test-unity command. The pod should be Ready and Running.\nIf Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\nDeleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f test/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity XT array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\nOpen your volume in Unity XT Management UI (Unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. apiVersion: v1 kind: PersistentVolume metadata: name: static-1 annotations: pv.kubernetes.io/provisioned-by: csi-unity.dellemc.com spec: accessModes: - ReadWriteOnce capacity: storage: 5Gi csi: driver: csi-unity.dellemc.com volumeHandle: existingvol-\u003cprotocol\u003e-\u003carray_id\u003e-\u003cvolume-id\u003e persistentVolumeReclaimPolicy: Retain claimRef: namespace: default name: static-pvc1 storageClassName: unity volumeMode: Filesystem Create PersistentVolumeClaim to use this PersistentVolume. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: static-pvc1 spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi volumeName: static-1 storageClassName: unity Then use this PVC as a volume in a pod. apiVersion: v1 kind: Pod metadata: name: static-prov-pod spec: containers: - name: test image: docker.io/centos:latest command: [ \"/bin/sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data0\" name: pvol volumes: - name: pvol persistentVolumeClaim: claimName: static-pvc1 After the pod becomes Ready and Running, you can start to use this pod and volume. Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class Following is the manifest to create Volume Snapshot Class :\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: unity-snapclass driver: csi-unity.dellemc.com deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap namespace: test-unity spec: volumeSnapshotClassName: unity-snapclass source: persistentVolumeClaimName: pvol Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-xxxxxxxxxxxxx creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Note : A set of annotated volume snapshot class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: test-unity spec: storageClassName: unity-iscsi dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Volume Expansion The CSI Unity XT driver supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: unity-expand-sc annotations: storageclass.kubernetes.io/is-default-class: false provisioner: csi-unity.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true # Set this attribute to true if you plan to expand any PVCs created using this storage class parameters: csi.storage.k8s.io/fstype: \"xfs\" To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: unity-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 30Gi # Updated size from 3Gi to 30Gi storageClassName: unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\nRaw block support The CSI Unity XT driver supports Raw Block Volumes. Raw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. The following is an example configuration:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: rawblockpvc namespace: default spec: accessModes: - ReadWriteOnce volumeMode: Block resources: requests: storage: 5Gi storageClassName: unity-iscsi apiVersion: v1 kind: Pod metadata: name: rawblockpod namespace: default spec: containers: - name: task-pv-container image: nginx ports: - containerPort: 80 name: \"http-server\" volumeDevices: - devicePath: /usr/share/nginx/html/device name: nov-eleventh-1-pv-storage volumes: - name: nov-eleventh-1-pv-storage persistentVolumeClaim: claimName: rawblockpvc Access modes allowed are ReadWriteOnce and ReadWriteMany. Raw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device.\nRaw Block volumes support online Volume Expansion, but it is up to the application to manage and reconfigure the file system (if any) to the new size. Access mode ReadOnlyMany is not supported with raw block since we cannot restrict volumes to be readonly from Unity XT.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity XT driver supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: vol0 namespace: test-unity spec: storageClassName: unity-nfs accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: cloned-pvc namespace: test-unity spec: storageClassName: unity-nfs dataSource: name: vol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Ephemeral Inline Volume The CSI Unity XT driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity XT driver.\nkind: Pod apiVersion: v1 metadata: name: test-unity-ephemeral-volume spec: containers: - name: test-container image: busybox command: [ \"sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data\" name: volume volumes: - name: volume csi: driver: csi-unity.dellemc.com fsType: \"ext4\" volumeAttributes: size: \"10Gi\" arrayId: APM************ protocol: iSCSI thinProvisioned: \"true\" isDataReductionEnabled: \"false\" tieringPolicy: \"1\" storagePool: pool_2 This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes: - name: volume csi: driver: csi-unity.dellemc.com csi.storage.k8s.io/fstype: \"nfs\" volumeAttributes: size: \"20Gi\" nasName: \"csi-nas-name\" Controller HA The CSI Unity XT driver supports controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default, the number of replicas is set to 2. You can set the controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator, you can change the replicas parameter in the spec.driver section in your Unity XT Custom Resource.\nWhen multiple replicas of controller pods are in a cluster each sidecar (Attacher, Provisioner, Resizer, and Snapshotter) tries to get a lease so only one instance of each sidecar is active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameters controller: # \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment nodeSelector: # \"controller.tolerations\" defines tolerations that would be applied to controller deployment tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameters controller: # \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" As said before you can configure where node driver pods would be assigned in a similar way in the node section of myvalues.yaml\nTopology The CSI Unity XT driver supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage User can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: unity-topology-fc provisioner: csi-unity.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-unity.dellemc.com/\u003carray_id\u003e-fc values: - \"true\" This example matches all nodes where the driver has a connection to the Unity XT array with array ID mentioned via Fiber Channel. Similarly, by replacing fc with iscsi in the key checks for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\nNote that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work properly.\nFor any additional information about the topology, see the Kubernetes Topology documentation.\nVolume Limit The CSI Driver for Dell Unity XT allows users to specify the maximum number of Unity XT volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-unity-volumes-per-node and specifying the volume limit for that node. kubectl label node \u003cnode_name\u003e max-unity-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxUnityVolumesPerNode attribute in values.yaml file.\nNOTE: To reflect the changes after setting the value either via node label or in values.yaml file, user has to bounce the driver controller and node pods using the command kubectl get pods -n unity --no-headers=true | awk '/unity-/{print $1}'| xargs kubectl delete -n unity pod. If the value is set both by node label and values.yaml file then node label value will get the precedence and user has to remove the node label in order to reflect the values.yaml value. The default value of maxUnityVolumesPerNode is 0. If maxUnityVolumesPerNode is set to zero, then Container Orchestration decides how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxUnityVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-unity-volumes-per-node is not set.\nNAT Support CSI Driver for Dell Unity XT is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nSingle Pod Access Mode for PersistentVolumes CSI Driver for Unity XT supports a new accessmode ReadWriteOncePod for PersistentVolumes and PersistentVolumeClaims. With this feature, CSI Driver for Unity XT restricts volume access to a single pod in the cluster\nPrerequisites\nEnable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet as the ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is only supported for CSI volumes. You can enable the feature by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\" Create a PVC with access mode set to ReadWriteOncePod like shown in the sample below kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-writer-only spec: accessModes: - ReadWriteOncePod # Allow only a single pod to access single-writer-only. resources: requests: storage: 1Gi Volume Health Monitoring CSI Driver for Unity XT supports volume health monitoring. This is an alpha feature and requires feature gate to be enabled by setting command line arguments --feature-gates=\"...,CSIVolumeHealth=true\".\nThis feature:\nReports on the condition of the underlying volumes via events when a volume condition is abnormal. We can watch the events on the describe of pvc kubectl describe pvc \u003cpvc name\u003e -n \u003cnamespace\u003e Collects the volume stats. We can see the volume usage in the node logs kubectl logs \u003cnodepod\u003e -n \u003cnamespacename\u003e -c driver By default this is disabled in CSI Driver for Unity XT. You will have to set the healthMonitor.enable flag for controller, node or for both in values.yaml to get the volume stats and volume condition. Dynamic Logging Configuration Helm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params Tenancy support for Unity XT NFS The CSI Unity XT driver supports the Tenancy feature of Unity XT that allows the user to associate specific worker nodes (in the cluster) and NFS storage volumes with Tenant.\nPrerequisites (to be manually created in Unity XT Array) before the driver installation:\nCreate Tenants Create Pools Create NAS Servers with Tenant and Pool mapping The following example describes the usage of Tenant in the NFS pod creation:\nInstall the csi driver using myvalues.yaml with the TenantName as follows: Example myvalues.yaml\nlogLevel: \"info\" certSecretCount: 1 kubeletConfigDir: /var/lib/kubelet controller: controllerCount: 2 volumeNamePrefix : csivol snapshot: snapNamePrefix: csi-snap tenantName: \"tenant3\" Create storageclass with NAS-Server and the Storage-Pool associated with TenantName as follows: Example storageclass.yaml\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: storageclass.kubernetes.io/is-default-class: \"false\" name: unity-nfs parameters: arrayId: \"APM0***XXXXXX\" hostIoSize: \"16384\" isDataReductionEnabled: \"false\" storagePool: pool_7 thinProvisioned: \"true\" tieringPolicy: \"0\" protocol: \"NFS\" nasServer: \"nas_5\" provisioner: csi-unity.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true Create the pod and pvc as follows: Example pvc.yaml\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvcname namespace: nginx spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 2Gi storageClassName: unity-nfs Example pod.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: podname namespace: nginx spec: replicas: 1 selector: matchLabels: app: podname template: metadata: labels: app: podname spec: containers: - args: - \"-c\" - \"while true; do dd if=/dev/urandom of=/data0/foo bs=1M count=1;done\" command: - /bin/bash image: \"docker.io/centos:latest\" name: test volumeMounts: - mountPath: /data0 name: pvcname volumes: - name: pvolx0 persistentVolumeClaim: claimName: pvcname With the usage shown in the example, the user will be able to create an NFS pod with PVC using the NAS and the Pool associated with the added Tenants specified in SC.\nNote: Current feature supports ONLY single Tenant for all the nodes in the cluster. Users may expect an error if PVC is created from the NAS server whose pool is mapped to the different tenants not associated with this SC.\nFor operator based installation, mention the TENANT_NAME in configmap as shown in the following example: Example configmap.yaml\napiVersion: v1 kind: ConfigMap metadata: name: unity-config-params namespace: test-unity data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"info\" ALLOW_RWO_MULTIPOD_ACCESS: \"false\" MAX_UNITY_VOLUMES_PER_NODE: \"0\" SYNC_NODE_INFO_TIME_INTERVAL: \"15\" TENANT_NAME: \"\" Note: csi-unity supports Tenancy in multi-array setup, provided the TenantName is the same across Unity XT instances.\n","categories":"","description":"Code features for Unity XT Driver","excerpt":"Code features for Unity XT Driver","ref":"/csm-docs/v3/csidriver/features/unity/","tags":"","title":"Unity XT"},{"body":"You can upgrade the CSI Driver for Dell Unity XT using Helm or Dell CSI Operator.\nNote:\nUser has to re-create existing custom-storage classes (if any) according to the latest format. User has to create Volumesnapshotclass after upgrade for taking Snapshots. Secret.yaml files can be updated according to Multiarray normalization parameters only after upgrading the driver. Using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nPreparing myvalues.yaml is the same as explained in the install section.\nTo upgrade the driver from csi-unity v2.5.0 to csi-unity v2.6.0\nGet the latest csi-unity v2.6.0 code from Github using git clone -b v2.6.0 https://github.com/dell/csi-unity.git. Copy the helm/csi-unity/values.yaml to the new location csi-unity/dell-csi-helm-installer and rename it to myvalues.yaml. Customize settings for installation by editing myvalues.yaml as needed. Navigate to csi-unity/dell-csi-hem-installer folder and execute this command: ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Using Operator Notes:\nWhile upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes. Upgrading the Operator does not upgrade the CSI Driver. To upgrade the driver:\nPlease upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here. ","categories":"","description":"Upgrade Unity XT CSI driver","excerpt":"Upgrade Unity XT CSI driver","ref":"/csm-docs/v3/csidriver/upgradation/drivers/unity/","tags":["upgrade","csi-driver"],"title":"Unity XT"},{"body":"Replication design and architecture Container Storage Modules (CSM) for Replication project consists of the following components:\nDellCSIReplicationGroup, a Kubernetes Custom Resource. CSM Replication controller which replicates the resources across (or within) Kubernetes clusters. CSM Replication sidecar container which is part of each CSI driver controller pod. repctl - Multi cluster Kubernetes client for managing replication related objects. DellCSIReplicationGroup DellCSIReplicationGroup (RG) is a cluster scoped Custom Resource that represents a protection group on the backend storage array. It is used to group volumes with the same replication related properties together. DellCSIReplicationGroup’s spec contains an action field which can be used to perform replication related operations on the backing protection groups on the storage arrays. This includes operations like Failover, Reprotect, Suspend, Synchronize, etc. Any replication related operation is always carried out on all the volumes present in the group.\nSpecification kind: DellCSIReplicationGroup apiVersion: replication.storage.dell.com/v1 metadata: name: rg-e6be24c0-145d-4b62-8674-639282ebdd13 spec: driverName: driver.dellemc.com # Name of the CSI driver (same as provisioner name in StorageClass) action: \"\" # Name of the replication action to be performed on the protection group protectionGroupAttributes: localAttributeKey: value protectionGroupId: protection-group-id # Identifier of the backing protection group on the Storage Array remoteClusterId: tgtClusterID # A unique identifier for the remote Kubernetes Cluster remoteProtectionGroupAttributes: remoteAttributeKey: value remoteProtectionGroupId: csi-rep-sg-test-5-ASYNC # Identifier for the protection group on the remote Storage Array Status The status sub resource of DellCSIReplicationGroup contains information about the state of replication \u0026 any actions which have been performed on the object.\nField Description state State of the Custom Resource replicationLinkState State of the replication on the storage arrays lastAction Result of the last performed action conditions List of recent conditions the CR instance has gone through status: conditions: - condition: Action REPROTECT_REMOTE succeeded time: \"2021-08-11T12:22:05Z\" - condition: Replication Link State:IsSource changed from (true) to (false) time: \"2021-08-11T12:18:50Z\" - condition: Action FAILOVER_REMOTE succeeded time: \"2021-08-11T12:18:50Z\" lastAction: condition: Action REPROTECT_REMOTE succeeded time: \"2021-08-11T12:22:05Z\" replicationLinkState: isSource: false lastSuccessfulUpdate: \"2021-08-11T17:18:12Z\" state: Synchronized state: Ready Here is a diagram representing how the state of the CustomResource changes based on actions: CSM Replication Sidecar CSM Replication sidecar is deployed as sidecar container in each CSI driver’s controller pod. This container is similar to Kubernetes CSI Sidecar containers and runs a Controller Manager which manages the following controllers:\nPersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller The PV \u0026 PVC controllers watch for PV/PVC creation events and use dell-csi-extensions APIs to communicate with the CSI Driver controller plugin to discover/create replication enabled volumes and protection groups on the backend storage array. The PersistentVolume controller then uses these details to create DellCSIReplicationGroup objects in the cluster. These controllers are also responsible for associating the PV \u0026 PVC objects with DellCSIReplicationGroup objects. This association is established by applying annotations \u0026 labels on the PV \u0026 PVC objects.\nThe RG controller manages DellCSIReplicationGroup instances and processes any change requests. It is primarily responsible for the following:\nPerform actions on the protection groups Monitor status of replication Updates to the status sub resource CSM Replication Controller CSM Replication Controller is a Kubernetes application deployed independently of CSI drivers and is responsible for the communication between Kubernetes clusters. One CSM Replication Controller manages replication operations for all CSI driver installations on the Kubernetes cluster.\nThe details about the clusters it needs to connect to are provided in the form of a ConfigMap with references to secrets containing the details(KubeConfig/ServiceAccount tokens) required to connect to the respective clusters.\nIt consists of Controller Manager which manages the following controllers:\nPersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller The PV controller is responsible for creating PV objects (representing the replicated volumes on the backend storage array) in the remote Kubernetes cluster. This controller also enables deletion of the remote PV object, if enabled through the storage class’ RemotePVRetentionPolicy, by propagating the deletion request across clusters.\nSimilarly, the RG controller is responsible for creating RG objects in the remote Kubernetes cluster. These RG objects represent the remote protection groups on the backend storage array. This controller can also propagate the deletion request of RG objects across clusters, if enabled through the storage class’ RemoteRGRetentionPolicy.\nBoth the PV \u0026 RG objects in the remote cluster have extra metadata associated with them in form of annotations \u0026 labels. This metadata includes information about the respective objects in the source cluster.\nThe PVC objects are never replicated across the clusters. Instead, the remote PV objects have annotations related to the source PVC objects. This information can be easily used to create the PVCs whenever required using repctl or kubectl.\nSupported Cluster Topologies Click here for details for the various types of supported cluster topologies\n","categories":"","description":"High level architecture for CSM for Replication\n","excerpt":"High level architecture for CSM for Replication\n","ref":"/csm-docs/docs/replication/architecture/","tags":"","title":"Architecture"},{"body":"Replication design and architecture Container Storage Modules (CSM) for Replication project consists of the following components:\nDellCSIReplicationGroup, a Kubernetes Custom Resource. CSM Replication controller which replicates the resources across (or within) Kubernetes clusters. CSM Replication sidecar container which is part of each CSI driver controller pod. repctl - Multi cluster Kubernetes client for managing replication related objects. DellCSIReplicationGroup DellCSIReplicationGroup (RG) is a cluster scoped Custom Resource that represents a protection group on the backend storage array. It is used to group volumes with the same replication related properties together. DellCSIReplicationGroup’s spec contains an action field which can be used to perform replication related operations on the backing protection groups on the storage arrays. This includes operations like Failover, Reprotect, Suspend, Synchronize, etc. Any replication related operation is always carried out on all the volumes present in the group.\nSpecification kind: DellCSIReplicationGroup apiVersion: replication.storage.dell.com/v1 metadata: name: rg-e6be24c0-145d-4b62-8674-639282ebdd13 spec: driverName: driver.dellemc.com # Name of the CSI driver (same as provisioner name in StorageClass) action: \"\" # Name of the replication action to be performed on the protection group protectionGroupAttributes: localAttributeKey: value protectionGroupId: protection-group-id # Identifier of the backing protection group on the Storage Array remoteClusterId: tgtClusterID # A unique identifier for the remote Kubernetes Cluster remoteProtectionGroupAttributes: remoteAttributeKey: value remoteProtectionGroupId: csi-rep-sg-test-5-ASYNC # Identifier for the protection group on the remote Storage Array Status The status sub resource of DellCSIReplicationGroup contains information about the state of replication \u0026 any actions which have been performed on the object.\nField Description state State of the Custom Resource replicationLinkState State of the replication on the storage arrays lastAction Result of the last performed action conditions List of recent conditions the CR instance has gone through status: conditions: - condition: Action REPROTECT_REMOTE succeeded time: \"2021-08-11T12:22:05Z\" - condition: Replication Link State:IsSource changed from (true) to (false) time: \"2021-08-11T12:18:50Z\" - condition: Action FAILOVER_REMOTE succeeded time: \"2021-08-11T12:18:50Z\" lastAction: condition: Action REPROTECT_REMOTE succeeded time: \"2021-08-11T12:22:05Z\" replicationLinkState: isSource: false lastSuccessfulUpdate: \"2021-08-11T17:18:12Z\" state: Synchronized state: Ready Here is a diagram representing how the state of the CustomResource changes based on actions: CSM Replication Sidecar CSM Replication sidecar is deployed as sidecar container in each CSI driver’s controller pod. This container is similar to Kubernetes CSI Sidecar containers and runs a Controller Manager which manages the following controllers:\nPersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller The PV \u0026 PVC controllers watch for PV/PVC creation events and use dell-csi-extensions APIs to communicate with the CSI Driver controller plugin to discover/create replication enabled volumes and protection groups on the backend storage array. The PersistentVolume controller then uses these details to create DellCSIReplicationGroup objects in the cluster. These controllers are also responsible for associating the PV \u0026 PVC objects with DellCSIReplicationGroup objects. This association is established by applying annotations \u0026 labels on the PV \u0026 PVC objects.\nThe RG controller manages DellCSIReplicationGroup instances and processes any change requests. It is primarily responsible for the following:\nPerform actions on the protection groups Monitor status of replication Updates to the status sub resource CSM Replication Controller CSM Replication Controller is a Kubernetes application deployed independently of CSI drivers and is responsible for the communication between Kubernetes clusters. One CSM Replication Controller manages replication operations for all CSI driver installations on the Kubernetes cluster.\nThe details about the clusters it needs to connect to are provided in the form of a ConfigMap with references to secrets containing the details(KubeConfig/ServiceAccount tokens) required to connect to the respective clusters.\nIt consists of Controller Manager which manages the following controllers:\nPersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller The PV controller is responsible for creating PV objects (representing the replicated volumes on the backend storage array) in the remote Kubernetes cluster. This controller also enables deletion of the remote PV object, if enabled through the storage class’ RemotePVRetentionPolicy, by propagating the deletion request across clusters.\nSimilarly, the RG controller is responsible for creating RG objects in the remote Kubernetes cluster. These RG objects represent the remote protection groups on the backend storage array. This controller can also propagate the deletion request of RG objects across clusters, if enabled through the storage class’ RemoteRGRetentionPolicy.\nBoth the PV \u0026 RG objects in the remote cluster have extra metadata associated with them in form of annotations \u0026 labels. This metadata includes information about the respective objects in the source cluster.\nThe PVC objects are never replicated across the clusters. Instead, the remote PV objects have annotations related to the source PVC objects. This information can be easily used to create the PVCs whenever required using repctl or kubectl.\nSupported Cluster Topologies Click here for details for the various types of supported cluster topologies\n","categories":"","description":"High level architecture for CSM for Replication\n","excerpt":"High level architecture for CSM for Replication\n","ref":"/csm-docs/v1/replication/architecture/","tags":"","title":"Architecture"},{"body":"Replication design and architecture Container Storage Modules (CSM) for Replication project consists of the following components:\nDellCSIReplicationGroup, a Kubernetes Custom Resource. CSM Replication controller which replicates the resources across (or within) Kubernetes clusters. CSM Replication sidecar container which is part of each CSI driver controller pod. repctl - Multi cluster Kubernetes client for managing replication related objects. DellCSIReplicationGroup DellCSIReplicationGroup (RG) is a cluster scoped Custom Resource that represents a protection group on the backend storage array. It is used to group volumes with the same replication related properties together. DellCSIReplicationGroup’s spec contains an action field which can be used to perform replication related operations on the backing protection groups on the storage arrays. This includes operations like Failover, Reprotect, Suspend, Synchronize, etc. Any replication related operation is always carried out on all the volumes present in the group.\nSpecification kind: DellCSIReplicationGroup apiVersion: replication.storage.dell.com/v1 metadata: name: rg-e6be24c0-145d-4b62-8674-639282ebdd13 spec: driverName: driver.dellemc.com # Name of the CSI driver (same as provisioner name in StorageClass) action: \"\" # Name of the replication action to be performed on the protection group protectionGroupAttributes: localAttributeKey: value protectionGroupId: protection-group-id # Identifier of the backing protection group on the Storage Array remoteClusterId: tgtClusterID # A unique identifier for the remote Kubernetes Cluster remoteProtectionGroupAttributes: remoteAttributeKey: value remoteProtectionGroupId: csi-rep-sg-test-5-ASYNC # Identifier for the protection group on the remote Storage Array Status The status sub resource of DellCSIReplicationGroup contains information about the state of replication \u0026 any actions which have been performed on the object.\nField Description state State of the Custom Resource replicationLinkState State of the replication on the storage arrays lastAction Result of the last performed action conditions List of recent conditions the CR instance has gone through status: conditions: - condition: Action REPROTECT_REMOTE succeeded time: \"2021-08-11T12:22:05Z\" - condition: Replication Link State:IsSource changed from (true) to (false) time: \"2021-08-11T12:18:50Z\" - condition: Action FAILOVER_REMOTE succeeded time: \"2021-08-11T12:18:50Z\" lastAction: condition: Action REPROTECT_REMOTE succeeded time: \"2021-08-11T12:22:05Z\" replicationLinkState: isSource: false lastSuccessfulUpdate: \"2021-08-11T17:18:12Z\" state: Synchronized state: Ready Here is a diagram representing how the state of the CustomResource changes based on actions: CSM Replication Sidecar CSM Replication sidecar is deployed as sidecar container in each CSI driver’s controller pod. This container is similar to Kubernetes CSI Sidecar containers and runs a Controller Manager which manages the following controllers:\nPersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller The PV \u0026 PVC controllers watch for PV/PVC creation events and use dell-csi-extensions APIs to communicate with the CSI Driver controller plugin to discover/create replication enabled volumes and protection groups on the backend storage array. The PersistentVolume controller then uses these details to create DellCSIReplicationGroup objects in the cluster. These controllers are also responsible for associating the PV \u0026 PVC objects with DellCSIReplicationGroup objects. This association is established by applying annotations \u0026 labels on the PV \u0026 PVC objects.\nThe RG controller manages DellCSIReplicationGroup instances and processes any change requests. It is primarily responsible for the following:\nPerform actions on the protection groups Monitor status of replication Updates to the status sub resource CSM Replication Controller CSM Replication Controller is a Kubernetes application deployed independently of CSI drivers and is responsible for the communication between Kubernetes clusters. One CSM Replication Controller manages replication operations for all CSI driver installations on the Kubernetes cluster.\nThe details about the clusters it needs to connect to are provided in the form of a ConfigMap with references to secrets containing the details(KubeConfig/ServiceAccount tokens) required to connect to the respective clusters.\nIt consists of Controller Manager which manages the following controllers:\nPersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller The PV controller is responsible for creating PV objects (representing the replicated volumes on the backend storage array) in the remote Kubernetes cluster. This controller also enables deletion of the remote PV object, if enabled through the storage class’ RemotePVRetentionPolicy, by propagating the deletion request across clusters.\nSimilarly, the RG controller is responsible for creating RG objects in the remote Kubernetes cluster. These RG objects represent the remote protection groups on the backend storage array. This controller can also propagate the deletion request of RG objects across clusters, if enabled through the storage class’ RemoteRGRetentionPolicy.\nBoth the PV \u0026 RG objects in the remote cluster have extra metadata associated with them in form of annotations \u0026 labels. This metadata includes information about the respective objects in the source cluster.\nThe PVC objects are never replicated across the clusters. Instead, the remote PV objects have annotations related to the source PVC objects. This information can be easily used to create the PVCs whenever required using repctl or kubectl.\nSupported Cluster Topologies Click here for details for the various types of supported cluster topologies\n","categories":"","description":"High level architecture for CSM for Replication\n","excerpt":"High level architecture for CSM for Replication\n","ref":"/csm-docs/v2/replication/architecture/","tags":"","title":"Architecture"},{"body":"Replication design and architecture Container Storage Modules (CSM) for Replication project consists of the following components:\nDellCSIReplicationGroup, a Kubernetes Custom Resource. CSM Replication controller which replicates the resources across (or within) Kubernetes clusters. CSM Replication sidecar container which is part of each CSI driver controller pod. repctl - Multi cluster Kubernetes client for managing replication related objects. DellCSIReplicationGroup DellCSIReplicationGroup (RG) is a cluster scoped Custom Resource that represents a protection group on the backend storage array. It is used to group volumes with the same replication related properties together. DellCSIReplicationGroup’s spec contains an action field which can be used to perform replication related operations on the backing protection groups on the storage arrays. This includes operations like Failover, Reprotect, Suspend, Synchronize, etc. Any replication related operation is always carried out on all the volumes present in the group.\nSpecification kind: DellCSIReplicationGroup apiVersion: replication.storage.dell.com/v1 metadata: name: rg-e6be24c0-145d-4b62-8674-639282ebdd13 spec: driverName: driver.dellemc.com # Name of the CSI driver (same as provisioner name in StorageClass) action: \"\" # Name of the replication action to be performed on the protection group protectionGroupAttributes: localAttributeKey: value protectionGroupId: protection-group-id # Identifier of the backing protection group on the Storage Array remoteClusterId: tgtClusterID # A unique identifier for the remote Kubernetes Cluster remoteProtectionGroupAttributes: remoteAttributeKey: value remoteProtectionGroupId: csi-rep-sg-test-5-ASYNC # Identifier for the protection group on the remote Storage Array Status The status sub resource of DellCSIReplicationGroup contains information about the state of replication \u0026 any actions which have been performed on the object.\nField Description state State of the Custom Resource replicationLinkState State of the replication on the storage arrays lastAction Result of the last performed action conditions List of recent conditions the CR instance has gone through status: conditions: - condition: Action REPROTECT_REMOTE succeeded time: \"2021-08-11T12:22:05Z\" - condition: Replication Link State:IsSource changed from (true) to (false) time: \"2021-08-11T12:18:50Z\" - condition: Action FAILOVER_REMOTE succeeded time: \"2021-08-11T12:18:50Z\" lastAction: condition: Action REPROTECT_REMOTE succeeded time: \"2021-08-11T12:22:05Z\" replicationLinkState: isSource: false lastSuccessfulUpdate: \"2021-08-11T17:18:12Z\" state: Synchronized state: Ready Here is a diagram representing how the state of the CustomResource changes based on actions: CSM Replication Sidecar CSM Replication sidecar is deployed as sidecar container in each CSI driver’s controller pod. This container is similar to Kubernetes CSI Sidecar containers and runs a Controller Manager which manages the following controllers:\nPersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller The PV \u0026 PVC controllers watch for PV/PVC creation events and use dell-csi-extensions APIs to communicate with the CSI Driver controller plugin to discover/create replication enabled volumes and protection groups on the backend storage array. The PersistentVolume controller then uses these details to create DellCSIReplicationGroup objects in the cluster. These controllers are also responsible for associating the PV \u0026 PVC objects with DellCSIReplicationGroup objects. This association is established by applying annotations \u0026 labels on the PV \u0026 PVC objects.\nThe RG controller manages DellCSIReplicationGroup instances and processes any change requests. It is primarily responsible for the following:\nPerform actions on the protection groups Monitor status of replication Updates to the status sub resource CSM Replication Controller CSM Replication Controller is a Kubernetes application deployed independently of CSI drivers and is responsible for the communication between Kubernetes clusters. One CSM Replication Controller manages replication operations for all CSI driver installations on the Kubernetes cluster.\nThe details about the clusters it needs to connect to are provided in the form of a ConfigMap with references to secrets containing the details(KubeConfig/ServiceAccount tokens) required to connect to the respective clusters.\nIt consists of Controller Manager which manages the following controllers:\nPersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller The PV controller is responsible for creating PV objects (representing the replicated volumes on the backend storage array) in the remote Kubernetes cluster. This controller also enables deletion of the remote PV object, if enabled through the storage class’ RemotePVRetentionPolicy, by propagating the deletion request across clusters.\nSimilarly, the RG controller is responsible for creating RG objects in the remote Kubernetes cluster. These RG objects represent the remote protection groups on the backend storage array. This controller can also propagate the deletion request of RG objects across clusters, if enabled through the storage class’ RemoteRGRetentionPolicy.\nBoth the PV \u0026 RG objects in the remote cluster have extra metadata associated with them in form of annotations \u0026 labels. This metadata includes information about the respective objects in the source cluster.\nThe PVC objects are never replicated across the clusters. Instead, the remote PV objects have annotations related to the source PVC objects. This information can be easily used to create the PVCs whenever required using repctl or kubectl.\nSupported Cluster Topologies Click here for details for the various types of supported cluster topologies\n","categories":"","description":"High level architecture for CSM for Replication\n","excerpt":"High level architecture for CSM for Replication\n","ref":"/csm-docs/v3/replication/architecture/","tags":"","title":"Architecture"},{"body":"Backup and Restore information for CSM Authorization can be found in this section.\n","categories":"","description":"Methods to backup and restore CSM Authorization","excerpt":"Methods to backup and restore CSM Authorization","ref":"/csm-docs/docs/authorization/backup-and-restore/","tags":["backup","restore","csm-authorization"],"title":"Backup and Restore"},{"body":"Backup and Restore information for CSM Authorization can be found in this section.\n","categories":"","description":"Methods to backup and restore CSM Authorization","excerpt":"Methods to backup and restore CSM Authorization","ref":"/csm-docs/v1/authorization/backup-and-restore/","tags":["backup","restore","csm-authorization"],"title":"Backup and Restore"},{"body":"Backup and Restore information for CSM Authorization can be found in this section.\n","categories":"","description":"Methods to backup and restore CSM Authorization","excerpt":"Methods to backup and restore CSM Authorization","ref":"/csm-docs/v2/authorization/backup-and-restore/","tags":["backup","restore","csm-authorization"],"title":"Backup and Restore"},{"body":"Backup and Restore information for CSM Authorization can be found in this section.\n","categories":"","description":"Methods to backup and restore CSM Authorization","excerpt":"Methods to backup and restore CSM Authorization","ref":"/csm-docs/v3/authorization/backup-and-restore/","tags":["backup","restore","csm-authorization"],"title":"Backup and Restore"},{"body":"You can migrate existing pre-provisioned volumes to another storage class by using volume migration feature.\nCurrently two versions of migration are supported:\nTo replicated storage class from NON replicated one. To NON replicated storage class from replicated one. Prerequisites Original volume is from the one of currently supported CSI drivers (see Support Matrix) Migrated sidecar is installed alongside with the driver, you can enable it in your myvalues.yaml file migration: enabled: true Support Matrix Migration Type PowerMax PowerStore PowerScale PowerFlex Unity NON_REPL_TO_REPL Yes No No No No REPL_TO_NON_REPL Yes No No No No Basic Usage To trigger migration procedure, you need to patch existing PersistentVolume with migration annotation (by default migration.storage.dell.com/migrate-to) and in value of said annotation specify StorageClass name you want to migrate to.\nFor example, if we have PV named test-pv already provisioned and we want to migrate it to replicated storage class named powermax-replication we can run:\nkubectl patch pv test-pv -p '{\"metadata\": {\"annotations\":{\"migration.storage.dell.com/migrate-to\":\"powermax-replication\"}}}' Patching PV resource will trigger migration sidecar that will call VolumeMigrate call from the CSI driver. After migration is finished new PersistentVolume will be created in cluster with name of original PV plus -to-\u003csc-name\u003e appended to it.\nIn our example, we will see this when running kubectl get pv:\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE test-pv 1Gi RWO Retain Bound default/test-pvc powermax 5m test-pv-to-powermax-replication 1Gi RWO Retain Available powermax-replication 10s When Volume Migration is finished, source PV will be updated with an EVENT that denotes that this has taken place.\nNewly created PV (test-pv-to-powermax-replication in our example) is available for consumption via static provisioning by any PVC that requests it.\nNamespace Considerations For Replication Replication Groups in CSM Replication can be made namespaced, meaning that one SC will generate one Replication Group per namespace. This is also important when migrating volumes from/to replcation storage class.\n“When just setting one annotation migration.storage.dell.com/migrate-to migrated volume is assumed to be used in same namespace as original PV and it’s PVC. In the case of being migrated to replication enabled storage class will be inserted in namespaced Replication Group inside PVC namespace.”\nHowever, you can define in which namespace migrated volume must be used after migration by setting migration.storage.dell.com/namespace. You can use the same annotation in a scenario where you only have a statically provisioned PV, and you don’t have it bound to any PVC, and you want to migrate it to another storage class.\nNon Disruptive Migration You can migrate your PVs without disrupting workflows if you use StatefulSet with multiple replicas to deploy application.\nInstruction (you can also use repctl for convenience):\nFind every PV for your StatefulSet and patch it with migration.storage.dell.com/migrate-to annotation that points to new storage class:\nkubectl patch pv \u003cpv-name\u003e -p '{\"metadata\": {\"annotations\":{\"migration.storage.dell.com/migrate-to\":\"powermax-replication\"}}}' Ensure you have a copy of StatefulSet manifest somewhere ready, we will need it later. If you don’t have it, you can get it from cluster:\nkubectl get sts \u003csts-name\u003e -n \u003cns-name\u003e -o yaml \u003e sts-manifest.yaml To not disrupt any workflows, we will need to delete StatefulSet without deleting any pods, to do so you can use the --cascade flag:\nkubectl delete sts \u003csts-name\u003e -n \u003cns-name\u003e --cascade=orphan Change the StorageClass in your manifest of StatefulSet to point to a new storage class, then apply it to the cluster:\nkubectl apply -f sts-manifest.yaml Find a PVC and pod of one replica of StatefulSet delete PVCs first and Pod after it:\nkubectl delete pvc \u003cpvc-name\u003e -n \u003cns-name\u003e kubectl delete pod \u003cpod-name\u003e -n \u003cns-name\u003e Wait for new pod to be created by StatefulSet, it should create new PVC that will use migrated PV.\nRepeat step 5 until all replicas use new PVCs.\nUsing repctl You can use repctl CLI tool to help you simplify running migration specific commands.\nSingle PV In its most simple usage, repctl can do the same operations as kubectl, for example, migrating the single PV ’test-pv’ from our example will look like:\n./repctl migrate pv test-pv --to-sc powermax-replication repctl will go and patch the resource for you. You can also provide --wait flag for it to wait until migrated PV is created in the cluster. repctl also can set migration.storage.dell.com/namespace for you if you provide --target-ns flag.\nAside from just migrating single PVs repctl can migrate PVCs and StatefulSets.\nPVC repctl can find PV for any given PVC for you and patch it. This could be done with similar command to single PV migration:\n./repctl migrate pvc test-pvc --to-sc powermax-replication -n default Notice that we provide original namespace (default in our example) for this command because PVCs are namespaced resource and we need namespace to be able to find it.\nStatefulSet repctl can help you migrate entire StatefulSet by automating migration process.\nYou can use this command to do so:\n./repctl migrate sts test-sts --to-sc powermax-replication -n default By default, it will find every Pod, PVC and PV for provided StatefulSet and patch every PV with annotation.\nYou can also optionally provide --ndu flag, with this flag provided repctl will do steps provided in Non Disruptive Migration section automatically.\n","categories":"","description":"Support for Array Migration of Volumes between storage classes\n","excerpt":"Support for Array Migration of Volumes between storage classes\n","ref":"/csm-docs/docs/replication/migration/migrating-volumes-same-array/","tags":"","title":"Between Storage classes"},{"body":"You can migrate existing pre-provisioned volumes to another storage class by using volume migration feature.\nCurrently two versions of migration are supported:\nTo replicated storage class from NON replicated one. To NON replicated storage class from replicated one. Prerequisites Original volume is from the one of currently supported CSI drivers (see Support Matrix) Migrated sidecar is installed alongside with the driver, you can enable it in your myvalues.yaml file migration: enabled: true Support Matrix Migration Type PowerMax PowerStore PowerScale PowerFlex Unity NON_REPL_TO_REPL Yes No No No No REPL_TO_NON_REPL Yes No No No No Basic Usage To trigger migration procedure, you need to patch existing PersistentVolume with migration annotation (by default migration.storage.dell.com/migrate-to) and in value of said annotation specify StorageClass name you want to migrate to.\nFor example, if we have PV named test-pv already provisioned and we want to migrate it to replicated storage class named powermax-replication we can run:\nkubectl patch pv test-pv -p '{\"metadata\": {\"annotations\":{\"migration.storage.dell.com/migrate-to\":\"powermax-replication\"}}}' Patching PV resource will trigger migration sidecar that will call VolumeMigrate call from the CSI driver. After migration is finished new PersistentVolume will be created in cluster with name of original PV plus -to-\u003csc-name\u003e appended to it.\nIn our example, we will see this when running kubectl get pv:\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE test-pv 1Gi RWO Retain Bound default/test-pvc powermax 5m test-pv-to-powermax-replication 1Gi RWO Retain Available powermax-replication 10s When Volume Migration is finished, source PV will be updated with an EVENT that denotes that this has taken place.\nNewly created PV (test-pv-to-powermax-replication in our example) is available for consumption via static provisioning by any PVC that requests it.\nNamespace Considerations For Replication Replication Groups in CSM Replication can be made namespaced, meaning that one SC will generate one Replication Group per namespace. This is also important when migrating volumes from/to replcation storage class.\n“When just setting one annotation migration.storage.dell.com/migrate-to migrated volume is assumed to be used in same namespace as original PV and it’s PVC. In the case of being migrated to replication enabled storage class will be inserted in namespaced Replication Group inside PVC namespace.”\nHowever, you can define in which namespace migrated volume must be used after migration by setting migration.storage.dell.com/namespace. You can use the same annotation in a scenario where you only have a statically provisioned PV, and you don’t have it bound to any PVC, and you want to migrate it to another storage class.\nNon Disruptive Migration You can migrate your PVs without disrupting workflows if you use StatefulSet with multiple replicas to deploy application.\nInstruction (you can also use repctl for convenience):\nFind every PV for your StatefulSet and patch it with migration.storage.dell.com/migrate-to annotation that points to new storage class:\nkubectl patch pv \u003cpv-name\u003e -p '{\"metadata\": {\"annotations\":{\"migration.storage.dell.com/migrate-to\":\"powermax-replication\"}}}' Ensure you have a copy of StatefulSet manifest somewhere ready, we will need it later. If you don’t have it, you can get it from cluster:\nkubectl get sts \u003csts-name\u003e -n \u003cns-name\u003e -o yaml \u003e sts-manifest.yaml To not disrupt any workflows, we will need to delete StatefulSet without deleting any pods, to do so you can use the --cascade flag:\nkubectl delete sts \u003csts-name\u003e -n \u003cns-name\u003e --cascade=orphan Change the StorageClass in your manifest of StatefulSet to point to a new storage class, then apply it to the cluster:\nkubectl apply -f sts-manifest.yaml Find a PVC and pod of one replica of StatefulSet delete PVCs first and Pod after it:\nkubectl delete pvc \u003cpvc-name\u003e -n \u003cns-name\u003e kubectl delete pod \u003cpod-name\u003e -n \u003cns-name\u003e Wait for new pod to be created by StatefulSet, it should create new PVC that will use migrated PV.\nRepeat step 5 until all replicas use new PVCs.\nUsing repctl You can use repctl CLI tool to help you simplify running migration specific commands.\nSingle PV In its most simple usage, repctl can do the same operations as kubectl, for example, migrating the single PV ’test-pv’ from our example will look like:\n./repctl migrate pv test-pv --to-sc powermax-replication repctl will go and patch the resource for you. You can also provide --wait flag for it to wait until migrated PV is created in the cluster. repctl also can set migration.storage.dell.com/namespace for you if you provide --target-ns flag.\nAside from just migrating single PVs repctl can migrate PVCs and StatefulSets.\nPVC repctl can find PV for any given PVC for you and patch it. This could be done with similar command to single PV migration:\n./repctl migrate pvc test-pvc --to-sc powermax-replication -n default Notice that we provide original namespace (default in our example) for this command because PVCs are namespaced resource and we need namespace to be able to find it.\nStatefulSet repctl can help you migrate entire StatefulSet by automating migration process.\nYou can use this command to do so:\n./repctl migrate sts test-sts --to-sc powermax-replication -n default By default, it will find every Pod, PVC and PV for provided StatefulSet and patch every PV with annotation.\nYou can also optionally provide --ndu flag, with this flag provided repctl will do steps provided in Non Disruptive Migration section automatically.\n","categories":"","description":"Support for Array Migration of Volumes between storage classes\n","excerpt":"Support for Array Migration of Volumes between storage classes\n","ref":"/csm-docs/v1/replication/migration/migrating-volumes-same-array/","tags":"","title":"Between Storage classes"},{"body":"You can migrate existing pre-provisioned volumes to another storage class by using volume migration feature.\nCurrently two versions of migration are supported:\nTo replicated storage class from NON replicated one. To NON replicated storage class from replicated one. Prerequisites Original volume is from the one of currently supported CSI drivers (see Support Matrix) Migrated sidecar is installed alongside with the driver, you can enable it in your myvalues.yaml file migration: enabled: true Support Matrix Migration Type PowerMax PowerStore PowerScale PowerFlex Unity NON_REPL_TO_REPL Yes No No No No REPL_TO_NON_REPL Yes No No No No Basic Usage To trigger migration procedure, you need to patch existing PersistentVolume with migration annotation (by default migration.storage.dell.com/migrate-to) and in value of said annotation specify StorageClass name you want to migrate to.\nFor example, if we have PV named test-pv already provisioned and we want to migrate it to replicated storage class named powermax-replication we can run:\nkubectl patch pv test-pv -p '{\"metadata\": {\"annotations\":{\"migration.storage.dell.com/migrate-to\":\"powermax-replication\"}}}' Patching PV resource will trigger migration sidecar that will call VolumeMigrate call from the CSI driver. After migration is finished new PersistentVolume will be created in cluster with name of original PV plus -to-\u003csc-name\u003e appended to it.\nIn our example, we will see this when running kubectl get pv:\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE test-pv 1Gi RWO Retain Bound default/test-pvc powermax 5m test-pv-to-powermax-replication 1Gi RWO Retain Available powermax-replication 10s When Volume Migration is finished, source PV will be updated with an EVENT that denotes that this has taken place.\nNewly created PV (test-pv-to-powermax-replication in our example) is available for consumption via static provisioning by any PVC that requests it.\nNamespace Considerations For Replication Replication Groups in CSM Replication can be made namespaced, meaning that one SC will generate one Replication Group per namespace. This is also important when migrating volumes from/to replcation storage class.\n“When just setting one annotation migration.storage.dell.com/migrate-to migrated volume is assumed to be used in same namespace as original PV and it’s PVC. In the case of being migrated to replication enabled storage class will be inserted in namespaced Replication Group inside PVC namespace.”\nHowever, you can define in which namespace migrated volume must be used after migration by setting migration.storage.dell.com/namespace. You can use the same annotation in a scenario where you only have a statically provisioned PV, and you don’t have it bound to any PVC, and you want to migrate it to another storage class.\nNon Disruptive Migration You can migrate your PVs without disrupting workflows if you use StatefulSet with multiple replicas to deploy application.\nInstruction (you can also use repctl for convenience):\nFind every PV for your StatefulSet and patch it with migration.storage.dell.com/migrate-to annotation that points to new storage class:\nkubectl patch pv \u003cpv-name\u003e -p '{\"metadata\": {\"annotations\":{\"migration.storage.dell.com/migrate-to\":\"powermax-replication\"}}}' Ensure you have a copy of StatefulSet manifest somewhere ready, we will need it later. If you don’t have it, you can get it from cluster:\nkubectl get sts \u003csts-name\u003e -n \u003cns-name\u003e -o yaml \u003e sts-manifest.yaml To not disrupt any workflows, we will need to delete StatefulSet without deleting any pods, to do so you can use the --cascade flag:\nkubectl delete sts \u003csts-name\u003e -n \u003cns-name\u003e --cascade=orphan Change the StorageClass in your manifest of StatefulSet to point to a new storage class, then apply it to the cluster:\nkubectl apply -f sts-manifest.yaml Find a PVC and pod of one replica of StatefulSet delete PVCs first and Pod after it:\nkubectl delete pvc \u003cpvc-name\u003e -n \u003cns-name\u003e kubectl delete pod \u003cpod-name\u003e -n \u003cns-name\u003e Wait for new pod to be created by StatefulSet, it should create new PVC that will use migrated PV.\nRepeat step 5 until all replicas use new PVCs.\nUsing repctl You can use repctl CLI tool to help you simplify running migration specific commands.\nSingle PV In its most simple usage, repctl can do the same operations as kubectl, for example, migrating the single PV ’test-pv’ from our example will look like:\n./repctl migrate pv test-pv --to-sc powermax-replication repctl will go and patch the resource for you. You can also provide --wait flag for it to wait until migrated PV is created in the cluster. repctl also can set migration.storage.dell.com/namespace for you if you provide --target-ns flag.\nAside from just migrating single PVs repctl can migrate PVCs and StatefulSets.\nPVC repctl can find PV for any given PVC for you and patch it. This could be done with similar command to single PV migration:\n./repctl migrate pvc test-pvc --to-sc powermax-replication -n default Notice that we provide original namespace (default in our example) for this command because PVCs are namespaced resource and we need namespace to be able to find it.\nStatefulSet repctl can help you migrate entire StatefulSet by automating migration process.\nYou can use this command to do so:\n./repctl migrate sts test-sts --to-sc powermax-replication -n default By default, it will find every Pod, PVC and PV for provided StatefulSet and patch every PV with annotation.\nYou can also optionally provide --ndu flag, with this flag provided repctl will do steps provided in Non Disruptive Migration section automatically.\n","categories":"","description":"Support for Array Migration of Volumes between storage classes\n","excerpt":"Support for Array Migration of Volumes between storage classes\n","ref":"/csm-docs/v2/replication/migration/migrating-volumes-same-array/","tags":"","title":"Between Storage classes"},{"body":"You can migrate existing pre-provisioned volumes to another storage class by using volume migration feature.\nCurrently two versions of migration are supported:\nTo replicated storage class from NON replicated one. To NON replicated storage class from replicated one. Prerequisites Original volume is from the one of currently supported CSI drivers (see Support Matrix) Migrated sidecar is installed alongside with the driver, you can enable it in your myvalues.yaml file migration: enabled: true Support Matrix Migration Type PowerMax PowerStore PowerScale PowerFlex Unity NON_REPL_TO_REPL Yes No No No No REPL_TO_NON_REPL Yes No No No No Basic Usage To trigger migration procedure, you need to patch existing PersistentVolume with migration annotation (by default migration.storage.dell.com/migrate-to) and in value of said annotation specify StorageClass name you want to migrate to.\nFor example, if we have PV named test-pv already provisioned and we want to migrate it to replicated storage class named powermax-replication we can run:\nkubectl patch pv test-pv -p '{\"metadata\": {\"annotations\":{\"migration.storage.dell.com/migrate-to\":\"powermax-replication\"}}}' Patching PV resource will trigger migration sidecar that will call VolumeMigrate call from the CSI driver. After migration is finished new PersistentVolume will be created in cluster with name of original PV plus -to-\u003csc-name\u003e appended to it.\nIn our example, we will see this when running kubectl get pv:\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE test-pv 1Gi RWO Retain Bound default/test-pvc powermax 5m test-pv-to-powermax-replication 1Gi RWO Retain Available powermax-replication 10s When Volume Migration is finished, source PV will be updated with an EVENT that denotes that this has taken place.\nNewly created PV (test-pv-to-powermax-replication in our example) is available for consumption via static provisioning by any PVC that requests it.\nNamespace Considerations For Replication Replication Groups in CSM Replication can be made namespaced, meaning that one SC will generate one Replication Group per namespace. This is also important when migrating volumes from/to replcation storage class.\n“When just setting one annotation migration.storage.dell.com/migrate-to migrated volume is assumed to be used in same namespace as original PV and it’s PVC. In the case of being migrated to replication enabled storage class will be inserted in namespaced Replication Group inside PVC namespace.”\nHowever, you can define in which namespace migrated volume must be used after migration by setting migration.storage.dell.com/namespace. You can use the same annotation in a scenario where you only have a statically provisioned PV, and you don’t have it bound to any PVC, and you want to migrate it to another storage class.\nNon Disruptive Migration You can migrate your PVs without disrupting workflows if you use StatefulSet with multiple replicas to deploy application.\nInstruction (you can also use repctl for convenience):\nFind every PV for your StatefulSet and patch it with migration.storage.dell.com/migrate-to annotation that points to new storage class:\nkubectl patch pv \u003cpv-name\u003e -p '{\"metadata\": {\"annotations\":{\"migration.storage.dell.com/migrate-to\":\"powermax-replication\"}}}' Ensure you have a copy of StatefulSet manifest somewhere ready, we will need it later. If you don’t have it, you can get it from cluster:\nkubectl get sts \u003csts-name\u003e -n \u003cns-name\u003e -o yaml \u003e sts-manifest.yaml To not disrupt any workflows, we will need to delete StatefulSet without deleting any pods, to do so you can use the --cascade flag:\nkubectl delete sts \u003csts-name\u003e -n \u003cns-name\u003e --cascade=orphan Change the StorageClass in your manifest of StatefulSet to point to a new storage class, then apply it to the cluster:\nkubectl apply -f sts-manifest.yaml Find a PVC and pod of one replica of StatefulSet delete PVCs first and Pod after it:\nkubectl delete pvc \u003cpvc-name\u003e -n \u003cns-name\u003e kubectl delete pod \u003cpod-name\u003e -n \u003cns-name\u003e Wait for new pod to be created by StatefulSet, it should create new PVC that will use migrated PV.\nRepeat step 5 until all replicas use new PVCs.\nUsing repctl You can use repctl CLI tool to help you simplify running migration specific commands.\nSingle PV In its most simple usage, repctl can do the same operations as kubectl, for example, migrating the single PV ’test-pv’ from our example will look like:\n./repctl migrate pv test-pv --to-sc powermax-replication repctl will go and patch the resource for you. You can also provide --wait flag for it to wait until migrated PV is created in the cluster. repctl also can set migration.storage.dell.com/namespace for you if you provide --target-ns flag.\nAside from just migrating single PVs repctl can migrate PVCs and StatefulSets.\nPVC repctl can find PV for any given PVC for you and patch it. This could be done with similar command to single PV migration:\n./repctl migrate pvc test-pvc --to-sc powermax-replication -n default Notice that we provide original namespace (default in our example) for this command because PVCs are namespaced resource and we need namespace to be able to find it.\nStatefulSet repctl can help you migrate entire StatefulSet by automating migration process.\nYou can use this command to do so:\n./repctl migrate sts test-sts --to-sc powermax-replication -n default By default, it will find every Pod, PVC and PV for provided StatefulSet and patch every PV with annotation.\nYou can also optionally provide --ndu flag, with this flag provided repctl will do steps provided in Non Disruptive Migration section automatically.\n","categories":"","description":"Support for Array Migration of Volumes between storage classes\n","excerpt":"Support for Array Migration of Volumes between storage classes\n","ref":"/csm-docs/v3/replication/migration/migrating-volumes-same-array/","tags":"","title":"Between Storage classes"},{"body":"This section provides the details and instructions on how to configure CSM Authorization.\n","categories":"","description":"Configure CSM Authorization","excerpt":"Configure CSM Authorization","ref":"/csm-docs/docs/authorization/configuration/","tags":"","title":"Configuration"},{"body":"This section provides the details and instructions on how to configure CSM Authorization.\n","categories":"","description":"Configure CSM Authorization","excerpt":"Configure CSM Authorization","ref":"/csm-docs/v1/authorization/configuration/","tags":"","title":"Configuration"},{"body":"This section provides the details and instructions on how to configure CSM Authorization.\n","categories":"","description":"Configure CSM Authorization","excerpt":"Configure CSM Authorization","ref":"/csm-docs/v2/authorization/configuration/","tags":"","title":"Configuration"},{"body":"This section provides the details and instructions on how to configure CSM Authorization.\n","categories":"","description":"Configure CSM Authorization","excerpt":"Configure CSM Authorization","ref":"/csm-docs/v3/authorization/configuration/","tags":"","title":"Configuration"},{"body":"The CSM Operator can optionally enable modules that are supported by the specific Dell CSI driver. By default, the modules are disabled but they can be enabled by setting any pre-requisite configuration options for the given module and setting the enabled flag to true in the custom resource. The steps include:\nDeploy the Dell CSM Operator (if it is not already deployed). Please follow the instructions available here. Configure any pre-requisite for the desired module(s). See the specific module below for more information Follow the instructions here to install Dell CSI Drivers via the CSM Operator. The module section in the ContainerStorageModule CR should be updated to enable the desired module(s). There are sample manifests provided which can be edited to do an easy installation of the driver along with the module. ","categories":"","description":"Installation of Dell CSM Modules using Dell CSM Operator","excerpt":"Installation of Dell CSM Modules using Dell CSM Operator","ref":"/csm-docs/docs/deployment/csmoperator/modules/","tags":"","title":"CSM Modules"},{"body":"The CSM Operator can optionally enable modules that are supported by the specific Dell CSI driver. By default, the modules are disabled but they can be enabled by setting any pre-requisite configuration options for the given module and setting the enabled flag to true in the custom resource. The steps include:\nDeploy the Dell CSM Operator (if it is not already deployed). Please follow the instructions available here. Configure any pre-requisite for the desired module(s). See the specific module below for more information Follow the instructions here to install Dell CSI Drivers via the CSM Operator. The module section in the ContainerStorageModule CR should be updated to enable the desired module(s). There are sample manifests provided which can be edited to do an easy installation of the driver along with the module. ","categories":"","description":"Installation of Dell CSM Modules using Dell CSM Operator","excerpt":"Installation of Dell CSM Modules using Dell CSM Operator","ref":"/csm-docs/v1/deployment/csmoperator/modules/","tags":"","title":"CSM Modules"},{"body":"The CSM Operator can optionally enable modules that are supported by the specific Dell CSI driver. By default, the modules are disabled but they can be enabled by setting any pre-requisite configuration options for the given module and setting the enabled flag to true in the custom resource. The steps include:\nDeploy the Dell CSM Operator (if it is not already deployed). Please follow the instructions available here. Configure any pre-requisite for the desired module(s). See the specific module below for more information Follow the instructions here to install Dell CSI Drivers via the CSM Operator. The module section in the ContainerStorageModule CR should be updated to enable the desired module(s). There are sample manifests provided which can be edited to do an easy installation of the driver along with the module. ","categories":"","description":"Installation of Dell CSM Modules using Dell CSM Operator","excerpt":"Installation of Dell CSM Modules using Dell CSM Operator","ref":"/csm-docs/v2/deployment/csmoperator/modules/","tags":"","title":"CSM Modules"},{"body":"The CSM Operator can optionally enable modules that are supported by the specific Dell CSI driver. By default, the modules are disabled but they can be enabled by setting any pre-requisite configuration options for the given module and setting the enabled flag to true in the custom resource. The steps include:\nDeploy the Dell CSM Operator (if it is not already deployed). Please follow the instructions available here. Configure any pre-requisite for the desired module(s). See the specific module below for more information Follow the instructions to install the Dell CSI Driver (such as PowerScale or PowerFlex) via the CSM Operator. The module section in the ContainerStorageModule CR should be updated to enable the desired module(s). There are sample manifests provided which can be edited to do an easy installation of the driver along with the module. ","categories":"","description":"Installation of Dell CSM Modules using Dell CSM Operator","excerpt":"Installation of Dell CSM Modules using Dell CSM Operator","ref":"/csm-docs/v3/deployment/csmoperator/modules/","tags":"","title":"CSM Modules"},{"body":" The CSM Authorization RPM will be deprecated in a future release. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nInstallation information for CSM Authorization can be found in this section.\n","categories":"","description":"Methods to install CSM Authorization","excerpt":"Methods to install CSM Authorization","ref":"/csm-docs/docs/authorization/deployment/","tags":["install","csm-authorization"],"title":"Deployment"},{"body":" The CSM Authorization RPM will be deprecated in a future release. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nInstallation information for CSM Authorization can be found in this section.\n","categories":"","description":"Methods to install CSM Authorization","excerpt":"Methods to install CSM Authorization","ref":"/csm-docs/v1/authorization/deployment/","tags":["install","csm-authorization"],"title":"Deployment"},{"body":"Installation information for CSM Authorization can be found in this section.\n","categories":"","description":"Methods to install CSM Authorization","excerpt":"Methods to install CSM Authorization","ref":"/csm-docs/v2/authorization/deployment/","tags":["install","csm-authorization"],"title":"Deployment"},{"body":"Installation information for CSM Authorization can be found in this section.\n","categories":"","description":"Methods to install CSM Authorization","excerpt":"Methods to install CSM Authorization","ref":"/csm-docs/v3/authorization/deployment/","tags":["install","csm-authorization"],"title":"Deployment"},{"body":"Installation information for all the drivers/modules can be found on the individual driver’s page in this section\n","categories":"","description":"Methods to install CSI Drivers","excerpt":"Methods to install CSI Drivers","ref":"/csm-docs/docs/csidriver/installation/","tags":["install","csi-driver"],"title":"Installation"},{"body":"The installation process consists of two steps:\nInstall Container Storage Modules (CSM) for Replication Controller Install CSI driver after enabling replication Before you begin Please read this document before proceeding with the installation. It provides detailed steps on how to set up communication between multiple clusters which will be required during or after the installation.\nInstall CSM Replication Controller You can use one of the following methods to install CSM Replication Controller:\nUsing repctl Installation script (Helm chart) We recommend using repctl for the installation, as it simplifies the installation workflow. This process also helps configure repctl for future use during management operations.\nUsing repctl Please follow the steps here to install \u0026 configure Dell Replication Controller using repctl.\nUsing the installation script Please follow the steps here to install \u0026 configure Dell Replication Controller using script.\nInstall CSI driver The following CSI drivers support replication:\nCSI driver for PowerMax CSI driver for PowerStore CSI driver for PowerScale CSI driver for PowerFlex Please follow the steps outlined in PowerMax, PowerStore, PowerScale, or PowerFlex pages during the driver installation.\nNote: Please ensure that replication CRDs are installed in the clusters where you are installing the CSI drivers. These CRDs are generally installed as part of the CSM Replication controller installation process.\nDynamic Log Level Change CSM Replication Controller can dynamically change its logs’ verbosity level. To set log level in runtime, you need to edit the controllers ConfigMap:\nkubectl edit cm dell-replication-controller-config -n dell-replication-controller And set the CSI_LOG_LEVEL field to the level of your choosing. CSM Replication controller supports following log levels:\n“PANIC” “FATAL” “ERROR” “WARN” “INFO” “DEBUG” “TRACE” Note: CSI-Replicator sidecar utilizes the same log level as CSI driver. To change the sidecars log level refer to corresponding csi drivers documentation.\n","categories":"","description":"Installation of CSM for Replication\n","excerpt":"Installation of CSM for Replication\n","ref":"/csm-docs/docs/replication/deployment/installation/","tags":"","title":"Installation"},{"body":"Installation information for all the drivers/modules can be found on the individual driver’s page in this section\n","categories":"","description":"Methods to install CSI Drivers","excerpt":"Methods to install CSI Drivers","ref":"/csm-docs/v1/csidriver/installation/","tags":["install","csi-driver"],"title":"Installation"},{"body":"The installation process consists of two steps:\nInstall Container Storage Modules (CSM) for Replication Controller Install CSI driver after enabling replication Before you begin Please read this document before proceeding with the installation. It provides detailed steps on how to set up communication between multiple clusters which will be required during or after the installation.\nInstall CSM Replication Controller You can use one of the following methods to install CSM Replication Controller:\nUsing repctl Installation script (Helm chart) We recommend using repctl for the installation, as it simplifies the installation workflow. This process also helps configure repctl for future use during management operations.\nUsing repctl Please follow the steps here to install \u0026 configure Dell Replication Controller using repctl.\nUsing the installation script Please follow the steps here to install \u0026 configure Dell Replication Controller using script.\nInstall CSI driver The following CSI drivers support replication:\nCSI driver for PowerMax CSI driver for PowerStore CSI driver for PowerScale CSI driver for PowerFlex Please follow the steps outlined in PowerMax, PowerStore, PowerScale, or PowerFlex pages during the driver installation.\nNote: Please ensure that replication CRDs are installed in the clusters where you are installing the CSI drivers. These CRDs are generally installed as part of the CSM Replication controller installation process.\nDynamic Log Level Change CSM Replication Controller can dynamically change its logs’ verbosity level. To set log level in runtime, you need to edit the controllers ConfigMap:\nkubectl edit cm dell-replication-controller-config -n dell-replication-controller And set the CSI_LOG_LEVEL field to the level of your choosing. CSM Replication controller supports following log levels:\n“PANIC” “FATAL” “ERROR” “WARN” “INFO” “DEBUG” “TRACE” Note: CSI-Replicator sidecar utilizes the same log level as CSI driver. To change the sidecars log level refer to corresponding csi drivers documentation.\n","categories":"","description":"Installation of CSM for Replication\n","excerpt":"Installation of CSM for Replication\n","ref":"/csm-docs/v1/replication/deployment/installation/","tags":"","title":"Installation"},{"body":"Installation information for all the drivers/modules can be found on the individual driver’s page in this section\n","categories":"","description":"Methods to install CSI Drivers","excerpt":"Methods to install CSI Drivers","ref":"/csm-docs/v2/csidriver/installation/","tags":["install","csi-driver"],"title":"Installation"},{"body":"The installation process consists of two steps:\nInstall Container Storage Modules (CSM) for Replication Controller Install CSI driver after enabling replication Before you begin Please read this document before proceeding with the installation. It provides detailed steps on how to set up communication between multiple clusters which will be required during or after the installation.\nInstall CSM Replication Controller You can use one of the following methods to install CSM Replication Controller:\nUsing repctl Installation script (Helm chart) We recommend using repctl for the installation, as it simplifies the installation workflow. This process also helps configure repctl for future use during management operations.\nUsing repctl Please follow the steps here to install \u0026 configure Dell Replication Controller using repctl.\nUsing the installation script Please follow the steps here to install \u0026 configure Dell Replication Controller using script.\nInstall CSI driver The following CSI drivers support replication:\nCSI driver for PowerMax CSI driver for PowerStore CSI driver for PowerScale CSI driver for PowerFlex Please follow the steps outlined in PowerMax, PowerStore, PowerScale, or PowerFlex pages during the driver installation.\nNote: Please ensure that replication CRDs are installed in the clusters where you are installing the CSI drivers. These CRDs are generally installed as part of the CSM Replication controller installation process.\nDynamic Log Level Change CSM Replication Controller can dynamically change its logs’ verbosity level. To set log level in runtime, you need to edit the controllers ConfigMap:\nkubectl edit cm dell-replication-controller-config -n dell-replication-controller And set the CSI_LOG_LEVEL field to the level of your choosing. CSM Replication controller supports following log levels:\n“PANIC” “FATAL” “ERROR” “WARN” “INFO” “DEBUG” “TRACE” Note: CSI-Replicator sidecar utilizes the same log level as CSI driver. To change the sidecars log level refer to corresponding csi drivers documentation.\n","categories":"","description":"Installation of CSM for Replication\n","excerpt":"Installation of CSM for Replication\n","ref":"/csm-docs/v2/replication/deployment/installation/","tags":"","title":"Installation"},{"body":"Installation information for all the drivers/modules can be found on the individual driver’s page in this section\n","categories":"","description":"Methods to install CSI Drivers","excerpt":"Methods to install CSI Drivers","ref":"/csm-docs/v3/csidriver/installation/","tags":["install","csi-driver"],"title":"Installation"},{"body":"The installation process consists of two steps:\nInstall Container Storage Modules (CSM) for Replication Controller Install CSI driver after enabling replication Before you begin Please read this document before proceeding with the installation. It provides detailed steps on how to set up communication between multiple clusters which will be required during or after the installation.\nInstall CSM Replication Controller You can use one of the following methods to install CSM Replication Controller:\nUsing repctl Installation script (Helm chart) We recommend using repctl for the installation, as it simplifies the installation workflow. This process also helps configure repctl for future use during management operations.\nUsing repctl Please follow the steps here to install \u0026 configure Dell Replication Controller using repctl.\nUsing the installation script Please follow the steps here to install \u0026 configure Dell Replication Controller using script.\nInstall CSI driver The following CSI drivers support replication:\nCSI driver for PowerMax CSI driver for PowerStore CSI driver for PowerScale CSI driver for PowerFlex Please follow the steps outlined in PowerMax, PowerStore, PowerScale, or PowerFlex pages during the driver installation.\nNote: Please ensure that replication CRDs are installed in the clusters where you are installing the CSI drivers. These CRDs are generally installed as part of the CSM Replication controller installation process.\nDynamic Log Level Change CSM Replication Controller can dynamically change its logs’ verbosity level. To set log level in runtime, you need to edit the controllers ConfigMap:\nkubectl edit cm dell-replication-controller-config -n dell-replication-controller And set the CSI_LOG_LEVEL field to the level of your choosing. CSM Replication controller supports following log levels:\n“PANIC” “FATAL” “ERROR” “WARN” “INFO” “DEBUG” “TRACE” Note: CSI-Replicator sidecar utilizes the same log level as CSI driver. To change the sidecars log level refer to corresponding csi drivers documentation.\n","categories":"","description":"Installation of CSM for Replication\n","excerpt":"Installation of CSM for Replication\n","ref":"/csm-docs/v3/replication/deployment/installation/","tags":"","title":"Installation"},{"body":"Welcome to Dell CSM’s interactive tutorials section! Try the different modules with a live system and experience the benefits of our solutions firsthand.\nOur interactive tutorials provide step-by-step guidance on how to use our CSM products in the context of PowerFlex Software-Defined-Storage.\nNOTE: It takes about 15 minutes for the lab to be ready to use.\n","categories":"","description":"Try CSM and more with interactive labs","excerpt":"Try CSM and more with interactive labs","ref":"/csm-docs/docs/interactive-tutorials/","tags":"","title":"Interactive Tutorials"},{"body":"Welcome to Dell CSM’s interactive tutorials section! Try the different modules with a live system and experience the benefits of our solutions firsthand.\nOur interactive tutorials provide step-by-step guidance on how to use our CSM products in the context of PowerFlex Software-Defined-Storage.\nNOTE: It takes about 15 minutes for the lab to be ready to use.\n","categories":"","description":"Try CSM and more with interactive labs","excerpt":"Try CSM and more with interactive labs","ref":"/csm-docs/v1/interactive-tutorials/","tags":"","title":"Interactive Tutorials"},{"body":"Welcome to Dell CSM’s interactive tutorials section! Try the different modules with a live system and experience the benefits of our solutions firsthand.\nOur interactive tutorials provide step-by-step guidance on how to use our CSM products in the context of PowerFlex Software-Defined-Storage.\nNOTE: It takes about 15 minutes for the lab to be ready to use.\n","categories":"","description":"Try CSM and more with interactive labs","excerpt":"Try CSM and more with interactive labs","ref":"/csm-docs/v2/interactive-tutorials/","tags":"","title":"Interactive Tutorials"},{"body":"Welcome to Dell CSM’s interactive tutorials section! Try the different modules with a live system and experience the benefits of our solutions firsthand.\nOur interactive tutorials provide step-by-step guidance on how to use our CSM products in the context of PowerFlex Software-Defined-Storage.\nNOTE: It takes about 15 minutes for the lab to be ready to use.\n","categories":"","description":"Try CSM and more with interactive labs","excerpt":"Try CSM and more with interactive labs","ref":"/csm-docs/v3/interactive-tutorials/","tags":"","title":"Interactive Tutorials"},{"body":"You can learn more and engage with the CSM community over different channels by:\nReading the official blogs and white paper are on https://infohub.delltechnologies.com/ Seeing the lastest CSM features in action from Dell Technologies’ YouTube channel Reaching-out the development team on Slack Learning from technical blogs on https://volumes.blog/ ","categories":"","description":"Blogs and Youtube videos for CSM","excerpt":"Blogs and Youtube videos for CSM","ref":"/csm-docs/docs/references/learn/","tags":"","title":"Learn"},{"body":"You can learn more and engage with the CSM community over different channels by:\nReading the official blogs and white paper are on https://infohub.delltechnologies.com/ Seeing the lastest CSM features in action from Dell Technologies’ YouTube channel Reaching-out the development team on Slack Learning from technical blogs on https://volumes.blog/ ","categories":"","description":"Blogs and Youtube videos for CSM","excerpt":"Blogs and Youtube videos for CSM","ref":"/csm-docs/v1/references/learn/","tags":"","title":"Learn"},{"body":"You can learn more and engage with the CSM community over different channels by:\nReading the official blogs and white paper are on https://infohub.delltechnologies.com/ Seeing the lastest CSM features in action from Dell Technologies’ YouTube channel Reaching-out the development team on Slack Learning from technical blogs on https://volumes.blog/ ","categories":"","description":"Blogs and Youtube videos for CSM","excerpt":"Blogs and Youtube videos for CSM","ref":"/csm-docs/v2/references/learn/","tags":"","title":"Learn"},{"body":"You can learn more and engage with the CSM community over different channels by:\nReading the official blogs and white paper are on https://infohub.delltechnologies.com/ Seeing the lastest CSM features in action from Dell Technologies’ YouTube channel Reaching-out the development team on Slack Learning from technical blogs on https://volumes.blog/ ","categories":"","description":"Blogs and Youtube videos for CSM","excerpt":"Blogs and Youtube videos for CSM","ref":"/csm-docs/v3/references/learn/","tags":"","title":"Learn"},{"body":"This section outlines the metrics collected by Container Storage Modules (CSM) for Observability in the areas of I/O Performance and Storage Capacity. All metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Metrics\n","ref":"/csm-docs/docs/observability/metrics/","tags":"","title":"Metrics"},{"body":"This section outlines the metrics collected by Container Storage Modules (CSM) for Observability in the areas of I/O Performance and Storage Capacity. All metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Metrics\n","ref":"/csm-docs/v1/observability/metrics/","tags":"","title":"Metrics"},{"body":"This section outlines the metrics collected by Container Storage Modules (CSM) for Observability in the areas of I/O Performance and Storage Capacity. All metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Metrics\n","ref":"/csm-docs/v2/observability/metrics/","tags":"","title":"Metrics"},{"body":"This section outlines the metrics collected by Container Storage Modules (CSM) for Observability in the areas of I/O Performance and Storage Capacity. All metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Metrics\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Metrics\n","ref":"/csm-docs/v3/observability/metrics/","tags":"","title":"Metrics"},{"body":"SyncIQ Policy Architecture When creating DellCSIReplicationGroup (RG) objects on the Kubernetes cluster(s) used for replication, a SyncIQ policy to facilitate this replication is created only on the source PowerScale storage array.\nThis singular SyncIQ policy on the source storage array and its matching Local Target policy on the target storage array provide information for the RGs to determine their status. Upon creation, the SyncIQ policy is set to a schedule of When source is modified. The SyncIQ policy is Enabled when the RG is created. The directory that is being replicated is read-write accessible on the source storage array, and is restricted to read-only on the target.\nReplication Group Deletion When deleting DellCSIReplicationGroup (RG) objects on the Kubernetes cluster(s) used for replication, deletion should only be performed on an empty RG. If there is any user-created or Kubernetes PV-generated data left inside of the replication group, the RG object will be held in a Deleting state until all user data has been cleared out on both source and target storage arrays.\nIf the RG’s folder on both source and target storage arrays is empty and the RG is given a delete command, it will perform a sync, then remove its SyncIQ policy from the source storage array, then delete the RG object on both source and target Kubernetes clusters.\nIf irregular Kubernetes cluster/storage array behavior causes the source and target to fall out-of-sync (ex: one of the sides is down), the RG deletion will become stuck. If forced removal of the RG is necessary, the finalizers can be removed manually to allow for deletion, but data and SyncIQ policies may remain on the storage arrays and require manual deletion. See this Knowledge Base Article for further information on manual deletion.\nPerforming Failover/Failback/Reprotect on PowerScale Failover, Failback, and Reprotect one-step operations are not natively supported on PowerScale, and are performed as a series of steps in CSM replication. When any of these operations are triggered, through the use of repctl or by editing the RG, the steps below are performed on the PowerScale storage arrays.\nFailover - Halt Replication and Allow Writes on Target Steps for performing Failover can be found in the Tools page under Executing Actions. There are some PowerScale-specific considerations to keep in mind:\nFailover on PowerScale does NOT halt writes on the source side. It is recommended that the storage administrator or end user manually stop writes to ensure no data is lost on the source side in the event of future failback. In the case of unplanned failover, the SyncIQ policy on the source PowerScale array will be left enabled and set to its previously defined When source is modified sync schedule. Storage admins must manually disable this SyncIQ policy when bringing the failed-over source array back online, or unexpected behavior may occur. The below steps are performed by CSM replication to perform a failover.\nSyncing data from source to target one final time before transition. (planned failover only) Disabling the SyncIQ policy on the source PowerScale storage array. (planned failover only) Enabling writes on the target PowerScale array’s Local Target policy. Failback - Discard Target Performing failback and discarding changes made to the target is to simply resume synchronization from the source. The steps CSM replication is following to perform this operation are as follows:\nEditing the SyncIQ policy on the source PowerScale array’s schedule from When source is modified to Manual. Performing Actions \u003e Disallow writes on the target PowerScale array’s Local Target policy that matches the SyncIQ policy undergoing failback. Editing the SyncIQ policy’s schedule from Manual to When source is modified and setting the time delay for synchronization as appropriate. Enabling the source PowerScale array’s SyncIQ policy. Failback - Discard Source Information on the methodology for performing a failback while taking changes made to the original target can be found in relevant PowerScale SyncIQ documentation. The steps CSM replication is following to perform this operation are as follows:\nEditing the SyncIQ policy on the source PowerScale array’s schedule from When source is modified to Manual. Enabling the SyncIQ policy that is undergoing failback, if it isn’t already enabled. Performing the Resync-prep action on the SyncIQ policy. This will create a new SyncIQ policy on the target PowerScale array, matching the original SyncIQ policy with an appended _mirror to its name. Starting a synchronization job on the target PowerScale array’s newly created _mirror policy. Running the Allow writes operation on the Local Target on the source PowerScale array that was created by the _mirror policy. Performing the Resync-prep action on the target PowerScale array’s _mirror policy. Deleting the _mirror SyncIQ policy. Editing the SyncIQ policy on the source PowerScale array’s schedule from Manual to When source is modified and setting the time delay for synchronization as appropriate. Reprotect - Set Original Target as New Source A reprotect operation is, in essence, doing away with the original source-target relationship and establishing a new one in the reverse direction. This is done only after failing over to the original target array is complete, and the original source array is up and ready to be made into a new replication destination. To accomplish this, CSM replication performs the following steps:\nDeleting the SyncIQ policy on the original source PowerScale array. Creating a new SyncIQ policy on the original target PowerScale array. This policy establishes the original target as a new source, and sets its replication destination to the original source (which can be considered the new target.) ","categories":"","description":"Platform-Specific Architecture for CSI PowerScale\n","excerpt":"Platform-Specific Architecture for CSI PowerScale\n","ref":"/csm-docs/docs/replication/architecture/powerscale/","tags":"","title":"PowerScale"},{"body":"SyncIQ Policy Architecture When creating DellCSIReplicationGroup (RG) objects on the Kubernetes cluster(s) used for replication, a SyncIQ policy to facilitate this replication is created only on the source PowerScale storage array.\nThis singular SyncIQ policy on the source storage array and its matching Local Target policy on the target storage array provide information for the RGs to determine their status. Upon creation, the SyncIQ policy is set to a schedule of When source is modified. The SyncIQ policy is Enabled when the RG is created. The directory that is being replicated is read-write accessible on the source storage array, and is restricted to read-only on the target.\nReplication Group Deletion When deleting DellCSIReplicationGroup (RG) objects on the Kubernetes cluster(s) used for replication, deletion should only be performed on an empty RG. If there is any user-created or Kubernetes PV-generated data left inside of the replication group, the RG object will be held in a Deleting state until all user data has been cleared out on both source and target storage arrays.\nIf the RG’s folder on both source and target storage arrays is empty and the RG is given a delete command, it will perform a sync, then remove its SyncIQ policy from the source storage array, then delete the RG object on both source and target Kubernetes clusters.\nIf irregular Kubernetes cluster/storage array behavior causes the source and target to fall out-of-sync (ex: one of the sides is down), the RG deletion will become stuck. If forced removal of the RG is necessary, the finalizers can be removed manually to allow for deletion, but data and SyncIQ policies may remain on the storage arrays and require manual deletion. See this Knowledge Base Article for further information on manual deletion.\nPerforming Failover/Failback/Reprotect on PowerScale Failover, Failback, and Reprotect one-step operations are not natively supported on PowerScale, and are performed as a series of steps in CSM replication. When any of these operations are triggered, through the use of repctl or by editing the RG, the steps below are performed on the PowerScale storage arrays.\nFailover - Halt Replication and Allow Writes on Target Steps for performing Failover can be found in the Tools page under Executing Actions. There are some PowerScale-specific considerations to keep in mind:\nFailover on PowerScale does NOT halt writes on the source side. It is recommended that the storage administrator or end user manually stop writes to ensure no data is lost on the source side in the event of future failback. In the case of unplanned failover, the SyncIQ policy on the source PowerScale array will be left enabled and set to its previously defined When source is modified sync schedule. Storage admins must manually disable this SyncIQ policy when bringing the failed-over source array back online, or unexpected behavior may occur. The below steps are performed by CSM replication to perform a failover.\nSyncing data from source to target one final time before transition. (planned failover only) Disabling the SyncIQ policy on the source PowerScale storage array. (planned failover only) Enabling writes on the target PowerScale array’s Local Target policy. Failback - Discard Target Performing failback and discarding changes made to the target is to simply resume synchronization from the source. The steps CSM replication is following to perform this operation are as follows:\nEditing the SyncIQ policy on the source PowerScale array’s schedule from When source is modified to Manual. Performing Actions \u003e Disallow writes on the target PowerScale array’s Local Target policy that matches the SyncIQ policy undergoing failback. Editing the SyncIQ policy’s schedule from Manual to When source is modified and setting the time delay for synchronization as appropriate. Enabling the source PowerScale array’s SyncIQ policy. Failback - Discard Source Information on the methodology for performing a failback while taking changes made to the original target can be found in relevant PowerScale SyncIQ documentation. The steps CSM replication is following to perform this operation are as follows:\nEditing the SyncIQ policy on the source PowerScale array’s schedule from When source is modified to Manual. Enabling the SyncIQ policy that is undergoing failback, if it isn’t already enabled. Performing the Resync-prep action on the SyncIQ policy. This will create a new SyncIQ policy on the target PowerScale array, matching the original SyncIQ policy with an appended _mirror to its name. Starting a synchronization job on the target PowerScale array’s newly created _mirror policy. Running the Allow writes operation on the Local Target on the source PowerScale array that was created by the _mirror policy. Performing the Resync-prep action on the target PowerScale array’s _mirror policy. Deleting the _mirror SyncIQ policy. Editing the SyncIQ policy on the source PowerScale array’s schedule from Manual to When source is modified and setting the time delay for synchronization as appropriate. Reprotect - Set Original Target as New Source A reprotect operation is, in essence, doing away with the original source-target relationship and establishing a new one in the reverse direction. This is done only after failing over to the original target array is complete, and the original source array is up and ready to be made into a new replication destination. To accomplish this, CSM replication performs the following steps:\nDeleting the SyncIQ policy on the original source PowerScale array. Creating a new SyncIQ policy on the original target PowerScale array. This policy establishes the original target as a new source, and sets its replication destination to the original source (which can be considered the new target.) ","categories":"","description":"Platform-Specific Architecture for CSI PowerScale\n","excerpt":"Platform-Specific Architecture for CSI PowerScale\n","ref":"/csm-docs/v1/replication/architecture/powerscale/","tags":"","title":"PowerScale"},{"body":"SyncIQ Policy Architecture When creating DellCSIReplicationGroup (RG) objects on the Kubernetes cluster(s) used for replication, a SyncIQ policy to facilitate this replication is created only on the source PowerScale storage array.\nThis singular SyncIQ policy on the source storage array and its matching Local Target policy on the target storage array provide information for the RGs to determine their status. Upon creation, the SyncIQ policy is set to a schedule of When source is modified. The SyncIQ policy is Enabled when the RG is created. The directory that is being replicated is read-write accessible on the source storage array, and is restricted to read-only on the target.\nReplication Group Deletion When deleting DellCSIReplicationGroup (RG) objects on the Kubernetes cluster(s) used for replication, deletion should only be performed on an empty RG. If there is any user-created or Kubernetes PV-generated data left inside of the replication group, the RG object will be held in a Deleting state until all user data has been cleared out on both source and target storage arrays.\nIf the RG’s folder on both source and target storage arrays is empty and the RG is given a delete command, it will perform a sync, then remove its SyncIQ policy from the source storage array, then delete the RG object on both source and target Kubernetes clusters.\nIf irregular Kubernetes cluster/storage array behavior causes the source and target to fall out-of-sync (ex: one of the sides is down), the RG deletion will become stuck. If forced removal of the RG is necessary, the finalizers can be removed manually to allow for deletion, but data and SyncIQ policies may remain on the storage arrays and require manual deletion. See this Knowledge Base Article for further information on manual deletion.\nPerforming Failover/Failback/Reprotect on PowerScale Failover, Failback, and Reprotect one-step operations are not natively supported on PowerScale, and are performed as a series of steps in CSM replication. When any of these operations are triggered, through the use of repctl or by editing the RG, the steps below are performed on the PowerScale storage arrays.\nFailover - Halt Replication and Allow Writes on Target Steps for performing Failover can be found in the Tools page under Executing Actions. There are some PowerScale-specific considerations to keep in mind:\nFailover on PowerScale does NOT halt writes on the source side. It is recommended that the storage administrator or end user manually stop writes to ensure no data is lost on the source side in the event of future failback. In the case of unplanned failover, the SyncIQ policy on the source PowerScale array will be left enabled and set to its previously defined When source is modified sync schedule. Storage admins must manually disable this SyncIQ policy when bringing the failed-over source array back online, or unexpected behavior may occur. The below steps are performed by CSM replication to perform a failover.\nSyncing data from source to target one final time before transition. (planned failover only) Disabling the SyncIQ policy on the source PowerScale storage array. (planned failover only) Enabling writes on the target PowerScale array’s Local Target policy. Failback - Discard Target Performing failback and discarding changes made to the target is to simply resume synchronization from the source. The steps CSM replication is following to perform this operation are as follows:\nEditing the SyncIQ policy on the source PowerScale array’s schedule from When source is modified to Manual. Performing Actions \u003e Disallow writes on the target PowerScale array’s Local Target policy that matches the SyncIQ policy undergoing failback. Editing the SyncIQ policy’s schedule from Manual to When source is modified and setting the time delay for synchronization as appropriate. Enabling the source PowerScale array’s SyncIQ policy. Failback - Discard Source Information on the methodology for performing a failback while taking changes made to the original target can be found in relevant PowerScale SyncIQ documentation. The steps CSM replication is following to perform this operation are as follows:\nEditing the SyncIQ policy on the source PowerScale array’s schedule from When source is modified to Manual. Enabling the SyncIQ policy that is undergoing failback, if it isn’t already enabled. Performing the Resync-prep action on the SyncIQ policy. This will create a new SyncIQ policy on the target PowerScale array, matching the original SyncIQ policy with an appended _mirror to its name. Starting a synchronization job on the target PowerScale array’s newly created _mirror policy. Running the Allow writes operation on the Local Target on the source PowerScale array that was created by the _mirror policy. Performing the Resync-prep action on the target PowerScale array’s _mirror policy. Deleting the _mirror SyncIQ policy. Editing the SyncIQ policy on the source PowerScale array’s schedule from Manual to When source is modified and setting the time delay for synchronization as appropriate. Reprotect - Set Original Target as New Source A reprotect operation is, in essence, doing away with the original source-target relationship and establishing a new one in the reverse direction. This is done only after failing over to the original target array is complete, and the original source array is up and ready to be made into a new replication destination. To accomplish this, CSM replication performs the following steps:\nDeleting the SyncIQ policy on the original source PowerScale array. Creating a new SyncIQ policy on the original target PowerScale array. This policy establishes the original target as a new source, and sets its replication destination to the original source (which can be considered the new target.) ","categories":"","description":"Platform-Specific Architecture for CSI PowerScale\n","excerpt":"Platform-Specific Architecture for CSI PowerScale\n","ref":"/csm-docs/v2/replication/architecture/powerscale/","tags":"","title":"PowerScale"},{"body":"SyncIQ Policy Architecture When creating DellCSIReplicationGroup (RG) objects on the Kubernetes cluster(s) used for replication, a SyncIQ policy to facilitate this replication is created only on the source PowerScale storage array.\nThis singular SyncIQ policy on the source storage array and its matching Local Target policy on the target storage array provide information for the RGs to determine their status. Upon creation, the SyncIQ policy is set to a schedule of When source is modified. The SyncIQ policy is Enabled when the RG is created. The directory that is being replicated is read-write accessible on the source storage array, and is restricted to read-only on the target.\nReplication Group Deletion When deleting DellCSIReplicationGroup (RG) objects on the Kubernetes cluster(s) used for replication, deletion should only be performed on an empty RG. If there is any user-created or Kubernetes PV-generated data left inside of the replication group, the RG object will be held in a Deleting state until all user data has been cleared out on both source and target storage arrays.\nIf the RG’s folder on both source and target storage arrays is empty and the RG is given a delete command, it will perform a sync, then remove its SyncIQ policy from the source storage array, then delete the RG object on both source and target Kubernetes clusters.\nIf irregular Kubernetes cluster/storage array behavior causes the source and target to fall out-of-sync (ex: one of the sides is down), the RG deletion will become stuck. If forced removal of the RG is necessary, the finalizers can be removed manually to allow for deletion, but data and SyncIQ policies may remain on the storage arrays and require manual deletion. See this Knowledge Base Article for further information on manual deletion.\nPerforming Failover/Failback/Reprotect on PowerScale Failover, Failback, and Reprotect one-step operations are not natively supported on PowerScale, and are performed as a series of steps in CSM replication. When any of these operations are triggered, through the use of repctl or by editing the RG, the steps below are performed on the PowerScale storage arrays.\nFailover - Halt Replication and Allow Writes on Target Steps for performing Failover can be found in the Tools page under Executing Actions. There are some PowerScale-specific considerations to keep in mind:\nFailover on PowerScale does NOT halt writes on the source side. It is recommended that the storage administrator or end user manually stop writes to ensure no data is lost on the source side in the event of future failback. In the case of unplanned failover, the SyncIQ policy on the source PowerScale array will be left enabled and set to its previously defined When source is modified sync schedule. Storage admins must manually disable this SyncIQ policy when bringing the failed-over source array back online, or unexpected behavior may occur. The below steps are performed by CSM replication to perform a failover.\nSyncing data from source to target one final time before transition. (planned failover only) Disabling the SyncIQ policy on the source PowerScale storage array. (planned failover only) Enabling writes on the target PowerScale array’s Local Target policy. Failback - Discard Target Performing failback and discarding changes made to the target is to simply resume synchronization from the source. The steps CSM replication is following to perform this operation are as follows:\nEditing the SyncIQ policy on the source PowerScale array’s schedule from When source is modified to Manual. Performing Actions \u003e Disallow writes on the target PowerScale array’s Local Target policy that matches the SyncIQ policy undergoing failback. Editing the SyncIQ policy’s schedule from Manual to When source is modified and setting the time delay for synchronization as appropriate. Enabling the source PowerScale array’s SyncIQ policy. Failback - Discard Source Information on the methodology for performing a failback while taking changes made to the original target can be found in relevant PowerScale SyncIQ documentation. The steps CSM replication is following to perform this operation are as follows:\nEditing the SyncIQ policy on the source PowerScale array’s schedule from When source is modified to Manual. Enabling the SyncIQ policy that is undergoing failback, if it isn’t already enabled. Performing the Resync-prep action on the SyncIQ policy. This will create a new SyncIQ policy on the target PowerScale array, matching the original SyncIQ policy with an appended _mirror to its name. Starting a synchronization job on the target PowerScale array’s newly created _mirror policy. Running the Allow writes operation on the Local Target on the source PowerScale array that was created by the _mirror policy. Performing the Resync-prep action on the target PowerScale array’s _mirror policy. Deleting the _mirror SyncIQ policy. Editing the SyncIQ policy on the source PowerScale array’s schedule from Manual to When source is modified and setting the time delay for synchronization as appropriate. Reprotect - Set Original Target as New Source A reprotect operation is, in essence, doing away with the original source-target relationship and establishing a new one in the reverse direction. This is done only after failing over to the original target array is complete, and the original source array is up and ready to be made into a new replication destination. To accomplish this, CSM replication performs the following steps:\nDeleting the SyncIQ policy on the original source PowerScale array. Creating a new SyncIQ policy on the original target PowerScale array. This policy establishes the original target as a new source, and sets its replication destination to the original source (which can be considered the new target.) ","categories":"","description":"Platform-Specific Architecture for CSI PowerScale\n","excerpt":"Platform-Specific Architecture for CSI PowerScale\n","ref":"/csm-docs/v3/replication/architecture/powerscale/","tags":"","title":"PowerScale"},{"body":" The CSM Authorization RPM will be deprecated in a future release. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nThis section outlines the deployment steps for Container Storage Modules (CSM) for Authorization. The deployment of CSM for Authorization is handled in 2 parts:\nDeploying the CSM for Authorization proxy server, to be controlled by storage administrators Configuring one to many supported Dell CSI drivers with CSM for Authorization Prerequisites The CSM for Authorization proxy server requires a Linux host with the following minimum resource allocations:\n32 GB of memory 4 CPU 200 GB local storage The following package needs to be installed on the Linux host:\ncontainer-selinux Use the appropriate package manager on the machine to install the package.\nUsing yum on CentOS/RedHat 7: yum install -y container-selinux\nUsing yum on CentOS/RedHat 8: yum install -y container-selinux\nDark Sites For environments where yum will not work, obtain the supported version of container-selinux for your OS version and install it.\nThe container-selinux RPMs for CentOS/RedHat 7 and 8 can be downloaded from https://centos.pkgs.org/7/centos-extras-x86_64/ and https://centos.pkgs.org/8-stream/centos-appstream-x86_64/, respectively.\nDeploying the CSM Authorization Proxy Server The first part of deploying CSM for Authorization is installing the proxy server. This activity and the administration of the proxy server will be owned by the storage administrator.\nThe CSM for Authorization proxy server is installed using a shell script after extracting from a tar archive.\nIf CSM for Authorization is being installed on a system where SELinux is enabled, you must ensure the proper SELinux policies have been installed.\nShell Script Installer The easiest way to obtain the tar archive with the shell script installer is directly from the GitHub repository’s releases section.\nAlternatively, the tar archive can be built from source by cloning the GitHub repository and using the following Makefile targets to build the installer:\nmake dist build-installer rpm package The build-installer step creates a binary at karavi-authorization/bin/deploy and embeds all components required for installation. The rpm step generates an RPM package and stores it at karavi-authorization/deploy/rpm/x86_64/. The package step bundles the install script, authorization package, pre-downloaded K3s-SELinux packages, and policies folder together for the installation in the packages/ directory. This allows CSM for Authorization to be installed in network-restricted environments.\nA Storage Administrator can execute the shell script, install_karavi_auth.sh as a root user or via sudo.\nInstalling the RPM Before installing the rpm, some network and security configuration inputs need to be provided in json format. The json file should be created in the location $HOME/.karavi/config.json having the following contents:\n{ \"web\": { \"jwtsigningsecret\": \"secret\" }, \"proxy\": { \"host\": \":8080\" }, \"zipkin\": { \"collectoruri\": \"http://zipkin-addr:9411/api/v2/spans\", \"probability\": 1 }, \"certificate\": { \"keyFile\": \"path_to_private_key_file\", \"crtFile\": \"path_to_host_cert_file\", \"rootCertificate\": \"path_to_root_CA_file\" }, \"hostname\": \"DNS-hostname\" } In an instance where a secure deployment is not required, an insecure deployment is possible. Please note that self-signed certificates will be created for you using cert-manager to allow TLS encryption for communication on the CSM for Authorization proxy server. However, this is not recommended for production environments. For an insecure deployment, the json file in the location $HOME/.karavi/config.json only requires the following contents:\n{ \"hostname\": \"DNS-hostname\" } Note:\nDNS-hostname refers to the hostname of the system in which the CSM for Authorization server will be installed. This hostname can be found by running nslookup \u003cIP_address\u003e There are a number of ways to create certificates. In a production environment, certificates are usually created and managed by an IT administrator. Otherwise, certificates can be created using OpenSSL. To install the rpm package on the system, you must first extract the contents of the tar file with the command:\ntar -xvf karavi_authorization_\u003cversion\u003e Afterwards, you must enter the extracted folder’s directory and run the shell script:\ncd karavi_authorization_\u003cversion\u003e sh install_karavi_auth.sh As an option, on version 1.6.0, the Nodeports for the ingress controller can be specified:\nsh install_karavi_auth.sh --traefik_web_port \u003cweb port number\u003e --traefik_websecure_port \u003cwebsecure port number\u003e Ex.:\nsh install_karavi_auth.sh --traefik_web_port 30001 --traefik_websecure_port 30002 After installation, application data will be stored on the system under /var/lib/rancher/k3s/storage/.\nIf errors occur during installation, review the Troubleshooting section.\nConfiguring the CSM for Authorization Proxy Server The first part of CSM for Authorization deployment is to configure the proxy server. This is controlled by the Storage Administrator.\nPlease follow the steps outlined in the proxy server configuration.\nConfiguring a Dell CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the supported CSI drivers. This is controlled by the Kubernetes tenant administrator.\nPlease follow the steps outlined in PowerFlex, PowerMax, or PowerScale to configure the CSI Driver to work with the Authorization sidecar.\nUpdating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically:\nParameter Type Default Description web.jwtsigningsecret String “secret” The secret used to sign JWT tokens Updating configuration parameters can be done by editing the karavi-config-secret on the CSM for the Authorization Server. The secret can be queried using k3s and kubectl like so:\nk3s kubectl -n karavi get secret/karavi-config-secret To update or add parameters, you must edit the base64 encoded data in the secret. The karavi-config-secret data can be decoded like so:\nk3s kubectl -n karavi get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d Save the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so:\ncat \u003cfile\u003e | base64 Copy the new, encoded data and edit the karavi-config-secret with the new data. Run this command to edit the secret:\nk3s kubectl -n karavi edit secret/karavi-config-secret Replace the data in config.yaml under the data field with your new, encoded data. Save the changes and CSM for Authorization will read the changed secret.\nNote: If you are updating the signing secret, the tenants need to be updated with new tokens via the karavictl generate token command like so. The --insecure flag is required if certificates were not provided in $HOME/.karavi/config.json\nkaravictl generate token --tenant $TenantName --insecure --addr DNS-hostname | sed -e 's/\"Token\": //' -e 's/[{}\"]//g' -e 's/\\\\n/\\n/g' | kubectl -n $namespace apply -f - CSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the karavi-config-secret but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM for Authorization logging settings during runtime, run the below command on the K3s cluster, make your changes, and save the updated configmap data.\nk3s kubectl -n karavi edit configmap/csm-config-params This edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion:\nkubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params Using PowerFlex as an example, kubectl -n vxflexos edit configmap/vxflexos-config-params can be used to update the logging level of the sidecar-proxy and the driver.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization RPM deployment\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/authorization/deployment/rpm/","tags":"","title":"RPM"},{"body":" The CSM Authorization RPM will be deprecated in a future release. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nThis section outlines the deployment steps for Container Storage Modules (CSM) for Authorization. The deployment of CSM for Authorization is handled in 2 parts:\nDeploying the CSM for Authorization proxy server, to be controlled by storage administrators Configuring one to many supported Dell CSI drivers with CSM for Authorization Prerequisites The CSM for Authorization proxy server requires a Linux host with the following minimum resource allocations:\n32 GB of memory 4 CPU 200 GB local storage The following package needs to be installed on the Linux host:\ncontainer-selinux Use the appropriate package manager on the machine to install the package.\nUsing yum on CentOS/RedHat 7: yum install -y container-selinux\nUsing yum on CentOS/RedHat 8: yum install -y container-selinux\nDark Sites For environments where yum will not work, obtain the supported version of container-selinux for your OS version and install it.\nThe container-selinux RPMs for CentOS/RedHat 7 and 8 can be downloaded from https://centos.pkgs.org/7/centos-extras-x86_64/ and https://centos.pkgs.org/8-stream/centos-appstream-x86_64/, respectively.\nDeploying the CSM Authorization Proxy Server The first part of deploying CSM for Authorization is installing the proxy server. This activity and the administration of the proxy server will be owned by the storage administrator.\nThe CSM for Authorization proxy server is installed using a shell script after extracting from a tar archive.\nIf CSM for Authorization is being installed on a system where SELinux is enabled, you must ensure the proper SELinux policies have been installed.\nShell Script Installer The easiest way to obtain the tar archive with the shell script installer is directly from the GitHub repository’s releases section.\nAlternatively, the tar archive can be built from source by cloning the GitHub repository and using the following Makefile targets to build the installer:\nmake dist build-installer rpm package The build-installer step creates a binary at karavi-authorization/bin/deploy and embeds all components required for installation. The rpm step generates an RPM package and stores it at karavi-authorization/deploy/rpm/x86_64/. The package step bundles the install script, authorization package, pre-downloaded K3s-SELinux packages, and policies folder together for the installation in the packages/ directory. This allows CSM for Authorization to be installed in network-restricted environments.\nA Storage Administrator can execute the shell script, install_karavi_auth.sh as a root user or via sudo.\nInstalling the RPM Before installing the rpm, some network and security configuration inputs need to be provided in json format. The json file should be created in the location $HOME/.karavi/config.json having the following contents:\n{ \"web\": { \"jwtsigningsecret\": \"secret\" }, \"proxy\": { \"host\": \":8080\" }, \"zipkin\": { \"collectoruri\": \"http://zipkin-addr:9411/api/v2/spans\", \"probability\": 1 }, \"certificate\": { \"keyFile\": \"path_to_private_key_file\", \"crtFile\": \"path_to_host_cert_file\", \"rootCertificate\": \"path_to_root_CA_file\" }, \"hostname\": \"DNS-hostname\" } In an instance where a secure deployment is not required, an insecure deployment is possible. Please note that self-signed certificates will be created for you using cert-manager to allow TLS encryption for communication on the CSM for Authorization proxy server. However, this is not recommended for production environments. For an insecure deployment, the json file in the location $HOME/.karavi/config.json only requires the following contents:\n{ \"hostname\": \"DNS-hostname\" } Note:\nDNS-hostname refers to the hostname of the system in which the CSM for Authorization server will be installed. This hostname can be found by running nslookup \u003cIP_address\u003e There are a number of ways to create certificates. In a production environment, certificates are usually created and managed by an IT administrator. Otherwise, certificates can be created using OpenSSL. To install the rpm package on the system, you must first extract the contents of the tar file with the command:\ntar -xvf karavi_authorization_\u003cversion\u003e Afterwards, you must enter the extracted folder’s directory and run the shell script:\ncd karavi_authorization_\u003cversion\u003e sh install_karavi_auth.sh As an option, on version 1.6.0, the Nodeports for the ingress controller can be specified:\nsh install_karavi_auth.sh --traefik_web_port \u003cweb port number\u003e --traefik_websecure_port \u003cwebsecure port number\u003e Ex.:\nsh install_karavi_auth.sh --traefik_web_port 30001 --traefik_websecure_port 30002 After installation, application data will be stored on the system under /var/lib/rancher/k3s/storage/.\nIf errors occur during installation, review the Troubleshooting section.\nConfiguring the CSM for Authorization Proxy Server The first part of CSM for Authorization deployment is to configure the proxy server. This is controlled by the Storage Administrator.\nPlease follow the steps outlined in the proxy server configuration.\nConfiguring a Dell CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the supported CSI drivers. This is controlled by the Kubernetes tenant administrator.\nPlease follow the steps outlined in PowerFlex, PowerMax, or PowerScale to configure the CSI Driver to work with the Authorization sidecar.\nUpdating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically:\nParameter Type Default Description web.jwtsigningsecret String “secret” The secret used to sign JWT tokens Updating configuration parameters can be done by editing the karavi-config-secret on the CSM for the Authorization Server. The secret can be queried using k3s and kubectl like so:\nk3s kubectl -n karavi get secret/karavi-config-secret To update or add parameters, you must edit the base64 encoded data in the secret. The karavi-config-secret data can be decoded like so:\nk3s kubectl -n karavi get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d Save the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so:\ncat \u003cfile\u003e | base64 Copy the new, encoded data and edit the karavi-config-secret with the new data. Run this command to edit the secret:\nk3s kubectl -n karavi edit secret/karavi-config-secret Replace the data in config.yaml under the data field with your new, encoded data. Save the changes and CSM for Authorization will read the changed secret.\nNote: If you are updating the signing secret, the tenants need to be updated with new tokens via the karavictl generate token command like so. The --insecure flag is required if certificates were not provided in $HOME/.karavi/config.json\nkaravictl generate token --tenant $TenantName --insecure --addr DNS-hostname | sed -e 's/\"Token\": //' -e 's/[{}\"]//g' -e 's/\\\\n/\\n/g' | kubectl -n $namespace apply -f - CSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the karavi-config-secret but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM for Authorization logging settings during runtime, run the below command on the K3s cluster, make your changes, and save the updated configmap data.\nk3s kubectl -n karavi edit configmap/csm-config-params This edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion:\nkubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params Using PowerFlex as an example, kubectl -n vxflexos edit configmap/vxflexos-config-params can be used to update the logging level of the sidecar-proxy and the driver.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization RPM deployment\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v1/authorization/deployment/rpm/","tags":"","title":"RPM"},{"body":"This section outlines the deployment steps for Container Storage Modules (CSM) for Authorization. The deployment of CSM for Authorization is handled in 2 parts:\nDeploying the CSM for Authorization proxy server, to be controlled by storage administrators Configuring one to many supported Dell CSI drivers with CSM for Authorization Prerequisites The CSM for Authorization proxy server requires a Linux host with the following minimum resource allocations:\n32 GB of memory 4 CPU 200 GB local storage The following package needs to be installed on the Linux host:\ncontainer-selinux Use the appropriate package manager on the machine to install the package.\nUsing yum on CentOS/RedHat 7: yum install -y container-selinux\nUsing yum on CentOS/RedHat 8: yum install -y container-selinux\nDark Sites For environments where yum will not work, obtain the supported version of container-selinux for your OS version and install it.\nThe container-selinux RPMs for CentOS/RedHat 7 and 8 can be downloaded from https://centos.pkgs.org/7/centos-extras-x86_64/ and https://centos.pkgs.org/8-stream/centos-appstream-x86_64/, respectively.\nDeploying the CSM Authorization Proxy Server The first part of deploying CSM for Authorization is installing the proxy server. This activity and the administration of the proxy server will be owned by the storage administrator.\nThe CSM for Authorization proxy server is installed using a shell script after extracting from a tar archive.\nIf CSM for Authorization is being installed on a system where SELinux is enabled, you must ensure the proper SELinux policies have been installed.\nShell Script Installer The easiest way to obtain the tar archive with the shell script installer is directly from the GitHub repository’s releases section.\nAlternatively, the tar archive can be built from source by cloning the GitHub repository and using the following Makefile targets to build the installer:\nmake dist build-installer rpm package The build-installer step creates a binary at karavi-authorization/bin/deploy and embeds all components required for installation. The rpm step generates an RPM package and stores it at karavi-authorization/deploy/rpm/x86_64/. The package step bundles the install script, authorization package, pre-downloaded K3s-SELinux packages, and policies folder together for the installation in the packages/ directory. This allows CSM for Authorization to be installed in network-restricted environments.\nA Storage Administrator can execute the shell script, install_karavi_auth.sh as a root user or via sudo.\nInstalling the RPM Before installing the rpm, some network and security configuration inputs need to be provided in json format. The json file should be created in the location $HOME/.karavi/config.json having the following contents:\n{ \"web\": { \"jwtsigningsecret\": \"secret\" }, \"proxy\": { \"host\": \":8080\" }, \"zipkin\": { \"collectoruri\": \"http://zipkin-addr:9411/api/v2/spans\", \"probability\": 1 }, \"certificate\": { \"keyFile\": \"path_to_private_key_file\", \"crtFile\": \"path_to_host_cert_file\", \"rootCertificate\": \"path_to_root_CA_file\" }, \"hostname\": \"DNS-hostname\" } In an instance where a secure deployment is not required, an insecure deployment is possible. Please note that self-signed certificates will be created for you using cert-manager to allow TLS encryption for communication on the CSM for Authorization proxy server. However, this is not recommended for production environments. For an insecure deployment, the json file in the location $HOME/.karavi/config.json only requires the following contents:\n{ \"hostname\": \"DNS-hostname\" } Note:\nDNS-hostname refers to the hostname of the system in which the CSM for Authorization server will be installed. This hostname can be found by running nslookup \u003cIP_address\u003e There are a number of ways to create certificates. In a production environment, certificates are usually created and managed by an IT administrator. Otherwise, certificates can be created using OpenSSL. To install the rpm package on the system, you must first extract the contents of the tar file with the command:\ntar -xvf karavi_authorization_\u003cversion\u003e Afterwards, you must enter the extracted folder’s directory and run the shell script:\ncd karavi_authorization_\u003cversion\u003e sh install_karavi_auth.sh As an option, on version 1.6.0, the Nodeports for the ingress controller can be specified:\nsh install_karavi_auth.sh --traefik_web_port \u003cweb port number\u003e --traefik_websecure_port \u003cwebsecure port number\u003e Ex.:\nsh install_karavi_auth.sh --traefik_web_port 30001 --traefik_websecure_port 30002 After installation, application data will be stored on the system under /var/lib/rancher/k3s/storage/.\nIf errors occur during installation, review the Troubleshooting section.\nConfiguring the CSM for Authorization Proxy Server The first part of CSM for Authorization deployment is to configure the proxy server. This is controlled by the Storage Administrator.\nPlease follow the steps outlined in the proxy server configuration.\nConfiguring a Dell CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the supported CSI drivers. This is controlled by the Kubernetes tenant administrator.\nPlease follow the steps outlined in PowerFlex, PowerMax, or PowerScale to configure the CSI Driver to work with the Authorization sidecar.\nUpdating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically:\nParameter Type Default Description web.jwtsigningsecret String “secret” The secret used to sign JWT tokens Updating configuration parameters can be done by editing the karavi-config-secret on the CSM for the Authorization Server. The secret can be queried using k3s and kubectl like so:\nk3s kubectl -n karavi get secret/karavi-config-secret To update or add parameters, you must edit the base64 encoded data in the secret. The karavi-config-secret data can be decoded like so:\nk3s kubectl -n karavi get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d Save the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so:\ncat \u003cfile\u003e | base64 Copy the new, encoded data and edit the karavi-config-secret with the new data. Run this command to edit the secret:\nk3s kubectl -n karavi edit secret/karavi-config-secret Replace the data in config.yaml under the data field with your new, encoded data. Save the changes and CSM for Authorization will read the changed secret.\nNote: If you are updating the signing secret, the tenants need to be updated with new tokens via the karavictl generate token command like so. The --insecure flag is required if certificates were not provided in $HOME/.karavi/config.json\nkaravictl generate token --tenant $TenantName --insecure --addr DNS-hostname | sed -e 's/\"Token\": //' -e 's/[{}\"]//g' -e 's/\\\\n/\\n/g' | kubectl -n $namespace apply -f - CSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the karavi-config-secret but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM for Authorization logging settings during runtime, run the below command on the K3s cluster, make your changes, and save the updated configmap data.\nk3s kubectl -n karavi edit configmap/csm-config-params This edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion:\nkubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params Using PowerFlex as an example, kubectl -n vxflexos edit configmap/vxflexos-config-params can be used to update the logging level of the sidecar-proxy and the driver.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization RPM deployment\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v2/authorization/deployment/rpm/","tags":"","title":"RPM"},{"body":" The CSM Authorization karavictl CLI is no longer actively maintained or supported. It will be deprecated in CSM 2.0.\nThis section outlines the deployment steps for Container Storage Modules (CSM) for Authorization. The deployment of CSM for Authorization is handled in 2 parts:\nDeploying the CSM for Authorization proxy server, to be controlled by storage administrators Configuring one to many supported Dell CSI drivers with CSM for Authorization Prerequisites The CSM for Authorization proxy server requires a Linux host with the following minimum resource allocations:\n32 GB of memory 4 CPU 200 GB local storage The following package needs to be installed on the Linux host:\ncontainer-selinux Use the appropriate package manager on the machine to install the package.\nUsing yum on CentOS/RedHat 7: yum install -y container-selinux\nUsing yum on CentOS/RedHat 8: yum install -y container-selinux\nDark Sites For environments where yum will not work, obtain the supported version of container-selinux for your OS version and install it.\nThe container-selinux RPMs for CentOS/RedHat 7 and 8 can be downloaded from https://centos.pkgs.org/7/centos-extras-x86_64/ and https://centos.pkgs.org/8-stream/centos-appstream-x86_64/, respectively.\nDeploying the CSM Authorization Proxy Server The first part of deploying CSM for Authorization is installing the proxy server. This activity and the administration of the proxy server will be owned by the storage administrator.\nThe CSM for Authorization proxy server is installed using a shell script after extracting from a tar archive.\nIf CSM for Authorization is being installed on a system where SELinux is enabled, you must ensure the proper SELinux policies have been installed.\nShell Script Installer The easiest way to obtain the tar archive with the shell script installer is directly from the GitHub repository’s releases section.\nAlternatively, the tar archive can be built from source by cloning the GitHub repository and using the following Makefile targets to build the installer:\nmake dist build-installer rpm package The build-installer step creates a binary at karavi-authorization/bin/deploy and embeds all components required for installation. The rpm step generates an RPM package and stores it at karavi-authorization/deploy/rpm/x86_64/. The package step bundles the install script, authorization package, pre-downloaded K3s-SELinux packages, and policies folder together for the installation in the packages/ directory. This allows CSM for Authorization to be installed in network-restricted environments.\nA Storage Administrator can execute the shell script, install_karavi_auth.sh as a root user or via sudo.\nInstalling the RPM Before installing the rpm, some network and security configuration inputs need to be provided in json format. The json file should be created in the location $HOME/.karavi/config.json having the following contents:\n{ \"web\": { \"jwtsigningsecret\": \"secret\" }, \"proxy\": { \"host\": \":8080\" }, \"zipkin\": { \"collectoruri\": \"http://DNS-hostname:9411/api/v2/spans\", \"probability\": 1 }, \"certificate\": { \"keyFile\": \"path_to_private_key_file\", \"crtFile\": \"path_to_host_cert_file\", \"rootCertificate\": \"path_to_root_CA_file\" }, \"hostname\": \"DNS-hostname\" } In an instance where a secure deployment is not required, an insecure deployment is possible. Please note that self-signed certificates will be created for you using cert-manager to allow TLS encryption for communication on the CSM for Authorization proxy server. However, this is not recommended for production environments. For an insecure deployment, the json file in the location $HOME/.karavi/config.json only requires the following contents:\n{ \"hostname\": \"DNS-hostname\" } Note:\nDNS-hostname refers to the hostname of the system in which the CSM for Authorization server will be installed. This hostname can be found by running nslookup \u003cIP_address\u003e There are a number of ways to create certificates. In a production environment, certificates are usually created and managed by an IT administrator. Otherwise, certificates can be created using OpenSSL. In order to configure secure grpc connectivity, an additional subdomain in the format grpc.DNS-hostname is also required. All traffic from grpc.DNS-hostname needs to be routed to DNS-hostname address, this can be configured by adding a new DNS entry for grpc.DNS-hostname or providing a temporary path in the systems /etc/hosts file. Note: The certificate provided in crtFile should be valid for both the DNS-hostname and the grpc.DNS-hostname address.\nFor example, create the certificate config file with alternate names (to include DNS-hostname and grpc.DNS-hostname) and then create the .crt file: ``` CN = DNS-hostname subjectAltName = @alt_names [alt_names] DNS.1 = grpc.DNS-hostname.com $ openssl x509 -req -in cert_request_file.csr -CA root_CA.pem -CAkey private_key_File.key -CAcreateserial -out DNS-hostname.com.crt -days 365 -sha256 ``` To install the rpm package on the system, you must first extract the contents of the tar file with the command:\ntar -xvf karavi_authorization_\u003cversion\u003e Afterwards, you must enter the extracted folder’s directory and run the shell script:\ncd karavi_authorization_\u003cversion\u003e sh install_karavi_auth.sh As an option, on version 1.6.0, the Nodeports for the ingress controller can be specified:\nsh install_karavi_auth.sh --traefik_web_port \u003cweb port number\u003e --traefik_websecure_port \u003cwebsecure port number\u003e Ex.: sh install_karavi_auth.sh --traefik_web_port 30001 --traefik_websecure_port 30002 5. After installation, application data will be stored on the system under `/var/lib/rancher/k3s/storage/`. If errors occur during installation, review the [Troubleshooting](../../troubleshooting) section. ## Configuring the CSM for Authorization Proxy Server The first part of CSM for Authorization deployment is to configure the proxy server. This is controlled by the Storage Administrator. Please follow the steps outlined in the [proxy server](../../configuration/proxy-server) configuration. ## Configuring a Dell CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the [supported](../../../authorization#supported-csi-drivers) CSI drivers. This is controlled by the Kubernetes tenant administrator. Please follow the steps outlined in [PowerFlex](../../configuration/powerflex), [PowerMax](../../configuration/powermax), or [PowerScale](../../configuration/powerscale) to configure the CSI Driver to work with the Authorization sidecar. ## Updating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically: | Parameter | Type | Default | Description | | --------- | ---- | ------- | ----------- | | web.jwtsigningsecret | String | \"secret\" |The secret used to sign JWT tokens | Updating configuration parameters can be done by editing the `karavi-config-secret` on the CSM for the Authorization Server. The secret can be queried using k3s and kubectl like so: `k3s kubectl -n karavi get secret/karavi-config-secret` To update or add parameters, you must edit the base64 encoded data in the secret. The` karavi-config-secret` data can be decoded like so: `k3s kubectl -n karavi get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d` Save the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so: `cat \u003cfile\u003e | base64` Copy the new, encoded data and edit the `karavi-config-secret` with the new data. Run this command to edit the secret: `k3s kubectl -n karavi edit secret/karavi-config-secret` Replace the data in `config.yaml` under the `data` field with your new, encoded data. Save the changes and CSM for Authorization will read the changed secret. \u003e__Note__: If you are updating the signing secret, the tenants need to be updated with new tokens via the `karavictl generate token` command like so. The `--insecure` flag is required if certificates were not provided in `$HOME/.karavi/config.json` `karavictl generate token --tenant $TenantName --insecure --addr grpc.DNS-hostname:443 | sed -e 's/\"Token\": //' -e 's/[{}\"]//g' -e 's/\\\\n/\\n/g' | kubectl -n $namespace apply -f -` ## CSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the `karavi-config-secret` but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM for Authorization logging settings during runtime, run the below command on the K3s cluster, make your changes, and save the updated configmap data. k3s kubectl -n karavi edit configmap/csm-config-params\nThis edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion: kubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params\nUsing PowerFlex as an example, `kubectl -n vxflexos edit configmap/vxflexos-config-params` can be used to update the logging level of the sidecar-proxy and the driver. ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization RPM deployment\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v3/authorization/deployment/rpm/","tags":"","title":"RPM"},{"body":"This section outlines the uninstallation steps for Application Mobility.\nUninstall the Application Mobility Helm Chart This command removes all the Kubernetes components associated with the chart.\nhelm delete [APPLICATION_MOBILITY_NAME] --namespace [APPLICATION_MOBILITY_NAMESPACE] ","categories":"","description":"Uninstallation\n","excerpt":"Uninstallation\n","ref":"/csm-docs/docs/applicationmobility/uninstallation/","tags":"","title":"Uninstallation"},{"body":" The CSM Authorization RPM will be deprecated in a future release. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nThis section outlines the uninstallation steps for Container Storage Modules (CSM) for Authorization.\nUninstalling the RPM To uninstall the rpm package on the system, you must first uninstall the K3s SELinux package if SELinux is enabled. To uninstall the K3s SELinux package, run:\nrpm -e k3s-selinux To uninstall the CSM Authorization rpm package on the system, run:\nrpm -e \u003crpm_file_name\u003e Uninstalling the sidecar-proxy in the CSI Driver To uninstall the sidecar-proxy in the CSI Driver, uninstall the driver and reinstall the driver using the original configuration secret.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization Uninstallation\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/authorization/uninstallation/","tags":"","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Resiliency.\nUninstalling the sidecar in the CSI Driver To uninstall the sidecar in the CSI Driver, uninstall the driver and reinstall the driver with the podmon feature disabled.\n","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency Uninstallation\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency Uninstallation\n","ref":"/csm-docs/docs/resiliency/uninstallation/","tags":"","title":"Uninstallation"},{"body":"Cleanup Kubernetes Worker Hosts Login to each worker host and perform these steps:\nRemove directory /root/.driver-sec\nThis directory was created when a CSI driver with Encryption first ran on the host.\nRemove entry from /root/.ssh/authorized_keys\nThis is an entry added when a CSI driver with Encryption first ran on the host. It ends with driver-sec, similarly to:\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDGvSWmTL7NORRDPAvtbMbvoHUBLnen9bRtJePbGk1boJ4XK39Qdvo2zFHZ/6t2+dSL7xKo2kcxX3ovj3RyOPuqNCob\r5CLYyuIqduooy+eSP8S1i0FbiDHvH/52yHglnGkBb8g8fmoMolYGW7k35mKOEItKlXruP5/hpP0rBDfBfrxe/K4aHicxv6GylP+uTSBjdj7bZrdgRAIlmDyIdvU4oU6L\rK9PDW5rufArlrZHaToHXLMbXbqswD08rgFt3tLiXjj2GgvU8ifWYYAeuijMp+hwwE0dYv45EgUNTlXUa7x2STFZrVn8MFkLKjtZ60Qjbb4JoijRpBQ5XEUkW9UoeGbV2\rs+lCpZ2bMkmdda/0UC1ckvyrLkD0yQotb8gafizdX+WrQRE+iqUv/NQ2mrSEHtLgvuvgZ3myFU5chRv498YxglYZsAZUdCQI2hQt+7smjYMaM0V200UT741U9lIlYxza\rocI5t+n01dWeVOCSOH/Q3uXxHKnFvWVZh7m6583R9LfdGfwshsnx4CNz22kp69hzwBPxehR+U/VXkDUWnoQgI8NSPc0fFyU58yLHnl91XT9alz8qrkFK7oggKy5RRX7c\rVQrpjsCPCu3fpVjvvwfspVOftbn/sNgY1J3lz0pdgvJ3yQs6pa+DODQyin5Rt//19rIGifPxi/Hk/k49Vw== driver-sec It can be removed with sed -i '/^ssh-rsa .* driver-sec$/d' /root/.ssh/authorized_keys.\nRemove Kubernetes Resources Remove the resources created in Kubernetes cluster for Encryption.\nRemove Vault Server Configuration Remove the configuration created in the Vault server for Encryption.\nRemove Rekey Controller Remove the resources created during the installation of the Rekey Controller.\n","categories":"","description":"Uninstallation\n","excerpt":"Uninstallation\n","ref":"/csm-docs/docs/secure/encryption/uninstallation/","tags":"","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Application Mobility.\nUninstall the Application Mobility Helm Chart This command removes all the Kubernetes components associated with the chart.\nhelm delete [APPLICATION_MOBILITY_NAME] --namespace [APPLICATION_MOBILITY_NAMESPACE] ","categories":"","description":"Uninstallation\n","excerpt":"Uninstallation\n","ref":"/csm-docs/v1/applicationmobility/uninstallation/","tags":"","title":"Uninstallation"},{"body":" The CSM Authorization RPM will be deprecated in a future release. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nThis section outlines the uninstallation steps for Container Storage Modules (CSM) for Authorization.\nUninstalling the RPM To uninstall the rpm package on the system, you must first uninstall the K3s SELinux package if SELinux is enabled. To uninstall the K3s SELinux package, run:\nrpm -e k3s-selinux To uninstall the CSM Authorization rpm package on the system, run:\nrpm -e \u003crpm_file_name\u003e Uninstalling the sidecar-proxy in the CSI Driver To uninstall the sidecar-proxy in the CSI Driver, uninstall the driver and reinstall the driver using the original configuration secret.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization Uninstallation\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v1/authorization/uninstallation/","tags":"","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Resiliency.\nUninstalling the sidecar in the CSI Driver To uninstall the sidecar in the CSI Driver, uninstall the driver and reinstall the driver with the podmon feature disabled.\n","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency Uninstallation\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency Uninstallation\n","ref":"/csm-docs/v1/resiliency/uninstallation/","tags":"","title":"Uninstallation"},{"body":"Cleanup Kubernetes Worker Hosts Login to each worker host and perform these steps:\nRemove directory /root/.driver-sec\nThis directory was created when a CSI driver with Encryption first ran on the host.\nRemove entry from /root/.ssh/authorized_keys\nThis is an entry added when a CSI driver with Encryption first ran on the host. It ends with driver-sec, similarly to:\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDGvSWmTL7NORRDPAvtbMbvoHUBLnen9bRtJePbGk1boJ4XK39Qdvo2zFHZ/6t2+dSL7xKo2kcxX3ovj3RyOPuqNCob 5CLYyuIqduooy+eSP8S1i0FbiDHvH/52yHglnGkBb8g8fmoMolYGW7k35mKOEItKlXruP5/hpP0rBDfBfrxe/K4aHicxv6GylP+uTSBjdj7bZrdgRAIlmDyIdvU4oU6L K9PDW5rufArlrZHaToHXLMbXbqswD08rgFt3tLiXjj2GgvU8ifWYYAeuijMp+hwwE0dYv45EgUNTlXUa7x2STFZrVn8MFkLKjtZ60Qjbb4JoijRpBQ5XEUkW9UoeGbV2 s+lCpZ2bMkmdda/0UC1ckvyrLkD0yQotb8gafizdX+WrQRE+iqUv/NQ2mrSEHtLgvuvgZ3myFU5chRv498YxglYZsAZUdCQI2hQt+7smjYMaM0V200UT741U9lIlYxza ocI5t+n01dWeVOCSOH/Q3uXxHKnFvWVZh7m6583R9LfdGfwshsnx4CNz22kp69hzwBPxehR+U/VXkDUWnoQgI8NSPc0fFyU58yLHnl91XT9alz8qrkFK7oggKy5RRX7c VQrpjsCPCu3fpVjvvwfspVOftbn/sNgY1J3lz0pdgvJ3yQs6pa+DODQyin5Rt//19rIGifPxi/Hk/k49Vw== driver-sec It can be removed with sed -i '/^ssh-rsa .* driver-sec$/d' /root/.ssh/authorized_keys.\nRemove Kubernetes Resources Remove the resources created in Kubernetes cluster for Encryption.\nRemove Vault Server Configuration Remove the configuration created in the Vault server for Encryption.\nRemove Rekey Controller Remove the resources created during the installation of the Rekey Controller.\n","categories":"","description":"Uninstallation\n","excerpt":"Uninstallation\n","ref":"/csm-docs/v1/secure/encryption/uninstallation/","tags":"","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Application Mobility.\nUninstall the Application Mobility Helm Chart This command removes all the Kubernetes components associated with the chart.\nhelm delete [APPLICATION_MOBILITY_NAME] --namespace [APPLICATION_MOBILITY_NAMESPACE] ","categories":"","description":"Uninstallation\n","excerpt":"Uninstallation\n","ref":"/csm-docs/v2/applicationmobility/uninstallation/","tags":"","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Authorization.\nUninstalling the RPM To uninstall the rpm package on the system, you must first uninstall the K3s SELinux package if SELinux is enabled. To uninstall the K3s SELinux package, run:\nrpm -e k3s-selinux To uninstall the CSM Authorization rpm package on the system, run:\nrpm -e \u003crpm_file_name\u003e Uninstalling the sidecar-proxy in the CSI Driver To uninstall the sidecar-proxy in the CSI Driver, uninstall the driver and reinstall the driver using the original configuration secret.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization Uninstallation\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v2/authorization/uninstallation/","tags":"","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Resiliency.\nUninstalling the sidecar in the CSI Driver To uninstall the sidecar in the CSI Driver, uninstall the driver and reinstall the driver with the podmon feature disabled.\n","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency Uninstallation\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency Uninstallation\n","ref":"/csm-docs/v2/resiliency/uninstallation/","tags":"","title":"Uninstallation"},{"body":"Cleanup Kubernetes Worker Hosts Login to each worker host and perform these steps:\nRemove directory /root/.driver-sec\nThis directory was created when a CSI driver with Encryption first ran on the host.\nRemove entry from /root/.ssh/authorized_keys\nThis is an entry added when a CSI driver with Encryption first ran on the host. It ends with driver-sec, similarly to:\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDGvSWmTL7NORRDPAvtbMbvoHUBLnen9bRtJePbGk1boJ4XK39Qdvo2zFHZ/6t2+dSL7xKo2kcxX3ovj3RyOPuqNCob 5CLYyuIqduooy+eSP8S1i0FbiDHvH/52yHglnGkBb8g8fmoMolYGW7k35mKOEItKlXruP5/hpP0rBDfBfrxe/K4aHicxv6GylP+uTSBjdj7bZrdgRAIlmDyIdvU4oU6L K9PDW5rufArlrZHaToHXLMbXbqswD08rgFt3tLiXjj2GgvU8ifWYYAeuijMp+hwwE0dYv45EgUNTlXUa7x2STFZrVn8MFkLKjtZ60Qjbb4JoijRpBQ5XEUkW9UoeGbV2 s+lCpZ2bMkmdda/0UC1ckvyrLkD0yQotb8gafizdX+WrQRE+iqUv/NQ2mrSEHtLgvuvgZ3myFU5chRv498YxglYZsAZUdCQI2hQt+7smjYMaM0V200UT741U9lIlYxza ocI5t+n01dWeVOCSOH/Q3uXxHKnFvWVZh7m6583R9LfdGfwshsnx4CNz22kp69hzwBPxehR+U/VXkDUWnoQgI8NSPc0fFyU58yLHnl91XT9alz8qrkFK7oggKy5RRX7c VQrpjsCPCu3fpVjvvwfspVOftbn/sNgY1J3lz0pdgvJ3yQs6pa+DODQyin5Rt//19rIGifPxi/Hk/k49Vw== driver-sec It can be removed with sed -i '/^ssh-rsa .* driver-sec$/d' /root/.ssh/authorized_keys.\nRemove Kubernetes Resources Remove the resources created in Kubernetes cluster for Encryption.\nRemove Vault Server Configuration Remove the configuration created in the Vault server for Encryption.\nRemove Rekey Controller Remove the resources created during the installation of the Rekey Controller.\n","categories":"","description":"Uninstallation\n","excerpt":"Uninstallation\n","ref":"/csm-docs/v2/secure/encryption/uninstallation/","tags":"","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Application Mobility.\nUninstall the Application Mobility Helm Chart This command removes all the Kubernetes components associated with the chart.\n$ helm delete [APPLICATION_MOBILITY_NAME] --namespace [APPLICATION_MOBILITY_NAMESPACE] ","categories":"","description":"Uninstallation\n","excerpt":"Uninstallation\n","ref":"/csm-docs/v3/applicationmobility/uninstallation/","tags":"","title":"Uninstallation"},{"body":" The CSM Authorization RPM is no longer actively maintained or supported. It will be deprecated in CSM 2.0. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nThis section outlines the uninstallation steps for Container Storage Modules (CSM) for Authorization.\nUninstalling the RPM To uninstall the rpm package on the system, you must first uninstall the K3s SELinux package if SELinux is enabled. To uninstall the K3s SELinux package, run:\nrpm -e k3s-selinux To uninstall the CSM Authorization rpm package on the system, run:\nrpm -e \u003crpm_file_name\u003e Uninstalling the sidecar-proxy in the CSI Driver To uninstall the sidecar-proxy in the CSI Driver, uninstall the driver and reinstall the driver using the original configuration secret.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization Uninstallation\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v3/authorization/uninstallation/","tags":"","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Resiliency.\nUninstalling the sidecar in the CSI Driver To uninstall the sidecar in the CSI Driver, uninstall the driver and reinstall the driver with the podmon feature disabled.\n","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency Uninstallation\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency Uninstallation\n","ref":"/csm-docs/v3/resiliency/uninstallation/","tags":"","title":"Uninstallation"},{"body":"Cleanup Kubernetes Worker Hosts Login to each worker host and perform these steps:\nRemove directory /root/.driver-sec\nThis directory was created when a CSI driver with Encryption first ran on the host.\nRemove entry from /root/.ssh/authorized_keys\nThis is an entry added when a CSI driver with Encryption first ran on the host. It ends with driver-sec, similarly to:\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDGvSWmTL7NORRDPAvtbMbvoHUBLnen9bRtJePbGk1boJ4XK39Qdvo2zFHZ/6t2+dSL7xKo2kcxX3ovj3RyOPuqNCob 5CLYyuIqduooy+eSP8S1i0FbiDHvH/52yHglnGkBb8g8fmoMolYGW7k35mKOEItKlXruP5/hpP0rBDfBfrxe/K4aHicxv6GylP+uTSBjdj7bZrdgRAIlmDyIdvU4oU6L K9PDW5rufArlrZHaToHXLMbXbqswD08rgFt3tLiXjj2GgvU8ifWYYAeuijMp+hwwE0dYv45EgUNTlXUa7x2STFZrVn8MFkLKjtZ60Qjbb4JoijRpBQ5XEUkW9UoeGbV2 s+lCpZ2bMkmdda/0UC1ckvyrLkD0yQotb8gafizdX+WrQRE+iqUv/NQ2mrSEHtLgvuvgZ3myFU5chRv498YxglYZsAZUdCQI2hQt+7smjYMaM0V200UT741U9lIlYxza ocI5t+n01dWeVOCSOH/Q3uXxHKnFvWVZh7m6583R9LfdGfwshsnx4CNz22kp69hzwBPxehR+U/VXkDUWnoQgI8NSPc0fFyU58yLHnl91XT9alz8qrkFK7oggKy5RRX7c VQrpjsCPCu3fpVjvvwfspVOftbn/sNgY1J3lz0pdgvJ3yQs6pa+DODQyin5Rt//19rIGifPxi/Hk/k49Vw== driver-sec It can be removed with sed -i '/^ssh-rsa .* driver-sec$/d' /root/.ssh/authorized_keys.\nRemove Kubernetes Resources Remove the resources created in Kubernetes cluster for Encryption.\nRemove Vault Server Configuration Remove the configuration created in the Vault server for Encryption.\nRemove Rekey Controller Remove the resources created during the installation of the Rekey Controller.\n","categories":"","description":"Uninstallation\n","excerpt":"Uninstallation\n","ref":"/csm-docs/v3/secure/encryption/uninstallation/","tags":"","title":"Uninstallation"},{"body":"CSM for Resiliency is primarily designed to detect pod failures due to some kind of node failure or node communication failure. The diagram below shows the hardware environment that is assumed in the design.\nA Kubernetes Control Plane is assumed to exist that provides the K8S API service used by CSM for Resiliency. There is an arbitrary number of worker nodes (two are shown in the diagram) that are connected to the Control Plane through a K8S Control Plane IP Network.\nThe worker nodes (e.g. Node1 and Node2) can run a mix of CSM for Resiliency monitored Application Pods as well as unmonitored Application Pods. Monitored Pods are designated by a specific label that is applied to each monitored pod. The label key and value are configurable for each driver type when CSM for Resiliency is installed and must be unique for each driver instance.\nThe Worker Nodes are assumed to also have a connection to a Storage System Array (such as PowerFlex.) It is often preferred that a separate network be used for storage access from the network used by the K8S control plane, and CSM for Resiliency takes advantage of the separate networks when available.\nAnti Use-Cases CSM for Resiliency does not generally try to handle any of the following errors:\nFailure of the Kubernetes control plane, the etcd database used by Kubernetes, or the like. Kubernetes is generally designed to provide a highly available container orchestration system, and it is assumed clients follow the standard and/or best practices in configuring their Kubernetes deployments.\nCSM for Resiliency is generally not designed to take action upon a failure solely of the Application Pod(s). Applications are still responsible for detecting and providing recovery mechanisms should their application fail. There are some specific recommendations for applications to be monitored by CSM for Resiliency that are described later.\nFailure Model CSM for Resiliency’s design is focused on detecting the following types of hardware failures, and when they occur, moving protected pods to hardware that is functioning correctly:\nNode failure. Node failure is defined to be similar to a Power Failure to the node which causes it to cease operation. This is differentiated from Node Communication Failures which require different treatments. Node failures are generally discovered by receipt of a Node event with a NoSchedule or NoExecute taint, or detection of such a taint when retrieving the Node via the K8S API.\nGenerally, it is difficult to distinguish from the outside if a node is truly down (not executing) versus if it has lost connectivity on all its interfaces. (We might add capabilities in the future to query BIOS interfaces such as iDRAC, or perhaps periodically writing to file systems mounted in node-podmon to detect I/O failures, in order to get additional insight as to node status.) However, if the node has simply lost all outside communication paths, the protected pods are possibly still running. We refer to these pods as “zombie pods”. CSM for Resiliency is designed to deal with zombie pods in a way that prevents them from interfering with replacement pods it may have made by fencing the failed nodes and when communication is re-established to the node, going through a cleaning procedure to remove the zombie pod artifacts before allowing the node to go back into service.\nK8S Control Plane Network Failure. Control Plane Network Failure often has the same K8S failure signature (the node is tainted with NoSchedule or NoExecute). However, if there is a separate Array I/O interface, CSM for Resiliency can often detect that the Array I/O Network may be active even though the Control Plane Network is down.\nArray I/O Network failure is detected by polling the array to determine if the array has a healthy connection to the node. The capabilities to do this vary greatly by array and communication protocol type (Fibre Channel, iSCSI, NFS, NVMe, or PowerFlex SDC IP protocol). By monitoring the Array I/O Network separately from the Control Plane Network, CSM for Resiliency has two different indicators of whether the node is healthy or not.\nK8S Control Plane Failure. Control Plane Failure is defined as failure of kubelet in a given node. K8S Control Plane failures are generally discovered by receipt of a Node event with a NoSchedule or NoExecute taint, or detection of such a taint when retrieving the Node via the K8S API.\nCSI Driver node pods. CSM for Resiliency monitors CSI driver node pods.If for any reason the CSI Driver node pods fail and enter the Not Ready state, it will taint the node with NoSchedule value. This will disable kubernetes scheduler to schedule new workloads on the given node, hence avoid workloads that needed CSI Driver pods to be in Ready state.\n","categories":"","description":"CSM for Resiliency Use Cases\n","excerpt":"CSM for Resiliency Use Cases\n","ref":"/csm-docs/docs/resiliency/usecases/","tags":"","title":"Use Cases"},{"body":"CSM for Resiliency is primarily designed to detect pod failures due to some kind of node failure or node communication failure. The diagram below shows the hardware environment that is assumed in the design.\nA Kubernetes Control Plane is assumed to exist that provides the K8S API service used by CSM for Resiliency. There is an arbitrary number of worker nodes (two are shown in the diagram) that are connected to the Control Plane through a K8S Control Plane IP Network.\nThe worker nodes (e.g. Node1 and Node2) can run a mix of CSM for Resiliency monitored Application Pods as well as unmonitored Application Pods. Monitored Pods are designated by a specific label that is applied to each monitored pod. The label key and value are configurable for each driver type when CSM for Resiliency is installed and must be unique for each driver instance.\nThe Worker Nodes are assumed to also have a connection to a Storage System Array (such as PowerFlex.) It is often preferred that a separate network be used for storage access from the network used by the K8S control plane, and CSM for Resiliency takes advantage of the separate networks when available.\nAnti Use-Cases CSM for Resiliency does not generally try to handle any of the following errors:\nFailure of the Kubernetes control plane, the etcd database used by Kubernetes, or the like. Kubernetes is generally designed to provide a highly available container orchestration system, and it is assumed clients follow the standard and/or best practices in configuring their Kubernetes deployments.\nCSM for Resiliency is generally not designed to take action upon a failure solely of the Application Pod(s). Applications are still responsible for detecting and providing recovery mechanisms should their application fail. There are some specific recommendations for applications to be monitored by CSM for Resiliency that are described later.\nFailure Model CSM for Resiliency’s design is focused on detecting the following types of hardware failures, and when they occur, moving protected pods to hardware that is functioning correctly:\nNode failure. Node failure is defined to be similar to a Power Failure to the node which causes it to cease operation. This is differentiated from Node Communication Failures which require different treatments. Node failures are generally discovered by receipt of a Node event with a NoSchedule or NoExecute taint, or detection of such a taint when retrieving the Node via the K8S API.\nGenerally, it is difficult to distinguish from the outside if a node is truly down (not executing) versus if it has lost connectivity on all its interfaces. (We might add capabilities in the future to query BIOS interfaces such as iDRAC, or perhaps periodically writing to file systems mounted in node-podmon to detect I/O failures, in order to get additional insight as to node status.) However, if the node has simply lost all outside communication paths, the protected pods are possibly still running. We refer to these pods as “zombie pods”. CSM for Resiliency is designed to deal with zombie pods in a way that prevents them from interfering with replacement pods it may have made by fencing the failed nodes and when communication is re-established to the node, going through a cleaning procedure to remove the zombie pod artifacts before allowing the node to go back into service.\nK8S Control Plane Network Failure. Control Plane Network Failure often has the same K8S failure signature (the node is tainted with NoSchedule or NoExecute). However, if there is a separate Array I/O interface, CSM for Resiliency can often detect that the Array I/O Network may be active even though the Control Plane Network is down.\nArray I/O Network failure is detected by polling the array to determine if the array has a healthy connection to the node. The capabilities to do this vary greatly by array and communication protocol type (Fibre Channel, iSCSI, NFS, NVMe, or PowerFlex SDC IP protocol). By monitoring the Array I/O Network separately from the Control Plane Network, CSM for Resiliency has two different indicators of whether the node is healthy or not.\nK8S Control Plane Failure. Control Plane Failure is defined as failure of kubelet in a given node. K8S Control Plane failures are generally discovered by receipt of a Node event with a NoSchedule or NoExecute taint, or detection of such a taint when retrieving the Node via the K8S API.\nCSI Driver node pods. CSM for Resiliency monitors CSI driver node pods.If for any reason the CSI Driver node pods fail and enter the Not Ready state, it will taint the node with NoSchedule value. This will disable kubernetes scheduler to schedule new workloads on the given node, hence avoid workloads that needed CSI Driver pods to be in Ready state.\n","categories":"","description":"CSM for Resiliency Use Cases\n","excerpt":"CSM for Resiliency Use Cases\n","ref":"/csm-docs/v1/resiliency/usecases/","tags":"","title":"Use Cases"},{"body":"CSM for Resiliency is primarily designed to detect pod failures due to some kind of node failure or node communication failure. The diagram below shows the hardware environment that is assumed in the design.\nA Kubernetes Control Plane is assumed to exist that provides the K8S API service used by CSM for Resiliency. There is an arbitrary number of worker nodes (two are shown in the diagram) that are connected to the Control Plane through a K8S Control Plane IP Network.\nThe worker nodes (e.g. Node1 and Node2) can run a mix of CSM for Resiliency monitored Application Pods as well as unmonitored Application Pods. Monitored Pods are designated by a specific label that is applied to each monitored pod. The label key and value are configurable for each driver type when CSM for Resiliency is installed and must be unique for each driver instance.\nThe Worker Nodes are assumed to also have a connection to a Storage System Array (such as PowerFlex.) It is often preferred that a separate network be used for storage access from the network used by the K8S control plane, and CSM for Resiliency takes advantage of the separate networks when available.\nAnti Use-Cases CSM for Resiliency does not generally try to handle any of the following errors:\nFailure of the Kubernetes control plane, the etcd database used by Kubernetes, or the like. Kubernetes is generally designed to provide a highly available container orchestration system, and it is assumed clients follow the standard and/or best practices in configuring their Kubernetes deployments.\nCSM for Resiliency is generally not designed to take action upon a failure solely of the Application Pod(s). Applications are still responsible for detecting and providing recovery mechanisms should their application fail. There are some specific recommendations for applications to be monitored by CSM for Resiliency that are described later.\nFailure Model CSM for Resiliency’s design is focused on detecting the following types of hardware failures, and when they occur, moving protected pods to hardware that is functioning correctly:\nNode failure. Node failure is defined to be similar to a Power Failure to the node which causes it to cease operation. This is differentiated from Node Communication Failures which require different treatments. Node failures are generally discovered by receipt of a Node event with a NoSchedule or NoExecute taint, or detection of such a taint when retrieving the Node via the K8S API.\nGenerally, it is difficult to distinguish from the outside if a node is truly down (not executing) versus if it has lost connectivity on all its interfaces. (We might add capabilities in the future to query BIOS interfaces such as iDRAC, or perhaps periodically writing to file systems mounted in node-podmon to detect I/O failures, in order to get additional insight as to node status.) However, if the node has simply lost all outside communication paths, the protected pods are possibly still running. We refer to these pods as “zombie pods”. CSM for Resiliency is designed to deal with zombie pods in a way that prevents them from interfering with replacement pods it may have made by fencing the failed nodes and when communication is re-established to the node, going through a cleaning procedure to remove the zombie pod artifacts before allowing the node to go back into service.\nK8S Control Plane Network Failure. Control Plane Network Failure often has the same K8S failure signature (the node is tainted with NoSchedule or NoExecute). However, if there is a separate Array I/O interface, CSM for Resiliency can often detect that the Array I/O Network may be active even though the Control Plane Network is down.\nArray I/O Network failure is detected by polling the array to determine if the array has a healthy connection to the node. The capabilities to do this vary greatly by array and communication protocol type (Fibre Channel, iSCSI, NFS, NVMe, or PowerFlex SDC IP protocol). By monitoring the Array I/O Network separately from the Control Plane Network, CSM for Resiliency has two different indicators of whether the node is healthy or not.\nK8S Control Plane Failure. Control Plane Failure is defined as failure of kubelet in a given node. K8S Control Plane failures are generally discovered by receipt of a Node event with a NoSchedule or NoExecute taint, or detection of such a taint when retrieving the Node via the K8S API.\nCSI Driver node pods. CSM for Resiliency monitors CSI driver node pods.If for any reason the CSI Driver node pods fail and enter the Not Ready state, it will taint the node with NoSchedule value. This will disable kubernetes scheduler to schedule new workloads on the given node, hence avoid workloads that needed CSI Driver pods to be in Ready state.\n","categories":"","description":"CSM for Resiliency Use Cases\n","excerpt":"CSM for Resiliency Use Cases\n","ref":"/csm-docs/v2/resiliency/usecases/","tags":"","title":"Use Cases"},{"body":"CSM for Resiliency is primarily designed to detect pod failures due to some kind of node failure or node communication failure. The diagram below shows the hardware environment that is assumed in the design.\nA Kubernetes Control Plane is assumed to exist that provides the K8S API service used by CSM for Resiliency. There is an arbitrary number of worker nodes (two are shown in the diagram) that are connected to the Control Plane through a K8S Control Plane IP Network.\nThe worker nodes (e.g. Node1 and Node2) can run a mix of CSM for Resiliency monitored Application Pods as well as unmonitored Application Pods. Monitored Pods are designated by a specific label that is applied to each monitored pod. The label key and value are configurable for each driver type when CSM for Resiliency is installed and must be unique for each driver instance.\nThe Worker Nodes are assumed to also have a connection to a Storage System Array (such as PowerFlex.) It is often preferred that a separate network be used for storage access from the network used by the K8S control plane, and CSM for Resiliency takes advantage of the separate networks when available.\nAnti Use-Cases CSM for Resiliency does not generally try to handle any of the following errors:\nFailure of the Kubernetes control plane, the etcd database used by Kubernetes, or the like. Kubernetes is generally designed to provide a highly available container orchestration system, and it is assumed clients follow the standard and/or best practices in configuring their Kubernetes deployments.\nCSM for Resiliency is generally not designed to take action upon a failure solely of the Application Pod(s). Applications are still responsible for detecting and providing recovery mechanisms should their application fail. There are some specific recommendations for applications to be monitored by CSM for Resiliency that are described later.\nFailure Model CSM for Resiliency’s design is focused on detecting the following types of hardware failures, and when they occur, moving protected pods to hardware that is functioning correctly:\nNode failure. Node failure is defined to be similar to a Power Failure to the node which causes it to cease operation. This is differentiated from Node Communication Failures which require different treatments. Node failures are generally discovered by receipt of a Node event with a NoSchedule or NoExecute taint, or detection of such a taint when retrieving the Node via the K8S API.\nGenerally, it is difficult to distinguish from the outside if a node is truly down (not executing) versus if it has lost connectivity on all its interfaces. (We might add capabilities in the future to query BIOS interfaces such as iDRAC, or perhaps periodically writing to file systems mounted in node-podmon to detect I/O failures, in order to get additional insight as to node status.) However, if the node has simply lost all outside communication paths, the protected pods are possibly still running. We refer to these pods as “zombie pods”. CSM for Resiliency is designed to deal with zombie pods in a way that prevents them from interfering with replacement pods it may have made by fencing the failed nodes and when communication is re-established to the node, going through a cleaning procedure to remove the zombie pod artifacts before allowing the node to go back into service.\nK8S Control Plane Network Failure. Control Plane Network Failure often has the same K8S failure signature (the node is tainted with NoSchedule or NoExecute). However, if there is a separate Array I/O interface, CSM for Resiliency can often detect that the Array I/O Network may be active even though the Control Plane Network is down.\nArray I/O Network failure is detected by polling the array to determine if the array has a healthy connection to the node. The capabilities to do this vary greatly by array and communication protocol type (Fibre Channel, iSCSI, NFS, NVMe, or PowerFlex SDC IP protocol). By monitoring the Array I/O Network separately from the Control Plane Network, CSM for Resiliency has two different indicators of whether the node is healthy or not.\nK8S Control Plane Failure. Control Plane Failure is defined as failure of kubelet in a given node. K8S Control Plane failures are generally discovered by receipt of a Node event with a NoSchedule or NoExecute taint, or detection of such a taint when retrieving the Node via the K8S API.\nCSI Driver node pods. CSM for Resiliency monitors CSI driver node pods.If for any reason the CSI Driver node pods fail and enter the Not Ready state, it will taint the node with NoSchedule value. This will disable kubernetes scheduler to schedule new workloads on the given node, hence avoid workloads that needed CSI Driver pods to be in Ready state.\n","categories":"","description":"CSM for Resiliency Use Cases\n","excerpt":"CSM for Resiliency Use Cases\n","ref":"/csm-docs/v3/resiliency/usecases/","tags":"","title":"Use Cases"},{"body":"The COSI Driver for Dell ObjectScale can be deployed by using the provided Helm v3 charts on Kubernetes platform.\nThe Helm chart installs the following components in a Deployment in the specified namespace:\nCOSI Driver for ObjectScale Dependencies Installing any of the CSI Driver components using Helm requires a few utilities to be installed on the system running the installation.\nDependency Usage kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver. helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3. ℹ️ NOTE: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\nPrerequisites Install Kubernetes (see supported versions) Install the Driver Steps\nRun git clone -b main https://github.com/dell/helm-charts.git to clone the git repository. Ensure that you have created the namespace where you want to install the driver. You can run kubectl create namespace dell-cosi to create a new one. The use of dell-cosi as the namespace is just an example. You can choose any name for the namespace. Copy the charts/cosi/values.yaml into a new location with name my-cosi-values.yaml, to customize settings for installation. Create new file called my-cosi-configuration.yaml, and copy the settings available in the Configuration File page. Edit my-cosi-values.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the COSI driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository. Parameter Description Required Default provisioner.logLevel The logging level for the COSI driver provisioner. yes 4 provisioner.logFormat The logging format for the COSI driver provisioner. yes \"text\" provisioner.image.reposiotry COSI driver provisioner container image repository. yes \"docker.io/dell/cosi\" provisioner.image.tag COSI driver provisioner container image tag. yes \"v0.1.0\" provisioner.image.pullPolicy COSI driver provisioner container image pull policy. Maps 1-to-1 with Kubernetes image pull policy. yes \"IfNotPresent\" sidecar.verbosity The logging verbosity for the COSI driver sidecar, higher values are more verbose, possible values are integers from -2,147,483,648 to 2,147,483,647. Generally the range used is between -4 and 12. However, there may be cases where numbers outside that range might provide more information. For additional information, refer to the COSI sidecar documentation. yes 5 sidecar.image.reposiotry COSI driver sidecar container image repository. yes \"gcr.io/k8s-staging-sig-storage/objectstorage-sidecar/objectstorage-sidecar\" sidecar.image.tag COSI driver sidecar container image tag. yes \"v20230130-v0.1.0-24-gc0cf995\" sidecar.image.pullPolicy COSI driver sidecar container image pull policy. Maps 1-to-1 with Kubernetes image pull policy. yes \"IfNotPresent\" configuration.create Specifies whether a secret with driver configuration should be created If set to false, you must set configuration.secretName field to an existing configuration secret name. yes true configuration.secretName Name can be used to specify an existing secret name to use for the driver configuration or override the generated name. no \"cosi-config\" configuration.data Data should be provided when installing chart, it will be used to create the Secret with the driver configuration. configuration.create must be set to true for this to work. no \"\" ℹ️ NOTE:\nWhenever the configuration.secretName parameter changes in my-cosi-values.yaml user needs to reinstall the driver. Whenever the configuration.data parameter changes in my-cosi-values.yaml user needs to reinstall the driver. Install the driver by running the following command (assuming that the current working directory is charts and my-cosi-settings.yaml is also present in charts directory). helm install dell-cosi ./cosi --namespace=dell-cosi --values ./my-cosi-values.yaml --set-file configuration.data=./my-cosi-configuration.yaml Bucket Classes, Bucket Access Classes The COSI driver for Dell ObjectScale version 1.2, dell-csi-helm-installer does not create any Bucket Classes nor Bucket Access Classes as part of the driver installation. A sample class manifests are available at samples/bucketclass/objectscale.yaml and samples/bucketaccessclass/objectscale.yaml. Use this sample manifest to create a Bucket Classes to provision storage. Remember to uncomment/update the manifest as per the requirements.\n","categories":"","description":"Installation of COSI Driver using Helm","excerpt":"Installation of COSI Driver using Helm","ref":"/csm-docs/docs/cosidriver/installation/helm/","tags":"","title":"COSI Driver installation using Helm"},{"body":"This section provides the details and instructions on how to install the CSI Driver components using the provided Helm charts and in the case of the CSI drivers, the Dell CSI Helm Installer.\nDependencies Installing any of the CSI Driver components using Helm requires a few utilities to be installed on the system running the installation.\nDependency Usage kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver. helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3. sshpass sshpass is used to check certain pre-requisites in worker nodes (in chosen drivers). Note: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","categories":"","description":"Installation of CSI Drivers using Helm \n","excerpt":"Installation of CSI Drivers using Helm \n","ref":"/csm-docs/docs/csidriver/installation/helm/","tags":"","title":"CSI Driver installation using Helm"},{"body":"The COSI Driver for Dell ObjectScale can be deployed by using the provided Helm v3 charts on Kubernetes platform.\nThe Helm chart installs the following components in a Deployment in the specified namespace:\nCOSI Driver for ObjectScale Notational Conventions\nThe keywords “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD NOT”, “RECOMMENDED”, “NOT RECOMMENDED”, “MAY”, and “OPTIONAL” are to be interpreted as described in RFC 2119 (Bradner, S., “Key words for use in RFCs to Indicate Requirement Levels”, BCP 14, RFC 2119, March 1997).\nDependencies Installing any of the CSI Driver components using Helm requires a few utilities to be installed on the system running the installation.\nDependency Usage kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver. helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3. ℹ️ NOTE: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\nPrerequisites Install Kubernetes (see supported versions) Install the Driver Steps\nRun git clone -b main https://github.com/dell/helm-charts.git to clone the git repository. Ensure that you have created the namespace where you want to install the driver. You can run kubectl create namespace dell-cosi to create a new one. The use of dell-cosi as the namespace is just an example. You can choose any name for the namespace. Copy the charts/cosi/values.yaml into a new location with name my-cosi-values.yaml, to customize settings for installation. Create new file called my-cosi-configuration.yaml, and copy the settings available in the Configuration File page. Edit my-cosi-values.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the COSI driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository. Parameter Description Required Default provisioner.logLevel The logging level for the COSI driver provisioner. yes 4 provisioner.logFormat The logging format for the COSI driver provisioner. yes \"text\" provisioner.image.reposiotry COSI driver provisioner container image repository. yes \"docker.io/dell/cosi\" provisioner.image.tag COSI driver provisioner container image tag. yes \"v0.1.0\" provisioner.image.pullPolicy COSI driver provisioner container image pull policy. Maps 1-to-1 with Kubernetes image pull policy. yes \"IfNotPresent\" sidecar.verbosity The logging verbosity for the COSI driver sidecar, higher values are more verbose, possible values are integers from -2,147,483,648 to 2,147,483,647. Generally the range used is between -4 and 12. However, there may be cases where numbers outside that range might provide more information. For additional information, refer to the COSI sidecar documentation. yes 5 sidecar.image.reposiotry COSI driver sidecar container image repository. yes \"gcr.io/k8s-staging-sig-storage/objectstorage-sidecar/objectstorage-sidecar\" sidecar.image.tag COSI driver sidecar container image tag. yes \"v20230130-v0.1.0-24-gc0cf995\" sidecar.image.pullPolicy COSI driver sidecar container image pull policy. Maps 1-to-1 with Kubernetes image pull policy. yes \"IfNotPresent\" configuration.create Specifies whether a secret with driver configuration should be created If set to false, you must set configuration.secretName field to an existing configuration secret name. yes true configuration.secretName Name can be used to specify an existing secret name to use for the driver configuration or override the generated name. no \"cosi-config\" configuration.data Data should be provided when installing chart, it will be used to create the Secret with the driver configuration. configuration.create must be set to true for this to work. no \"\" ℹ️ NOTE:\nWhenever the configuration.secretName parameter changes in my-cosi-values.yaml user needs to reinstall the driver. Whenever the configuration.data parameter changes in my-cosi-values.yaml user needs to reinstall the driver. Install the driver by running the following command (assuming that the current working directory is charts and my-cosi-settings.yaml is also present in charts directory). helm install dell-cosi ./cosi --namespace=dell-cosi --values ./my-cosi-values.yaml --set-file configuration.data=./my-cosi-configuration.yaml Bucket Classes, Bucket Access Classes The COSI driver for Dell ObjectScale version 1.2, dell-csi-helm-installer does not create any Bucket Classes nor Bucket Access Classes as part of the driver installation. A sample class manifests are available at samples/bucketclass/objectscale.yaml and samples/bucketaccessclass/objectscale.yaml. Use this sample manifest to create a Bucket Classes to provision storage. Remember to uncomment/update the manifest as per the requirements.\n","categories":"","description":"Installation of COSI Driver using Helm","excerpt":"Installation of COSI Driver using Helm","ref":"/csm-docs/v1/cosidriver/installation/helm/","tags":"","title":"COSI Driver installation using Helm"},{"body":"This section provides the details and instructions on how to install the CSI Driver components using the provided Helm charts and in the case of the CSI drivers, the Dell CSI Helm Installer.\nDependencies Installing any of the CSI Driver components using Helm requires a few utilities to be installed on the system running the installation.\nDependency Usage kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver. helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3. sshpass sshpass is used to check certain pre-requisites in worker nodes (in chosen drivers). Note: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","categories":"","description":"Installation of CSI Drivers using Helm \n","excerpt":"Installation of CSI Drivers using Helm \n","ref":"/csm-docs/v1/csidriver/installation/helm/","tags":"","title":"CSI Driver installation using Helm"},{"body":"This section provides the details and instructions on how to install the CSI Driver components using the provided Helm charts and in the case of the CSI drivers, the Dell CSI Helm Installer.\nDependencies Installing any of the CSI Driver components using Helm requires a few utilities to be installed on the system running the installation.\nDependency Usage kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver. helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3. sshpass sshpass is used to check certain pre-requisites in worker nodes (in chosen drivers). Note: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","categories":"","description":"Installation of CSI Drivers using Helm \n","excerpt":"Installation of CSI Drivers using Helm \n","ref":"/csm-docs/v2/csidriver/installation/helm/","tags":"","title":"CSI Driver installation using Helm"},{"body":"This section provides the details and instructions on how to install the CSI Driver components using the provided Helm charts and in the case of the CSI drivers, the Dell CSI Helm Installer.\nDependencies Installing any of the CSI Driver components using Helm requires a few utilities to be installed on the system running the installation.\nDependency Usage kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver. helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3. sshpass sshpass is used to check certain pre-requisites in worker nodes (in chosen drivers). Note: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","categories":"","description":"Installation of CSI Drivers using Helm \n","excerpt":"Installation of CSI Drivers using Helm \n","ref":"/csm-docs/v3/csidriver/installation/helm/","tags":"","title":"CSI Driver installation using Helm"},{"body":"Replication Cluster Topologies Container Storage Modules (CSM) for Replication project supports the replication of volumes within a single Kubernetes cluster or between two different Kubernetes clusters. The replication controller can support multiple clusters at once, but a single volume can be replicated to a maximum of two clusters.\nEach cluster should be assigned the unique identifier clusterId. The rules for naming are as follows:\nmust be 63 characters or fewer (cannot be empty) must begin and end with an alphanumeric character ([a-z, 0-9, A-Z]) could contain dashes (-), underscores (_), dots (.), and alphanumerics between must be unique across clusters Single Cluster Replication Cluster Configuration When configuring replication within a single cluster, you need to create a ConfigMap with at least the clusterId field configured to point to the current cluster:\napiVersion: v1 data: config.yaml: | clusterId: cluster-A targets: [] kind: ConfigMap metadata: name: dell-replication-controller-config namespace: dell-replication-controller Note that the targets parameter is left empty since we don’t require any target clusters to work within a single cluster. This also means that you don’t need to create any Secrets that contain connection information to such clusters, since in this use case, we are limited to a single cluster.\nYou can find more info about configs and secrets for cluster communication in configmaps-secrets.\nStorage Class Configuration To create volumes that would be replicated within a single cluster, you need to create a special StorageClass. This StorageClass should contain the usual replication parameter replication.storage.dell.com/remoteClusterID, and it should be set to self to indicate that we want to replicate the volume inside the current cluster.\nAlso, you would need to create another storage class in the same cluster that would serve as a target storage class. This means that all replicated volumes would be derived from it. Its replication.storage.dell.com/remoteClusterID parameter should be also set to self.\nYou can find out more about replication StorageClasses and replication specific parameters in storageclasses.\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a single cluster replication, replicated resources (PersistentVolumes, ReplicationGroups) would be created in the same cluster with the replicated- prefix added to them. For example:\nkubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s replicated-csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication-tgt 23s kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:23:18Z rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z Multiple Cluster Replication Cluster Configuration Similar to a single cluster scenario, you need to create ConfigMap, but this time you need to provide at least one target cluster. You can provide as many as you like, but be mindful that a single volume can be replicated to only one of them.\nFor example:\napiVersion: v1 data: config.yaml: | clusterId: cluster-A targets: - clusterId: cluster-B address: 192.168.111.21 secretRef: secretClusterB kind: ConfigMap metadata: name: dell-replication-controller-config namespace: dell-replication-controller Note that target cluster information contains a field called secretRef. This field points to a secret available in the current cluster that contains connection information of cluster-B in the form of a kubeconfig file.\nYou can find more information about how to create such secrets in configmaps-secrets.\nStorage Class Configuration To create replicated volumes in the multi-cluster configuration you still need to have a special storage class. Replication parameter replication.storage.dell.com/remoteClusterID should be set to the cluster-id of whatever cluster you want to replicate your volumes.\nFor multi-cluster replication, we can choose one of the target cluster ids we specified in ConfigMap. In our example replication parameter, the target cluster id should be equal to cluster-B.\nYou can find more information about other replication parameters available in storage classes here.\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a multi-cluster replication, replicated resources would be created in both source and target clusters under the same names. For example:\n[CLUSTER-A]\nkubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z [CLUSTER-B]\nkubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication 18s kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 30s Ready SYNCHRONIZED 2021-08-03T11:22:18Z ","categories":"","description":"Supported Cluster Topologies with CSM Replication\n","excerpt":"Supported Cluster Topologies with CSM Replication\n","ref":"/csm-docs/docs/replication/cluster-topologies/","tags":"","title":"Cluster Topologies"},{"body":"Replication Cluster Topologies Container Storage Modules (CSM) for Replication project supports the replication of volumes within a single Kubernetes cluster or between two different Kubernetes clusters. The replication controller can support multiple clusters at once, but a single volume can be replicated to a maximum of two clusters.\nEach cluster should be assigned the unique identifier clusterId. The rules for naming are as follows:\nmust be 63 characters or fewer (cannot be empty) must begin and end with an alphanumeric character ([a-z, 0-9, A-Z]) could contain dashes (-), underscores (_), dots (.), and alphanumerics between must be unique across clusters Single Cluster Replication Cluster Configuration When configuring replication within a single cluster, you need to create a ConfigMap with at least the clusterId field configured to point to the current cluster:\napiVersion: v1 data: config.yaml: | clusterId: cluster-A targets: [] kind: ConfigMap metadata: name: dell-replication-controller-config namespace: dell-replication-controller Note that the targets parameter is left empty since we don’t require any target clusters to work within a single cluster. This also means that you don’t need to create any Secrets that contain connection information to such clusters, since in this use case, we are limited to a single cluster.\nYou can find more info about configs and secrets for cluster communication in configmaps-secrets.\nStorage Class Configuration To create volumes that would be replicated within a single cluster, you need to create a special StorageClass. This StorageClass should contain the usual replication parameter replication.storage.dell.com/remoteClusterID, and it should be set to self to indicate that we want to replicate the volume inside the current cluster.\nAlso, you would need to create another storage class in the same cluster that would serve as a target storage class. This means that all replicated volumes would be derived from it. Its replication.storage.dell.com/remoteClusterID parameter should be also set to self.\nYou can find out more about replication StorageClasses and replication specific parameters in storageclasses.\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a single cluster replication, replicated resources (PersistentVolumes, ReplicationGroups) would be created in the same cluster with the replicated- prefix added to them. For example:\nkubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s replicated-csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication-tgt 23s kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:23:18Z rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z Multiple Cluster Replication Cluster Configuration Similar to a single cluster scenario, you need to create ConfigMap, but this time you need to provide at least one target cluster. You can provide as many as you like, but be mindful that a single volume can be replicated to only one of them.\nFor example:\napiVersion: v1 data: config.yaml: | clusterId: cluster-A targets: - clusterId: cluster-B address: 192.168.111.21 secretRef: secretClusterB kind: ConfigMap metadata: name: dell-replication-controller-config namespace: dell-replication-controller Note that target cluster information contains a field called secretRef. This field points to a secret available in the current cluster that contains connection information of cluster-B in the form of a kubeconfig file.\nYou can find more information about how to create such secrets in configmaps-secrets.\nStorage Class Configuration To create replicated volumes in the multi-cluster configuration you still need to have a special storage class. Replication parameter replication.storage.dell.com/remoteClusterID should be set to the cluster-id of whatever cluster you want to replicate your volumes.\nFor multi-cluster replication, we can choose one of the target cluster ids we specified in ConfigMap. In our example replication parameter, the target cluster id should be equal to cluster-B.\nYou can find more information about other replication parameters available in storage classes here.\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a multi-cluster replication, replicated resources would be created in both source and target clusters under the same names. For example:\n[CLUSTER-A]\nkubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z [CLUSTER-B]\nkubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication 18s kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 30s Ready SYNCHRONIZED 2021-08-03T11:22:18Z ","categories":"","description":"Supported Cluster Topologies with CSM Replication\n","excerpt":"Supported Cluster Topologies with CSM Replication\n","ref":"/csm-docs/v1/replication/cluster-topologies/","tags":"","title":"Cluster Topologies"},{"body":"Replication Cluster Topologies Container Storage Modules (CSM) for Replication project supports the replication of volumes within a single Kubernetes cluster or between two different Kubernetes clusters. The replication controller can support multiple clusters at once, but a single volume can be replicated to a maximum of two clusters.\nEach cluster should be assigned the unique identifier clusterId. The rules for naming are as follows:\nmust be 63 characters or fewer (cannot be empty) must begin and end with an alphanumeric character ([a-z, 0-9, A-Z]) could contain dashes (-), underscores (_), dots (.), and alphanumerics between must be unique across clusters Single Cluster Replication Cluster Configuration When configuring replication within a single cluster, you need to create a ConfigMap with at least the clusterId field configured to point to the current cluster:\napiVersion: v1 data: config.yaml: | clusterId: cluster-A targets: [] kind: ConfigMap metadata: name: dell-replication-controller-config namespace: dell-replication-controller Note that the targets parameter is left empty since we don’t require any target clusters to work within a single cluster. This also means that you don’t need to create any Secrets that contain connection information to such clusters, since in this use case, we are limited to a single cluster.\nYou can find more info about configs and secrets for cluster communication in configmaps-secrets.\nStorage Class Configuration To create volumes that would be replicated within a single cluster, you need to create a special StorageClass. This StorageClass should contain the usual replication parameter replication.storage.dell.com/remoteClusterID, and it should be set to self to indicate that we want to replicate the volume inside the current cluster.\nAlso, you would need to create another storage class in the same cluster that would serve as a target storage class. This means that all replicated volumes would be derived from it. Its replication.storage.dell.com/remoteClusterID parameter should be also set to self.\nYou can find out more about replication StorageClasses and replication specific parameters in storageclasses.\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a single cluster replication, replicated resources (PersistentVolumes, ReplicationGroups) would be created in the same cluster with the replicated- prefix added to them. For example:\nkubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s replicated-csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication-tgt 23s kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:23:18Z rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z Multiple Cluster Replication Cluster Configuration Similar to a single cluster scenario, you need to create ConfigMap, but this time you need to provide at least one target cluster. You can provide as many as you like, but be mindful that a single volume can be replicated to only one of them.\nFor example:\napiVersion: v1 data: config.yaml: | clusterId: cluster-A targets: - clusterId: cluster-B address: 192.168.111.21 secretRef: secretClusterB kind: ConfigMap metadata: name: dell-replication-controller-config namespace: dell-replication-controller Note that target cluster information contains a field called secretRef. This field points to a secret available in the current cluster that contains connection information of cluster-B in the form of a kubeconfig file.\nYou can find more information about how to create such secrets in configmaps-secrets.\nStorage Class Configuration To create replicated volumes in the multi-cluster configuration you still need to have a special storage class. Replication parameter replication.storage.dell.com/remoteClusterID should be set to the cluster-id of whatever cluster you want to replicate your volumes.\nFor multi-cluster replication, we can choose one of the target cluster ids we specified in ConfigMap. In our example replication parameter, the target cluster id should be equal to cluster-B.\nYou can find more information about other replication parameters available in storage classes here.\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a multi-cluster replication, replicated resources would be created in both source and target clusters under the same names. For example:\n[CLUSTER-A]\nkubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z [CLUSTER-B]\nkubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication 18s kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 30s Ready SYNCHRONIZED 2021-08-03T11:22:18Z ","categories":"","description":"Supported Cluster Topologies with CSM Replication\n","excerpt":"Supported Cluster Topologies with CSM Replication\n","ref":"/csm-docs/v2/replication/cluster-topologies/","tags":"","title":"Cluster Topologies"},{"body":"Replication Cluster Topologies Container Storage Modules (CSM) for Replication project supports the replication of volumes within a single Kubernetes cluster or between two different Kubernetes clusters. The replication controller can support multiple clusters at once, but a single volume can be replicated to a maximum of two clusters.\nEach cluster should be assigned the unique identifier clusterId. The rules for naming are as follows:\nmust be 63 characters or fewer (cannot be empty) must begin and end with an alphanumeric character ([a-z, 0-9, A-Z]) could contain dashes (-), underscores (_), dots (.), and alphanumerics between must be unique across clusters Single Cluster Replication Cluster Configuration When configuring replication within a single cluster, you need to create a ConfigMap with at least the clusterId field configured to point to the current cluster:\napiVersion: v1 data: config.yaml: | clusterId: cluster-A targets: [] kind: ConfigMap metadata: name: dell-replication-controller-config namespace: dell-replication-controller Note that the targets parameter is left empty since we don’t require any target clusters to work within a single cluster. This also means that you don’t need to create any Secrets that contain connection information to such clusters, since in this use case, we are limited to a single cluster.\nYou can find more info about configs and secrets for cluster communication in configmaps-secrets.\nStorage Class Configuration To create volumes that would be replicated within a single cluster, you need to create a special StorageClass. This StorageClass should contain the usual replication parameter replication.storage.dell.com/remoteClusterID, and it should be set to self to indicate that we want to replicate the volume inside the current cluster.\nAlso, you would need to create another storage class in the same cluster that would serve as a target storage class. This means that all replicated volumes would be derived from it. Its replication.storage.dell.com/remoteClusterID parameter should be also set to self.\nYou can find out more about replication StorageClasses and replication specific parameters in storageclasses.\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a single cluster replication, replicated resources (PersistentVolumes, ReplicationGroups) would be created in the same cluster with the replicated- prefix added to them. For example:\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s replicated-csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication-tgt 23s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:23:18Z rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z Multiple Cluster Replication Cluster Configuration Similar to a single cluster scenario, you need to create ConfigMap, but this time you need to provide at least one target cluster. You can provide as many as you like, but be mindful that a single volume can be replicated to only one of them.\nFor example:\napiVersion: v1 data: config.yaml: | clusterId: cluster-A targets: - clusterId: cluster-B address: 192.168.111.21 secretRef: secretClusterB kind: ConfigMap metadata: name: dell-replication-controller-config namespace: dell-replication-controller Note that target cluster information contains a field called secretRef. This field points to a secret available in the current cluster that contains connection information of cluster-B in the form of a kubeconfig file.\nYou can find more information about how to create such secrets in configmaps-secrets.\nStorage Class Configuration To create replicated volumes in the multi-cluster configuration you still need to have a special storage class. Replication parameter replication.storage.dell.com/remoteClusterID should be set to the cluster-id of whatever cluster you want to replicate your volumes.\nFor multi-cluster replication, we can choose one of the target cluster ids we specified in ConfigMap. In our example replication parameter, the target cluster id should be equal to cluster-B.\nYou can find more information about other replication parameters available in storage classes here.\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a multi-cluster replication, replicated resources would be created in both source and target clusters under the same names. For example:\n[CLUSTER-A] $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z [CLUSTER-B] $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication 18s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 30s Ready SYNCHRONIZED 2021-08-03T11:22:18Z ","categories":"","description":"Supported Cluster Topologies with CSM Replication\n","excerpt":"Supported Cluster Topologies with CSM Replication\n","ref":"/csm-docs/v3/replication/cluster-topologies/","tags":"","title":"Cluster Topologies"},{"body":"Communication between clusters Container Storage Modules (CSM) for Replication Controller requires access to remote clusters for replicating various objects. There are two ways to set up this communication:\nUsing Normal Kubernetes users Using ServiceAccount token You need to create secrets (using either of the two methods) in each cluster involved in replication and provide their references in ConfigMap objects which are used to configure the respective CSM Replication Controllers.\nImportant: Direct network visibility between clusters required for CSM-Replication to work. Cluster-1’s API URL has to be pingable from cluster-2 pods and vice versa. If private networks are used and/or DNS is not set up properly - you may need to modify /etc/hosts file from within controller’s pod. This can be achieved by using helm installation method. Refer to this link.\nNote: If you are using a single stretched cluster, then you can skip all the following steps\nInject configuration using repctl This is the simplest way to configure CSM Replication Controller. repctl simplifies the complex configuration process greatly by enabling creation of secrets and updating their references in multiple clusters.\nRecommended method Use repctl to create secrets using service account tokens and update ConfigMaps in multiple clusters in one command. Run the following command:\nrepctl cluster inject --use-sa This will create secrets using the token for the dell-replication-controller-sa ServiceAccount and update the ConfigMap in all the clusters which have been configured for repctl.\nInject KubeConfigs from repctl configuration repctl is usually configured to communicate with multiple Kubernetes clusters and is provided with a set of KubeConfig files for each cluster. You can use repctl to inject secrets created using these files in each of the configured cluster. Run the following command:\nrepctl cluster inject Note: For a detailed walkthrough of the simplified installation process using repctl, please refer this link\nUnderstanding the Config file If you are setting up replication between two clusters (ex: Cluster A \u0026 Cluster B), a suitable configuration file (deploy/config.yaml) should look like this:\nCluster A clusterId: cluster-A # This cluster's Identifier targets: - clusterId: cluster-B # Identifier for the remote cluster B address: 192.168.111.21 # Address of the remote cluster secretRef: secretClusterB # Name of the secret required for communication with Cluster B Cluster B clusterId: cluster-B # This cluster's Identifier targets: - clusterId: cluster-A # Identifier for the remote cluster A address: 192.168.111.20 # Address of the remote cluster secretRef: secretClusterA # Name of the secret required for communication with Cluster A Manual configuration Generating KubeConfig We provide a helper script which can help create KubeConfig files for a normal user as well as a Service Account.\nUsing a Certificate Signing Request for a user cd scripts ./gen_kubeconfig.sh -u \u003cCN user\u003e -c \u003cCSR\u003e -k \u003ckey\u003e # where \"CN user\" is the name of the user \u0026 key is the private key of the user Create kubeconfig file for a Service Account cd scripts ./gen_kubeconfig.sh -s \u003csa-name\u003e -n \u003cnamespace\u003e Once you have created the KubeConfig file, you can use it to create the secret.\nSecrets using normal Kubernetes users You can create a normal Kubernetes user for your remote Kubernetes cluster and use it for inter cluster communication. The process of creating users is outside the scope of this document. Once you have the user created, you can provide it the RBAC privileges required by the controller.\nExample Continuing from our earlier example with Cluster A \u0026 Cluster B:\nCreate a user in Cluster B \u0026 generate a kubeconfig file for it using the helper script Create a ClusterRole in Cluster B using the following command: kubectl apply -f deploy/role.yaml Create a ClusterRoleBinding in Cluster B for the user: kubectl create clusterrolebinding \u003cname\u003e --clusterrole=dell-replication-manager-role --user=\u003cuser-name\u003e Create a secret in Cluster A using the kubeconfig file for this user: kubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller Secrets using ServiceAccount tokens You can use service account tokens to establish communication between various clusters. We recommend using the token for the dell-replication-controller-sa service account in the dell-replication-controller namespace after the installation as it already has all the required RBAC privileges.\nExample Use the following command to first create a KubeConfig file using the helper script in Cluster B:\n./gen_kubeconfig.sh -s dell-replication-controller-sa -n dell-replication-controller Once the KubeConfig file has been generated successfully, use the following command in Cluster A to to create the secret:\nkubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller ","categories":"","description":"Configuration\n","excerpt":"Configuration\n","ref":"/csm-docs/docs/replication/deployment/configmap-secrets/","tags":"","title":"ConfigMap \u0026 Secrets"},{"body":"Communication between clusters Container Storage Modules (CSM) for Replication Controller requires access to remote clusters for replicating various objects. There are two ways to set up this communication:\nUsing Normal Kubernetes users Using ServiceAccount token You need to create secrets (using either of the two methods) in each cluster involved in replication and provide their references in ConfigMap objects which are used to configure the respective CSM Replication Controllers.\nImportant: Direct network visibility between clusters required for CSM-Replication to work. Cluster-1’s API URL has to be pingable from cluster-2 pods and vice versa. If private networks are used and/or DNS is not set up properly - you may need to modify /etc/hosts file from within controller’s pod. This can be achieved by using helm installation method. Refer to this link.\nNote: If you are using a single stretched cluster, then you can skip all the following steps\nInject configuration using repctl This is the simplest way to configure CSM Replication Controller. repctl simplifies the complex configuration process greatly by enabling creation of secrets and updating their references in multiple clusters.\nRecommended method Use repctl to create secrets using service account tokens and update ConfigMaps in multiple clusters in one command. Run the following command:\nrepctl cluster inject --use-sa This will create secrets using the token for the dell-replication-controller-sa ServiceAccount and update the ConfigMap in all the clusters which have been configured for repctl.\nInject KubeConfigs from repctl configuration repctl is usually configured to communicate with multiple Kubernetes clusters and is provided with a set of KubeConfig files for each cluster. You can use repctl to inject secrets created using these files in each of the configured cluster. Run the following command:\nrepctl cluster inject Note: For a detailed walkthrough of the simplified installation process using repctl, please refer this link\nUnderstanding the Config file If you are setting up replication between two clusters (ex: Cluster A \u0026 Cluster B), a suitable configuration file (deploy/config.yaml) should look like this:\nCluster A clusterId: cluster-A # This cluster's Identifier targets: - clusterId: cluster-B # Identifier for the remote cluster B address: 192.168.111.21 # Address of the remote cluster secretRef: secretClusterB # Name of the secret required for communication with Cluster B Cluster B clusterId: cluster-B # This cluster's Identifier targets: - clusterId: cluster-A # Identifier for the remote cluster A address: 192.168.111.20 # Address of the remote cluster secretRef: secretClusterA # Name of the secret required for communication with Cluster A Manual configuration Generating KubeConfig We provide a helper script which can help create KubeConfig files for a normal user as well as a Service Account.\nUsing a Certificate Signing Request for a user cd scripts ./gen_kubeconfig.sh -u \u003cCN user\u003e -c \u003cCSR\u003e -k \u003ckey\u003e # where \"CN user\" is the name of the user \u0026 key is the private key of the user Create kubeconfig file for a Service Account cd scripts ./gen_kubeconfig.sh -s \u003csa-name\u003e -n \u003cnamespace\u003e Once you have created the KubeConfig file, you can use it to create the secret.\nSecrets using normal Kubernetes users You can create a normal Kubernetes user for your remote Kubernetes cluster and use it for inter cluster communication. The process of creating users is outside the scope of this document. Once you have the user created, you can provide it the RBAC privileges required by the controller.\nExample Continuing from our earlier example with Cluster A \u0026 Cluster B:\nCreate a user in Cluster B \u0026 generate a kubeconfig file for it using the helper script Create a ClusterRole in Cluster B using the following command: kubectl apply -f deploy/role.yaml Create a ClusterRoleBinding in Cluster B for the user: kubectl create clusterrolebinding \u003cname\u003e --clusterrole=dell-replication-manager-role --user=\u003cuser-name\u003e Create a secret in Cluster A using the kubeconfig file for this user: kubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller Secrets using ServiceAccount tokens You can use service account tokens to establish communication between various clusters. We recommend using the token for the dell-replication-controller-sa service account in the dell-replication-controller namespace after the installation as it already has all the required RBAC privileges.\nExample Use the following command to first create a KubeConfig file using the helper script in Cluster B:\n./gen_kubeconfig.sh -s dell-replication-controller-sa -n dell-replication-controller Once the KubeConfig file has been generated successfully, use the following command in Cluster A to to create the secret:\nkubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller ","categories":"","description":"Configuration\n","excerpt":"Configuration\n","ref":"/csm-docs/v1/replication/deployment/configmap-secrets/","tags":"","title":"ConfigMap \u0026 Secrets"},{"body":"Communication between clusters Container Storage Modules (CSM) for Replication Controller requires access to remote clusters for replicating various objects. There are two ways to set up this communication:\nUsing Normal Kubernetes users Using ServiceAccount token You need to create secrets (using either of the two methods) in each cluster involved in replication and provide their references in ConfigMap objects which are used to configure the respective CSM Replication Controllers.\nImportant: Direct network visibility between clusters required for CSM-Replication to work. Cluster-1’s API URL has to be pingable from cluster-2 pods and vice versa. If private networks are used and/or DNS is not set up properly - you may need to modify /etc/hosts file from within controller’s pod. This can be achieved by using helm installation method. Refer to this link.\nNote: If you are using a single stretched cluster, then you can skip all the following steps\nInject configuration using repctl This is the simplest way to configure CSM Replication Controller. repctl simplifies the complex configuration process greatly by enabling creation of secrets and updating their references in multiple clusters.\nRecommended method Use repctl to create secrets using service account tokens and update ConfigMaps in multiple clusters in one command. Run the following command:\nrepctl cluster inject --use-sa This will create secrets using the token for the dell-replication-controller-sa ServiceAccount and update the ConfigMap in all the clusters which have been configured for repctl.\nInject KubeConfigs from repctl configuration repctl is usually configured to communicate with multiple Kubernetes clusters and is provided with a set of KubeConfig files for each cluster. You can use repctl to inject secrets created using these files in each of the configured cluster. Run the following command:\nrepctl cluster inject Note: For a detailed walkthrough of the simplified installation process using repctl, please refer this link\nUnderstanding the Config file If you are setting up replication between two clusters (ex: Cluster A \u0026 Cluster B), a suitable configuration file (deploy/config.yaml) should look like this:\nCluster A clusterId: cluster-A # This cluster's Identifier targets: - clusterId: cluster-B # Identifier for the remote cluster B address: 192.168.111.21 # Address of the remote cluster secretRef: secretClusterB # Name of the secret required for communication with Cluster B Cluster B clusterId: cluster-B # This cluster's Identifier targets: - clusterId: cluster-A # Identifier for the remote cluster A address: 192.168.111.20 # Address of the remote cluster secretRef: secretClusterA # Name of the secret required for communication with Cluster A Manual configuration Generating KubeConfig We provide a helper script which can help create KubeConfig files for a normal user as well as a Service Account.\nUsing a Certificate Signing Request for a user cd scripts ./gen-kubeconfig.sh -u \u003cCN user\u003e -c \u003cCSR\u003e -k \u003ckey\u003e # where \"CN user\" is the name of the user \u0026 key is the private key of the user Create kubeconfig file for a Service Account cd scripts ./gen-kubeconfig.sh -s \u003csa-name\u003e -n \u003cnamespace\u003e Once you have created the KubeConfig file, you can use it to create the secret.\nSecrets using normal Kubernetes users You can create a normal Kubernetes user for your remote Kubernetes cluster and use it for inter cluster communication. The process of creating users is outside the scope of this document. Once you have the user created, you can provide it the RBAC privileges required by the controller.\nExample Continuing from our earlier example with Cluster A \u0026 Cluster B:\nCreate a user in Cluster B \u0026 generate a kubeconfig file for it using the helper script Create a ClusterRole in Cluster B using the following command: kubectl apply -f deploy/role.yaml Create a ClusterRoleBinding in Cluster B for the user: kubectl create clusterrolebinding \u003cname\u003e --clusterrole=dell-replication-manager-role --user=\u003cuser-name\u003e Create a secret in Cluster A using the kubeconfig file for this user: kubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller Secrets using ServiceAccount tokens You can use service account tokens to establish communication between various clusters. We recommend using the token for the dell-replication-controller-sa service account in the dell-replication-controller namespace after the installation as it already has all the required RBAC privileges.\nExample Use the following command to first create a KubeConfig file using the helper script in Cluster B:\n./gen-kubeconfig.sh -s dell-replication-controller-sa -n dell-replication-controller Once the KubeConfig file has been generated successfully, use the following command in Cluster A to to create the secret:\nkubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller ","categories":"","description":"Configuration\n","excerpt":"Configuration\n","ref":"/csm-docs/v2/replication/deployment/configmap-secrets/","tags":"","title":"ConfigMap \u0026 Secrets"},{"body":"Communication between clusters Container Storage Modules (CSM) for Replication Controller requires access to remote clusters for replicating various objects. There are two ways to set up this communication:\nUsing Normal Kubernetes users Using ServiceAccount token You need to create secrets (using either of the two methods) in each cluster involved in replication and provide their references in ConfigMap objects which are used to configure the respective CSM Replication Controllers.\nImportant: Direct network visibility between clusters required for CSM-Replication to work. Cluster-1’s API URL has to be pingable from cluster-2 pods and vice versa. If private networks are used and/or DNS is not set up properly - you may need to modify /etc/hosts file from within controller’s pod. This can be achieved by using helm installation method. Refer to this link.\nNote: If you are using a single stretched cluster, then you can skip all the following steps\nInject configuration using repctl This is the simplest way to configure CSM Replication Controller. repctl simplifies the complex configuration process greatly by enabling creation of secrets and updating their references in multiple clusters.\nRecommended method Use repctl to create secrets using service account tokens and update ConfigMaps in multiple clusters in one command. Run the following command:\nrepctl cluster inject --use-sa This will create secrets using the token for the dell-replication-controller-sa ServiceAccount and update the ConfigMap in all the clusters which have been configured for repctl.\nInject KubeConfigs from repctl configuration repctl is usually configured to communicate with multiple Kubernetes clusters and is provided with a set of KubeConfig files for each cluster. You can use repctl to inject secrets created using these files in each of the configured cluster. Run the following command:\nrepctl cluster inject Note: For a detailed walkthrough of the simplified installation process using repctl, please refer this link\nUnderstanding the Config file If you are setting up replication between two clusters (ex: Cluster A \u0026 Cluster B), a suitable configuration file (deploy/config.yaml) should look like this:\nCluster A clusterId: cluster-A # This cluster's Identifier targets: - clusterId: cluster-B # Identifier for the remote cluster B address: 192.168.111.21 # Address of the remote cluster secretRef: secretClusterB # Name of the secret required for communication with Cluster B Cluster B clusterId: cluster-B # This cluster's Identifier targets: - clusterId: cluster-A # Identifier for the remote cluster A address: 192.168.111.20 # Address of the remote cluster secretRef: secretClusterA # Name of the secret required for communication with Cluster A Manual configuration Generating KubeConfig We provide a helper script which can help create KubeConfig files for a normal user as well as a Service Account.\nUsing a Certificate Signing Request for a user cd scripts ./gen_kubeconfig.sh -u \u003cCN user\u003e -c \u003cCSR\u003e -k \u003ckey\u003e # where \"CN user\" is the name of the user \u0026 key is the private key of the user Create kubeconfig file for a Service Account cd scripts ./gen_kubeconfig.sh -s \u003csa-name\u003e -n \u003cnamespace\u003e Once you have created the KubeConfig file, you can use it to create the secret.\nSecrets using normal Kubernetes users You can create a normal Kubernetes user for your remote Kubernetes cluster and use it for inter cluster communication. The process of creating users is outside the scope of this document. Once you have the user created, you can provide it the RBAC privileges required by the controller.\nExample Continuing from our earlier example with Cluster A \u0026 Cluster B:\nCreate a user in Cluster B \u0026 generate a kubeconfig file for it using the helper script Create a ClusterRole in Cluster B using the following command: kubectl apply -f deploy/role.yaml Create a ClusterRoleBinding in Cluster B for the user: kubectl create clusterrolebinding \u003cname\u003e --clusterrole=dell-replication-manager-role --user=\u003cuser-name\u003e Create a secret in Cluster A using the kubeconfig file for this user: kubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller Secrets using ServiceAccount tokens You can use service account tokens to establish communication between various clusters. We recommend using the token for the dell-replication-controller-sa service account in the dell-replication-controller namespace after the installation as it already has all the required RBAC privileges.\nExample Use the following command to first create a KubeConfig file using the helper script in Cluster B:\n./gen_kubeconfig.sh -s dell-replication-controller-sa -n dell-replication-controller Once the KubeConfig file has been generated successfully, use the following command in Cluster A to to create the secret:\nkubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller ","categories":"","description":"Configuration\n","excerpt":"Configuration\n","ref":"/csm-docs/v3/replication/deployment/configmap-secrets/","tags":"","title":"ConfigMap \u0026 Secrets"},{"body":"CSM Docs is an open-source project and we thrive to build a welcoming and open community for anyone who wants to use the project or contribute to it.\nContributing to CSM Docs Become one of the contributors to this project!\nYou can contribute to this project in several ways. Here are some examples:\nContribute to the CSM documentation. Report an issue. Feature requests. CSM docs reside in https://github.com/dell/csm-docs.\nCSM project resides in https://github.com/dell/csm.\nDon’t Break the website view. Commit directly. Compromise backward compatibility. Disrespect your Community Team members. Forget to keep things simple. Do Keep it simple. Good work, your best every time. Squash your commits, avoid merges. Keep open communication with other Committers. Ask questions. Test your changes locally and make sure it is not breaking anything. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose.\nBranching strategy The CSM documentation portal follows a release branch strategy where a branch is created for each release and all documentation changes made for a release are done on that branch. The release branch is then merged into the main branch at the time of the release. In some situations it may be sufficient to merge a non-release branch to main if it fixes some issue in the documentation for the current released version.\nBranch Naming Convention Branch Type Example Comment main main Release release-1.0 hotfix: release-1.1 patch: release-1.0.1 Feature feature-9-olp-support “9” referring to GitHub issue ID Bug Fix bugfix-110-remove-docker-compose “110” referring to GitHub issue ID Steps for working on the main branch Fork the repository. Create a branch off of the main branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream main branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream main branch. Once your pull request has merged with the required approvals, your branch can be deleted. Steps for working on a release branch Fork the repository. Create a branch off of the release branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream release branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream release branch. Once your pull request has merged with the required approvals, your branch can be deleted. Previewing your changes Install latest Hugo version extended version. Note: Please note we have to install an extended version.\nCreate a local copy of the csm-docs repository using git clone. Update docsy submodules inside themes folder using git submodule update --recursive --init Change to the csm-docs folder and run hugo server By default, local changes will be reflected at http://localhost:1313/. Hugo will watch for changes to the content and automatically refreshes the site. Note: To bind it to different server address use hugo server --bind 0.0.0.0, default is 127.0.0.1\nAfter testing the changes locally, raise a pull request after editing the pages and pushing it to GitHub. Community guidelines This project follows https://github.com/dell/csm/blob/main/docs/CODE_OF_CONDUCT.md.\nBest Practices Linking the URLs Hardcoded relative links like [troubleshooting observability](../../observability/troubleshooting.md) will behave unexpectedly compared to how they would work on our local file system. To avoid broken links in the portal, use regular relative URLs in links that will be left unchanged by Hugo.\nStyle guide Use sentence case wherever applicable. Use the numbered lists for items in sequential order and bulletins for the other lists. Check for grammar and spelling. Embed the code within backticks. Use only high-resolution images. ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) docs Contribution Guidelines\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) docs …","ref":"/csm-docs/docs/references/contributionguidelines/","tags":"","title":"Contribution Guidelines"},{"body":"CSM Docs is an open-source project and we thrive to build a welcoming and open community for anyone who wants to use the project or contribute to it.\nContributing to CSM Docs Become one of the contributors to this project!\nYou can contribute to this project in several ways. Here are some examples:\nContribute to the CSM documentation. Report an issue. Feature requests. CSM docs reside in https://github.com/dell/csm-docs.\nCSM project resides in https://github.com/dell/csm.\nDon’t Break the website view. Commit directly. Compromise backward compatibility. Disrespect your Community Team members. Forget to keep things simple. Do Keep it simple. Good work, your best every time. Squash your commits, avoid merges. Keep open communication with other Committers. Ask questions. Test your changes locally and make sure it is not breaking anything. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose.\nBranching strategy The CSM documentation portal follows a release branch strategy where a branch is created for each release and all documentation changes made for a release are done on that branch. The release branch is then merged into the main branch at the time of the release. In some situations it may be sufficient to merge a non-release branch to main if it fixes some issue in the documentation for the current released version.\nBranch Naming Convention Branch Type Example Comment main main Release release-1.0 hotfix: release-1.1 patch: release-1.0.1 Feature feature-9-olp-support “9” referring to GitHub issue ID Bug Fix bugfix-110-remove-docker-compose “110” referring to GitHub issue ID Steps for working on the main branch Fork the repository. Create a branch off of the main branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream main branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream main branch. Once your pull request has merged with the required approvals, your branch can be deleted. Steps for working on a release branch Fork the repository. Create a branch off of the release branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream release branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream release branch. Once your pull request has merged with the required approvals, your branch can be deleted. Previewing your changes Install latest Hugo version extended version. Note: Please note we have to install an extended version.\nCreate a local copy of the csm-docs repository using git clone. Update docsy submodules inside themes folder using git submodule update --recursive --init Change to the csm-docs folder and run hugo server By default, local changes will be reflected at http://localhost:1313/. Hugo will watch for changes to the content and automatically refreshes the site. Note: To bind it to different server address use hugo server --bind 0.0.0.0, default is 127.0.0.1\nAfter testing the changes locally, raise a pull request after editing the pages and pushing it to GitHub. Community guidelines This project follows https://github.com/dell/csm/blob/main/docs/CODE_OF_CONDUCT.md.\nBest Practices Linking the URLs Hardcoded relative links like [troubleshooting observability](../../observability/troubleshooting.md) will behave unexpectedly compared to how they would work on our local file system. To avoid broken links in the portal, use regular relative URLs in links that will be left unchanged by Hugo.\nStyle guide Use sentence case wherever applicable. Use the numbered lists for items in sequential order and bulletins for the other lists. Check for grammar and spelling. Embed the code within backticks. Use only high-resolution images. ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) docs Contribution Guidelines\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) docs …","ref":"/csm-docs/v1/references/contributionguidelines/","tags":"","title":"Contribution Guidelines"},{"body":"CSM Docs is an open-source project and we thrive to build a welcoming and open community for anyone who wants to use the project or contribute to it.\nContributing to CSM Docs Become one of the contributors to this project!\nYou can contribute to this project in several ways. Here are some examples:\nContribute to the CSM documentation. Report an issue. Feature requests. CSM docs reside in https://github.com/dell/csm-docs.\nCSM project resides in https://github.com/dell/csm.\nDon’t Break the website view. Commit directly. Compromise backward compatibility. Disrespect your Community Team members. Forget to keep things simple. Do Keep it simple. Good work, your best every time. Squash your commits, avoid merges. Keep open communication with other Committers. Ask questions. Test your changes locally and make sure it is not breaking anything. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose.\nBranching strategy The CSM documentation portal follows a release branch strategy where a branch is created for each release and all documentation changes made for a release are done on that branch. The release branch is then merged into the main branch at the time of the release. In some situations it may be sufficient to merge a non-release branch to main if it fixes some issue in the documentation for the current released version.\nBranch Naming Convention Branch Type Example Comment main main Release release-1.0 hotfix: release-1.1 patch: release-1.0.1 Feature feature-9-olp-support “9” referring to GitHub issue ID Bug Fix bugfix-110-remove-docker-compose “110” referring to GitHub issue ID Steps for working on the main branch Fork the repository. Create a branch off of the main branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream main branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream main branch. Once your pull request has merged with the required approvals, your branch can be deleted. Steps for working on a release branch Fork the repository. Create a branch off of the release branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream release branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream release branch. Once your pull request has merged with the required approvals, your branch can be deleted. Previewing your changes Install latest Hugo version extended version. Note: Please note we have to install an extended version.\nCreate a local copy of the csm-docs repository using git clone. Update docsy submodules inside themes folder using git submodule update --recursive --init Change to the csm-docs folder and run hugo server By default, local changes will be reflected at http://localhost:1313/. Hugo will watch for changes to the content and automatically refreshes the site. Note: To bind it to different server address use hugo server --bind 0.0.0.0, default is 127.0.0.1\nAfter testing the changes locally, raise a pull request after editing the pages and pushing it to GitHub. Community guidelines This project follows https://github.com/dell/csm/blob/main/docs/CODE_OF_CONDUCT.md.\nBest Practices Linking the URLs Hardcoded relative links like [troubleshooting observability](../../observability/troubleshooting.md) will behave unexpectedly compared to how they would work on our local file system. To avoid broken links in the portal, use regular relative URLs in links that will be left unchanged by Hugo.\nStyle guide Use sentence case wherever applicable. Use the numbered lists for items in sequential order and bulletins for the other lists. Check for grammar and spelling. Embed the code within backticks. Use only high-resolution images. ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) docs Contribution Guidelines\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) docs …","ref":"/csm-docs/v2/references/contributionguidelines/","tags":"","title":"Contribution Guidelines"},{"body":"CSM Docs is an open-source project and we thrive to build a welcoming and open community for anyone who wants to use the project or contribute to it.\nContributing to CSM Docs Become one of the contributors to this project!\nYou can contribute to this project in several ways. Here are some examples:\nContribute to the CSM documentation. Report an issue. Feature requests. CSM docs reside in https://github.com/dell/csm-docs.\nCSM project resides in https://github.com/dell/csm.\nDon’t Break the website view. Commit directly. Compromise backward compatibility. Disrespect your Community Team members. Forget to keep things simple. Do Keep it simple. Good work, your best every time. Squash your commits, avoid merges. Keep open communication with other Committers. Ask questions. Test your changes locally and make sure it is not breaking anything. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose.\nBranching strategy The CSM documentation portal follows a release branch strategy where a branch is created for each release and all documentation changes made for a release are done on that branch. The release branch is then merged into the main branch at the time of the release. In some situations it may be sufficient to merge a non-release branch to main if it fixes some issue in the documentation for the current released version.\nBranch Naming Convention Branch Type Example Comment main main Release release-1.0 hotfix: release-1.1 patch: release-1.0.1 Feature feature-9-olp-support “9” referring to GitHub issue ID Bug Fix bugfix-110-remove-docker-compose “110” referring to GitHub issue ID Steps for working on the main branch Fork the repository. Create a branch off of the main branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream main branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream main branch. Once your pull request has merged with the required approvals, your branch can be deleted. Steps for working on a release branch Fork the repository. Create a branch off of the release branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream release branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream release branch. Once your pull request has merged with the required approvals, your branch can be deleted. Previewing your changes Install latest Hugo version extended version. Note: Please note we have to install an extended version.\nCreate a local copy of the csm-docs repository using git clone. Update docsy submodules inside themes folder using git submodule update --recursive --init Change to the csm-docs folder and run hugo server By default, local changes will be reflected at http://localhost:1313/. Hugo will watch for changes to the content and automatically refreshes the site. Note: To bind it to different server address use hugo server --bind 0.0.0.0, default is 127.0.0.1\nAfter testing the changes locally, raise a pull request after editing the pages and pushing it to GitHub. Community guidelines This project follows https://github.com/dell/csm/blob/main/docs/CODE_OF_CONDUCT.md.\nBest Practices Linking the URLs Hardcoded relative links like [troubleshooting observability](../../observability/troubleshooting.md) will behave unexpectedly compared to how they would work on our local file system. To avoid broken links in the portal, use regular relative URLs in links that will be left unchanged by Hugo.\nStyle guide Use sentence case wherever applicable. Use the numbered lists for items in sequential order and bulletins for the other lists. Check for grammar and spelling. Embed the code within backticks. Use only high-resolution images. ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) docs Contribution Guidelines\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) docs …","ref":"/csm-docs/v3/references/contributionguidelines/","tags":"","title":"Contribution Guidelines"},{"body":"The COSI Driver by Dell implements an interface between COSI (spec v1alpha1) enabled Container Orchestrator and Dell Storage Arrays. It is a plug-in that is installed into Kubernetes to provide object storage using Dell storage systems.\nDell COSI Driver is a multi-backend driver, meaning that it can connect to multiple Object Storage Platform (OSP) Instances and provide access to them using the same COSI interface.\nFeatures and capabilities Supported Container Orchestrator Platforms ℹ️ NOTE: during technical preview, no certification is performed. The platforms listed below were tested by developers using integration test suite.\nCOSI Kubernetes 1.27 K3s 1.27 COSI Driver Capabilities Features ObjectScale Bucket Creation yes Bucket Deletion yes Bucket Access Granting yes Bucket Access Revoking yes Backend Storage Details Protocol ObjectScale AWS S3 yes GCS N/A Azure Blob N/A Supported Storage Platforms Storage Platform Versions ObjectScale 1.2.x Bucket Lifecycle Workflow Create Bucket → Delete Bucket Create Bucket → Grant Access → Revoke Access → Delete Bucket ","categories":"","description":"About Dell Technologies (Dell) COSI Driver","excerpt":"About Dell Technologies (Dell) COSI Driver","ref":"/csm-docs/docs/cosidriver/","tags":"","title":"COSI Driver"},{"body":"The COSI Driver by Dell implements an interface between COSI (spec v1alpha1) enabled Container Orchestrator and Dell Storage Arrays. It is a plug-in that is installed into Kubernetes to provide object storage using Dell storage systems.\nDell COSI Driver is a multi-backend driver, meaning that it can connect to multiple Object Storage Platform (OSP) Instances and provide access to them using the same COSI interface.\nFeatures and capabilities Supported Container Orchestrator Platforms ℹ️ NOTE: during technical preview, no certification is performed. The platforms listed below were tested by developers using integration test suite.\nCOSI Kubernetes 1.27 K3s 1.27 COSI Driver Capabilities Features ObjectScale Bucket Creation yes Bucket Deletion yes Bucket Access Granting yes Bucket Access Revoking yes Backend Storage Details Protocol ObjectScale AWS S3 yes GCS N/A Azure Blob N/A Supported Storage Platforms Storage Platform Versions ObjectScale 1.2.x Bucket Lifecycle Workflow Create Bucket → Delete Bucket Create Bucket → Grant Access → Revoke Access → Delete Bucket ","categories":"","description":"About Dell Technologies (Dell) COSI Driver","excerpt":"About Dell Technologies (Dell) COSI Driver","ref":"/csm-docs/v1/cosidriver/","tags":"","title":"COSI Driver"},{"body":"The CSI Drivers by Dell implement an interface between CSI (CSI spec v1.6) enabled Container Orchestrator (CO) and Dell Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using the Dell storage system.\nFeatures and capabilities Supported Container Orchestrator Platforms PowerMax PowerFlex Unity XT PowerScale PowerStore Kubernetes 1.26, 1.27, 1.28 1.26, 1.27, 1.28 1.26, 1.27, 1.28 1.26, 1.27, 1.28 1.26, 1.27, 1.28 Red Hat OpenShift 4.13, 4.14 4.13, 4.14 4.13, 4.14 4.13, 4.14 4.13, 4.14 Mirantis Kubernetes Engine 3.6.x 3.6.x 3.6.x 3.5.x, 3.6.x 3.6.x Google Anthos 1.15 1.15 no 1.15 1.15 VMware Tanzu no no NFS NFS NFS,iSCSI Rancher Kubernetes Engine 1.4.x 1.4.x 1.4.x 1.4.x 1.4.x Amazon Elastic Kubernetes Service\nAnywhere yes yes yes yes yes OS dependencies iscsi-initiator-utils\nmultipathd or powerpath\nnvme-cli\nnfs-utils SDC iscsi-initiator-utils\nmultipathd\nnfs-utils nfs-utils iscsi-initiator-utils\nmultipathd\nnvme-cli\nnfs-utils Notes:\nThe required OS dependencies are only for the protocol needed (e.g. if NVMe isn’t the storage access protocol then nvme-cli is not required). The host operating system/version being used must align with what each Dell Storage platform supports. Please visit E-Lab Navigator for specific Dell Storage platform host operating system level support matrices. CSI Driver Capabilities Features PowerMax PowerFlex Unity XT PowerScale PowerStore CSI Driver version 2.9.1 2.9.2 2.9.1 2.9.1 2.9.1 Static Provisioning yes yes yes yes yes Dynamic Provisioning yes yes yes yes yes Expand Persistent Volume yes yes yes yes yes Create VolumeSnapshot yes for LUN\nno for NFS yes yes yes yes Create Volume from Snapshot yes for LUN\nno for NFS yes yes yes yes Delete Snapshot yes for LUN\nno for NFS yes yes yes yes Access Mode for volumeMode: Filesystem RWO, RWOP\nROX, RWX with NFS ONLY RWO, ROX, RWOP\nRWX with NFS ONLY RWO, ROX, RWOP\nRWX with NFS ONLY RWO, RWX, ROX, RWOP RWO, RWOP\nROX, RWX with NFS ONLY Access Mode for volumeMode: Block RWO, RWX, ROX, RWOP RWO, RWX, ROX, RWOP RWO, RWX, ROX, RWOP Not Supported RWO, RWX, ROX, RWOP CSI Volume Cloning yes for LUN\nno for NFS yes yes yes yes CSI Raw Block Volume yes yes yes no yes CSI Ephemeral Volume no yes yes yes yes Topology yes yes yes yes yes Multi-array yes yes yes yes yes Volume Health Monitoring yes yes yes yes yes Storage Capacity Tracking yes yes yes yes yes Volume Limit yes yes yes yes yes Supported Storage Platforms PowerMax PowerFlex Unity XT PowerScale PowerStore Storage Array PowerMax 2500/8500 PowerMaxOS 10 (6079) , PowerMaxOS 10.0.1 (6079) , PowerMaxOS 10.1 (6079)\nPowerMax 2000/8000 - 5978.711.xxx, 5978.479.xxx Unisphere 10.0,10.0.1,10.1 3.6.x, 4.0.x, 4.5.x 5.1.x, 5.2.x, 5.3.0 OneFS 9.3, 9.4, 9.5.0.x (x \u003e= 5) 3.0, 3.2, 3.5 Backend Storage Details Features PowerMax PowerFlex Unity XT PowerScale PowerStore Fibre Channel yes N/A yes N/A yes iSCSI yes N/A yes N/A yes NVMeTCP N/A N/A N/A N/A yes NVMeFC N/A N/A N/A N/A yes NFS yes - SDNAS only (not eNAS) yes yes yes yes Other N/A ScaleIO protocol N/A N/A N/A Supported FS ext4 / xfs / NFS ext4 / xfs / NFS ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS Thin / Thick provisioning Thin Thin Thin/Thick N/A Thin Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering Policy\nNFS Host IO size\nSnapshot Retention duration Access Zone\nNFS version (3 or 4);Configurable Export IPs iSCSI CHAP Auto RDM(vSphere) Yes(over FC) N/A N/A N/A N/A Community Qualified Platforms cert-csi results OS CO Storage Platform Protocol CSM Ticket 1079 Debian 10 K3s v1.24.7+k3s1 Unity VSA 5.3.1.0.5.008 iSCSI CSI v1.8.0 ","categories":"","description":"About Dell Technologies (Dell) CSI Drivers","excerpt":"About Dell Technologies (Dell) CSI Drivers","ref":"/csm-docs/docs/csidriver/","tags":"","title":"CSI Drivers"},{"body":"The CSI Drivers by Dell implement an interface between CSI (CSI spec v1.5) enabled Container Orchestrator (CO) and Dell Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using the Dell storage system.\nFeatures and capabilities Supported Operating Systems/Container Orchestrator Platforms PowerMax PowerFlex Unity XT PowerScale PowerStore Kubernetes 1.25, 1.26, 1.27 1.25, 1.26, 1.27 1.25, 1.26, 1.27 1.25, 1.26, 1.27 1.25, 1.26, 1.27 RHEL 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x Ubuntu 20.04 20.04 20.04 20.04, 22.04 20.04 CentOS 7.8, 7.9 7.8, 7.9 7.9 7.8, 7.9 7.8, 7.9 SLES 15SP4 15SP4 15SP4 15SP4 15SP4 Red Hat OpenShift 4.12, 4.12 EUS, 4.13 4.12, 4.12 EUS, 4.13 4.12, 4.12 EUS, 4.13 4.12, 4.12 EUS, 4.13 4.12, 4.13, 4.13 EUS Mirantis Kubernetes Engine 3.6.x 3.6.x 3.6.x 3.5.x, 3.6.x 3.6.x Google Anthos 1.15 1.15 no 1.15 1.15 VMware Tanzu no no NFS NFS NFS,iSCSI Rancher Kubernetes Engine 1.4.1 1.4.7 1.4.8 1.4.7 1.4.5 Amazon Elastic Kubernetes Service\nAnywhere yes yes yes yes yes Kubernetes K3s Engine on Debian OS no no 1.26, 1.27 no no CSI Driver Capabilities Features PowerMax PowerFlex Unity XT PowerScale PowerStore CSI Driver version 2.8.0 2.8.0 2.8.0 2.8.0 2.8.0 Static Provisioning yes yes yes yes yes Dynamic Provisioning yes yes yes yes yes Expand Persistent Volume yes yes yes yes yes Create VolumeSnapshot yes yes yes yes yes Create Volume from Snapshot yes yes yes yes yes Delete Snapshot yes yes yes yes yes Access Mode for volumeMode: Filesystem RWO, RWOP\nROX, RWX with NFS ONLY RWO, ROX, RWOP\nRWX with NFS ONLY RWO, ROX, RWOP\nRWX with NFS ONLY RWO, RWX, ROX, RWOP RWO, RWOP\nROX, RWX with NFS ONLY Access Mode for volumeMode: Block RWO, RWX, ROX, RWOP RWO, RWX, ROX, RWOP RWO, RWX, ROX, RWOP Not Supported RWO, RWX, ROX, RWOP CSI Volume Cloning yes yes yes yes yes CSI Raw Block Volume yes yes yes no yes CSI Ephemeral Volume no yes yes yes yes Topology yes yes yes yes yes Multi-array yes yes yes yes yes Volume Health Monitoring yes yes yes yes yes Storage Capacity Tracking yes yes yes yes yes Volume Limit yes yes yes yes yes Supported Storage Platforms PowerMax PowerFlex Unity XT PowerScale PowerStore Storage Array PowerMax 2500/8500 PowerMaxOS 10 (6079) , PowerMaxOS 10.0.1 (6079) PowerMax 2000/8000 - 5978.711.xxx, 5978.479.xxx Unisphere 10.0,10.0.1 3.5.x, 3.6.x, 4.0.x, 4.5 5.1.x, 5.2.x, 5.3.0 OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4, 9.5.0.5, 9.5.0.6 2.0.x, 2.1.x, 3.0, 3.2, 3.5 Note: To connect to a PowerFlex 4.5 array, the SDC image will need to be changed to dellemc/sdc:4.5.\nIf using helm to install, you will need to make this change in your values.yaml file. See helm install documentation for details. If using CSM-Operator to install, you will need to make this change in your samples file. See operator install documentation for details. Backend Storage Details Features PowerMax PowerFlex Unity XT PowerScale PowerStore Fibre Channel yes N/A yes N/A yes iSCSI yes N/A yes N/A yes NVMeTCP N/A N/A N/A N/A yes NVMeFC N/A N/A N/A N/A yes NFS yes - SDNAS only (not eNAS) yes yes yes yes Other N/A ScaleIO protocol N/A N/A N/A Supported FS ext4 / xfs / NFS ext4 / xfs / NFS ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS Thin / Thick provisioning Thin Thin Thin/Thick N/A Thin Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering Policy\nNFS Host IO size\nSnapshot Retention duration Access Zone\nNFS version (3 or 4);Configurable Export IPs iSCSI CHAP Auto RDM(vSphere) Yes(over FC) N/A N/A N/A N/A ","categories":"","description":"About Dell Technologies (Dell) CSI Drivers","excerpt":"About Dell Technologies (Dell) CSI Drivers","ref":"/csm-docs/v1/csidriver/","tags":"","title":"CSI Drivers"},{"body":"The CSI Drivers by Dell implement an interface between CSI (CSI spec v1.5) enabled Container Orchestrator (CO) and Dell Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using Dell storage system.\nFeatures and capabilities Supported Operating Systems/Container Orchestrator Platforms PowerMax PowerFlex Unity XT PowerScale PowerStore Kubernetes 1.25, 1.26, 1.27 1.25, 1.26, 1.27 1.25, 1.26, 1.27 1.25, 1.26, 1.27 1.25, 1.26, 1.27 RHEL 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x Ubuntu 20.04 20.04 18.04, 20.04 20.04, 22.04 20.04 CentOS 7.8, 7.9 7.8, 7.9 7.9 7.8, 7.9 7.8, 7.9 SLES 15SP4 15SP4 15SP4 15SP3 15SP4 Red Hat OpenShift 4.11, 4.12, 4.12 EUS 4.11, 4.12, 4.12 EUS 4.11, 4.12, 4.12 EUS 4.11, 4.12, 4.12 EUS 4.11, 4.12, 4.12 EUS Mirantis Kubernetes Engine 3.6.x 3.5.x,3.6.x 3.6.x 3.5.x, 3.6.x 3.6.x Google Anthos 1.14 1.12 no 1.15 1.15 VMware Tanzu no no NFS NFS NFS,iSCSI Rancher Kubernetes Engine 1.4.1 1.4.1 1.4.5 1.4.1 1.4.5 Amazon Elastic Kubernetes Service\nAnywhere yes yes yes yes yes Kubernetes K3s Engine on Debian OS no no 1.25, 1.26, 1.27 no no CSI Driver Capabilities Features PowerMax PowerFlex Unity XT PowerScale PowerStore CSI Driver version 2.7.0 2.7.1 2.7.0 2.7.0 2.7.0 Static Provisioning yes yes yes yes yes Dynamic Provisioning yes yes yes yes yes Expand Persistent Volume yes yes yes yes yes Create VolumeSnapshot yes yes yes yes yes Create Volume from Snapshot yes yes yes yes yes Delete Snapshot yes yes yes yes yes Access Mode FC/iSCSI: RWO/\nRWOP\nRaw block: RWO/\nRWX/\nROX/\nRWOP RWO/ROX/RWOP\nRWX (Raw block only) RWO/ROX/RWOP\nRWX (Raw block \u0026 NFS only) RWO/RWX/ROX/\nRWOP RWO/RWOP\n(FC/iSCSI)\nRWO/\nRWX/\nROX/\nRWOP\n(RawBlock, NFS) CSI Volume Cloning yes yes yes yes yes CSI Raw Block Volume yes yes yes no yes CSI Ephemeral Volume no yes yes yes yes Topology yes yes yes yes yes Multi-array yes yes yes yes yes Volume Health Monitoring yes yes yes yes yes Storage Capacity Tracking no no no yes yes Supported Storage Platforms PowerMax PowerFlex Unity XT PowerScale PowerStore Storage Array PowerMax 2500/8500 PowerMaxOS 10 (6079) , PowerMaxOS 10.0.1 (6079) PowerMax 2000/8000 - 5978.711.xxx, 5978.479.xxx Unisphere 10.0,10.0.1 3.5.x, 3.6.x, 4.0 5.1.x, 5.2.x, 5.3.0 OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4 2.0.x, 2.1.x, 3.0, 3.2, 3.5 Backend Storage Details Features PowerMax PowerFlex Unity XT PowerScale PowerStore Fibre Channel yes N/A yes N/A yes iSCSI yes N/A yes N/A yes NVMeTCP N/A N/A N/A N/A yes NVMeFC N/A N/A N/A N/A yes NFS N/A N/A yes yes yes Other N/A ScaleIO protocol N/A N/A N/A Supported FS ext4 / xfs ext4 / xfs ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS Thin / Thick provisioning Thin Thin Thin/Thick N/A Thin Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering Policy\nNFS Host IO size\nSnapshot Retention duration Access Zone\nNFS version (3 or 4);Configurable Export IPs iSCSI CHAP Auto RDM(vSphere) Yes(over FC) N/A N/A N/A N/A ","categories":"","description":"About Dell Technologies (Dell) CSI Drivers","excerpt":"About Dell Technologies (Dell) CSI Drivers","ref":"/csm-docs/v2/csidriver/","tags":"","title":"CSI Drivers"},{"body":"The CSI Drivers by Dell implement an interface between CSI (CSI spec v1.5) enabled Container Orchestrator (CO) and Dell Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using Dell storage system.\nFeatures and capabilities Supported Operating Systems/Container Orchestrator Platforms PowerMax PowerFlex Unity XT PowerScale PowerStore Kubernetes 1.24, 1.25, 1.26 1.24, 1.25, 1.26 1.24, 1.25, 1.26 1.24, 1.25, 1.26 1.24, 1.25, 1.26 RHEL 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x Ubuntu 20.04 20.04 18.04, 20.04 20.04, 22.04 20.04 CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9 SLES 15SP4 15SP4 15SP4 15SP3 15SP4 Red Hat OpenShift 4.10, 4.10 EUS, 4.11 4.10, 4.10 EUS, 4.11 4.10, 4.10 EUS, 4.11 4.10, 4.10 EUS, 4.11 4.10, 4.10 EUS, 4.11 Mirantis Kubernetes Engine 3.6.x 3.5.x,3.6.x 3.6.x 3.5.x, 3.6.x 3.6.x Google Anthos 1.14 1.12 no 1.12 1.14 VMware Tanzu no no NFS NFS NFS,iSCSI Rancher Kubernetes Engine 1.4.1 1.4.1 1.4.1 1.4.1 1.4.1 Amazon Elastic Kubernetes Service\nAnywhere no yes yes yes yes CSI Driver Capabilities Features PowerMax PowerFlex Unity XT PowerScale PowerStore CSI Driver version 2.6.0 2.6.0 2.6.0 2.6.1 2.6.0 Static Provisioning yes yes yes yes yes Dynamic Provisioning yes yes yes yes yes Expand Persistent Volume yes yes yes yes yes Create VolumeSnapshot yes yes yes yes yes Create Volume from Snapshot yes yes yes yes yes Delete Snapshot yes yes yes yes yes Access Mode FC/iSCSI: RWO/\nRWOP\nRaw block: RWO/\nRWX/\nROX/\nRWOP RWO/ROX/RWOP\nRWX (Raw block only) RWO/ROX/RWOP\nRWX (Raw block \u0026 NFS only) RWO/RWX/ROX/\nRWOP RWO/RWOP\n(FC/iSCSI)\nRWO/\nRWX/\nROX/\nRWOP\n(RawBlock, NFS) CSI Volume Cloning yes yes yes yes yes CSI Raw Block Volume yes yes yes no yes CSI Ephemeral Volume no yes yes yes yes Topology yes yes yes yes yes Multi-array yes yes yes yes yes Volume Health Monitoring yes yes yes yes yes Storage Capacity Tracking no no no no yes Supported Storage Platforms PowerMax PowerFlex Unity XT PowerScale PowerStore Storage Array PowerMax 2000/8000 PowerMax 2500/8500 5978.479.479, 5978.711.711, 6079.xxx.xxx\nUnisphere 10.0 3.5.x, 3.6.x, 4.0 5.0.x, 5.1.x, 5.2.x OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4 1.0.x, 2.0.x, 2.1.x, 3.0, 3.2 Backend Storage Details Features PowerMax PowerFlex Unity XT PowerScale PowerStore Fibre Channel yes N/A yes N/A yes iSCSI yes N/A yes N/A yes NVMeTCP N/A N/A N/A N/A yes NVMeFC N/A N/A N/A N/A yes NFS N/A N/A yes yes yes Other N/A ScaleIO protocol N/A N/A N/A Supported FS ext4 / xfs ext4 / xfs ext3 / ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS Thin / Thick provisioning Thin Thin Thin/Thick N/A Thin Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering Policy\nNFS Host IO size\nSnapshot Retention duration Access Zone\nNFS version (3 or 4);Configurable Export IPs iSCSI CHAP Auto RDM(vSphere) Yes(over FC) N/A N/A N/A N/A ","categories":"","description":"About Dell Technologies (Dell) CSI Drivers","excerpt":"About Dell Technologies (Dell) CSI Drivers","ref":"/csm-docs/v3/csidriver/","tags":"","title":"CSI Drivers"},{"body":"CSM for Observability can be deployed in one of four ways:\nHelm CSM for Observability Installer CSM for Observability Offline Installer Operator Post Installation Dependencies The following third-party components are required in the same Kubernetes cluster where CSM for Observability has been deployed:\nPrometheus Grafana Other Deployment Methods There are various ways to deploy these components. We recommend following the Helm deployments according to the specifications defined below.\nTip: CSM for Observability must be deployed first. Once the module has been deployed, you can proceed to deploying/configuring Prometheus and Grafana.\nPrometheus The Prometheus service should be running on the same Kubernetes cluster as the CSM for Observability services. As part of the CSM for Observability deployment, the OpenTelemetry Collector gets deployed. CSM for Observability pushes metrics to the OpenTelemetry Collector where the metrics are consumed by Prometheus. Prometheus must be configured to scrape the metrics data from the OpenTelemetry Collector.\nSupported Version Image Helm Chart 2.34.0 prom/prometheus:v2.34.0 Prometheus Helm chart Note: It is the user’s responsibility to provide persistent storage for Prometheus if they want to preserve historical data.\nPrometheus Helm Deployment Here is a sample minimal configuration for Prometheus. Please note that the configuration below uses insecure skip verify. If you wish to properly configure TLS, you will need to provide a ca_file in the Prometheus configuration. The certificate provided as part of the CSM for Observability deployment should be signed by this same CA. For more information about Prometheus configuration, see Prometheus configuration.\nCreate a values file named prometheus-values.yaml.\n# prometheus-values.yaml alertmanager: enabled: false nodeExporter: enabled: false pushgateway: enabled: false kubeStateMetrics: enabled: false configmapReload: prometheus: enabled: false server: enabled: true image: repository: quay.io/prometheus/prometheus tag: v2.34.0 pullPolicy: IfNotPresent persistentVolume: enabled: false service: type: NodePort servicePort: 9090 extraScrapeConfigs: | - job_name: 'karavi-metrics-[CSI-DRIVER]' scrape_interval: 5s scheme: https static_configs: - targets: ['otel-collector:8443'] tls_config: insecure_skip_verify: true If using Rancher, create a ServiceMonitor.\napiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: otel-collector namespace: powerflex spec: endpoints: - path: /metrics port: exporter-https scheme: https tlsConfig: insecureSkipVerify: true selector: matchLabels: app.kubernetes.io/instance: karavi-observability app.kubernetes.io/name: otel-collector Add the Prometheus Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add stable https://charts.helm.sh/stable helm repo update Install the Helm chart.\nOn your terminal, run the command below:\nhelm install prometheus prometheus-community/prometheus -n [CSM_NAMESPACE] -f prometheus-values.yaml Grafana The Grafana dashboards require Grafana to be deployed in the same Kubernetes cluster as CSM for Observability. Below are the configuration details required to properly set up Grafana to work with CSM for Observability.\nSupported Version Helm Chart 8.5.0 Grafana Helm chart Grafana must be configured with the following data sources/plugins:\nName Additional Information Prometheus data source Prometheus data source Data Table plugin Data Table plugin Pie Chart plugin Pie Chart plugin SimpleJson data source SimpleJson data source Settings for the Grafana Prometheus data source:\nSetting Value Additional Information Name Prometheus Type prometheus URL http://PROMETHEUS_IP:PORT The IP/PORT of your running Prometheus instance Access Proxy Settings for the Grafana SimpleJson data source:\nSetting Value Name Karavi-Topology URL Access CSM for Observability Topology service at https://karavi-topology.namespace.svc.cluster.local:8443 Skip TLS Verify Enabled (If not using CA certificate) With CA Cert Enabled (If using CA certificate) Grafana Helm Deployment Below are the steps to deploy a new Grafana instance into your Kubernetes cluster:\nCreate a ConfigMap.\nWhen using a network that requires a decryption certificate, the Grafana server MUST be configured with the necessary certificate. If no certificate is required, skip to step 2.\nCreate a Config file named grafana-configmap.yaml The file should look like this: # grafana-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: certs-configmap namespace: [CSM_NAMESPACE] labels: certs-configmap: \"1\" data: ca-certificates.crt: |- -----BEGIN CERTIFICATE----- ReplaceMeWithActualCaCERT= -----END CERTIFICATE----- NOTE: you need an actual CA Cert for it to work\nOn your terminal, run the commands below:\nkubectl create -f grafana-configmap.yaml Create a values file.\nCreate a Config file named grafana-values.yaml The file should look like this:\n# grafana-values.yaml image: repository: grafana/grafana tag: 8.5.0 sha: \"\" pullPolicy: IfNotPresent service: type: NodePort ## Administrator credentials when not using an existing Secret adminUser: admin adminPassword: admin ## Pass the plugins you want to be installed as a list. ## plugins: - grafana-simple-json-datasource - briangann-datatable-panel - grafana-piechart-panel ## Configure grafana datasources ## ref: http://docs.grafana.org/administration/provisioning/#datasources ## datasources: datasources.yaml: apiVersion: 1 datasources: - name: Karavi-Topology type: grafana-simple-json-datasource access: proxy url: 'https://karavi-topology:8443' isDefault: null version: 1 editable: true jsonData: tlsSkipVerify: true - name: Prometheus type: prometheus access: proxy url: 'http://prometheus-server:9090' isDefault: null version: 1 editable: true testFramework: enabled: false sidecar: datasources: enabled: true dashboards: enabled: true ## Additional grafana server ConfigMap mounts ## Defines additional mounts with ConfigMap. ConfigMap must be manually created in the namespace. extraConfigmapMounts: [] # If you created a ConfigMap on the previous step, delete [] and uncomment the lines below # - name: certs-configmap # mountPath: /etc/ssl/certs/ca-certificates.crt # subPath: ca-certificates.crt # configMap: certs-configmap # readOnly: true Add the Grafana Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add grafana https://grafana.github.io/helm-charts helm repo update Install the Helm chart.\nOn your terminal, run the commands below:\nhelm install grafana grafana/grafana -n [CSM_NAMESPACE] -f grafana-values.yaml Other Deployment Methods Grafana Labs Operator Deployment Rancher Monitoring and Alerting Deployment Importing CSM for Observability Dashboards Once Grafana is properly configured, you can import the pre-built observability dashboards. Log into Grafana and click the + icon in the side menu. Then click Import. From here you can upload the JSON files or paste the JSON text directly into the text area. Below are the locations of the dashboards that can be imported:\nDashboard Description PowerFlex: I/O Performance by Kubernetes Node Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by Kubernetes node PowerFlex: I/O Performance by Provisioned Volume Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume PowerFlex: Storage Pool Consumption By CSI Driver Provides visibility into the total, used and available capacity for a storage class and associated underlying storage construct PowerStore: I/O Performance by Provisioned Volume Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume PowerStore: I/O Performance by File System Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by filesystem PowerStore: Array and Storage Class Consumption By CSI Driver Provides visibility into the total, used and available capacity for a storage class and associated underlying storage construct PowerScale: I/O Performance by Cluster Provides visibility into the I/O performance metrics (IOPS, bandwidth) by cluster PowerScale: Capacity by Cluster Provides visibility into the total, used, available capacity and directory quota capacity by cluster PowerScale: Capacity by Quota Provides visibility into the subscribed, remaining capacity and usage by quota PowerMax: PowerMax Capacity Provides visibility into the subscribed, used, available capacity for a storage class and associated underlying storage construct PowerMax: PowerMax Performance Provides visibility into the I/O performance metrics (IOPS, bandwidth) by storage group and volume CSI Driver Provisioned Volume Topology Provides visibility into Dell CSI (Container Storage Interface) driver provisioned volume characteristics in Kubernetes correlated with volumes on the storage system. Dynamic Configuration Some parameters can be configured/updated during runtime without restarting the CSM for Observability services. These parameters will be stored in ConfigMaps that can be updated on the Kubernetes cluster. This will automatically change the settings on the services.\nConfigMap Observability Service Parameters karavi-metrics-powerflex-configmap karavi-metrics-powerflex COLLECTOR_ADDRPROVISIONER_NAMESPOWERFLEX_SDC_METRICS_ENABLEDPOWERFLEX_SDC_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_POLL_FREQUENCYPOWERFLEX_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMAT karavi-metrics-powerstore-configmap karavi-metrics-powerstore COLLECTOR_ADDRPROVISIONER_NAMESPOWERSTORE_VOLUME_METRICS_ENABLEDPOWERSTORE_VOLUME_IO_POLL_FREQUENCYPOWERSTORE_SPACE_POLL_FREQUENCYPOWERSTORE_ARRAY_POLL_FREQUENCYPOWERSTORE_FILE_SYSTEM_POLL_FREQUENCYPOWERSTORE_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY karavi-metrics-powerscale-configmap karavi-metrics-powerscale COLLECTOR_ADDR PROVISIONER_NAMES POWERSCALE_MAX_CONCURRENT_QUERIES POWERSCALE_CAPACITY_METRICS_ENABLED POWERSCALE_PERFORMANCE_METRICS_ENABLED POWERSCALE_CLUSTER_CAPACITY_POLL_FREQUENCY POWERSCALE_CLUSTER_PERFORMANCE_POLL_FREQUENCY POWERSCALE_QUOTA_CAPACITY_POLL_FREQUENCY POWERSCALE_ISICLIENT_INSECURE POWERSCALE_ISICLIENT_AUTH_TYPE POWERSCALE_ISICLIENT_VERBOSE LOG_LEVEL LOG_FORMAT karavi-metrics-powermax-configmap karavi-metrics-powermax COLLECTOR_ADDR PROVISIONER_NAMES POWERMAX_MAX_CONCURRENT_QUERIES POWERMAX_CAPACITY_METRICS_ENABLED POWERMAX_PERFORMANCE_METRICS_ENABLED POWERMAX_CAPACITY_POLL_FREQUENCY POWERMAX_PERFORMANCE_POLL_FREQUENCY LOG_LEVEL LOG_FORMAT karavi-topology-configmap karavi-topology PROVISIONER_NAMESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY To update any of these settings, run the following command on the Kubernetes cluster then save the updated ConfigMap data.\nkubectl edit configmap [CONFIG_MAP_NAME] -n [CSM_NAMESPACE] Tracing CSM for Observability is instrumented to report trace data to Zipkin. This helps gather timing data needed to troubleshoot latency problems with CSM for Observability. Follow the instructions below to enable the reporting of trace data:\nDeploy a Zipkin instance in the CSM namespace and expose the service as NodePort for external access.\napiVersion: apps/v1 kind: Deployment metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance template: metadata: labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance spec: containers: - name: zipkin image: \"openzipkin/zipkin\" imagePullPolicy: IfNotPresent env: - name: \"STORAGE_TYPE\" value: \"mem\" - name: \"TRANSPORT_TYPE\" value: \"http\" --- apiVersion: v1 kind: Service metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: ports: - port: 9411 targetPort: 9411 protocol: TCP type: \"NodePort\" selector: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance Add the Zipkin URI to the CSM for Observability ConfigMaps. Based on the manifest above, Zipkin will be running on port 9411.\nNote: Zipkin tracing is currently not supported for the collection of PowerFlex metrics.\nUpdate the ConfigMaps from the table above. Here is an example updating the karavi-topology-configmap based on the deployment manifest above.\nkubectl edit configmap/karavi-topology-configmap -n [CSM_NAMESPACE] Update the ZIPKIN_URI and ZIPKIN_PROBABILITY values and save the ConfigMap.\nZIPKIN_URI: \"http://zipkin:9411/api/v2/spans\" ZIPKIN_SERVICE_NAME: \"karavi-topology\" ZIPKIN_PROBABILITY: \"1.0\" Once the ConfigMaps are updated, the changes will automatically be applied and tracing can be seen by accessing Zipkin on the exposed port.\nUpdating Storage System Credentials If the storage system credentials have been updated in the relevant CSI Driver, CSM for Observability must be updated with those new credentials as follows:\nWhen CSM for Observability uses the Authorization module In this case, all storage system requests made by CSM for Observability will be routed through the Authorization module. The following must be performed:\nUpdate the Authorization Module Token CSI Driver for Dell PowerFlex Delete the current proxy-authz-tokens Secret from the CSM namespace.\nkubectl delete secret proxy-authz-tokens -n [CSM_NAMESPACE] Copy the proxy-authz-tokens Secret from the CSI Driver for Dell PowerFlex to the CSM namespace.\nkubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerScale Delete the current isilon-proxy-authz-tokens Secret from the CSM namespace.\nkubectl delete secret isilon-proxy-authz-tokens -n [CSM_NAMESPACE] Copy the isilon-proxy-authz-tokens Secret from the CSI Driver for Dell PowerScale namespace to the CSM namespace.\nkubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/'| sed 's/name: proxy-authz-tokens/name: isilon-proxy-authz-tokens/' | kubectl create -f CSI Driver for Dell PowerMax Delete the current powermax-proxy-authz-tokens Secret from the CSM namespace.\nkubectl delete secret powermax-proxy-authz-tokens -n [CSM_NAMESPACE] Copy the powermax-proxy-authz-tokens Secret from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nkubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/'| sed 's/name: proxy-authz-tokens/name: powermax-proxy-authz-tokens/' | kubectl create -f Update Storage Systems If the list of storage systems managed by a Dell CSI Driver have changed, the following steps can be performed to update CSM for Observability to reference the updated systems:\nCSI Driver for Dell PowerFlex Delete the current karavi-authorization-config Secret from the CSM namespace.\nkubectl delete secret karavi-authorization-config -n [CSM_NAMESPACE] Copy the karavi-authorization-config Secret from the CSI Driver for Dell PowerFlex namespace to CSM for Observability namespace.\nkubectl get secret karavi-authorization-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerScale Delete the current isilon-karavi-authorization-config Secret from the CSM namespace.\nkubectl delete secret isilon-karavi-authorization-config -n [CSM_NAMESPACE] Copy the isilon-karavi-authorization-config Secret from the CSI Driver for Dell PowerScale namespace to CSM for Observability namespace.\nkubectl get secret karavi-authorization-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: isilon-karavi-authorization-config/' | kubectl create -f CSI Driver for Dell PowerMax Delete the current powermax-karavi-authorization-config secret from the CSM namespace.\nkubectl delete secret powermax-karavi-authorization-config -n [CSM_NAMESPACE] Copy powermax-karavi-authorization-config secret from the CSI Driver for Dell PowerMax to the CSM namespace.\nkubectl get secret karavi-authorization-config proxy-server-root-certificate -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: powermax-karavi-authorization-config/' | kubectl create -f - When CSM for Observability does not use the Authorization module In this case all storage system requests made by CSM for Observability will not be routed through the Authorization module. The following must be performed:\nCSI Driver for Dell PowerFlex Delete the current vxflexos-config Secret from the CSM namespace.\nkubectl delete secret vxflexos-config -n [CSM_NAMESPACE] Copy the vxflexos-config Secret from the CSI Driver for Dell PowerFlex namespace to the CSM namespace.\nkubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default vxflexos-config, please use the following command to copy secret:\nkubectl get secret [VXFLEXOS-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG]/name: vxflexos-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerStore Delete the current powerstore-config Secret from the CSM namespace.\nkubectl delete secret powerstore-config -n [CSM_NAMESPACE] Copy the powerstore-config Secret from the CSI Driver for Dell PowerStore namespace to the CSM namespace.\nkubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default powerstore-config, please use the following command to copy secret:\nkubectl get secret [POWERSTORE-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERSTORE-CONFIG]/name: powerstore-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerScale Delete the current isilon-creds Secret from the CSM namespace.\nkubectl delete secret isilon-creds -n [CSM_NAMESPACE] Copy the isilon-creds Secret from the CSI Driver for Dell PowerScale namespace to the CSM namespace.\nkubectl get secret isilon-creds -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default isilon-creds, please use the following command to copy secret:\nkubectl get secret [ISILON-CREDS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CREDS]/name: isilon-creds/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerMax Delete the secrets in powermax-reverseproxy-config configmap from the CSM namespace.\nfor secret in $(kubectl get configmap powermax-reverseproxy-config -n [CSM_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl delete secret $secret -n [CSM_NAMESPACE] done Delete the current powermax-reverseproxy-config configmap from the CSM namespace.\nkubectl delete configmap powermax-reverseproxy-config -n [CSM_NAMESPACE] Copy the configmap powermax-reverseproxy-config from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nNote: Observability for PowerMax works only with CSI PowerMax driver with Proxy in StandAlone mode.\nkubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy configmap:\nkubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-REVERSEPROXY-CONFIG]/name: powermax-reverseproxy-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the secrets in powermax-reverseproxy-config from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nfor secret in $(kubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy secrets:\nfor secret in $(kubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Deployment\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Deployment\n","ref":"/csm-docs/docs/observability/deployment/","tags":"","title":"Deployment"},{"body":"CSM for Observability can be deployed in one of three ways:\nHelm CSM for Observability Installer CSM for Observability Offline Installer Post Installation Dependencies The following third-party components are required in the same Kubernetes cluster where CSM for Observability has been deployed:\nPrometheus Grafana Other Deployment Methods There are various ways to deploy these components. We recommend following the Helm deployments according to the specifications defined below.\nTip: CSM for Observability must be deployed first. Once the module has been deployed, you can proceed to deploying/configuring Prometheus and Grafana.\nPrometheus The Prometheus service should be running on the same Kubernetes cluster as the CSM for Observability services. As part of the CSM for Observability deployment, the OpenTelemetry Collector gets deployed. CSM for Observability pushes metrics to the OpenTelemetry Collector where the metrics are consumed by Prometheus. Prometheus must be configured to scrape the metrics data from the OpenTelemetry Collector.\nSupported Version Image Helm Chart 2.34.0 prom/prometheus:v2.34.0 Prometheus Helm chart Note: It is the user’s responsibility to provide persistent storage for Prometheus if they want to preserve historical data.\nPrometheus Helm Deployment Here is a sample minimal configuration for Prometheus. Please note that the configuration below uses insecure skip verify. If you wish to properly configure TLS, you will need to provide a ca_file in the Prometheus configuration. The certificate provided as part of the CSM for Observability deployment should be signed by this same CA. For more information about Prometheus configuration, see Prometheus configuration.\nCreate a values file named prometheus-values.yaml.\n# prometheus-values.yaml alertmanager: enabled: false nodeExporter: enabled: false pushgateway: enabled: false kubeStateMetrics: enabled: false configmapReload: prometheus: enabled: false server: enabled: true image: repository: quay.io/prometheus/prometheus tag: v2.34.0 pullPolicy: IfNotPresent persistentVolume: enabled: false service: type: NodePort servicePort: 9090 extraScrapeConfigs: | - job_name: 'karavi-metrics-[CSI-DRIVER]' scrape_interval: 5s scheme: https static_configs: - targets: ['otel-collector:8443'] tls_config: insecure_skip_verify: true If using Rancher, create a ServiceMonitor.\napiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: otel-collector namespace: powerflex spec: endpoints: - path: /metrics port: exporter-https scheme: https tlsConfig: insecureSkipVerify: true selector: matchLabels: app.kubernetes.io/instance: karavi-observability app.kubernetes.io/name: otel-collector Add the Prometheus Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add stable https://charts.helm.sh/stable helm repo update Install the Helm chart.\nOn your terminal, run the command below:\nhelm install prometheus prometheus-community/prometheus -n [CSM_NAMESPACE] -f prometheus-values.yaml Grafana The Grafana dashboards require Grafana to be deployed in the same Kubernetes cluster as CSM for Observability. Below are the configuration details required to properly set up Grafana to work with CSM for Observability.\nSupported Version Helm Chart 8.5.0 Grafana Helm chart Grafana must be configured with the following data sources/plugins:\nName Additional Information Prometheus data source Prometheus data source Data Table plugin Data Table plugin Pie Chart plugin Pie Chart plugin SimpleJson data source SimpleJson data source Settings for the Grafana Prometheus data source:\nSetting Value Additional Information Name Prometheus Type prometheus URL http://PROMETHEUS_IP:PORT The IP/PORT of your running Prometheus instance Access Proxy Settings for the Grafana SimpleJson data source:\nSetting Value Name Karavi-Topology URL Access CSM for Observability Topology service at https://karavi-topology.namespace.svc.cluster.local:8443 Skip TLS Verify Enabled (If not using CA certificate) With CA Cert Enabled (If using CA certificate) Grafana Helm Deployment Below are the steps to deploy a new Grafana instance into your Kubernetes cluster:\nCreate a ConfigMap.\nWhen using a network that requires a decryption certificate, the Grafana server MUST be configured with the necessary certificate. If no certificate is required, skip to step 2.\nCreate a Config file named grafana-configmap.yaml The file should look like this: # grafana-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: certs-configmap namespace: [CSM_NAMESPACE] labels: certs-configmap: \"1\" data: ca-certificates.crt: |- -----BEGIN CERTIFICATE----- ReplaceMeWithActualCaCERT= -----END CERTIFICATE----- NOTE: you need an actual CA Cert for it to work\nOn your terminal, run the commands below:\nkubectl create -f grafana-configmap.yaml Create a values file.\nCreate a Config file named grafana-values.yaml The file should look like this:\n# grafana-values.yaml image: repository: grafana/grafana tag: 8.5.0 sha: \"\" pullPolicy: IfNotPresent service: type: NodePort ## Administrator credentials when not using an existing Secret adminUser: admin adminPassword: admin ## Pass the plugins you want to be installed as a list. ## plugins: - grafana-simple-json-datasource - briangann-datatable-panel - grafana-piechart-panel ## Configure grafana datasources ## ref: http://docs.grafana.org/administration/provisioning/#datasources ## datasources: datasources.yaml: apiVersion: 1 datasources: - name: Karavi-Topology type: grafana-simple-json-datasource access: proxy url: 'https://karavi-topology:8443' isDefault: null version: 1 editable: true jsonData: tlsSkipVerify: true - name: Prometheus type: prometheus access: proxy url: 'http://prometheus-server:9090' isDefault: null version: 1 editable: true testFramework: enabled: false sidecar: datasources: enabled: true dashboards: enabled: true ## Additional grafana server ConfigMap mounts ## Defines additional mounts with ConfigMap. ConfigMap must be manually created in the namespace. extraConfigmapMounts: [] # If you created a ConfigMap on the previous step, delete [] and uncomment the lines below # - name: certs-configmap # mountPath: /etc/ssl/certs/ca-certificates.crt # subPath: ca-certificates.crt # configMap: certs-configmap # readOnly: true Add the Grafana Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add grafana https://grafana.github.io/helm-charts helm repo update Install the Helm chart.\nOn your terminal, run the commands below:\nhelm install grafana grafana/grafana -n [CSM_NAMESPACE] -f grafana-values.yaml Other Deployment Methods Grafana Labs Operator Deployment Rancher Monitoring and Alerting Deployment Importing CSM for Observability Dashboards Once Grafana is properly configured, you can import the pre-built observability dashboards. Log into Grafana and click the + icon in the side menu. Then click Import. From here you can upload the JSON files or paste the JSON text directly into the text area. Below are the locations of the dashboards that can be imported:\nDashboard Description PowerFlex: I/O Performance by Kubernetes Node Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by Kubernetes node PowerFlex: I/O Performance by Provisioned Volume Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume PowerFlex: Storage Pool Consumption By CSI Driver Provides visibility into the total, used and available capacity for a storage class and associated underlying storage construct PowerStore: I/O Performance by Provisioned Volume Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume PowerStore: I/O Performance by File System Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by filesystem PowerStore: Array and Storage Class Consumption By CSI Driver Provides visibility into the total, used and available capacity for a storage class and associated underlying storage construct PowerScale: I/O Performance by Cluster Provides visibility into the I/O performance metrics (IOPS, bandwidth) by cluster PowerScale: Capacity by Cluster Provides visibility into the total, used, available capacity and directory quota capacity by cluster PowerScale: Capacity by Quota Provides visibility into the subscribed, remaining capacity and usage by quota PowerMax: PowerMax Capacity Provides visibility into the subscribed, used, available capacity for a storage class and associated underlying storage construct PowerMax: PowerMax Performance Provides visibility into the I/O performance metrics (IOPS, bandwidth) by storage group and volume CSI Driver Provisioned Volume Topology Provides visibility into Dell CSI (Container Storage Interface) driver provisioned volume characteristics in Kubernetes correlated with volumes on the storage system. Dynamic Configuration Some parameters can be configured/updated during runtime without restarting the CSM for Observability services. These parameters will be stored in ConfigMaps that can be updated on the Kubernetes cluster. This will automatically change the settings on the services.\nConfigMap Observability Service Parameters karavi-metrics-powerflex-configmap karavi-metrics-powerflex COLLECTOR_ADDRPROVISIONER_NAMESPOWERFLEX_SDC_METRICS_ENABLEDPOWERFLEX_SDC_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_POLL_FREQUENCYPOWERFLEX_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMAT karavi-metrics-powerstore-configmap karavi-metrics-powerstore COLLECTOR_ADDRPROVISIONER_NAMESPOWERSTORE_VOLUME_METRICS_ENABLEDPOWERSTORE_VOLUME_IO_POLL_FREQUENCYPOWERSTORE_SPACE_POLL_FREQUENCYPOWERSTORE_ARRAY_POLL_FREQUENCYPOWERSTORE_FILE_SYSTEM_POLL_FREQUENCYPOWERSTORE_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY karavi-metrics-powerscale-configmap karavi-metrics-powerscale COLLECTOR_ADDR PROVISIONER_NAMES POWERSCALE_MAX_CONCURRENT_QUERIES POWERSCALE_CAPACITY_METRICS_ENABLED POWERSCALE_PERFORMANCE_METRICS_ENABLED POWERSCALE_CLUSTER_CAPACITY_POLL_FREQUENCY POWERSCALE_CLUSTER_PERFORMANCE_POLL_FREQUENCY POWERSCALE_QUOTA_CAPACITY_POLL_FREQUENCY POWERSCALE_ISICLIENT_INSECURE POWERSCALE_ISICLIENT_AUTH_TYPE POWERSCALE_ISICLIENT_VERBOSE LOG_LEVEL LOG_FORMAT karavi-metrics-powermax-configmap karavi-metrics-powermax COLLECTOR_ADDR PROVISIONER_NAMES POWERMAX_MAX_CONCURRENT_QUERIES POWERMAX_CAPACITY_METRICS_ENABLED POWERMAX_PERFORMANCE_METRICS_ENABLED POWERMAX_CAPACITY_POLL_FREQUENCY POWERMAX_PERFORMANCE_POLL_FREQUENCY LOG_LEVEL LOG_FORMAT karavi-topology-configmap karavi-topology PROVISIONER_NAMESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY To update any of these settings, run the following command on the Kubernetes cluster then save the updated ConfigMap data.\nkubectl edit configmap [CONFIG_MAP_NAME] -n [CSM_NAMESPACE] Tracing CSM for Observability is instrumented to report trace data to Zipkin. This helps gather timing data needed to troubleshoot latency problems with CSM for Observability. Follow the instructions below to enable the reporting of trace data:\nDeploy a Zipkin instance in the CSM namespace and expose the service as NodePort for external access.\napiVersion: apps/v1 kind: Deployment metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance template: metadata: labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance spec: containers: - name: zipkin image: \"openzipkin/zipkin\" imagePullPolicy: IfNotPresent env: - name: \"STORAGE_TYPE\" value: \"mem\" - name: \"TRANSPORT_TYPE\" value: \"http\" --- apiVersion: v1 kind: Service metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: ports: - port: 9411 targetPort: 9411 protocol: TCP type: \"NodePort\" selector: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance Add the Zipkin URI to the CSM for Observability ConfigMaps. Based on the manifest above, Zipkin will be running on port 9411.\nNote: Zipkin tracing is currently not supported for the collection of PowerFlex metrics.\nUpdate the ConfigMaps from the table above. Here is an example updating the karavi-topology-configmap based on the deployment manifest above.\nkubectl edit configmap/karavi-topology-configmap -n [CSM_NAMESPACE] Update the ZIPKIN_URI and ZIPKIN_PROBABILITY values and save the ConfigMap.\nZIPKIN_URI: \"http://zipkin:9411/api/v2/spans\" ZIPKIN_SERVICE_NAME: \"karavi-topology\" ZIPKIN_PROBABILITY: \"1.0\" Once the ConfigMaps are updated, the changes will automatically be applied and tracing can be seen by accessing Zipkin on the exposed port.\nUpdating Storage System Credentials If the storage system credentials have been updated in the relevant CSI Driver, CSM for Observability must be updated with those new credentials as follows:\nWhen CSM for Observability uses the Authorization module In this case, all storage system requests made by CSM for Observability will be routed through the Authorization module. The following must be performed:\nUpdate the Authorization Module Token CSI Driver for Dell PowerFlex Delete the current proxy-authz-tokens Secret from the CSM namespace.\nkubectl delete secret proxy-authz-tokens -n [CSM_NAMESPACE] Copy the proxy-authz-tokens Secret from the CSI Driver for Dell PowerFlex to the CSM namespace.\nkubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerScale Delete the current isilon-proxy-authz-tokens Secret from the CSM namespace.\nkubectl delete secret isilon-proxy-authz-tokens -n [CSM_NAMESPACE] Copy the isilon-proxy-authz-tokens Secret from the CSI Driver for Dell PowerScale namespace to the CSM namespace.\nkubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/'| sed 's/name: proxy-authz-tokens/name: isilon-proxy-authz-tokens/' | kubectl create -f CSI Driver for Dell PowerMax Delete the current powermax-proxy-authz-tokens Secret from the CSM namespace.\nkubectl delete secret powermax-proxy-authz-tokens -n [CSM_NAMESPACE] Copy the powermax-proxy-authz-tokens Secret from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nkubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/'| sed 's/name: proxy-authz-tokens/name: powermax-proxy-authz-tokens/' | kubectl create -f Update Storage Systems If the list of storage systems managed by a Dell CSI Driver have changed, the following steps can be performed to update CSM for Observability to reference the updated systems:\nCSI Driver for Dell PowerFlex Delete the current karavi-authorization-config Secret from the CSM namespace.\nkubectl delete secret karavi-authorization-config -n [CSM_NAMESPACE] Copy the karavi-authorization-config Secret from the CSI Driver for Dell PowerFlex namespace to CSM for Observability namespace.\nkubectl get secret karavi-authorization-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerScale Delete the current isilon-karavi-authorization-config Secret from the CSM namespace.\nkubectl delete secret isilon-karavi-authorization-config -n [CSM_NAMESPACE] Copy the isilon-karavi-authorization-config Secret from the CSI Driver for Dell PowerScale namespace to CSM for Observability namespace.\nkubectl get secret karavi-authorization-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: isilon-karavi-authorization-config/' | kubectl create -f CSI Driver for Dell PowerMax Delete the current powermax-karavi-authorization-config secret from the CSM namespace.\nkubectl delete secret powermax-karavi-authorization-config -n [CSM_NAMESPACE] Copy powermax-karavi-authorization-config secret from the CSI Driver for Dell PowerMax to the CSM namespace.\nkubectl get secret karavi-authorization-config proxy-server-root-certificate -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: powermax-karavi-authorization-config/' | kubectl create -f - When CSM for Observability does not use the Authorization module In this case all storage system requests made by CSM for Observability will not be routed through the Authorization module. The following must be performed:\nCSI Driver for Dell PowerFlex Delete the current vxflexos-config Secret from the CSM namespace.\nkubectl delete secret vxflexos-config -n [CSM_NAMESPACE] Copy the vxflexos-config Secret from the CSI Driver for Dell PowerFlex namespace to the CSM namespace.\nkubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default vxflexos-config, please use the following command to copy secret:\nkubectl get secret [VXFLEXOS-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG]/name: vxflexos-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerStore Delete the current powerstore-config Secret from the CSM namespace.\nkubectl delete secret powerstore-config -n [CSM_NAMESPACE] Copy the powerstore-config Secret from the CSI Driver for Dell PowerStore namespace to the CSM namespace.\nkubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default powerstore-config, please use the following command to copy secret:\nkubectl get secret [POWERSTORE-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERSTORE-CONFIG]/name: powerstore-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerScale Delete the current isilon-creds Secret from the CSM namespace.\nkubectl delete secret isilon-creds -n [CSM_NAMESPACE] Copy the isilon-creds Secret from the CSI Driver for Dell PowerScale namespace to the CSM namespace.\nkubectl get secret isilon-creds -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default isilon-creds, please use the following command to copy secret:\nkubectl get secret [ISILON-CREDS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CREDS]/name: isilon-creds/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerMax Delete the secrets in powermax-reverseproxy-config configmap from the CSM namespace.\nfor secret in $(kubectl get configmap powermax-reverseproxy-config -n [CSM_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl delete secret $secret -n [CSM_NAMESPACE] done Delete the current powermax-reverseproxy-config configmap from the CSM namespace.\nkubectl delete configmap powermax-reverseproxy-config -n [CSM_NAMESPACE] Copy the configmap powermax-reverseproxy-config from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nNote: Observability for PowerMax works only with CSI PowerMax driver with Proxy in StandAlone mode.\nkubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy configmap:\nkubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-REVERSEPROXY-CONFIG]/name: powermax-reverseproxy-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the secrets in powermax-reverseproxy-config from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nfor secret in $(kubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy secrets:\nfor secret in $(kubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Deployment\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Deployment\n","ref":"/csm-docs/v1/observability/deployment/","tags":"","title":"Deployment"},{"body":"CSM for Resiliency is installed as part of the Dell CSI driver installation. The drivers can be installed either by a helm chart or by the Dell CSI Operator. Currently, only Helm chart installation is supported.\nFor information on the PowerFlex CSI driver, see PowerFlex CSI Driver.\nFor information on the Unity XT CSI driver, see Unity XT CSI Driver.\nFor information on the PowerScale CSI driver, see PowerScale CSI Driver.\nFor information on the PowerStore CSI driver, see PowerStore CSI Driver.\nConfigure all the helm chart parameters described below before installing the drivers.\nHelm Chart Installation The drivers that support Helm chart installation allow CSM for Resiliency to be optionally installed by variables in the chart. There is a podmon block specified in the values.yaml file of the chart that will look similar to the text below by default:\n# Enable this feature only after contact support for additional information podmon: enabled: true image: dellemc/podmon:v1.3.0 controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" To install CSM for Resiliency with the driver, the following changes are required:\nEnable CSM for Resiliency by changing the podmon.enabled boolean to true. This will enable both controller-podmon and node-podmon. Specify the podmon image to be used as podmon.image. Specify arguments to controller-podmon in the podmon.controller.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon. Specify arguments to node-podmon in the podmon.node.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon. Podmon Arguments Argument Required Description Applicability enabled Required Boolean “true” enables CSM for Resiliency installation with the driver in a helm installation. top level image Required Must be set to a repository where the podmon image can be pulled. controller \u0026 node mode Required Must be set to “controller” for controller-podmon and “node” for node-podmon. controller \u0026 node csisock Required This should be left as set in the helm template for the driver. For controller: -csisock=unix:/var/run/csi/csi.sock For node it will vary depending on the driver’s identity: -csisock=unix:/var/lib/kubelet/plugins\n/vxflexos.emc.dell.com/csi_sock controller \u0026 node leaderelection Required Boolean value that should be set true for controller and false for node. The default value is true. controller \u0026 node skipArrayConnectionValidation Optional Boolean value that if set to true will cause controllerPodCleanup to skip the validation that no I/O is ongoing before cleaning up the pod. If set to true will cause controllerPodCleanup on K8S Control Plane failure (kubelet service down). controller labelKey Optional String value that sets the label key used to denote pods to be monitored by CSM for Resiliency. It will make life easier if this key is the same for all driver types, and drivers are differentiated by different labelValues (see below). If the label keys are the same across all drivers you can do kubectl get pods -A -l labelKey to find all the CSM for Resiliency protected pods. labelKey defaults to “podmon.dellemc.com/driver”. controller \u0026 node labelValue Required String that sets the value that denotes pods to be monitored by CSM for Resiliency. This must be specific for each driver. Defaults to “csi-vxflexos” for CSI Driver for Dell PowerFlex and “csi-unity” for CSI Driver for Dell Unity XT controller \u0026 node arrayConnectivityPollRate Optional The minimum polling rate in seconds to determine if the array has connectivity to a node. Should not be set to less than 5 seconds. See the specific section for each array type for additional guidance. controller \u0026 node arrayConnectivityConnectionLossThreshold Optional Gives the number of failed connection polls that will be deemed to indicate array connectivity loss. Should not be set to less than 3. See the specific section for each array type for additional guidance. controller driver-config-params Required String that set the path to a file containing configuration parameter(for instance, Log levels) for a driver. controller \u0026 node ignoreVolumelessPods Optional Boolean value that if set to true will enable CSM for Resiliency to ignore pods without persistent volume attached to the pod. controller \u0026 node PowerFlex Specific Recommendations PowerFlex supports a very robust array connection validation mechanism that can detect changes in connectivity in about two seconds and can detect whether I/O has occurred over a five-second sample. For that reason it is recommended to set “skipArrayConnectionValidation=false” (which is the default) and to set “arrayConnectivityPollRate=5” (5 seconds) and “arrayConnectivityConnectionLossThreshold=3” to 3 or more.\nHere is a typical installation used for testing:\npodmon: image: dellemc/podmon enabled: true controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=controller\" - \"--arrayConnectivityPollRate=5\" - \"--arrayConnectivityConnectionLossThreshold=3\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" Unity XT Specific Recommendations Here is a typical installation used for testing:\npodmon: image: dellemc/podmon enabled: true controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-unity\" - \"--driverPath=csi-unity.dellemc.com\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/unity-config/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/unity.emc.dell.com/csi_sock\" - \"--labelvalue=csi-unity\" - \"--driverPath=csi-unity.dellemc.com\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/unity-config/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" PowerScale Specific Recommendations Here is a typical installation used for testing:\npodmon: image: dellemc/podmon enabled: true controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-isilon\" - \"--arrayConnectivityPollRate=60\" - \"--driverPath=csi-isilon.dellemc.com\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/csi-isilon-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/csi-isilon/csi_sock\" - \"--labelvalue=csi-isilon\" - \"--arrayConnectivityPollRate=60\" - \"--driverPath=csi-isilon.dellemc.com\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/csi-isilon-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" PowerStore Specific Recommendations Here is a typical installation used for testing:\npodmon: enabled: true image: dellemc/podmon controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-powerstore\" - \"--arrayConnectivityPollRate=60\" - \"--driverPath=csi-powerstore.dellemc.com\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/powerstore-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/csi-powerstore.dellemc.com/csi_sock\" - \"--labelvalue=csi-powerstore\" - \"--arrayConnectivityPollRate=60\" - \"--driverPath=csi-powerstore.dellemc.com\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/powerstore-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" Dynamic parameters CSM for Resiliency has configuration parameters that can be updated dynamically, such as the logging level and format. This can be done by editing the Dell CSI Driver’s parameters ConfigMap. The ConfigMap can be queried using kubectl. For example, the Dell Powerflex CSI Driver ConfigMaps can be found using this command: kubectl get -n vxflexos configmap. The ConfigMap to edit will have this pattern: -config-params (e.g., vxflexos-config-params).\nTo update or add parameters, you can use the kubectl edit command. For example, kubectl edit -n vxflexos configmap vxflexos-config-params.\nThis is a list of parameters that can be adjusted for CSM for Resiliency:\nParameter Type Default Description PODMON_CONTROLLER_LOG_FORMAT String “text” Logging format output for the controller podmon sidecar. Should be “text” or “json” PODMON_CONTROLLER_LOG_LEVEL String “debug” Logging level for the controller podmon sidecar. Standard values: ‘info’, ’error’, ‘warning’, ‘debug’, ’trace’ PODMON_NODE_LOG_FORMAT String “text” Logging format output for the node podmon sidecar. Should be “text” or “json” PODMON_NODE_LOG_LEVEL String “debug” Logging level for the node podmon sidecar. Standard values: ‘info’, ’error’, ‘warning’, ‘debug’, ’trace’ PODMON_ARRAY_CONNECTIVITY_POLL_RATE Integer (\u003e0) 15 An interval in seconds to poll the underlying array PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD Integer (\u003e0) 3 A value representing the number of failed connection poll intervals before marking the array connectivity as lost PODMON_SKIP_ARRAY_CONNECTION_VALIDATION Boolean false Flag to disable the array connectivity check, set to true for NoSchedule or NoExecute taint due to K8S Control Plane failure (kubelet failure) Here is an example of the parameters:\nPODMON_CONTROLLER_LOG_FORMAT: \"text\" PODMON_CONTROLLER_LOG_LEVEL: \"info\" PODMON_NODE_LOG_FORMAT: \"text\" PODMON_NODE_LOG_LEVEL: \"info\" PODMON_ARRAY_CONNECTIVITY_POLL_RATE: 20 PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD: 2 PODMON_SKIP_ARRAY_CONNECTION_VALIDATION: true ","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency installation\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency installation\n","ref":"/csm-docs/v1/resiliency/deployment/","tags":"","title":"Deployment"},{"body":"CSM for Observability can be deployed in one of three ways:\nHelm CSM for Observability Installer CSM for Observability Offline Installer Post Installation Dependencies The following third-party components are required in the same Kubernetes cluster where CSM for Observability has been deployed:\nPrometheus Grafana Other Deployment Methods There are various ways to deploy these components. We recommend following the Helm deployments according to the specifications defined below.\nTip: CSM for Observability must be deployed first. Once the module has been deployed, you can proceed to deploying/configuring Prometheus and Grafana.\nPrometheus The Prometheus service should be running on the same Kubernetes cluster as the CSM for Observability services. As part of the CSM for Observability deployment, the OpenTelemetry Collector gets deployed. CSM for Observability pushes metrics to the OpenTelemetry Collector where the metrics are consumed by Prometheus. Prometheus must be configured to scrape the metrics data from the OpenTelemetry Collector.\nSupported Version Image Helm Chart 2.34.0 prom/prometheus:v2.34.0 Prometheus Helm chart Note: It is the user’s responsibility to provide persistent storage for Prometheus if they want to preserve historical data.\nPrometheus Helm Deployment Here is a sample minimal configuration for Prometheus. Please note that the configuration below uses insecure skip verify. If you wish to properly configure TLS, you will need to provide a ca_file in the Prometheus configuration. The certificate provided as part of the CSM for Observability deployment should be signed by this same CA. For more information about Prometheus configuration, see Prometheus configuration.\nCreate a values file named prometheus-values.yaml.\n# prometheus-values.yaml alertmanager: enabled: false nodeExporter: enabled: false pushgateway: enabled: false kubeStateMetrics: enabled: false configmapReload: prometheus: enabled: false server: enabled: true image: repository: quay.io/prometheus/prometheus tag: v2.34.0 pullPolicy: IfNotPresent persistentVolume: enabled: false service: type: NodePort servicePort: 9090 extraScrapeConfigs: | - job_name: 'karavi-metrics-[CSI-DRIVER]' scrape_interval: 5s scheme: https static_configs: - targets: ['otel-collector:8443'] tls_config: insecure_skip_verify: true If using Rancher, create a ServiceMonitor.\napiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: otel-collector namespace: powerflex spec: endpoints: - path: /metrics port: exporter-https scheme: https tlsConfig: insecureSkipVerify: true selector: matchLabels: app.kubernetes.io/instance: karavi-observability app.kubernetes.io/name: otel-collector Add the Prometheus Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add stable https://charts.helm.sh/stable helm repo update Install the Helm chart.\nOn your terminal, run the command below:\nhelm install prometheus prometheus-community/prometheus -n [CSM_NAMESPACE] -f prometheus-values.yaml Grafana The Grafana dashboards require Grafana to be deployed in the same Kubernetes cluster as CSM for Observability. Below are the configuration details required to properly set up Grafana to work with CSM for Observability.\nSupported Version Helm Chart 8.5.0 Grafana Helm chart Grafana must be configured with the following data sources/plugins:\nName Additional Information Prometheus data source Prometheus data source Data Table plugin Data Table plugin Pie Chart plugin Pie Chart plugin SimpleJson data source SimpleJson data source Settings for the Grafana Prometheus data source:\nSetting Value Additional Information Name Prometheus Type prometheus URL http://PROMETHEUS_IP:PORT The IP/PORT of your running Prometheus instance Access Proxy Settings for the Grafana SimpleJson data source:\nSetting Value Name Karavi-Topology URL Access CSM for Observability Topology service at https://karavi-topology.namespace.svc.cluster.local:8443 Skip TLS Verify Enabled (If not using CA certificate) With CA Cert Enabled (If using CA certificate) Grafana Helm Deployment Below are the steps to deploy a new Grafana instance into your Kubernetes cluster:\nCreate a ConfigMap.\nWhen using a network that requires a decryption certificate, the Grafana server MUST be configured with the necessary certificate. If no certificate is required, skip to step 2.\nCreate a Config file named grafana-configmap.yaml The file should look like this: # grafana-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: certs-configmap namespace: [CSM_NAMESPACE] labels: certs-configmap: \"1\" data: ca-certificates.crt: |- -----BEGIN CERTIFICATE----- ReplaceMeWithActualCaCERT= -----END CERTIFICATE----- NOTE: you need an actual CA Cert for it to work\nOn your terminal, run the commands below:\nkubectl create -f grafana-configmap.yaml Create a values file.\nCreate a Config file named grafana-values.yaml The file should look like this:\n# grafana-values.yaml image: repository: grafana/grafana tag: 8.5.0 sha: \"\" pullPolicy: IfNotPresent service: type: NodePort ## Administrator credentials when not using an existing Secret adminUser: admin adminPassword: admin ## Pass the plugins you want to be installed as a list. ## plugins: - grafana-simple-json-datasource - briangann-datatable-panel - grafana-piechart-panel ## Configure grafana datasources ## ref: http://docs.grafana.org/administration/provisioning/#datasources ## datasources: datasources.yaml: apiVersion: 1 datasources: - name: Karavi-Topology type: grafana-simple-json-datasource access: proxy url: 'https://karavi-topology:8443' isDefault: null version: 1 editable: true jsonData: tlsSkipVerify: true - name: Prometheus type: prometheus access: proxy url: 'http://prometheus-server:9090' isDefault: null version: 1 editable: true testFramework: enabled: false sidecar: datasources: enabled: true dashboards: enabled: true ## Additional grafana server ConfigMap mounts ## Defines additional mounts with ConfigMap. ConfigMap must be manually created in the namespace. extraConfigmapMounts: [] # If you created a ConfigMap on the previous step, delete [] and uncomment the lines below # - name: certs-configmap # mountPath: /etc/ssl/certs/ca-certificates.crt # subPath: ca-certificates.crt # configMap: certs-configmap # readOnly: true Add the Grafana Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add grafana https://grafana.github.io/helm-charts helm repo update Install the Helm chart.\nOn your terminal, run the commands below:\nhelm install grafana grafana/grafana -n [CSM_NAMESPACE] -f grafana-values.yaml Other Deployment Methods Grafana Labs Operator Deployment Rancher Monitoring and Alerting Deployment Importing CSM for Observability Dashboards Once Grafana is properly configured, you can import the pre-built observability dashboards. Log into Grafana and click the + icon in the side menu. Then click Import. From here you can upload the JSON files or paste the JSON text directly into the text area. Below are the locations of the dashboards that can be imported:\nDashboard Description PowerFlex: I/O Performance by Kubernetes Node Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by Kubernetes node PowerFlex: I/O Performance by Provisioned Volume Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume PowerFlex: Storage Pool Consumption By CSI Driver Provides visibility into the total, used and available capacity for a storage class and associated underlying storage construct PowerStore: I/O Performance by Provisioned Volume Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume PowerStore: I/O Performance by File System Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by filesystem PowerStore: Array and Storage Class Consumption By CSI Driver Provides visibility into the total, used and available capacity for a storage class and associated underlying storage construct PowerScale: I/O Performance by Cluster Provides visibility into the I/O performance metrics (IOPS, bandwidth) by cluster PowerScale: Capacity by Cluster Provides visibility into the total, used, available capacity and directory quota capacity by cluster PowerScale: Capacity by Quota Provides visibility into the subscribed, remaining capacity and usage by quota PowerMax: PowerMax Capacity Provides visibility into the subscribed, used, available capacity for a storage class and associated underlying storage construct PowerMax: PowerMax Performance Provides visibility into the I/O performance metrics (IOPS, bandwidth) by storage group and volume CSI Driver Provisioned Volume Topology Provides visibility into Dell CSI (Container Storage Interface) driver provisioned volume characteristics in Kubernetes correlated with volumes on the storage system. Dynamic Configuration Some parameters can be configured/updated during runtime without restarting the CSM for Observability services. These parameters will be stored in ConfigMaps that can be updated on the Kubernetes cluster. This will automatically change the settings on the services.\nConfigMap Observability Service Parameters karavi-metrics-powerflex-configmap karavi-metrics-powerflex COLLECTOR_ADDRPROVISIONER_NAMESPOWERFLEX_SDC_METRICS_ENABLEDPOWERFLEX_SDC_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_POLL_FREQUENCYPOWERFLEX_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMAT karavi-metrics-powerstore-configmap karavi-metrics-powerstore COLLECTOR_ADDRPROVISIONER_NAMESPOWERSTORE_VOLUME_METRICS_ENABLEDPOWERSTORE_VOLUME_IO_POLL_FREQUENCYPOWERSTORE_SPACE_POLL_FREQUENCYPOWERSTORE_ARRAY_POLL_FREQUENCYPOWERSTORE_FILE_SYSTEM_POLL_FREQUENCYPOWERSTORE_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY karavi-metrics-powerscale-configmap karavi-metrics-powerscale COLLECTOR_ADDR PROVISIONER_NAMES POWERSCALE_MAX_CONCURRENT_QUERIES POWERSCALE_CAPACITY_METRICS_ENABLED POWERSCALE_PERFORMANCE_METRICS_ENABLED POWERSCALE_CLUSTER_CAPACITY_POLL_FREQUENCY POWERSCALE_CLUSTER_PERFORMANCE_POLL_FREQUENCY POWERSCALE_QUOTA_CAPACITY_POLL_FREQUENCY POWERSCALE_ISICLIENT_INSECURE POWERSCALE_ISICLIENT_AUTH_TYPE POWERSCALE_ISICLIENT_VERBOSE LOG_LEVEL LOG_FORMAT karavi-metrics-powermax-configmap karavi-metrics-powermax COLLECTOR_ADDR PROVISIONER_NAMES POWERMAX_MAX_CONCURRENT_QUERIES POWERMAX_CAPACITY_METRICS_ENABLED POWERMAX_PERFORMANCE_METRICS_ENABLED POWERMAX_CAPACITY_POLL_FREQUENCY POWERMAX_PERFORMANCE_POLL_FREQUENCY LOG_LEVEL LOG_FORMAT karavi-topology-configmap karavi-topology PROVISIONER_NAMESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY To update any of these settings, run the following command on the Kubernetes cluster then save the updated ConfigMap data.\nkubectl edit configmap [CONFIG_MAP_NAME] -n [CSM_NAMESPACE] Tracing CSM for Observability is instrumented to report trace data to Zipkin. This helps gather timing data needed to troubleshoot latency problems with CSM for Observability. Follow the instructions below to enable the reporting of trace data:\nDeploy a Zipkin instance in the CSM namespace and expose the service as NodePort for external access.\napiVersion: apps/v1 kind: Deployment metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance template: metadata: labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance spec: containers: - name: zipkin image: \"openzipkin/zipkin\" imagePullPolicy: IfNotPresent env: - name: \"STORAGE_TYPE\" value: \"mem\" - name: \"TRANSPORT_TYPE\" value: \"http\" --- apiVersion: v1 kind: Service metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: ports: - port: 9411 targetPort: 9411 protocol: TCP type: \"NodePort\" selector: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance Add the Zipkin URI to the CSM for Observability ConfigMaps. Based on the manifest above, Zipkin will be running on port 9411.\nNote: Zipkin tracing is currently not supported for the collection of PowerFlex metrics.\nUpdate the ConfigMaps from the table above. Here is an example updating the karavi-topology-configmap based on the deployment manifest above.\nkubectl edit configmap/karavi-topology-configmap -n [CSM_NAMESPACE] Update the ZIPKIN_URI and ZIPKIN_PROBABILITY values and save the ConfigMap.\nZIPKIN_URI: \"http://zipkin:9411/api/v2/spans\" ZIPKIN_SERVICE_NAME: \"karavi-topology\" ZIPKIN_PROBABILITY: \"1.0\" Once the ConfigMaps are updated, the changes will automatically be applied and tracing can be seen by accessing Zipkin on the exposed port.\nUpdating Storage System Credentials If the storage system credentials have been updated in the relevant CSI Driver, CSM for Observability must be updated with those new credentials as follows:\nWhen CSM for Observability uses the Authorization module In this case, all storage system requests made by CSM for Observability will be routed through the Authorization module. The following must be performed:\nUpdate the Authorization Module Token CSI Driver for Dell PowerFlex Delete the current proxy-authz-tokens Secret from the CSM namespace.\nkubectl delete secret proxy-authz-tokens -n [CSM_NAMESPACE] Copy the proxy-authz-tokens Secret from the CSI Driver for Dell PowerFlex to the CSM namespace.\nkubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerScale Delete the current isilon-proxy-authz-tokens Secret from the CSM namespace.\nkubectl delete secret isilon-proxy-authz-tokens -n [CSM_NAMESPACE] Copy the isilon-proxy-authz-tokens Secret from the CSI Driver for Dell PowerScale namespace to the CSM namespace.\nkubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/'| sed 's/name: proxy-authz-tokens/name: isilon-proxy-authz-tokens/' | kubectl create -f CSI Driver for Dell PowerMax Delete the current powermax-proxy-authz-tokens Secret from the CSM namespace.\nkubectl delete secret powermax-proxy-authz-tokens -n [CSM_NAMESPACE] Copy the powermax-proxy-authz-tokens Secret from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nkubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/'| sed 's/name: proxy-authz-tokens/name: powermax-proxy-authz-tokens/' | kubectl create -f Update Storage Systems If the list of storage systems managed by a Dell CSI Driver have changed, the following steps can be performed to update CSM for Observability to reference the updated systems:\nCSI Driver for Dell PowerFlex Delete the current karavi-authorization-config Secret from the CSM namespace.\nkubectl delete secret karavi-authorization-config -n [CSM_NAMESPACE] Copy the karavi-authorization-config Secret from the CSI Driver for Dell PowerFlex namespace to CSM for Observability namespace.\nkubectl get secret karavi-authorization-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerScale Delete the current isilon-karavi-authorization-config Secret from the CSM namespace.\nkubectl delete secret isilon-karavi-authorization-config -n [CSM_NAMESPACE] Copy the isilon-karavi-authorization-config Secret from the CSI Driver for Dell PowerScale namespace to CSM for Observability namespace.\nkubectl get secret karavi-authorization-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: isilon-karavi-authorization-config/' | kubectl create -f CSI Driver for Dell PowerMax Delete the current powermax-karavi-authorization-config secret from the CSM namespace.\nkubectl delete secret powermax-karavi-authorization-config -n [CSM_NAMESPACE] Copy powermax-karavi-authorization-config secret from the CSI Driver for Dell PowerMax to the CSM namespace.\nkubectl get secret karavi-authorization-config proxy-server-root-certificate -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: powermax-karavi-authorization-config/' | kubectl create -f - When CSM for Observability does not use the Authorization module In this case all storage system requests made by CSM for Observability will not be routed through the Authorization module. The following must be performed:\nCSI Driver for Dell PowerFlex Delete the current vxflexos-config Secret from the CSM namespace.\nkubectl delete secret vxflexos-config -n [CSM_NAMESPACE] Copy the vxflexos-config Secret from the CSI Driver for Dell PowerFlex namespace to the CSM namespace.\nkubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default vxflexos-config, please use the following command to copy secret:\nkubectl get secret [VXFLEXOS-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG]/name: vxflexos-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerStore Delete the current powerstore-config Secret from the CSM namespace.\nkubectl delete secret powerstore-config -n [CSM_NAMESPACE] Copy the powerstore-config Secret from the CSI Driver for Dell PowerStore namespace to the CSM namespace.\nkubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default powerstore-config, please use the following command to copy secret:\nkubectl get secret [POWERSTORE-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERSTORE-CONFIG]/name: powerstore-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerScale Delete the current isilon-creds Secret from the CSM namespace.\nkubectl delete secret isilon-creds -n [CSM_NAMESPACE] Copy the isilon-creds Secret from the CSI Driver for Dell PowerScale namespace to the CSM namespace.\nkubectl get secret isilon-creds -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default isilon-creds, please use the following command to copy secret:\nkubectl get secret [ISILON-CREDS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CREDS]/name: isilon-creds/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerMax Delete the secrets in powermax-reverseproxy-config configmap from the CSM namespace.\nfor secret in $(kubectl get configmap powermax-reverseproxy-config -n [CSM_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl delete secret $secret -n [CSM_NAMESPACE] done Delete the current powermax-reverseproxy-config configmap from the CSM namespace.\nkubectl delete configmap powermax-reverseproxy-config -n [CSM_NAMESPACE] Copy the configmap powermax-reverseproxy-config from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nNote: Observability for PowerMax works only with CSI PowerMax driver with Proxy in StandAlone mode.\nkubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy configmap:\nkubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-REVERSEPROXY-CONFIG]/name: powermax-reverseproxy-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the secrets in powermax-reverseproxy-config from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nfor secret in $(kubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy secrets:\nfor secret in $(kubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Deployment\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Deployment\n","ref":"/csm-docs/v2/observability/deployment/","tags":"","title":"Deployment"},{"body":"CSM for Resiliency is installed as part of the Dell CSI driver installation. The drivers can be installed either by a helm chart or by the Dell CSI Operator. Currently, only Helm chart installation is supported.\nFor information on the PowerFlex CSI driver, see PowerFlex CSI Driver.\nFor information on the Unity XT CSI driver, see Unity XT CSI Driver.\nFor information on the PowerScale CSI driver, see PowerScale CSI Driver.\nFor information on the PowerStore CSI driver, see PowerStore CSI Driver.\nConfigure all the helm chart parameters described below before installing the drivers.\nHelm Chart Installation The drivers that support Helm chart installation allow CSM for Resiliency to be optionally installed by variables in the chart. There is a podmon block specified in the values.yaml file of the chart that will look similar to the text below by default:\n# Enable this feature only after contact support for additional information podmon: enabled: true image: dellemc/podmon:v1.3.0 controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" To install CSM for Resiliency with the driver, the following changes are required:\nEnable CSM for Resiliency by changing the podmon.enabled boolean to true. This will enable both controller-podmon and node-podmon. Specify the podmon image to be used as podmon.image. Specify arguments to controller-podmon in the podmon.controller.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon. Specify arguments to node-podmon in the podmon.node.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon. Podmon Arguments Argument Required Description Applicability enabled Required Boolean “true” enables CSM for Resiliency installation with the driver in a helm installation. top level image Required Must be set to a repository where the podmon image can be pulled. controller \u0026 node mode Required Must be set to “controller” for controller-podmon and “node” for node-podmon. controller \u0026 node csisock Required This should be left as set in the helm template for the driver. For controller: -csisock=unix:/var/run/csi/csi.sock For node it will vary depending on the driver’s identity: -csisock=unix:/var/lib/kubelet/plugins\n/vxflexos.emc.dell.com/csi_sock controller \u0026 node leaderelection Required Boolean value that should be set true for controller and false for node. The default value is true. controller \u0026 node skipArrayConnectionValidation Optional Boolean value that if set to true will cause controllerPodCleanup to skip the validation that no I/O is ongoing before cleaning up the pod. If set to true will cause controllerPodCleanup on K8S Control Plane failure (kubelet service down). controller labelKey Optional String value that sets the label key used to denote pods to be monitored by CSM for Resiliency. It will make life easier if this key is the same for all driver types, and drivers are differentiated by different labelValues (see below). If the label keys are the same across all drivers you can do kubectl get pods -A -l labelKey to find all the CSM for Resiliency protected pods. labelKey defaults to “podmon.dellemc.com/driver”. controller \u0026 node labelValue Required String that sets the value that denotes pods to be monitored by CSM for Resiliency. This must be specific for each driver. Defaults to “csi-vxflexos” for CSI Driver for Dell PowerFlex and “csi-unity” for CSI Driver for Dell Unity XT controller \u0026 node arrayConnectivityPollRate Optional The minimum polling rate in seconds to determine if the array has connectivity to a node. Should not be set to less than 5 seconds. See the specific section for each array type for additional guidance. controller \u0026 node arrayConnectivityConnectionLossThreshold Optional Gives the number of failed connection polls that will be deemed to indicate array connectivity loss. Should not be set to less than 3. See the specific section for each array type for additional guidance. controller driver-config-params Required String that set the path to a file containing configuration parameter(for instance, Log levels) for a driver. controller \u0026 node ignoreVolumelessPods Optional Boolean value that if set to true will enable CSM for Resiliency to ignore pods without persistent volume attached to the pod. controller \u0026 node PowerFlex Specific Recommendations PowerFlex supports a very robust array connection validation mechanism that can detect changes in connectivity in about two seconds and can detect whether I/O has occurred over a five-second sample. For that reason it is recommended to set “skipArrayConnectionValidation=false” (which is the default) and to set “arrayConnectivityPollRate=5” (5 seconds) and “arrayConnectivityConnectionLossThreshold=3” to 3 or more.\nHere is a typical installation used for testing:\npodmon: image: dellemc/podmon enabled: true controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=controller\" - \"--arrayConnectivityPollRate=5\" - \"--arrayConnectivityConnectionLossThreshold=3\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" Unity XT Specific Recommendations Here is a typical installation used for testing:\npodmon: image: dellemc/podmon enabled: true controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-unity\" - \"--driverPath=csi-unity.dellemc.com\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/unity-config/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/unity.emc.dell.com/csi_sock\" - \"--labelvalue=csi-unity\" - \"--driverPath=csi-unity.dellemc.com\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/unity-config/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" PowerScale Specific Recommendations Here is a typical installation used for testing:\npodmon: image: dellemc/podmon enabled: true controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-isilon\" - \"--arrayConnectivityPollRate=60\" - \"--driverPath=csi-isilon.dellemc.com\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/csi-isilon-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/csi-isilon/csi_sock\" - \"--labelvalue=csi-isilon\" - \"--arrayConnectivityPollRate=60\" - \"--driverPath=csi-isilon.dellemc.com\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/csi-isilon-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" PowerStore Specific Recommendations Here is a typical installation used for testing:\npodmon: enabled: true image: dellemc/podmon controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-powerstore\" - \"--arrayConnectivityPollRate=60\" - \"--driverPath=csi-powerstore.dellemc.com\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/powerstore-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/csi-powerstore.dellemc.com/csi_sock\" - \"--labelvalue=csi-powerstore\" - \"--arrayConnectivityPollRate=60\" - \"--driverPath=csi-powerstore.dellemc.com\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/powerstore-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" Dynamic parameters CSM for Resiliency has configuration parameters that can be updated dynamically, such as the logging level and format. This can be done by editing the Dell CSI Driver’s parameters ConfigMap. The ConfigMap can be queried using kubectl. For example, the Dell Powerflex CSI Driver ConfigMaps can be found using this command: kubectl get -n vxflexos configmap. The ConfigMap to edit will have this pattern: -config-params (e.g., vxflexos-config-params).\nTo update or add parameters, you can use the kubectl edit command. For example, kubectl edit -n vxflexos configmap vxflexos-config-params.\nThis is a list of parameters that can be adjusted for CSM for Resiliency:\nParameter Type Default Description PODMON_CONTROLLER_LOG_FORMAT String “text” Logging format output for the controller podmon sidecar. Should be “text” or “json” PODMON_CONTROLLER_LOG_LEVEL String “debug” Logging level for the controller podmon sidecar. Standard values: ‘info’, ’error’, ‘warning’, ‘debug’, ’trace’ PODMON_NODE_LOG_FORMAT String “text” Logging format output for the node podmon sidecar. Should be “text” or “json” PODMON_NODE_LOG_LEVEL String “debug” Logging level for the node podmon sidecar. Standard values: ‘info’, ’error’, ‘warning’, ‘debug’, ’trace’ PODMON_ARRAY_CONNECTIVITY_POLL_RATE Integer (\u003e0) 15 An interval in seconds to poll the underlying array PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD Integer (\u003e0) 3 A value representing the number of failed connection poll intervals before marking the array connectivity as lost PODMON_SKIP_ARRAY_CONNECTION_VALIDATION Boolean false Flag to disable the array connectivity check, set to true for NoSchedule or NoExecute taint due to K8S Control Plane failure (kubelet failure) Here is an example of the parameters:\nPODMON_CONTROLLER_LOG_FORMAT: \"text\" PODMON_CONTROLLER_LOG_LEVEL: \"info\" PODMON_NODE_LOG_FORMAT: \"text\" PODMON_NODE_LOG_LEVEL: \"info\" PODMON_ARRAY_CONNECTIVITY_POLL_RATE: 20 PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD: 2 PODMON_SKIP_ARRAY_CONNECTION_VALIDATION: true ","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency installation\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency installation\n","ref":"/csm-docs/v2/resiliency/deployment/","tags":"","title":"Deployment"},{"body":"CSM for Observability can be deployed in one of three ways:\nHelm CSM for Observability Installer CSM for Observability Offline Installer Post Installation Dependencies The following third-party components are required in the same Kubernetes cluster where CSM for Observability has been deployed:\nPrometheus Grafana Other Deployment Methods There are various ways to deploy these components. We recommend following the Helm deployments according to the specifications defined below.\nTip: CSM for Observability must be deployed first. Once the module has been deployed, you can proceed to deploying/configuring Prometheus and Grafana.\nPrometheus The Prometheus service should be running on the same Kubernetes cluster as the CSM for Observability services. As part of the CSM for Observability deployment, the OpenTelemetry Collector gets deployed. CSM for Observability pushes metrics to the OpenTelemetry Collector where the metrics are consumed by Prometheus. Prometheus must be configured to scrape the metrics data from the OpenTelemetry Collector.\nSupported Version Image Helm Chart 2.34.0 prom/prometheus:v2.34.0 Prometheus Helm chart Note: It is the user’s responsibility to provide persistent storage for Prometheus if they want to preserve historical data.\nPrometheus Helm Deployment Here is a sample minimal configuration for Prometheus. Please note that the configuration below uses insecure skip verify. If you wish to properly configure TLS, you will need to provide a ca_file in the Prometheus configuration. The certificate provided as part of the CSM for Observability deployment should be signed by this same CA. For more information about Prometheus configuration, see Prometheus configuration.\nCreate a values file named prometheus-values.yaml.\n# prometheus-values.yaml alertmanager: enabled: false nodeExporter: enabled: false pushgateway: enabled: false kubeStateMetrics: enabled: false configmapReload: prometheus: enabled: false server: enabled: true image: repository: quay.io/prometheus/prometheus tag: v2.34.0 pullPolicy: IfNotPresent persistentVolume: enabled: false service: type: NodePort servicePort: 9090 extraScrapeConfigs: | - job_name: 'karavi-metrics-[CSI-DRIVER]' scrape_interval: 5s scheme: https static_configs: - targets: ['otel-collector:8443'] tls_config: insecure_skip_verify: true If using Rancher, create a ServiceMonitor.\napiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: otel-collector namespace: powerflex spec: endpoints: - path: /metrics port: exporter-https scheme: https tlsConfig: insecureSkipVerify: true selector: matchLabels: app.kubernetes.io/instance: karavi-observability app.kubernetes.io/name: otel-collector Add the Prometheus Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add stable https://charts.helm.sh/stable helm repo update Install the Helm chart.\nOn your terminal, run the command below:\nhelm install prometheus prometheus-community/prometheus -n [CSM_NAMESPACE] -f prometheus-values.yaml Grafana The Grafana dashboards require Grafana to be deployed in the same Kubernetes cluster as CSM for Observability. Below are the configuration details required to properly set up Grafana to work with CSM for Observability.\nSupported Version Helm Chart 8.5.0 Grafana Helm chart Grafana must be configured with the following data sources/plugins:\nName Additional Information Prometheus data source Prometheus data source Data Table plugin Data Table plugin Pie Chart plugin Pie Chart plugin SimpleJson data source SimpleJson data source Settings for the Grafana Prometheus data source:\nSetting Value Additional Information Name Prometheus Type prometheus URL http://PROMETHEUS_IP:PORT The IP/PORT of your running Prometheus instance Access Proxy Settings for the Grafana SimpleJson data source:\nSetting Value Name Karavi-Topology URL Access CSM for Observability Topology service at https://karavi-topology.namespace.svc.cluster.local:8443 Skip TLS Verify Enabled (If not using CA certificate) With CA Cert Enabled (If using CA certificate) Grafana Helm Deployment Below are the steps to deploy a new Grafana instance into your Kubernetes cluster:\nCreate a ConfigMap.\nWhen using a network that requires a decryption certificate, the Grafana server MUST be configured with the necessary certificate. If no certificate is required, skip to step 2.\nCreate a Config file named grafana-configmap.yaml The file should look like this: # grafana-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: certs-configmap namespace: [CSM_NAMESPACE] labels: certs-configmap: \"1\" data: ca-certificates.crt: |- -----BEGIN CERTIFICATE----- ReplaceMeWithActualCaCERT= -----END CERTIFICATE----- NOTE: you need an actual CA Cert for it to work\nOn your terminal, run the commands below:\nkubectl create -f grafana-configmap.yaml Create a values file.\nCreate a Config file named grafana-values.yaml The file should look like this:\n# grafana-values.yaml image: repository: grafana/grafana tag: 8.5.0 sha: \"\" pullPolicy: IfNotPresent service: type: NodePort ## Administrator credentials when not using an existing Secret adminUser: admin adminPassword: admin ## Pass the plugins you want to be installed as a list. ## plugins: - grafana-simple-json-datasource - briangann-datatable-panel - grafana-piechart-panel ## Configure grafana datasources ## ref: http://docs.grafana.org/administration/provisioning/#datasources ## datasources: datasources.yaml: apiVersion: 1 datasources: - name: Karavi-Topology type: grafana-simple-json-datasource access: proxy url: 'https://karavi-topology:8443' isDefault: null version: 1 editable: true jsonData: tlsSkipVerify: true - name: Prometheus type: prometheus access: proxy url: 'http://prometheus-server:9090' isDefault: null version: 1 editable: true testFramework: enabled: false sidecar: datasources: enabled: true dashboards: enabled: true ## Additional grafana server ConfigMap mounts ## Defines additional mounts with ConfigMap. ConfigMap must be manually created in the namespace. extraConfigmapMounts: [] # If you created a ConfigMap on the previous step, delete [] and uncomment the lines below # - name: certs-configmap # mountPath: /etc/ssl/certs/ca-certificates.crt # subPath: ca-certificates.crt # configMap: certs-configmap # readOnly: true Add the Grafana Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add grafana https://grafana.github.io/helm-charts helm repo update Install the Helm chart.\nOn your terminal, run the commands below:\nhelm install grafana grafana/grafana -n [CSM_NAMESPACE] -f grafana-values.yaml Other Deployment Methods Grafana Labs Operator Deployment Rancher Monitoring and Alerting Deployment Importing CSM for Observability Dashboards Once Grafana is properly configured, you can import the pre-built observability dashboards. Log into Grafana and click the + icon in the side menu. Then click Import. From here you can upload the JSON files or paste the JSON text directly into the text area. Below are the locations of the dashboards that can be imported:\nDashboard Description PowerFlex: I/O Performance by Kubernetes Node Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by Kubernetes node PowerFlex: I/O Performance by Provisioned Volume Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume PowerFlex: Storage Pool Consumption By CSI Driver Provides visibility into the total, used and available capacity for a storage class and associated underlying storage construct PowerStore: I/O Performance by Provisioned Volume Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume PowerStore: I/O Performance by File System Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by filesystem PowerStore: Array and Storage Class Consumption By CSI Driver Provides visibility into the total, used and available capacity for a storage class and associated underlying storage construct PowerScale: I/O Performance by Cluster Provides visibility into the I/O performance metrics (IOPS, bandwidth) by cluster PowerScale: Capacity by Cluster Provides visibility into the total, used, available capacity and directory quota capacity by cluster PowerScale: Capacity by Quota Provides visibility into the subscribed, remaining capacity and usage by quota PowerMax: PowerMax Capacity Provides visibility into the subscribed, used, available capacity for a storage class and associated underlying storage construct PowerMax: PowerMax Performance Provides visibility into the I/O performance metrics (IOPS, bandwidth) by storage group and volume CSI Driver Provisioned Volume Topology Provides visibility into Dell CSI (Container Storage Interface) driver provisioned volume characteristics in Kubernetes correlated with volumes on the storage system. Dynamic Configuration Some parameters can be configured/updated during runtime without restarting the CSM for Observability services. These parameters will be stored in ConfigMaps that can be updated on the Kubernetes cluster. This will automatically change the settings on the services.\nConfigMap Observability Service Parameters karavi-metrics-powerflex-configmap karavi-metrics-powerflex COLLECTOR_ADDRPROVISIONER_NAMESPOWERFLEX_SDC_METRICS_ENABLEDPOWERFLEX_SDC_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_POLL_FREQUENCYPOWERFLEX_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMAT karavi-metrics-powerstore-configmap karavi-metrics-powerstore COLLECTOR_ADDRPROVISIONER_NAMESPOWERSTORE_VOLUME_METRICS_ENABLEDPOWERSTORE_VOLUME_IO_POLL_FREQUENCYPOWERSTORE_SPACE_POLL_FREQUENCYPOWERSTORE_ARRAY_POLL_FREQUENCYPOWERSTORE_FILE_SYSTEM_POLL_FREQUENCYPOWERSTORE_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY karavi-metrics-powerscale-configmap karavi-metrics-powerscale COLLECTOR_ADDR PROVISIONER_NAMES POWERSCALE_MAX_CONCURRENT_QUERIES POWERSCALE_CAPACITY_METRICS_ENABLED POWERSCALE_PERFORMANCE_METRICS_ENABLED POWERSCALE_CLUSTER_CAPACITY_POLL_FREQUENCY POWERSCALE_CLUSTER_PERFORMANCE_POLL_FREQUENCY POWERSCALE_QUOTA_CAPACITY_POLL_FREQUENCY POWERSCALE_ISICLIENT_INSECURE POWERSCALE_ISICLIENT_AUTH_TYPE POWERSCALE_ISICLIENT_VERBOSE LOG_LEVEL LOG_FORMAT karavi-metrics-powermax-configmap karavi-metrics-powermax COLLECTOR_ADDR PROVISIONER_NAMES POWERMAX_MAX_CONCURRENT_QUERIES POWERMAX_CAPACITY_METRICS_ENABLED POWERMAX_PERFORMANCE_METRICS_ENABLED POWERMAX_CAPACITY_POLL_FREQUENCY POWERMAX_PERFORMANCE_POLL_FREQUENCY LOG_LEVEL LOG_FORMAT karavi-topology-configmap karavi-topology PROVISIONER_NAMESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY To update any of these settings, run the following command on the Kubernetes cluster then save the updated ConfigMap data.\nkubectl edit configmap [CONFIG_MAP_NAME] -n [CSM_NAMESPACE] Tracing CSM for Observability is instrumented to report trace data to Zipkin. This helps gather timing data needed to troubleshoot latency problems with CSM for Observability. Follow the instructions below to enable the reporting of trace data:\nDeploy a Zipkin instance in the CSM namespace and expose the service as NodePort for external access.\napiVersion: apps/v1 kind: Deployment metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance template: metadata: labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance spec: containers: - name: zipkin image: \"openzipkin/zipkin\" imagePullPolicy: IfNotPresent env: - name: \"STORAGE_TYPE\" value: \"mem\" - name: \"TRANSPORT_TYPE\" value: \"http\" --- apiVersion: v1 kind: Service metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: ports: - port: 9411 targetPort: 9411 protocol: TCP type: \"NodePort\" selector: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance Add the Zipkin URI to the CSM for Observability ConfigMaps. Based on the manifest above, Zipkin will be running on port 9411.\nNote: Zipkin tracing is currently not supported for the collection of PowerFlex metrics.\nUpdate the ConfigMaps from the table above. Here is an example updating the karavi-topology-configmap based on the deployment manifest above.\nkubectl edit configmap/karavi-topology-configmap -n [CSM_NAMESPACE] Update the ZIPKIN_URI and ZIPKIN_PROBABILITY values and save the ConfigMap.\nZIPKIN_URI: \"http://zipkin:9411/api/v2/spans\" ZIPKIN_SERVICE_NAME: \"karavi-topology\" ZIPKIN_PROBABILITY: \"1.0\" Once the ConfigMaps are updated, the changes will automatically be applied and tracing can be seen by accessing Zipkin on the exposed port.\nUpdating Storage System Credentials If the storage system credentials have been updated in the relevant CSI Driver, CSM for Observability must be updated with those new credentials as follows:\nWhen CSM for Observability uses the Authorization module In this case, all storage system requests made by CSM for Observability will be routed through the Authorization module. The following must be performed:\nUpdate the Authorization Module Token CSI Driver for Dell PowerFlex Delete the current proxy-authz-tokens Secret from the CSM namespace.\n$ kubectl delete secret proxy-authz-tokens -n [CSM_NAMESPACE] Copy the proxy-authz-tokens Secret from the CSI Driver for Dell PowerFlex to the CSM namespace.\n$ kubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerScale Delete the current isilon-proxy-authz-tokens Secret from the CSM namespace.\n$ kubectl delete secret isilon-proxy-authz-tokens -n [CSM_NAMESPACE] Copy the isilon-proxy-authz-tokens Secret from the CSI Driver for Dell PowerScale namespace to the CSM namespace.\n$ kubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/'| sed 's/name: proxy-authz-tokens/name: isilon-proxy-authz-tokens/' | kubectl create -f CSI Driver for Dell PowerMax Delete the current powermax-proxy-authz-tokens Secret from the CSM namespace.\n$ kubectl delete secret powermax-proxy-authz-tokens -n [CSM_NAMESPACE] Copy the powermax-proxy-authz-tokens Secret from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\n$ kubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/'| sed 's/name: proxy-authz-tokens/name: powermax-proxy-authz-tokens/' | kubectl create -f Update Storage Systems If the list of storage systems managed by a Dell CSI Driver have changed, the following steps can be performed to update CSM for Observability to reference the updated systems:\nCSI Driver for Dell PowerFlex Delete the current karavi-authorization-config Secret from the CSM namespace.\n$ kubectl delete secret karavi-authorization-config -n [CSM_NAMESPACE] Copy the karavi-authorization-config Secret from the CSI Driver for Dell PowerFlex namespace to CSM for Observability namespace.\n$ kubectl get secret karavi-authorization-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerScale Delete the current isilon-karavi-authorization-config Secret from the CSM namespace.\n$ kubectl delete secret isilon-karavi-authorization-config -n [CSM_NAMESPACE] Copy the isilon-karavi-authorization-config Secret from the CSI Driver for Dell PowerScale namespace to CSM for Observability namespace.\n$ kubectl get secret karavi-authorization-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: isilon-karavi-authorization-config/' | kubectl create -f CSI Driver for Dell PowerMax Delete the current powermax-karavi-authorization-config secret from the CSM namespace.\n$ kubectl delete secret powermax-karavi-authorization-config -n [CSM_NAMESPACE] Copy powermax-karavi-authorization-config secret from the CSI Driver for Dell PowerMax to the CSM namespace.\n$ kubectl get secret karavi-authorization-config proxy-server-root-certificate -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: powermax-karavi-authorization-config/' | kubectl create -f - When CSM for Observability does not use the Authorization module In this case all storage system requests made by CSM for Observability will not be routed through the Authorization module. The following must be performed:\nCSI Driver for Dell PowerFlex Delete the current vxflexos-config Secret from the CSM namespace.\n$ kubectl delete secret vxflexos-config -n [CSM_NAMESPACE] Copy the vxflexos-config Secret from the CSI Driver for Dell PowerFlex namespace to the CSM namespace.\n$ kubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default vxflexos-config, please use the following command to copy secret:\n$ kubectl get secret [VXFLEXOS-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG]/name: vxflexos-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerStore Delete the current powerstore-config Secret from the CSM namespace.\n$ kubectl delete secret powerstore-config -n [CSM_NAMESPACE] Copy the powerstore-config Secret from the CSI Driver for Dell PowerStore namespace to the CSM namespace.\n$ kubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default powerstore-config, please use the following command to copy secret:\n$ kubectl get secret [POWERSTORE-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERSTORE-CONFIG]/name: powerstore-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerScale Delete the current isilon-creds Secret from the CSM namespace.\n$ kubectl delete secret isilon-creds -n [CSM_NAMESPACE] Copy the isilon-creds Secret from the CSI Driver for Dell PowerScale namespace to the CSM namespace.\n$ kubectl get secret isilon-creds -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default isilon-creds, please use the following command to copy secret:\n$ kubectl get secret [ISILON-CREDS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CREDS]/name: isilon-creds/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for Dell PowerMax Delete the secrets in powermax-reverseproxy-config configmap from the CSM namespace.\nfor secret in $(kubectl get configmap powermax-reverseproxy-config -n [CSM_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl delete secret $secret -n [CSM_NAMESPACE] done Delete the current powermax-reverseproxy-config configmap from the CSM namespace.\n$ kubectl delete configmap powermax-reverseproxy-config -n [CSM_NAMESPACE] Copy the configmap powermax-reverseproxy-config from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nNote: Observability for PowerMax works only with CSI PowerMax driver with Proxy in StandAlone mode.\n$ kubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy configmap:\n$ kubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-REVERSEPROXY-CONFIG]/name: powermax-reverseproxy-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the secrets in powermax-reverseproxy-config from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nfor secret in $(kubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy secrets:\nfor secret in $(kubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Deployment\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Deployment\n","ref":"/csm-docs/v3/observability/deployment/","tags":"","title":"Deployment"},{"body":"CSM for Resiliency is installed as part of the Dell CSI driver installation. The drivers can be installed either by a helm chart or by the Dell CSI Operator. Currently, only Helm chart installation is supported.\nFor information on the PowerFlex CSI driver, see PowerFlex CSI Driver.\nFor information on the Unity XT CSI driver, see Unity XT CSI Driver.\nFor information on the PowerScale CSI driver, see PowerScale CSI Driver.\nFor information on the PowerStore CSI driver, see PowerStore CSI Driver.\nConfigure all the helm chart parameters described below before installing the drivers.\nHelm Chart Installation The drivers that support Helm chart installation allow CSM for Resiliency to be optionally installed by variables in the chart. There is a podmon block specified in the values.yaml file of the chart that will look similar to the text below by default:\n# Enable this feature only after contact support for additional information podmon: enabled: true image: dellemc/podmon:v1.3.0 controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" To install CSM for Resiliency with the driver, the following changes are required:\nEnable CSM for Resiliency by changing the podmon.enabled boolean to true. This will enable both controller-podmon and node-podmon. Specify the podmon image to be used as podmon.image. Specify arguments to controller-podmon in the podmon.controller.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon. Specify arguments to node-podmon in the podmon.node.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon. Podmon Arguments Argument Required Description Applicability enabled Required Boolean “true” enables CSM for Resiliency installation with the driver in a helm installation. top level image Required Must be set to a repository where the podmon image can be pulled. controller \u0026 node mode Required Must be set to “controller” for controller-podmon and “node” for node-podmon. controller \u0026 node csisock Required This should be left as set in the helm template for the driver. For controller: -csisock=unix:/var/run/csi/csi.sock For node it will vary depending on the driver’s identity: -csisock=unix:/var/lib/kubelet/plugins\n/vxflexos.emc.dell.com/csi_sock controller \u0026 node leaderelection Required Boolean value that should be set true for controller and false for node. The default value is true. controller \u0026 node skipArrayConnectionValidation Optional Boolean value that if set to true will cause controllerPodCleanup to skip the validation that no I/O is ongoing before cleaning up the pod. If set to true will cause controllerPodCleanup on K8S Control Plane failure (kubelet service down). controller labelKey Optional String value that sets the label key used to denote pods to be monitored by CSM for Resiliency. It will make life easier if this key is the same for all driver types, and drivers are differentiated by different labelValues (see below). If the label keys are the same across all drivers you can do kubectl get pods -A -l labelKey to find all the CSM for Resiliency protected pods. labelKey defaults to “podmon.dellemc.com/driver”. controller \u0026 node labelValue Required String that sets the value that denotes pods to be monitored by CSM for Resiliency. This must be specific for each driver. Defaults to “csi-vxflexos” for CSI Driver for Dell PowerFlex and “csi-unity” for CSI Driver for Dell Unity XT controller \u0026 node arrayConnectivityPollRate Optional The minimum polling rate in seconds to determine if the array has connectivity to a node. Should not be set to less than 5 seconds. See the specific section for each array type for additional guidance. controller \u0026 node arrayConnectivityConnectionLossThreshold Optional Gives the number of failed connection polls that will be deemed to indicate array connectivity loss. Should not be set to less than 3. See the specific section for each array type for additional guidance. controller driver-config-params Required String that set the path to a file containing configuration parameter(for instance, Log levels) for a driver. controller \u0026 node ignoreVolumelessPods Optional Boolean value that if set to true will enable CSM for Resiliency to ignore pods without persistent volume attached to the pod. controller \u0026 node PowerFlex Specific Recommendations PowerFlex supports a very robust array connection validation mechanism that can detect changes in connectivity in about two seconds and can detect whether I/O has occurred over a five-second sample. For that reason it is recommended to set “skipArrayConnectionValidation=false” (which is the default) and to set “arrayConnectivityPollRate=5” (5 seconds) and “arrayConnectivityConnectionLossThreshold=3” to 3 or more.\nHere is a typical installation used for testing:\npodmon: image: dellemc/podmon enabled: true controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=controller\" - \"--arrayConnectivityPollRate=5\" - \"--arrayConnectivityConnectionLossThreshold=3\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" Unity XT Specific Recommendations Here is a typical installation used for testing:\npodmon: image: dellemc/podmon enabled: true controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-unity\" - \"--driverPath=csi-unity.dellemc.com\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/unity-config/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/unity.emc.dell.com/csi_sock\" - \"--labelvalue=csi-unity\" - \"--driverPath=csi-unity.dellemc.com\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/unity-config/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" PowerScale Specific Recommendations Here is a typical installation used for testing:\npodmon: image: dellemc/podmon enabled: true controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-isilon\" - \"--arrayConnectivityPollRate=60\" - \"--driverPath=csi-isilon.dellemc.com\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/csi-isilon-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/csi-isilon/csi_sock\" - \"--labelvalue=csi-isilon\" - \"--arrayConnectivityPollRate=60\" - \"--driverPath=csi-isilon.dellemc.com\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/csi-isilon-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" PowerStore Specific Recommendations Here is a typical installation used for testing:\npodmon: enabled: true image: dellemc/podmon controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-powerstore\" - \"--arrayConnectivityPollRate=60\" - \"--driverPath=csi-powerstore.dellemc.com\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/powerstore-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/csi-powerstore.dellemc.com/csi_sock\" - \"--labelvalue=csi-powerstore\" - \"--arrayConnectivityPollRate=60\" - \"--driverPath=csi-powerstore.dellemc.com\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/powerstore-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" Dynamic parameters CSM for Resiliency has configuration parameters that can be updated dynamically, such as the logging level and format. This can be done by editing the Dell CSI Driver’s parameters ConfigMap. The ConfigMap can be queried using kubectl. For example, the Dell Powerflex CSI Driver ConfigMaps can be found using this command: kubectl get -n vxflexos configmap. The ConfigMap to edit will have this pattern: -config-params (e.g., vxflexos-config-params).\nTo update or add parameters, you can use the kubectl edit command. For example, kubectl edit -n vxflexos configmap vxflexos-config-params.\nThis is a list of parameters that can be adjusted for CSM for Resiliency:\nParameter Type Default Description PODMON_CONTROLLER_LOG_FORMAT String “text” Logging format output for the controller podmon sidecar. Should be “text” or “json” PODMON_CONTROLLER_LOG_LEVEL String “debug” Logging level for the controller podmon sidecar. Standard values: ‘info’, ’error’, ‘warning’, ‘debug’, ’trace’ PODMON_NODE_LOG_FORMAT String “text” Logging format output for the node podmon sidecar. Should be “text” or “json” PODMON_NODE_LOG_LEVEL String “debug” Logging level for the node podmon sidecar. Standard values: ‘info’, ’error’, ‘warning’, ‘debug’, ’trace’ PODMON_ARRAY_CONNECTIVITY_POLL_RATE Integer (\u003e0) 15 An interval in seconds to poll the underlying array PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD Integer (\u003e0) 3 A value representing the number of failed connection poll intervals before marking the array connectivity as lost PODMON_SKIP_ARRAY_CONNECTION_VALIDATION Boolean false Flag to disable the array connectivity check, set to true for NoSchedule or NoExecute taint due to K8S Control Plane failure (kubelet failure) Here is an example of the parameters:\nPODMON_CONTROLLER_LOG_FORMAT: \"text\" PODMON_CONTROLLER_LOG_LEVEL: \"info\" PODMON_NODE_LOG_FORMAT: \"text\" PODMON_NODE_LOG_LEVEL: \"info\" PODMON_ARRAY_CONNECTIVITY_POLL_RATE: 20 PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD: 2 PODMON_SKIP_ARRAY_CONNECTION_VALIDATION: true ","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency installation\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency installation\n","ref":"/csm-docs/v3/resiliency/deployment/","tags":"","title":"Deployment"},{"body":"The Container Storage Modules (CSM) for Observability Helm chart bootstraps an Observability deployment on a Kubernetes cluster using the Helm package manager.\nPrerequisites Helm 3.x The deployment of one or more supported Dell CSI drivers Install the CSM for Observability Helm Chart Steps\nCreate a namespace where you want to install the module\nkubectl create namespace karavi Install cert-manager CRDs\nkubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.10.0/cert-manager.crds.yaml Add the Dell Helm Charts repo\nhelm repo add dell https://dell.github.io/helm-charts Copy only the deployed CSI driver entities to the Observability namespace\nPowerFlex Copy the config Secret from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default vxflexos-config, please use the following command to copy secret:\nkubectl get secret [VXFLEXOS-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG]/name: vxflexos-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerFlex, perform the following steps:\nCopy the driver configuration parameters ConfigMap from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get configmap vxflexos-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default vxflexos-config-params, please use the following command to copy configmap:\nkubectl get configmap [VXFLEXOS-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG-PARAMS]/name: vxflexos-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the karavi-authorization-config, proxy-server-root-certificate, proxy-authz-tokens Secret from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - PowerStore Copy the config Secret from the CSI PowerStore namespace into the CSM for Observability namespace:\nkubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default powerstore-config, please use the following command to copy secret:\nkubectl get secret [POWERSTORE-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERSTORE-CONFIG]/name: powerstore-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - PowerScale Copy the config Secret from the CSI PowerScale namespace into the CSM for Observability namespace:\nkubectl get secret isilon-creds -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default isilon-creds, please use the following command to copy secret:\nkubectl get secret [ISILON-CREDS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CREDS]/name: isilon-creds/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerScale, perform these steps:\nCopy the driver configuration parameters ConfigMap from the CSI PowerScale namespace into the CSM for Observability namespace:\nkubectl get configmap isilon-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default isilon-config-params, please use the following command to copy configmap:\nkubectl get configmap [ISILON-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CONFIG-PARAMS]/name: isilon-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the karavi-authorization-config, proxy-server-root-certificate, proxy-authz-tokens Secret from the CSI PowerScale namespace into the CSM for Observability namespace:\nkubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: isilon-karavi-authorization-config/' | sed 's/name: proxy-server-root-certificate/name: isilon-proxy-server-root-certificate/' | sed 's/name: proxy-authz-tokens/name: isilon-proxy-authz-tokens/' | kubectl create -f - PowerMax Copy the configmap powermax-reverseproxy-config from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nNote: Observability for PowerMax works only with CSI PowerMax driver with Proxy in StandAlone mode.\nkubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy configmap:\nkubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-REVERSEPROXY-CONFIG]/name: powermax-reverseproxy-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the secrets in powermax-reverseproxy-config from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nfor secret in $(kubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy secrets:\nfor secret in $(kubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If CSM for Authorization is enabled for CSI PowerMax, perform these steps:\nCopy the driver configuration parameters ConfigMap from the CSI PowerMax namespace into the CSM for Observability namespace:\nkubectl get configmap powermax-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-config-params, please use the following command to copy configmap:\nkubectl get configmap [POWERMAX-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-CONFIG-PARAMS]/name: powermax-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the karavi-authorization-config, proxy-server-root-certificate, proxy-authz-tokens Secret from the CSI PowerMax namespace into the CSM for Observability namespace:\nkubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: powermax-karavi-authorization-config/' | sed 's/name: proxy-server-root-certificate/name: powermax-proxy-server-root-certificate/' | sed 's/name: proxy-authz-tokens/name: powermax-proxy-authz-tokens/' | kubectl create -f - Configure the parameters and install the CSM for Observability Helm Chart\nA default values.yaml file is located here that can be used for installation. This can be copied into a file named myvalues.yaml and either used as is or modified accordingly.\nNote:\nThe default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured in your values file for CSM Observability. If CSM for Authorization is enabled for CSI PowerScale, the karaviMetricsPowerscale.authorization parameters must be properly configured in your values file for CSM Observability. If CSM for Authorization is enabled for CSI PowerMax, the karaviMetricsPowerMax.authorization parameters must be properly configured in your values file for CSM Observability. helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] -f myvalues.yaml Alternatively, you can specify each parameter using the ‘–set key=value[,key=value]’ and/or ‘–set-file key=value[,key=value] arguments to ‘helm install’. For example:\nhelm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] \\ --set-file karaviTopology.certificateFile=\u003clocation-of-karavi-topology-certificate-file\u003e \\ --set-file karaviTopology.privateKeyFile=\u003clocation-of-karavi-topology-private-key-file\u003e \\ --set-file otelCollector.certificateFile=\u003clocation-of-otel-collector-certificate-file\u003e \\ --set-file otelCollector.privateKeyFile=\u003clocation-of-otel-collector-private-key-file\u003e Configuration The following table lists the configurable parameters of the CSM for Observability Helm chart and their default values.\nParameter Description Default karaviTopology.image Location of the csm-topology Docker image dellemc/csm-topology:v1.0 karaviTopology.enabled Enable the CSM for Observability Topology service true karaviTopology.provisionerNames Provisioner Names used to filter the Persistent Volumes created on the Kubernetes cluster (must be a comma-separated list) csi-vxflexos.dellemc.com karaviTopology.service.type Kubernetes service type ClusterIP karaviTopology.certificateFile Optional valid CA public certificate file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’. karaviTopology.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’. karaviTopology.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO karaviTopology.logFormat Output logs in the specified format (Valid values: text, json) text otelCollector.certificateFile Optional valid CA public certificate file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’. otelCollector.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’. otelCollector.service.type Kubernetes service type ClusterIP karaviMetricsPowerflex.image CSM Metrics for PowerFlex Service image dellemc/csm-metrics-powerflex:v1.0 karaviMetricsPowerflex.enabled Enable CSM Metrics for PowerFlex service true karaviMetricsPowerflex.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680 karaviMetricsPowerflex.provisionerNames Provisioner Names used to filter for determining PowerFlex SDC nodes( Must be a Comma-separated list) csi-vxflexos.dellemc.com karaviMetricsPowerflex.sdcPollFrequencySeconds The polling frequency (in seconds) to gather SDC metrics 10 karaviMetricsPowerflex.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10 karaviMetricsPowerflex.storageClassPoolPollFrequencySeconds The polling frequency (in seconds) to gather storage class/pool metrics 10 karaviMetricsPowerflex.concurrentPowerflexQueries The number of simultaneous metrics queries to make to Powerflex(MUST be less than 10; otherwise, several request errors from Powerflex will ensue. 10 karaviMetricsPowerflex.authorization.enabled Authorization is an optional feature to apply credential shielding of the backend PowerFlex. false karaviMetricsPowerflex.authorization.proxyHost Hostname of the csm-authorization server. karaviMetricsPowerflex.authorization.skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. karaviMetricsPowerflex.sdcMetricsEnabled Enable PowerFlex SDC Metrics Collection true karaviMetricsPowerflex.volumeMetricsEnabled Enable PowerFlex Volume Metrics Collection true karaviMetricsPowerflex.storageClassPoolMetricsEnabled Enable PowerFlex Storage Class/Pool Metrics Collection true karaviMetricsPowerflex.endpoint Endpoint for pod leader election karavi-metrics-powerflex karaviMetricsPowerflex.service.type Kubernetes service type ClusterIP karaviMetricsPowerflex.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO karaviMetricsPowerflex.logFormat Output logs in the specified format (Valid values: text, json) text karaviMetricsPowerstore.image CSM Metrics for PowerStore Service image dellemc/csm-metrics-powerstore:v1.0 karaviMetricsPowerstore.enabled Enable CSM Metrics for PowerStore service true karaviMetricsPowerstore.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680 karaviMetricsPowerstore.provisionerNames Provisioner Names used to filter for determining PowerStore volumes (must be a Comma-separated list) csi-powerstore.dellemc.com karaviMetricsPowerstore.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10 karaviMetricsPowerstore.concurrentPowerstoreQueries The number of simultaneous metrics queries to make to PowerStore (must be less than 10; otherwise, several request errors from PowerStore will ensue.) 10 karaviMetricsPowerstore.volumeMetricsEnabled Enable PowerStore Volume Metrics Collection true karaviMetricsPowerstore.endpoint Endpoint for pod leader election karavi-metrics-powerstore karaviMetricsPowerstore.service.type Kubernetes service type ClusterIP karaviMetricsPowerstore.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO karaviMetricsPowerstore.logFormat Output logs in the specified format (Valid values: text, json) text karaviMetricsPowerstore.zipkin.uri URI of a Zipkin instance where tracing data can be forwarded karaviMetricsPowerstore.zipkin.serviceName Service name used for Zipkin tracing data metrics-powerstore karaviMetricsPowerstore.zipkin.probability Percentage of trace information to send to Zipkin (Valid range: 0.0 to 1.0) 0 karaviMetricsPowerscale.image CSM Metrics for PowerScale Service image dellemc/csm-metrics-powerscale:v1.0 karaviMetricsPowerscale.enabled Enable CSM Metrics for PowerScale service true karaviMetricsPowerscale.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680 karaviMetricsPowerscale.provisionerNames Provisioner Names used to filter for determining PowerScale volumes (must be a Comma-separated list) csi-isilon.dellemc.com karaviMetricsPowerscale.capacityMetricsEnabled Enable PowerScale capacity metric Collection true karaviMetricsPowerscale.performanceMetricsEnabled Enable PowerScale performance metric Collection true karaviMetricsPowerscale.clusterCapacityPollFrequencySeconds The polling frequency (in seconds) to gather cluster capacity metrics 30 karaviMetricsPowerscale.clusterPerformancePollFrequencySeconds The polling frequency (in seconds) to gather cluster performance metrics 20 karaviMetricsPowerscale.quotaCapacityPollFrequencySeconds The polling frequency (in seconds) to gather volume capacity metrics 30 karaviMetricsPowerscale.concurrentPowerscaleQueries The number of simultaneous metrics queries to make to PowerScale(MUST be less than 10; otherwise, several request errors from PowerScale will ensue.) 10 karaviMetricsPowerscale.endpoint Endpoint for pod leader election karavi-metrics-powerscale karaviMetricsPowerscale.service.type Kubernetes service type ClusterIP karaviMetricsPowerscale.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO karaviMetricsPowerscale.logFormat Output logs in the specified format (Valid values: text, json) text karaviMetricsPowerscale.isiClientOptions.isiSkipCertificateValidation Skip OneFS API server’s certificates true karaviMetricsPowerscale.isiClientOptions.isiAuthType 0 to enable session-based Authentication; 1 to enables basic Authentication 1 karaviMetricsPowerscale.isiClientOptions.isiLogVerbose Decide High/Medium/Low content of the OneFS REST API message 0 karaviMetricsPowerscale.authorization.enabled Authorization is an optional feature to apply credential shielding of the backend PowerScale. false karaviMetricsPowerscale.authorization.proxyHost Hostname of the csm-authorization server. karaviMetricsPowerscale.authorization.skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. karaviMetricsPowerMax.capacityMetricsEnabled Enable PowerMax capacity metric Collection true karaviMetricsPowerMax.performanceMetricsEnabled Enable PowerMax performance metric Collection true karaviMetricsPowerMax.capacityPollFrequencySeconds The polling frequency (in seconds) to gather capacity metrics 20 karaviMetricsPowerMax.performancePollFrequencySeconds The polling frequency (in seconds) to gather performance metrics 20 karaviMetricsPowerMax.concurrentPowerMaxQueries The number of simultaneous metrics queries to make to PowerMax (MUST be less than 10; otherwise, several request errors from PowerMax will ensue.) 10 karaviMetricsPowerMax.authorization.enabled Authorization is an optional feature to apply credential shielding of the backend PowerMax. false karaviMetricsPowerMax.authorization.proxyHost Hostname of the csm-authorization server. karaviMetricsPowerMax.authorization.skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Helm deployment\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Helm deployment …","ref":"/csm-docs/docs/observability/deployment/helm/","tags":"","title":"Helm"},{"body":"CSM for Resiliency is installed as part of the Dell CSI driver installation.\nFor information on the PowerFlex CSI driver, see PowerFlex CSI Driver.\nFor information on the Unity XT CSI driver, see Unity XT CSI Driver.\nFor information on the PowerScale CSI driver, see PowerScale CSI Driver.\nFor information on the PowerStore CSI driver, see PowerStore CSI Driver.\nConfigure all the helm chart parameters described below before installing the drivers.\nHelm Chart Installation The drivers that support Helm chart installation allow CSM for Resiliency to be optionally installed by variables in the chart. There is a podmon block specified in the values.yaml file of the chart that will look similar to the text below by default:\n# Enable this feature only after contact support for additional information podmon: enabled: true controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" To install CSM for Resiliency with the driver, the following changes are required:\nEnable CSM for Resiliency by changing the podmon.enabled boolean to true. This will enable both controller-podmon and node-podmon. If you need to change the registry, specify the podmon image to be used in images.podmon Specify arguments to controller-podmon in the podmon.controller.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon. Specify arguments to node-podmon in the podmon.node.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon. Podmon Arguments Argument Required Description Applicability enabled Required Boolean “true” enables CSM for Resiliency installation with the driver in a helm installation. top level mode Required Must be set to “controller” for controller-podmon and “node” for node-podmon. controller \u0026 node csisock Required This should be left as set in the helm template for the driver. For controller: -csisock=unix:/var/run/csi/csi.sock For node it will vary depending on the driver’s identity: -csisock=unix:/var/lib/kubelet/plugins\n/vxflexos.emc.dell.com/csi_sock controller \u0026 node leaderelection Required Boolean value that should be set true for controller and false for node. The default value is true. controller \u0026 node skipArrayConnectionValidation Optional Boolean value that if set to true will cause controllerPodCleanup to skip the validation that no I/O is ongoing before cleaning up the pod. If set to true will cause controllerPodCleanup on K8S Control Plane failure (kubelet service down). controller labelKey Optional String value that sets the label key used to denote pods to be monitored by CSM for Resiliency. It will make life easier if this key is the same for all driver types, and drivers are differentiated by different labelValues (see below). If the label keys are the same across all drivers you can do kubectl get pods -A -l labelKey to find all the CSM for Resiliency protected pods. labelKey defaults to “podmon.dellemc.com/driver”. controller \u0026 node labelValue Required String that sets the value that denotes pods to be monitored by CSM for Resiliency. This must be specific for each driver. Defaults to “csi-vxflexos” for CSI Driver for Dell PowerFlex and “csi-unity” for CSI Driver for Dell Unity XT controller \u0026 node arrayConnectivityPollRate Optional The minimum polling rate in seconds to determine if the array has connectivity to a node. Should not be set to less than 5 seconds. See the specific section for each array type for additional guidance. controller \u0026 node arrayConnectivityConnectionLossThreshold Optional Gives the number of failed connection polls that will be deemed to indicate array connectivity loss. Should not be set to less than 3. See the specific section for each array type for additional guidance. controller driver-config-params Required String that set the path to a file containing configuration parameter(for instance, Log levels) for a driver. controller \u0026 node ignoreVolumelessPods Optional Boolean value that if set to true will enable CSM for Resiliency to ignore pods without persistent volume attached to the pod. controller \u0026 node PowerFlex Specific Recommendations PowerFlex supports a very robust array connection validation mechanism that can detect changes in connectivity in about two seconds and can detect whether I/O has occurred over a five-second sample. For that reason it is recommended to set “skipArrayConnectionValidation=false” (which is the default) and to set “arrayConnectivityPollRate=5” (5 seconds) and “arrayConnectivityConnectionLossThreshold=3” to 3 or more.\nHere is a typical installation used for testing:\npodmon: enabled: true controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=controller\" - \"--arrayConnectivityPollRate=5\" - \"--arrayConnectivityConnectionLossThreshold=3\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" Unity XT Specific Recommendations Here is a typical installation used for testing:\npodmon: enabled: true controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-unity\" - \"--driverPath=csi-unity.dellemc.com\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/unity-config/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/unity.emc.dell.com/csi_sock\" - \"--labelvalue=csi-unity\" - \"--driverPath=csi-unity.dellemc.com\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/unity-config/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" PowerScale Specific Recommendations Here is a typical installation used for testing:\npodmon: enabled: true controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-isilon\" - \"--arrayConnectivityPollRate=60\" - \"--driverPath=csi-isilon.dellemc.com\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/csi-isilon-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/csi-isilon/csi_sock\" - \"--labelvalue=csi-isilon\" - \"--arrayConnectivityPollRate=60\" - \"--driverPath=csi-isilon.dellemc.com\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/csi-isilon-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" PowerStore Specific Recommendations Here is a typical installation used for testing:\npodmon: enabled: true controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-powerstore\" - \"--arrayConnectivityPollRate=60\" - \"--driverPath=csi-powerstore.dellemc.com\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/powerstore-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/csi-powerstore.dellemc.com/csi_sock\" - \"--labelvalue=csi-powerstore\" - \"--arrayConnectivityPollRate=60\" - \"--driverPath=csi-powerstore.dellemc.com\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/powerstore-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" Dynamic parameters CSM for Resiliency has configuration parameters that can be updated dynamically, such as the logging level and format. This can be done by editing the Dell CSI Driver’s parameters ConfigMap. The ConfigMap can be queried using kubectl. For example, the Dell Powerflex CSI Driver ConfigMaps can be found using this command: kubectl get -n vxflexos configmap. The ConfigMap to edit will have this pattern: -config-params (e.g., vxflexos-config-params).\nTo update or add parameters, you can use the kubectl edit command. For example, kubectl edit -n vxflexos configmap vxflexos-config-params.\nThis is a list of parameters that can be adjusted for CSM for Resiliency:\nParameter Type Default Description PODMON_CONTROLLER_LOG_FORMAT String “text” Logging format output for the controller podmon sidecar. Should be “text” or “json” PODMON_CONTROLLER_LOG_LEVEL String “debug” Logging level for the controller podmon sidecar. Standard values: ‘info’, ’error’, ‘warning’, ‘debug’, ’trace’ PODMON_NODE_LOG_FORMAT String “text” Logging format output for the node podmon sidecar. Should be “text” or “json” PODMON_NODE_LOG_LEVEL String “debug” Logging level for the node podmon sidecar. Standard values: ‘info’, ’error’, ‘warning’, ‘debug’, ’trace’ PODMON_ARRAY_CONNECTIVITY_POLL_RATE Integer (\u003e0) 15 An interval in seconds to poll the underlying array PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD Integer (\u003e0) 3 A value representing the number of failed connection poll intervals before marking the array connectivity as lost PODMON_SKIP_ARRAY_CONNECTION_VALIDATION Boolean false Flag to disable the array connectivity check, set to true for NoSchedule or NoExecute taint due to K8S Control Plane failure (kubelet failure) Here is an example of the parameters:\nPODMON_CONTROLLER_LOG_FORMAT: \"text\" PODMON_CONTROLLER_LOG_LEVEL: \"info\" PODMON_NODE_LOG_FORMAT: \"text\" PODMON_NODE_LOG_LEVEL: \"info\" PODMON_ARRAY_CONNECTIVITY_POLL_RATE: 20 PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD: 2 PODMON_SKIP_ARRAY_CONNECTION_VALIDATION: true ","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency installation\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency installation\n","ref":"/csm-docs/docs/resiliency/deployment/helm/","tags":"","title":"Helm"},{"body":"The Container Storage Modules (CSM) for Observability Helm chart bootstraps an Observability deployment on a Kubernetes cluster using the Helm package manager.\nPrerequisites Helm 3.3 The deployment of one or more supported Dell CSI drivers Install the CSM for Observability Helm Chart Steps\nCreate a namespace where you want to install the module\nkubectl create namespace karavi Install cert-manager CRDs\nkubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.10.0/cert-manager.crds.yaml Add the Dell Helm Charts repo\nhelm repo add dell https://dell.github.io/helm-charts Copy only the deployed CSI driver entities to the Observability namespace\nPowerFlex Copy the config Secret from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default vxflexos-config, please use the following command to copy secret:\nkubectl get secret [VXFLEXOS-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG]/name: vxflexos-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerFlex, perform the following steps:\nCopy the driver configuration parameters ConfigMap from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get configmap vxflexos-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default vxflexos-config-params, please use the following command to copy configmap:\nkubectl get configmap [VXFLEXOS-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG-PARAMS]/name: vxflexos-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the karavi-authorization-config, proxy-server-root-certificate, proxy-authz-tokens Secret from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - PowerStore Copy the config Secret from the CSI PowerStore namespace into the CSM for Observability namespace:\nkubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default powerstore-config, please use the following command to copy secret:\nkubectl get secret [POWERSTORE-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERSTORE-CONFIG]/name: powerstore-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - PowerScale Copy the config Secret from the CSI PowerScale namespace into the CSM for Observability namespace:\nkubectl get secret isilon-creds -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default isilon-creds, please use the following command to copy secret:\nkubectl get secret [ISILON-CREDS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CREDS]/name: isilon-creds/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerScale, perform these steps:\nCopy the driver configuration parameters ConfigMap from the CSI PowerScale namespace into the CSM for Observability namespace:\nkubectl get configmap isilon-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default isilon-config-params, please use the following command to copy configmap:\nkubectl get configmap [ISILON-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CONFIG-PARAMS]/name: isilon-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the karavi-authorization-config, proxy-server-root-certificate, proxy-authz-tokens Secret from the CSI PowerScale namespace into the CSM for Observability namespace:\nkubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: isilon-karavi-authorization-config/' | sed 's/name: proxy-server-root-certificate/name: isilon-proxy-server-root-certificate/' | sed 's/name: proxy-authz-tokens/name: isilon-proxy-authz-tokens/' | kubectl create -f - PowerMax Copy the configmap powermax-reverseproxy-config from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nNote: Observability for PowerMax works only with CSI PowerMax driver with Proxy in StandAlone mode.\nkubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy configmap:\nkubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-REVERSEPROXY-CONFIG]/name: powermax-reverseproxy-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the secrets in powermax-reverseproxy-config from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nfor secret in $(kubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy secrets:\nfor secret in $(kubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If CSM for Authorization is enabled for CSI PowerMax, perform these steps:\nCopy the driver configuration parameters ConfigMap from the CSI PowerMax namespace into the CSM for Observability namespace:\nkubectl get configmap powermax-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-config-params, please use the following command to copy configmap:\nkubectl get configmap [POWERMAX-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-CONFIG-PARAMS]/name: powermax-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the karavi-authorization-config, proxy-server-root-certificate, proxy-authz-tokens Secret from the CSI PowerMax namespace into the CSM for Observability namespace:\nkubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: powermax-karavi-authorization-config/' | sed 's/name: proxy-server-root-certificate/name: powermax-proxy-server-root-certificate/' | sed 's/name: proxy-authz-tokens/name: powermax-proxy-authz-tokens/' | kubectl create -f - Configure the parameters and install the CSM for Observability Helm Chart\nA default values.yaml file is located here that can be used for installation. This can be copied into a file named myvalues.yaml and either used as is or modified accordingly.\nNote:\nThe default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured in your values file for CSM Observability. If CSM for Authorization is enabled for CSI PowerScale, the karaviMetricsPowerscale.authorization parameters must be properly configured in your values file for CSM Observability. If CSM for Authorization is enabled for CSI PowerMax, the karaviMetricsPowerMax.authorization parameters must be properly configured in your values file for CSM Observability. helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] -f myvalues.yaml Alternatively, you can specify each parameter using the ‘–set key=value[,key=value]’ and/or ‘–set-file key=value[,key=value] arguments to ‘helm install’. For example:\nhelm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] \\ --set-file karaviTopology.certificateFile=\u003clocation-of-karavi-topology-certificate-file\u003e \\ --set-file karaviTopology.privateKeyFile=\u003clocation-of-karavi-topology-private-key-file\u003e \\ --set-file otelCollector.certificateFile=\u003clocation-of-otel-collector-certificate-file\u003e \\ --set-file otelCollector.privateKeyFile=\u003clocation-of-otel-collector-private-key-file\u003e Configuration The following table lists the configurable parameters of the CSM for Observability Helm chart and their default values.\nParameter Description Default karaviTopology.image Location of the csm-topology Docker image dellemc/csm-topology:v1.0 karaviTopology.enabled Enable the CSM for Observability Topology service true karaviTopology.provisionerNames Provisioner Names used to filter the Persistent Volumes created on the Kubernetes cluster (must be a comma-separated list) csi-vxflexos.dellemc.com karaviTopology.service.type Kubernetes service type ClusterIP karaviTopology.certificateFile Optional valid CA public certificate file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’. karaviTopology.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’. karaviTopology.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO karaviTopology.logFormat Output logs in the specified format (Valid values: text, json) text otelCollector.certificateFile Optional valid CA public certificate file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’. otelCollector.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’. otelCollector.service.type Kubernetes service type ClusterIP karaviMetricsPowerflex.image CSM Metrics for PowerFlex Service image dellemc/csm-metrics-powerflex:v1.0 karaviMetricsPowerflex.enabled Enable CSM Metrics for PowerFlex service true karaviMetricsPowerflex.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680 karaviMetricsPowerflex.provisionerNames Provisioner Names used to filter for determining PowerFlex SDC nodes( Must be a Comma-separated list) csi-vxflexos.dellemc.com karaviMetricsPowerflex.sdcPollFrequencySeconds The polling frequency (in seconds) to gather SDC metrics 10 karaviMetricsPowerflex.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10 karaviMetricsPowerflex.storageClassPoolPollFrequencySeconds The polling frequency (in seconds) to gather storage class/pool metrics 10 karaviMetricsPowerflex.concurrentPowerflexQueries The number of simultaneous metrics queries to make to Powerflex(MUST be less than 10; otherwise, several request errors from Powerflex will ensue. 10 karaviMetricsPowerflex.authorization.enabled Authorization is an optional feature to apply credential shielding of the backend PowerFlex. false karaviMetricsPowerflex.authorization.proxyHost Hostname of the csm-authorization server. karaviMetricsPowerflex.authorization.skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. karaviMetricsPowerflex.sdcMetricsEnabled Enable PowerFlex SDC Metrics Collection true karaviMetricsPowerflex.volumeMetricsEnabled Enable PowerFlex Volume Metrics Collection true karaviMetricsPowerflex.storageClassPoolMetricsEnabled Enable PowerFlex Storage Class/Pool Metrics Collection true karaviMetricsPowerflex.endpoint Endpoint for pod leader election karavi-metrics-powerflex karaviMetricsPowerflex.service.type Kubernetes service type ClusterIP karaviMetricsPowerflex.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO karaviMetricsPowerflex.logFormat Output logs in the specified format (Valid values: text, json) text karaviMetricsPowerstore.image CSM Metrics for PowerStore Service image dellemc/csm-metrics-powerstore:v1.0 karaviMetricsPowerstore.enabled Enable CSM Metrics for PowerStore service true karaviMetricsPowerstore.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680 karaviMetricsPowerstore.provisionerNames Provisioner Names used to filter for determining PowerStore volumes (must be a Comma-separated list) csi-powerstore.dellemc.com karaviMetricsPowerstore.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10 karaviMetricsPowerstore.concurrentPowerstoreQueries The number of simultaneous metrics queries to make to PowerStore (must be less than 10; otherwise, several request errors from PowerStore will ensue.) 10 karaviMetricsPowerstore.volumeMetricsEnabled Enable PowerStore Volume Metrics Collection true karaviMetricsPowerstore.endpoint Endpoint for pod leader election karavi-metrics-powerstore karaviMetricsPowerstore.service.type Kubernetes service type ClusterIP karaviMetricsPowerstore.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO karaviMetricsPowerstore.logFormat Output logs in the specified format (Valid values: text, json) text karaviMetricsPowerstore.zipkin.uri URI of a Zipkin instance where tracing data can be forwarded karaviMetricsPowerstore.zipkin.serviceName Service name used for Zipkin tracing data metrics-powerstore karaviMetricsPowerstore.zipkin.probability Percentage of trace information to send to Zipkin (Valid range: 0.0 to 1.0) 0 karaviMetricsPowerscale.image CSM Metrics for PowerScale Service image dellemc/csm-metrics-powerscale:v1.0 karaviMetricsPowerscale.enabled Enable CSM Metrics for PowerScale service true karaviMetricsPowerscale.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680 karaviMetricsPowerscale.provisionerNames Provisioner Names used to filter for determining PowerScale volumes (must be a Comma-separated list) csi-isilon.dellemc.com karaviMetricsPowerscale.capacityMetricsEnabled Enable PowerScale capacity metric Collection true karaviMetricsPowerscale.performanceMetricsEnabled Enable PowerScale performance metric Collection true karaviMetricsPowerscale.clusterCapacityPollFrequencySeconds The polling frequency (in seconds) to gather cluster capacity metrics 30 karaviMetricsPowerscale.clusterPerformancePollFrequencySeconds The polling frequency (in seconds) to gather cluster performance metrics 20 karaviMetricsPowerscale.quotaCapacityPollFrequencySeconds The polling frequency (in seconds) to gather volume capacity metrics 30 karaviMetricsPowerscale.concurrentPowerscaleQueries The number of simultaneous metrics queries to make to PowerScale(MUST be less than 10; otherwise, several request errors from PowerScale will ensue.) 10 karaviMetricsPowerscale.endpoint Endpoint for pod leader election karavi-metrics-powerscale karaviMetricsPowerscale.service.type Kubernetes service type ClusterIP karaviMetricsPowerscale.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO karaviMetricsPowerscale.logFormat Output logs in the specified format (Valid values: text, json) text karaviMetricsPowerscale.isiClientOptions.isiSkipCertificateValidation Skip OneFS API server’s certificates true karaviMetricsPowerscale.isiClientOptions.isiAuthType 0 to enable session-based Authentication; 1 to enables basic Authentication 1 karaviMetricsPowerscale.isiClientOptions.isiLogVerbose Decide High/Medium/Low content of the OneFS REST API message 0 karaviMetricsPowerscale.authorization.enabled Authorization is an optional feature to apply credential shielding of the backend PowerScale. false karaviMetricsPowerscale.authorization.proxyHost Hostname of the csm-authorization server. karaviMetricsPowerscale.authorization.skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. karaviMetricsPowerMax.capacityMetricsEnabled Enable PowerMax capacity metric Collection true karaviMetricsPowerMax.performanceMetricsEnabled Enable PowerMax performance metric Collection true karaviMetricsPowerMax.capacityPollFrequencySeconds The polling frequency (in seconds) to gather capacity metrics 20 karaviMetricsPowerMax.performancePollFrequencySeconds The polling frequency (in seconds) to gather performance metrics 20 karaviMetricsPowerMax.concurrentPowerMaxQueries The number of simultaneous metrics queries to make to PowerMax (MUST be less than 10; otherwise, several request errors from PowerMax will ensue.) 10 karaviMetricsPowerMax.authorization.enabled Authorization is an optional feature to apply credential shielding of the backend PowerMax. false karaviMetricsPowerMax.authorization.proxyHost Hostname of the csm-authorization server. karaviMetricsPowerMax.authorization.skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Helm deployment\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Helm deployment …","ref":"/csm-docs/v1/observability/deployment/helm/","tags":"","title":"Helm"},{"body":"The Container Storage Modules (CSM) for Observability Helm chart bootstraps an Observability deployment on a Kubernetes cluster using the Helm package manager.\nPrerequisites Helm 3.3 The deployment of one or more supported Dell CSI drivers Install the CSM for Observability Helm Chart Steps\nCreate a namespace where you want to install the module\nkubectl create namespace karavi Install cert-manager CRDs\nkubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.10.0/cert-manager.crds.yaml Add the Dell Helm Charts repo\nhelm repo add dell https://dell.github.io/helm-charts Copy only the deployed CSI driver entities to the Observability namespace\nPowerFlex Copy the config Secret from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default vxflexos-config, please use the following command to copy secret:\nkubectl get secret [VXFLEXOS-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG]/name: vxflexos-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerFlex, perform the following steps:\nCopy the driver configuration parameters ConfigMap from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get configmap vxflexos-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default vxflexos-config-params, please use the following command to copy configmap:\nkubectl get configmap [VXFLEXOS-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG-PARAMS]/name: vxflexos-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the karavi-authorization-config, proxy-server-root-certificate, proxy-authz-tokens Secret from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - PowerStore Copy the config Secret from the CSI PowerStore namespace into the CSM for Observability namespace:\nkubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default powerstore-config, please use the following command to copy secret:\nkubectl get secret [POWERSTORE-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERSTORE-CONFIG]/name: powerstore-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - PowerScale Copy the config Secret from the CSI PowerScale namespace into the CSM for Observability namespace:\nkubectl get secret isilon-creds -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default isilon-creds, please use the following command to copy secret:\nkubectl get secret [ISILON-CREDS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CREDS]/name: isilon-creds/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerScale, perform these steps:\nCopy the driver configuration parameters ConfigMap from the CSI PowerScale namespace into the CSM for Observability namespace:\nkubectl get configmap isilon-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default isilon-config-params, please use the following command to copy configmap:\nkubectl get configmap [ISILON-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CONFIG-PARAMS]/name: isilon-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the karavi-authorization-config, proxy-server-root-certificate, proxy-authz-tokens Secret from the CSI PowerScale namespace into the CSM for Observability namespace:\nkubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: isilon-karavi-authorization-config/' | sed 's/name: proxy-server-root-certificate/name: isilon-proxy-server-root-certificate/' | sed 's/name: proxy-authz-tokens/name: isilon-proxy-authz-tokens/' | kubectl create -f - PowerMax Copy the configmap powermax-reverseproxy-config from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nNote: Observability for PowerMax works only with CSI PowerMax driver with Proxy in StandAlone mode.\nkubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy configmap:\nkubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-REVERSEPROXY-CONFIG]/name: powermax-reverseproxy-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the secrets in powermax-reverseproxy-config from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nfor secret in $(kubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy secrets:\nfor secret in $(kubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If CSM for Authorization is enabled for CSI PowerMax, perform these steps:\nCopy the driver configuration parameters ConfigMap from the CSI PowerMax namespace into the CSM for Observability namespace:\nkubectl get configmap powermax-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-config-params, please use the following command to copy configmap:\nkubectl get configmap [POWERMAX-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-CONFIG-PARAMS]/name: powermax-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the karavi-authorization-config, proxy-server-root-certificate, proxy-authz-tokens Secret from the CSI PowerMax namespace into the CSM for Observability namespace:\nkubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: powermax-karavi-authorization-config/' | sed 's/name: proxy-server-root-certificate/name: powermax-proxy-server-root-certificate/' | sed 's/name: proxy-authz-tokens/name: powermax-proxy-authz-tokens/' | kubectl create -f - Configure the parameters and install the CSM for Observability Helm Chart\nA default values.yaml file is located here that can be used for installation. This can be copied into a file named myvalues.yaml and either used as is or modified accordingly.\nNote:\nThe default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured in your values file for CSM Observability. If CSM for Authorization is enabled for CSI PowerScale, the karaviMetricsPowerscale.authorization parameters must be properly configured in your values file for CSM Observability. If CSM for Authorization is enabled for CSI PowerMax, the karaviMetricsPowerMax.authorization parameters must be properly configured in your values file for CSM Observability. helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] -f myvalues.yaml Alternatively, you can specify each parameter using the ‘–set key=value[,key=value]’ and/or ‘–set-file key=value[,key=value] arguments to ‘helm install’. For example:\nhelm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] \\ --set-file karaviTopology.certificateFile=\u003clocation-of-karavi-topology-certificate-file\u003e \\ --set-file karaviTopology.privateKeyFile=\u003clocation-of-karavi-topology-private-key-file\u003e \\ --set-file otelCollector.certificateFile=\u003clocation-of-otel-collector-certificate-file\u003e \\ --set-file otelCollector.privateKeyFile=\u003clocation-of-otel-collector-private-key-file\u003e Configuration The following table lists the configurable parameters of the CSM for Observability Helm chart and their default values.\nParameter Description Default karaviTopology.image Location of the csm-topology Docker image dellemc/csm-topology:v1.0 karaviTopology.enabled Enable the CSM for Observability Topology service true karaviTopology.provisionerNames Provisioner Names used to filter the Persistent Volumes created on the Kubernetes cluster (must be a comma-separated list) csi-vxflexos.dellemc.com karaviTopology.service.type Kubernetes service type ClusterIP karaviTopology.certificateFile Optional valid CA public certificate file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’. karaviTopology.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’. karaviTopology.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO karaviTopology.logFormat Output logs in the specified format (Valid values: text, json) text otelCollector.certificateFile Optional valid CA public certificate file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’. otelCollector.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’. otelCollector.service.type Kubernetes service type ClusterIP karaviMetricsPowerflex.image CSM Metrics for PowerFlex Service image dellemc/csm-metrics-powerflex:v1.0 karaviMetricsPowerflex.enabled Enable CSM Metrics for PowerFlex service true karaviMetricsPowerflex.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680 karaviMetricsPowerflex.provisionerNames Provisioner Names used to filter for determining PowerFlex SDC nodes( Must be a Comma-separated list) csi-vxflexos.dellemc.com karaviMetricsPowerflex.sdcPollFrequencySeconds The polling frequency (in seconds) to gather SDC metrics 10 karaviMetricsPowerflex.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10 karaviMetricsPowerflex.storageClassPoolPollFrequencySeconds The polling frequency (in seconds) to gather storage class/pool metrics 10 karaviMetricsPowerflex.concurrentPowerflexQueries The number of simultaneous metrics queries to make to Powerflex(MUST be less than 10; otherwise, several request errors from Powerflex will ensue. 10 karaviMetricsPowerflex.authorization.enabled Authorization is an optional feature to apply credential shielding of the backend PowerFlex. false karaviMetricsPowerflex.authorization.proxyHost Hostname of the csm-authorization server. karaviMetricsPowerflex.authorization.skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. karaviMetricsPowerflex.sdcMetricsEnabled Enable PowerFlex SDC Metrics Collection true karaviMetricsPowerflex.volumeMetricsEnabled Enable PowerFlex Volume Metrics Collection true karaviMetricsPowerflex.storageClassPoolMetricsEnabled Enable PowerFlex Storage Class/Pool Metrics Collection true karaviMetricsPowerflex.endpoint Endpoint for pod leader election karavi-metrics-powerflex karaviMetricsPowerflex.service.type Kubernetes service type ClusterIP karaviMetricsPowerflex.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO karaviMetricsPowerflex.logFormat Output logs in the specified format (Valid values: text, json) text karaviMetricsPowerstore.image CSM Metrics for PowerStore Service image dellemc/csm-metrics-powerstore:v1.0 karaviMetricsPowerstore.enabled Enable CSM Metrics for PowerStore service true karaviMetricsPowerstore.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680 karaviMetricsPowerstore.provisionerNames Provisioner Names used to filter for determining PowerStore volumes (must be a Comma-separated list) csi-powerstore.dellemc.com karaviMetricsPowerstore.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10 karaviMetricsPowerstore.concurrentPowerstoreQueries The number of simultaneous metrics queries to make to PowerStore (must be less than 10; otherwise, several request errors from PowerStore will ensue.) 10 karaviMetricsPowerstore.volumeMetricsEnabled Enable PowerStore Volume Metrics Collection true karaviMetricsPowerstore.endpoint Endpoint for pod leader election karavi-metrics-powerstore karaviMetricsPowerstore.service.type Kubernetes service type ClusterIP karaviMetricsPowerstore.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO karaviMetricsPowerstore.logFormat Output logs in the specified format (Valid values: text, json) text karaviMetricsPowerstore.zipkin.uri URI of a Zipkin instance where tracing data can be forwarded karaviMetricsPowerstore.zipkin.serviceName Service name used for Zipkin tracing data metrics-powerstore karaviMetricsPowerstore.zipkin.probability Percentage of trace information to send to Zipkin (Valid range: 0.0 to 1.0) 0 karaviMetricsPowerscale.image CSM Metrics for PowerScale Service image dellemc/csm-metrics-powerscale:v1.0 karaviMetricsPowerscale.enabled Enable CSM Metrics for PowerScale service true karaviMetricsPowerscale.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680 karaviMetricsPowerscale.provisionerNames Provisioner Names used to filter for determining PowerScale volumes (must be a Comma-separated list) csi-isilon.dellemc.com karaviMetricsPowerscale.capacityMetricsEnabled Enable PowerScale capacity metric Collection true karaviMetricsPowerscale.performanceMetricsEnabled Enable PowerScale performance metric Collection true karaviMetricsPowerscale.clusterCapacityPollFrequencySeconds The polling frequency (in seconds) to gather cluster capacity metrics 30 karaviMetricsPowerscale.clusterPerformancePollFrequencySeconds The polling frequency (in seconds) to gather cluster performance metrics 20 karaviMetricsPowerscale.quotaCapacityPollFrequencySeconds The polling frequency (in seconds) to gather volume capacity metrics 30 karaviMetricsPowerscale.concurrentPowerscaleQueries The number of simultaneous metrics queries to make to PowerScale(MUST be less than 10; otherwise, several request errors from PowerScale will ensue.) 10 karaviMetricsPowerscale.endpoint Endpoint for pod leader election karavi-metrics-powerscale karaviMetricsPowerscale.service.type Kubernetes service type ClusterIP karaviMetricsPowerscale.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO karaviMetricsPowerscale.logFormat Output logs in the specified format (Valid values: text, json) text karaviMetricsPowerscale.isiClientOptions.isiSkipCertificateValidation Skip OneFS API server’s certificates true karaviMetricsPowerscale.isiClientOptions.isiAuthType 0 to enable session-based Authentication; 1 to enables basic Authentication 1 karaviMetricsPowerscale.isiClientOptions.isiLogVerbose Decide High/Medium/Low content of the OneFS REST API message 0 karaviMetricsPowerscale.authorization.enabled Authorization is an optional feature to apply credential shielding of the backend PowerScale. false karaviMetricsPowerscale.authorization.proxyHost Hostname of the csm-authorization server. karaviMetricsPowerscale.authorization.skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. karaviMetricsPowerMax.capacityMetricsEnabled Enable PowerMax capacity metric Collection true karaviMetricsPowerMax.performanceMetricsEnabled Enable PowerMax performance metric Collection true karaviMetricsPowerMax.capacityPollFrequencySeconds The polling frequency (in seconds) to gather capacity metrics 20 karaviMetricsPowerMax.performancePollFrequencySeconds The polling frequency (in seconds) to gather performance metrics 20 karaviMetricsPowerMax.concurrentPowerMaxQueries The number of simultaneous metrics queries to make to PowerMax (MUST be less than 10; otherwise, several request errors from PowerMax will ensue.) 10 karaviMetricsPowerMax.authorization.enabled Authorization is an optional feature to apply credential shielding of the backend PowerMax. false karaviMetricsPowerMax.authorization.proxyHost Hostname of the csm-authorization server. karaviMetricsPowerMax.authorization.skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Helm deployment\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Helm deployment …","ref":"/csm-docs/v2/observability/deployment/helm/","tags":"","title":"Helm"},{"body":"The Container Storage Modules (CSM) for Observability Helm chart bootstraps an Observability deployment on a Kubernetes cluster using the Helm package manager.\nPrerequisites Helm 3.3 The deployment of one or more supported Dell CSI drivers Install the CSM for Observability Helm Chart Steps\nCreate a namespace where you want to install the module kubectl create namespace karavi\nInstall cert-manager CRDs kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.10.0/cert-manager.crds.yaml\nAdd the Dell Helm Charts repo helm repo add dell https://dell.github.io/helm-charts\nCopy only the deployed CSI driver entities to the Observability namespace\nPowerFlex Copy the config Secret from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\nIf the CSI driver secret name is not the default vxflexos-config, please use the following command to copy secret:\nkubectl get secret [VXFLEXOS-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG]/name: vxflexos-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\nIf CSM for Authorization is enabled for CSI PowerFlex, perform the following steps:\nCopy the driver configuration parameters ConfigMap from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get configmap vxflexos-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\nIf the CSI driver configmap name is not the default vxflexos-config-params, please use the following command to copy configmap:\nkubectl get configmap [VXFLEXOS-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG-PARAMS]/name: vxflexos-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\nCopy the karavi-authorization-config, proxy-server-root-certificate, proxy-authz-tokens Secret from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\nPowerStore Copy the config Secret from the CSI PowerStore namespace into the CSM for Observability namespace:\nkubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\nIf the CSI driver secret name is not the default powerstore-config, please use the following command to copy secret:\nkubectl get secret [POWERSTORE-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERSTORE-CONFIG]/name: powerstore-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\nPowerScale Copy the config Secret from the CSI PowerScale namespace into the CSM for Observability namespace:\nkubectl get secret isilon-creds -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\nIf the CSI driver secret name is not the default isilon-creds, please use the following command to copy secret:\nkubectl get secret [ISILON-CREDS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CREDS]/name: isilon-creds/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\nIf CSM for Authorization is enabled for CSI PowerScale, perform these steps:\nCopy the driver configuration parameters ConfigMap from the CSI PowerScale namespace into the CSM for Observability namespace:\nkubectl get configmap isilon-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\nIf the CSI driver configmap name is not the default isilon-config-params, please use the following command to copy configmap:\nkubectl get configmap [ISILON-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CONFIG-PARAMS]/name: isilon-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\nCopy the karavi-authorization-config, proxy-server-root-certificate, proxy-authz-tokens Secret from the CSI PowerScale namespace into the CSM for Observability namespace:\nkubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: isilon-karavi-authorization-config/' | sed 's/name: proxy-server-root-certificate/name: isilon-proxy-server-root-certificate/' | sed 's/name: proxy-authz-tokens/name: isilon-proxy-authz-tokens/' | kubectl create -f -\nPowerMax Copy the configmap powermax-reverseproxy-config from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nNote: Observability for PowerMax works only with CSI PowerMax driver with Proxy in StandAlone mode.\nkubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\nIf the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy configmap:\nkubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-REVERSEPROXY-CONFIG]/name: powermax-reverseproxy-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\nCopy the secrets in powermax-reverseproxy-config from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nfor secret in $(kubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy secrets:\nfor secret in $(kubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If CSM for Authorization is enabled for CSI PowerMax, perform these steps:\nCopy the driver configuration parameters ConfigMap from the CSI PowerMax namespace into the CSM for Observability namespace:\nkubectl get configmap powermax-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\nIf the CSI driver configmap name is not the default powermax-config-params, please use the following command to copy configmap:\nkubectl get configmap [POWERMAX-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-CONFIG-PARAMS]/name: powermax-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\nCopy the karavi-authorization-config, proxy-server-root-certificate, proxy-authz-tokens Secret from the CSI PowerMax namespace into the CSM for Observability namespace:\nkubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: powermax-karavi-authorization-config/' | sed 's/name: proxy-server-root-certificate/name: powermax-proxy-server-root-certificate/' | sed 's/name: proxy-authz-tokens/name: powermax-proxy-authz-tokens/' | kubectl create -f -\nConfigure the parameters and install the CSM for Observability Helm Chart\nA default values.yaml file is located here that can be used for installation. This can be copied into a file named myvalues.yaml and either used as is or modified accordingly.\nNote:\nThe default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured in your values file for CSM Observability. If CSM for Authorization is enabled for CSI PowerScale, the karaviMetricsPowerscale.authorization parameters must be properly configured in your values file for CSM Observability. If CSM for Authorization is enabled for CSI PowerMax, the karaviMetricsPowermax.authorization parameters must be properly configured in your values file for CSM Observability. $ helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] -f myvalues.yaml Alternatively, you can specify each parameter using the ‘–set key=value[,key=value]’ and/or ‘–set-file key=value[,key=value] arguments to ‘helm install’. For example:\n$ helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] \\ --set-file karaviTopology.certificateFile=\u003clocation-of-karavi-topology-certificate-file\u003e \\ --set-file karaviTopology.privateKeyFile=\u003clocation-of-karavi-topology-private-key-file\u003e \\ --set-file otelCollector.certificateFile=\u003clocation-of-otel-collector-certificate-file\u003e \\ --set-file otelCollector.privateKeyFile=\u003clocation-of-otel-collector-private-key-file\u003e Configuration The following table lists the configurable parameters of the CSM for Observability Helm chart and their default values.\nParameter Description Default karaviTopology.image Location of the csm-topology Docker image dellemc/csm-topology:v1.0 karaviTopology.enabled Enable the CSM for Observability Topology service true karaviTopology.provisionerNames Provisioner Names used to filter the Persistent Volumes created on the Kubernetes cluster (must be a comma-separated list) csi-vxflexos.dellemc.com karaviTopology.service.type Kubernetes service type ClusterIP karaviTopology.certificateFile Optional valid CA public certificate file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’. karaviTopology.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’. karaviTopology.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO karaviTopology.logFormat Output logs in the specified format (Valid values: text, json) text otelCollector.certificateFile Optional valid CA public certificate file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’. otelCollector.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’. otelCollector.service.type Kubernetes service type ClusterIP karaviMetricsPowerflex.image CSM Metrics for PowerFlex Service image dellemc/csm-metrics-powerflex:v1.0 karaviMetricsPowerflex.enabled Enable CSM Metrics for PowerFlex service true karaviMetricsPowerflex.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680 karaviMetricsPowerflex.provisionerNames Provisioner Names used to filter for determining PowerFlex SDC nodes( Must be a Comma-separated list) csi-vxflexos.dellemc.com karaviMetricsPowerflex.sdcPollFrequencySeconds The polling frequency (in seconds) to gather SDC metrics 10 karaviMetricsPowerflex.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10 karaviMetricsPowerflex.storageClassPoolPollFrequencySeconds The polling frequency (in seconds) to gather storage class/pool metrics 10 karaviMetricsPowerflex.concurrentPowerflexQueries The number of simultaneous metrics queries to make to Powerflex(MUST be less than 10; otherwise, several request errors from Powerflex will ensue. 10 karaviMetricsPowerflex.authorization.enabled Authorization is an optional feature to apply credential shielding of the backend PowerFlex. false karaviMetricsPowerflex.authorization.proxyHost Hostname of the csm-authorization server. karaviMetricsPowerflex.authorization.skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. karaviMetricsPowerflex.sdcMetricsEnabled Enable PowerFlex SDC Metrics Collection true karaviMetricsPowerflex.volumeMetricsEnabled Enable PowerFlex Volume Metrics Collection true karaviMetricsPowerflex.storageClassPoolMetricsEnabled Enable PowerFlex Storage Class/Pool Metrics Collection true karaviMetricsPowerflex.endpoint Endpoint for pod leader election karavi-metrics-powerflex karaviMetricsPowerflex.service.type Kubernetes service type ClusterIP karaviMetricsPowerflex.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO karaviMetricsPowerflex.logFormat Output logs in the specified format (Valid values: text, json) text karaviMetricsPowerstore.image CSM Metrics for PowerStore Service image dellemc/csm-metrics-powerstore:v1.0 karaviMetricsPowerstore.enabled Enable CSM Metrics for PowerStore service true karaviMetricsPowerstore.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680 karaviMetricsPowerstore.provisionerNames Provisioner Names used to filter for determining PowerStore volumes (must be a Comma-separated list) csi-powerstore.dellemc.com karaviMetricsPowerstore.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10 karaviMetricsPowerstore.concurrentPowerstoreQueries The number of simultaneous metrics queries to make to PowerStore (must be less than 10; otherwise, several request errors from PowerStore will ensue.) 10 karaviMetricsPowerstore.volumeMetricsEnabled Enable PowerStore Volume Metrics Collection true karaviMetricsPowerstore.endpoint Endpoint for pod leader election karavi-metrics-powerstore karaviMetricsPowerstore.service.type Kubernetes service type ClusterIP karaviMetricsPowerstore.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO karaviMetricsPowerstore.logFormat Output logs in the specified format (Valid values: text, json) text karaviMetricsPowerstore.zipkin.uri URI of a Zipkin instance where tracing data can be forwarded karaviMetricsPowerstore.zipkin.serviceName Service name used for Zipkin tracing data metrics-powerstore karaviMetricsPowerstore.zipkin.probability Percentage of trace information to send to Zipkin (Valid range: 0.0 to 1.0) 0 karaviMetricsPowerscale.image CSM Metrics for PowerScale Service image dellemc/csm-metrics-powerscale:v1.0 karaviMetricsPowerscale.enabled Enable CSM Metrics for PowerScale service true karaviMetricsPowerscale.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680 karaviMetricsPowerscale.provisionerNames Provisioner Names used to filter for determining PowerScale volumes (must be a Comma-separated list) csi-isilon.dellemc.com karaviMetricsPowerscale.capacityMetricsEnabled Enable PowerScale capacity metric Collection true karaviMetricsPowerscale.performanceMetricsEnabled Enable PowerScale performance metric Collection true karaviMetricsPowerscale.clusterCapacityPollFrequencySeconds The polling frequency (in seconds) to gather cluster capacity metrics 30 karaviMetricsPowerscale.clusterPerformancePollFrequencySeconds The polling frequency (in seconds) to gather cluster performance metrics 20 karaviMetricsPowerscale.quotaCapacityPollFrequencySeconds The polling frequency (in seconds) to gather volume capacity metrics 30 karaviMetricsPowerscale.concurrentPowerscaleQueries The number of simultaneous metrics queries to make to PowerScale(MUST be less than 10; otherwise, several request errors from PowerScale will ensue.) 10 karaviMetricsPowerscale.endpoint Endpoint for pod leader election karavi-metrics-powerscale karaviMetricsPowerscale.service.type Kubernetes service type ClusterIP karaviMetricsPowerscale.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO karaviMetricsPowerscale.logFormat Output logs in the specified format (Valid values: text, json) text karaviMetricsPowerscale.isiClientOptions.isiSkipCertificateValidation Skip OneFS API server’s certificates true karaviMetricsPowerscale.isiClientOptions.isiAuthType 0 to enable session-based Authentication; 1 to enables basic Authentication 1 karaviMetricsPowerscale.isiClientOptions.isiLogVerbose Decide High/Medium/Low content of the OneFS REST API message 0 karaviMetricsPowerscale.authorization.enabled Authorization is an optional feature to apply credential shielding of the backend PowerScale. false karaviMetricsPowerscale.authorization.proxyHost Hostname of the csm-authorization server. karaviMetricsPowerscale.authorization.skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. karaviMetricsPowermax.capacityMetricsEnabled Enable PowerMax capacity metric Collection true karaviMetricsPowermax.performanceMetricsEnabled Enable PowerMax performance metric Collection true karaviMetricsPowermax.capacityPollFrequencySeconds The polling frequency (in seconds) to gather capacity metrics 20 karaviMetricsPowermax.performancePollFrequencySeconds The polling frequency (in seconds) to gather performance metrics 20 karaviMetricsPowermax.concurrentPowermaxQueries The number of simultaneous metrics queries to make to PowerMax (MUST be less than 10; otherwise, several request errors from PowerMax will ensue.) 10 karaviMetricsPowermax.authorization.enabled Authorization is an optional feature to apply credential shielding of the backend PowerMax. false karaviMetricsPowermax.authorization.proxyHost Hostname of the csm-authorization server. karaviMetricsPowermax.authorization.skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Helm deployment\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Helm deployment …","ref":"/csm-docs/v3/observability/deployment/helm/","tags":"","title":"Helm"},{"body":" The Container Storage Modules (CSM) for Observability installer bootstraps Helm to create a more simplified and robust deployment option that does the following:\nVerifies CSM for Observability is not yet installed Verifies the Kubernetes/Openshift versions are supported Verifies the Helm version is supported Adds the Dell Helm chart repository Refreshes the Helm chart repositories to download any recent changes Creates the CSM namespace (if not already created) Copies the secrets from the CSI driver namespaces into the CSM namespace (if not already copied) Installs the CertManager CRDs (if not already installed) Installs the CSM for Observability Helm chart Waits for the CSM for Observability pods to become ready If the Authorization module is enabled for the CSI drivers installed in the same Kubernetes cluster, the installer will perform the current steps to enable CSM for Observability to use the same Authorization instance:\nVerifies the karavictl binary is available. Verifies the appropriate Secrets and ConfigMap exist in the CSI driver namespace. Updates the CSM for Observability deployment to use the existing Authorization instance if not already enabled during the initial installation of CSM for Observability. Prerequisites Helm 3.x The deployment of one or more supported Dell CSI drivers Online Installer Follow the instructions below to install CSM for Observability in an environment that has an Internet connection and is capable of downloading the required Helm chart and Docker images. The installer expects CSI drivers are using the default secret and configmap names.\nDependencies A Linux-based system, with Internet access, will be used to execute the script to install CSM for Observability into a Kubernetes/Openshift environment that also has Internet access.\nDependency Usage kubectl kubectl will be used to verify the Kubernetes/OpenShift environment helm helm will be used to install the CSM for Observability helm chart jq jq will be used to parse the CSM for Authorization configuration file during installation Installer Usage ./karavi-observability-install.sh --help Help for ./karavi-observability-install.sh Usage: ./karavi-observability-install.sh mode options... Mode: install Installs Karavi Observability and enables Karavi Authorization if already installed enable-authorization Updates existing installation of Karavi Observability with Karavi Authorization upgrade Upgrades existing installation of Karavi Observability to the latest release Options: Required --namespace[=]\u003cnamespace\u003e Namespace where Karavi Observability will be installed Optional --csi-powerflex-namespace[=]\u003ccsi powerflex namespace\u003e Namespace where CSI PowerFlex is installed, default is 'vxflexos' --csi-powerstore-namespace[=]\u003ccsi powerstore namespace\u003e Namespace where CSI PowerStore is installed, default is 'csi-powerstore' --csi-powerscale-namespace[=]\u003ccsi powerscale namespace\u003e Namespace where CSI PowerScale is installed, default is 'isilon' --csi-powermax-namespace[=]\u003ccsi powermax namespace\u003e Namespace where CSI PowerMax is installed, default is 'powermax' --set-file Set values from files used during helm installation (can be specified multiple times) --skip-verify Skip verification of the environment --values[=]\u003cvalues.yaml\u003e Values file, which defines configuration values --verbose Display verbose logging --version[=]\u003chelm chart version\u003e Helm chart version to install, default value will be latest --help Help Note: CSM for Authorization currently does not support the Observability module for PowerStore. Therefore setting enable-authorization is not supported in this case.\nExecuting the Installer To perform an online installation of CSM for Observability, the following steps should be performed:\nClone the GitHub repository:\ngit clone https://github.com/dell/karavi-observability.git Change to the installer directory:\ncd karavi-observability/installer Execute the installation script. The following example will install CSM for Observability into the CSM namespace.\nA sample values.yaml file is located here. This can be copied into a file named myvalues.yaml and modified accordingly for the installer command below. Configuration options are outlined in the Helm chart deployment section.\nNote:\nThe default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured in myvalues.yaml for CSM Observability. If CSM for Authorization is enabled for CSI PowerScale, the karaviMetricsPowerscale.authorization parameters must be properly configured in myvalues.yaml for CSM Observability. If CSM for Authorization is enabled for CSI PowerMax, the karaviMetricsPowerMax.authorization parameters must be properly configured in myvalues.yaml for CSM Observability. ./karavi-observability-install.sh install --namespace [CSM_NAMESPACE] --values myvalues.yaml --------------------------------------------------------------------------------- \u003e Installing Karavi Observability in namespace karavi on 1.27 --------------------------------------------------------------------------------- | |- Karavi Observability is not installed Success | |- Karavi Authorization will be enabled during installation | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying helm version Success | |- Configure helm chart repository | |--\u003e Adding helm repository https://dell.github.io/helm-charts Success | |--\u003e Updating helm repositories Success | |- Creating namespace karavi Success | |- CSI Driver for PowerFlex is installed Success | |- Copying Secret from vxflexos to karavi Success | |- CSI Driver for PowerStore is installed Success | |- Copying Secret from powerstore to karavi Success | |- CSI Driver for PowerScale is installed Success | |- Copying Secret from isilon to karavi Success | |- CSI Driver for PowerMax is installed Success | |- Copying ConfigMap from powermax to karavi Success | |- Copying Secret from powermax to karavi Success | |- Installing CertManager CRDs Success | |- Enabling Karavi Authorization for Karavi Observability | |--\u003e Copying ConfigMap from vxflexos to karavi Success | |--\u003e Copying Karavi Authorization Secrets from vxflexos to karavi Success | |--\u003e Copying ConfigMap from isilon to karavi Success | |--\u003e Copying Karavi Authorization Secrets from isilon to karavi Success | |--\u003e Copying ConfigMap from powermax to karavi Success | |--\u003e Copying Karavi Authorization Secrets from powermax to karavi Success | |- Installing Karavi Observability helm chart Success | |- Waiting for pods in namespace karavi to be ready Success ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Installer\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Installer\n","ref":"/csm-docs/docs/observability/deployment/online/","tags":"","title":"Installer"},{"body":" The Container Storage Modules (CSM) for Observability installer bootstraps Helm to create a more simplified and robust deployment option that does the following:\nVerifies CSM for Observability is not yet installed Verifies the Kubernetes/Openshift versions are supported Verifies the Helm version is supported Adds the Dell Helm chart repository Refreshes the Helm chart repositories to download any recent changes Creates the CSM namespace (if not already created) Copies the secrets from the CSI driver namespaces into the CSM namespace (if not already copied) Installs the CertManager CRDs (if not already installed) Installs the CSM for Observability Helm chart Waits for the CSM for Observability pods to become ready If the Authorization module is enabled for the CSI drivers installed in the same Kubernetes cluster, the installer will perform the current steps to enable CSM for Observability to use the same Authorization instance:\nVerifies the karavictl binary is available. Verifies the appropriate Secrets and ConfigMap exist in the CSI driver namespace. Updates the CSM for Observability deployment to use the existing Authorization instance if not already enabled during the initial installation of CSM for Observability. Prerequisites Helm 3.3 The deployment of one or more supported Dell CSI drivers Online Installer Follow the instructions below to install CSM for Observability in an environment that has an Internet connection and is capable of downloading the required Helm chart and Docker images. The installer expects CSI drivers are using the default secret and configmap names.\nDependencies A Linux-based system, with Internet access, will be used to execute the script to install CSM for Observability into a Kubernetes/Openshift environment that also has Internet access.\nDependency Usage kubectl kubectl will be used to verify the Kubernetes/OpenShift environment helm helm will be used to install the CSM for Observability helm chart jq jq will be used to parse the CSM for Authorization configuration file during installation Installer Usage ./karavi-observability-install.sh --help Help for ./karavi-observability-install.sh Usage: ./karavi-observability-install.sh mode options... Mode: install Installs Karavi Observability and enables Karavi Authorization if already installed enable-authorization Updates existing installation of Karavi Observability with Karavi Authorization upgrade Upgrades existing installation of Karavi Observability to the latest release Options: Required --namespace[=]\u003cnamespace\u003e Namespace where Karavi Observability will be installed Optional --csi-powerflex-namespace[=]\u003ccsi powerflex namespace\u003e Namespace where CSI PowerFlex is installed, default is 'vxflexos' --csi-powerstore-namespace[=]\u003ccsi powerstore namespace\u003e Namespace where CSI PowerStore is installed, default is 'csi-powerstore' --csi-powerscale-namespace[=]\u003ccsi powerscale namespace\u003e Namespace where CSI PowerScale is installed, default is 'isilon' --csi-powermax-namespace[=]\u003ccsi powermax namespace\u003e Namespace where CSI PowerMax is installed, default is 'powermax' --set-file Set values from files used during helm installation (can be specified multiple times) --skip-verify Skip verification of the environment --values[=]\u003cvalues.yaml\u003e Values file, which defines configuration values --verbose Display verbose logging --version[=]\u003chelm chart version\u003e Helm chart version to install, default value will be latest --help Help Note: CSM for Authorization currently does not support the Observability module for PowerStore. Therefore setting enable-authorization is not supported in this case.\nExecuting the Installer To perform an online installation of CSM for Observability, the following steps should be performed:\nClone the GitHub repository:\ngit clone https://github.com/dell/karavi-observability.git Change to the installer directory:\ncd karavi-observability/installer Execute the installation script. The following example will install CSM for Observability into the CSM namespace.\nA sample values.yaml file is located here. This can be copied into a file named myvalues.yaml and modified accordingly for the installer command below. Configuration options are outlined in the Helm chart deployment section.\nNote:\nThe default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured in myvalues.yaml for CSM Observability. If CSM for Authorization is enabled for CSI PowerScale, the karaviMetricsPowerscale.authorization parameters must be properly configured in myvalues.yaml for CSM Observability. If CSM for Authorization is enabled for CSI PowerMax, the karaviMetricsPowerMax.authorization parameters must be properly configured in myvalues.yaml for CSM Observability. ./karavi-observability-install.sh install --namespace [CSM_NAMESPACE] --values myvalues.yaml --------------------------------------------------------------------------------- \u003e Installing Karavi Observability in namespace karavi on 1.27 --------------------------------------------------------------------------------- | |- Karavi Observability is not installed Success | |- Karavi Authorization will be enabled during installation | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying helm version Success | |- Configure helm chart repository | |--\u003e Adding helm repository https://dell.github.io/helm-charts Success | |--\u003e Updating helm repositories Success | |- Creating namespace karavi Success | |- CSI Driver for PowerFlex is installed Success | |- Copying Secret from vxflexos to karavi Success | |- CSI Driver for PowerStore is installed Success | |- Copying Secret from powerstore to karavi Success | |- CSI Driver for PowerScale is installed Success | |- Copying Secret from isilon to karavi Success | |- CSI Driver for PowerMax is installed Success | |- Copying ConfigMap from powermax to karavi Success | |- Copying Secret from powermax to karavi Success | |- Installing CertManager CRDs Success | |- Enabling Karavi Authorization for Karavi Observability | |--\u003e Copying ConfigMap from vxflexos to karavi Success | |--\u003e Copying Karavi Authorization Secrets from vxflexos to karavi Success | |--\u003e Copying ConfigMap from isilon to karavi Success | |--\u003e Copying Karavi Authorization Secrets from isilon to karavi Success | |--\u003e Copying ConfigMap from powermax to karavi Success | |--\u003e Copying Karavi Authorization Secrets from powermax to karavi Success | |- Installing Karavi Observability helm chart Success | |- Waiting for pods in namespace karavi to be ready Success ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Installer\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Installer\n","ref":"/csm-docs/v1/observability/deployment/online/","tags":"","title":"Installer"},{"body":" The Container Storage Modules (CSM) for Observability installer bootstraps Helm to create a more simplified and robust deployment option that does the following:\nVerifies CSM for Observability is not yet installed Verifies the Kubernetes/Openshift versions are supported Verifies the Helm version is supported Adds the Dell Helm chart repository Refreshes the Helm chart repositories to download any recent changes Creates the CSM namespace (if not already created) Copies the secrets from the CSI driver namespaces into the CSM namespace (if not already copied) Installs the CertManager CRDs (if not already installed) Installs the CSM for Observability Helm chart Waits for the CSM for Observability pods to become ready If the Authorization module is enabled for the CSI drivers installed in the same Kubernetes cluster, the installer will perform the current steps to enable CSM for Observability to use the same Authorization instance:\nVerifies the karavictl binary is available. Verifies the appropriate Secrets and ConfigMap exist in the CSI driver namespace. Updates the CSM for Observability deployment to use the existing Authorization instance if not already enabled during the initial installation of CSM for Observability. Prerequisites Helm 3.3 The deployment of one or more supported Dell CSI drivers Online Installer The following instructions can be followed to install CSM for Observability in an environment that has an internet connection and is capable of downloading the required Helm chart and Docker images.\nThe installer expects CSI drivers are using the default secret and configmap names.\nDependencies A Linux-based system, with internet access, will be used to execute the script to install CSM for Observability into a Kubernetes/Openshift environment that also has internet access.\nDependency Usage kubectl kubectl will be used to verify the Kubernetes/OpenShift environment helm helm will be used to install the CSM for Observability helm chart jq jq will be used to parse the CSM for Authorization configuration file during installation Installer Usage ./karavi-observability-install.sh --help Help for ./karavi-observability-install.sh Usage: ./karavi-observability-install.sh mode options... Mode: install Installs Karavi Observability and enables Karavi Authorization if already installed enable-authorization Updates existing installation of Karavi Observability with Karavi Authorization upgrade Upgrades existing installation of Karavi Observability to the latest release Options: Required --namespace[=]\u003cnamespace\u003e Namespace where Karavi Observability will be installed Optional --csi-powerflex-namespace[=]\u003ccsi powerflex namespace\u003e Namespace where CSI PowerFlex is installed, default is 'vxflexos' --csi-powerstore-namespace[=]\u003ccsi powerstore namespace\u003e Namespace where CSI PowerStore is installed, default is 'csi-powerstore' --csi-powerscale-namespace[=]\u003ccsi powerscale namespace\u003e Namespace where CSI PowerScale is installed, default is 'isilon' --csi-powermax-namespace[=]\u003ccsi powermax namespace\u003e Namespace where CSI PowerMax is installed, default is 'powermax' --set-file Set values from files used during helm installation (can be specified multiple times) --skip-verify Skip verification of the environment --values[=]\u003cvalues.yaml\u003e Values file, which defines configuration values --verbose Display verbose logging --version[=]\u003chelm chart version\u003e Helm chart version to install, default value will be latest --help Help Note: CSM for Authorization currently does not support the Observability module for PowerStore. Therefore setting enable-authorization is not supported in this case.\nExecuting the Installer To perform an online installation of CSM for Observability, the following steps should be performed:\nClone the GitHub repository:\ngit clone https://github.com/dell/karavi-observability.git Change to the installer directory:\ncd karavi-observability/installer Execute the installation script. The following example will install CSM for Observability into the CSM namespace.\nA sample values.yaml file is located here. This can be copied into a file named myvalues.yaml and modified accordingly for the installer command below. Configuration options are outlined in the Helm chart deployment section.\nNote:\nThe default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured in myvalues.yaml for CSM Observability. If CSM for Authorization is enabled for CSI PowerScale, the karaviMetricsPowerscale.authorization parameters must be properly configured in myvalues.yaml for CSM Observability. If CSM for Authorization is enabled for CSI PowerMax, the karaviMetricsPowerMax.authorization parameters must be properly configured in myvalues.yaml for CSM Observability. ./karavi-observability-install.sh install --namespace [CSM_NAMESPACE] --values myvalues.yaml --------------------------------------------------------------------------------- \u003e Installing Karavi Observability in namespace karavi on 1.21 --------------------------------------------------------------------------------- | |- Karavi Observability is not installed Success | |- Karavi Authorization will be enabled during installation | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying helm version Success | |- Configure helm chart repository | |--\u003e Adding helm repository https://dell.github.io/helm-charts Success | |--\u003e Updating helm repositories Success | |- Creating namespace karavi Success | |- CSI Driver for PowerFlex is installed Success | |- Copying Secret from vxflexos to karavi Success | |- CSI Driver for PowerStore is installed Success | |- Copying Secret from powerstore to karavi Success | |- CSI Driver for PowerScale is installed Success | |- Copying Secret from isilon to karavi Success | |- CSI Driver for PowerMax is installed Success | |- Copying ConfigMap from powermax to karavi Success | |- Copying Secret from powermax to karavi Success | |- Installing CertManager CRDs Success | |- Enabling Karavi Authorization for Karavi Observability | |--\u003e Copying ConfigMap from vxflexos to karavi Success | |--\u003e Copying Karavi Authorization Secrets from vxflexos to karavi Success | |--\u003e Copying ConfigMap from isilon to karavi Success | |--\u003e Copying Karavi Authorization Secrets from isilon to karavi Success | |--\u003e Copying ConfigMap from powermax to karavi Success | |--\u003e Copying Karavi Authorization Secrets from powermax to karavi Success | |- Installing Karavi Observability helm chart Success | |- Waiting for pods in namespace karavi to be ready Success ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Installer\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Installer\n","ref":"/csm-docs/v2/observability/deployment/online/","tags":"","title":"Installer"},{"body":" The CSM Authorization karavictl CLI is no longer actively maintained or supported. It will be deprecated in CSM 2.0.\nThe Container Storage Modules (CSM) for Observability installer bootstraps Helm to create a more simplified and robust deployment option that does the following:\nVerifies CSM for Observability is not yet installed Verifies the Kubernetes/Openshift versions are supported Verifies the Helm version is supported Adds the Dell Helm chart repository Refreshes the Helm chart repositories to download any recent changes Creates the CSM namespace (if not already created) Copies the secrets from the CSI driver namespaces into the CSM namespace (if not already copied) Installs the CertManager CRDs (if not already installed) Installs the CSM for Observability Helm chart Waits for the CSM for Observability pods to become ready If the Authorization module is enabled for the CSI drivers installed in the same Kubernetes cluster, the installer will perform the current steps to enable CSM for Observability to use the same Authorization instance:\nVerifies the karavictl binary is available. Verifies the appropriate Secrets and ConfigMap exist in the CSI driver namespace. Updates the CSM for Observability deployment to use the existing Authorization instance if not already enabled during the initial installation of CSM for Observability. Prerequisites Helm 3.3 The deployment of one or more supported Dell CSI drivers Online Installer The following instructions can be followed to install CSM for Observability in an environment that has an internet connection and is capable of downloading the required Helm chart and Docker images.\nThe installer expects CSI drivers are using the default secret and configmap names.\nDependencies A Linux-based system, with internet access, will be used to execute the script to install CSM for Observability into a Kubernetes/Openshift environment that also has internet access.\nDependency Usage kubectl kubectl will be used to verify the Kubernetes/OpenShift environment helm helm will be used to install the CSM for Observability helm chart jq jq will be used to parse the CSM for Authorization configuration file during installation Installer Usage [user@system /home/user/karavi-observability/installer]# ./karavi-observability-install.sh --help Help for ./karavi-observability-install.sh Usage: ./karavi-observability-install.sh mode options... Mode: install Installs Karavi Observability and enables Karavi Authorization if already installed enable-authorization Updates existing installation of Karavi Observability with Karavi Authorization upgrade Upgrades existing installation of Karavi Observability to the latest release Options: Required --namespace[=]\u003cnamespace\u003e Namespace where Karavi Observability will be installed Optional --csi-powerflex-namespace[=]\u003ccsi powerflex namespace\u003e Namespace where CSI PowerFlex is installed, default is 'vxflexos' --csi-powerstore-namespace[=]\u003ccsi powerstore namespace\u003e Namespace where CSI PowerStore is installed, default is 'csi-powerstore' --csi-powerscale-namespace[=]\u003ccsi powerscale namespace\u003e Namespace where CSI PowerScale is installed, default is 'isilon' --csi-powermax-namespace[=]\u003ccsi powermax namespace\u003e Namespace where CSI PowerMax is installed, default is 'powermax' --set-file Set values from files used during helm installation (can be specified multiple times) --skip-verify Skip verification of the environment --values[=]\u003cvalues.yaml\u003e Values file, which defines configuration values --verbose Display verbose logging --version[=]\u003chelm chart version\u003e Helm chart version to install, default value will be latest --help Help Note: CSM for Authorization currently does not support the Observability module for PowerStore. Therefore setting enable-authorization is not supported in this case.\nExecuting the Installer To perform an online installation of CSM for Observability, the following steps should be performed:\nClone the GitHub repository:\n[user@system /home/user]# git clone https://github.com/dell/karavi-observability.git Change to the installer directory:\n[user@system /home/user]# cd karavi-observability/installer Execute the installation script. The following example will install CSM for Observability into the CSM namespace.\nA sample values.yaml file is located here. This can be copied into a file named myvalues.yaml and modified accordingly for the installer command below. Configuration options are outlined in the Helm chart deployment section.\nNote:\nThe default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured in myvalues.yaml for CSM Observability. If CSM for Authorization is enabled for CSI PowerScale, the karaviMetricsPowerscale.authorization parameters must be properly configured in myvalues.yaml for CSM Observability. If CSM for Authorization is enabled for CSI PowerMax, the karaviMetricsPowermax.authorization parameters must be properly configured in myvalues.yaml for CSM Observability. [user@system /home/user/karavi-observability/installer]# ./karavi-observability-install.sh install --namespace [CSM_NAMESPACE] --values myvalues.yaml --------------------------------------------------------------------------------- \u003e Installing Karavi Observability in namespace karavi on 1.21 --------------------------------------------------------------------------------- | |- Karavi Observability is not installed Success | |- Karavi Authorization will be enabled during installation | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying helm version Success | |- Configure helm chart repository | |--\u003e Adding helm repository https://dell.github.io/helm-charts Success | |--\u003e Updating helm repositories Success | |- Creating namespace karavi Success | |- CSI Driver for PowerFlex is installed Success | |- Copying Secret from vxflexos to karavi Success | |- CSI Driver for PowerStore is installed Success | |- Copying Secret from powerstore to karavi Success | |- CSI Driver for PowerScale is installed Success | |- Copying Secret from isilon to karavi Success | |- CSI Driver for PowerMax is installed Success | |- Copying ConfigMap from powermax to karavi Success | |- Copying Secret from powermax to karavi Success | |- Installing CertManager CRDs Success | |- Enabling Karavi Authorization for Karavi Observability | |--\u003e Copying ConfigMap from vxflexos to karavi Success | |--\u003e Copying Karavi Authorization Secrets from vxflexos to karavi Success | |--\u003e Copying ConfigMap from isilon to karavi Success | |--\u003e Copying Karavi Authorization Secrets from isilon to karavi Success | |--\u003e Copying ConfigMap from powermax to karavi Success | |--\u003e Copying Karavi Authorization Secrets from powermax to karavi Success | |- Installing Karavi Observability helm chart Success | |- Waiting for pods in namespace karavi to be ready Success ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Installer\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Installer\n","ref":"/csm-docs/v3/observability/deployment/online/","tags":"","title":"Installer"},{"body":"The following instructions can be followed when a Helm chart will be installed in an environment that does not have an Internet connection and will be unable to download the Helm chart and related Docker images.\nPrerequisites Helm 3.x The deployment of one or more supported Dell CSI drivers Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\nOne Linux-based system, with Internet access, will be used to create the bundle. This involves the user invoking a script that utilizes docker to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker to restore container images from file and push them to a registry If one Linux system has both Internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\nDependency Usage docker docker will be used to pull images from public image registries, tag them, and push them to a private registry.\nRequired on both the system building the offline bundle as well as the system preparing for installation. Tested version is docker 18.09+ Executing the Installer To perform an offline installation of a Helm chart, the following steps should be performed:\nBuild an offline bundle. Unpack the offline bundle and prepare for installation. Perform a Helm installation. Build the Offline Bundle Copy the offline-installer.sh script to a local Linux system using curl or wget:\ncurl https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh --output offline-installer.sh or\nwget -O offline-installer.sh https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh Set the file as executable.\nchmod +x offline-installer.sh Build the bundle by providing the Helm chart name as the argument. Below is a sample output that may be different on your machine.\n./offline-installer.sh -c dell/karavi-observability * * Adding Helm repository https://dell.github.io/helm-charts * * Downloading Helm chart dell/karavi-observability to directory /home/user/offline-karavi-observability-bundle/helm-original * * Downloading and saving Docker images dellemc/csm-topology:v1.6.0 dellemc/csm-metrics-powerflex:v1.6.0 dellemc/csm-metrics-powerstore:v1.6.0 dellemc/csm-metrics-powerscale:v1.3.0 dellemc/csm-metrics-powermax:v1.1.0 otel/opentelemetry-collector:0.42.0 nginxinc/nginx-unprivileged:1.20 * * Compressing offline-karavi-observability-bundle.tar.gz Unpack the Offline Bundle Copy the bundle file to another Linux system that has access to the internal Docker registry and that can install the Helm chart. From that Linux system, unpack the bundle.\ntar -xzf offline-karavi-observability-bundle.tar.gz Change directory into the new directory created from unpacking the bundle:\ncd offline-karavi-observability-bundle Prepare the bundle by providing the internal Docker registry URL. Below is a sample output that may be different on your machine.\n./offline-installer.sh -p \u003cmy-registry\u003e:5000 * * Loading, tagging, and pushing Docker images to registry \u003cmy-registry\u003e:5000/ dellemc/csm-topology:v1.6.0 -\u003e \u003cmy-registry\u003e:5000/csm-topology:v1.6.0 dellemc/csm-metrics-powerflex:v1.6.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerflex:v1.6.0 dellemc/csm-metrics-powerstore:v1.6.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerstore:v1.6.0 dellemc/csm-metrics-powerscale:v1.3.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerscale:v1.3.0 dellemc/csm-metrics-powermax:v1.1.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powermax:v1.1.0 otel/opentelemetry-collector:0.42.0 -\u003e \u003cmy-registry\u003e:5000/opentelemetry-collector:0.42.0 nginxinc/nginx-unprivileged:1.20 -\u003e \u003cmy-registry\u003e:5000/nginx-unprivileged:1.20 Perform Helm installation Change directory to helm which contains the updated Helm chart directory:\ncd helm Install necessary cert-manager CustomResourceDefinitions provided:\nkubectl apply --validate=false -f cert-manager.crds.yaml Copy the CSI Driver Secret(s)\nCopy the CSI Driver Secret from the namespace where CSI Driver is installed to the namespace where CSM for Observability is to be installed.\nCSI Driver for PowerFlex:\nkubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default vxflexos-config, please use the following command to copy secret:\nkubectl get secret [VXFLEXOS-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG]/name: vxflexos-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerFlex, perform these steps:\nkubectl get configmap vxflexos-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default vxflexos-config-params, please use the following command to copy configmap:\nkubectl get configmap [VXFLEXOS-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG-PARAMS]/name: vxflexos-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - kubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for PowerStore:\nkubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default powerstore-config, please use the following command to copy secret:\nkubectl get secret [POWERSTORE-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERSTORE-CONFIG]/name: powerstore-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for PowerScale:\nkubectl get secret isilon-creds -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default isilon-creds, please use the following command to copy secret:\nkubectl get secret [ISILON-CREDS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CREDS]/name: isilon-creds/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerScale, perform these steps:\nkubectl get configmap isilon-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default isilon-config-params, please use the following command to copy configmap:\nkubectl get configmap [ISILON-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CONFIG-PARAMS]/name: isilon-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - kubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: isilon-karavi-authorization-config/' | sed 's/name: proxy-server-root-certificate/name: isilon-proxy-server-root-certificate/' | sed 's/name: proxy-authz-tokens/name: isilon-proxy-authz-tokens/' | kubectl create -f - CSI Driver for PowerMax:\nCopy the configmap from the CSI Driver for Dell PowerMax namespace to the CSM namespace. Note: Observability for PowerMax works only with CSI PowerMax driver with Proxy in StandAlone mode.\nkubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy configmap:\nkubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-REVERSEPROXY-CONFIG]/name: powermax-reverseproxy-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the secrets from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nfor secret in $(kubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy secrets:\nfor secret in $(kubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If CSM for Authorization is enabled for CSI PowerMax, perform these steps:\nkubectl get configmap powermax-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-config-params, use the following command to copy the configmap:\nkubectl get configmap [POWERMAX-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-CONFIG-PARAMS]/name: powermax-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - kubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: powermax-karavi-authorization-config/' | sed 's/name: proxy-server-root-certificate/name: powermax-proxy-server-root-certificate/' | sed 's/name: proxy-authz-tokens/name: powermax-proxy-authz-tokens/' | kubectl create -f - After the images have been made available and the Helm chart configuration is updated, follow the instructions within the Helm chart’s repository to complete the installation.\nNote:\nOptionally, you could provide your own configurations. A sample values.yaml file is located here. The default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured. If CSM for Authorization is enabled for CSI PowerScale, the karaviMetricsPowerscale.authorization parameters must be properly configured. If CSM for Authorization is enabled for CSI PowerMax, the karaviMetricsPowerMax.authorization parameters must be properly configured. helm install -n install-namespace app-name karavi-observability NAME: app-name LAST DEPLOYED: Fri Nov 6 08:48:13 2020 NAMESPACE: install-namespace STATUS: deployed REVISION: 1 TEST SUITE: None ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Offline Installer\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Offline …","ref":"/csm-docs/docs/observability/deployment/offline/","tags":"","title":"Offline Installer"},{"body":"The following instructions can be followed when a Helm chart will be installed in an environment that does not have an Internet connection and will be unable to download the Helm chart and related Docker images.\nPrerequisites Helm 3.3 The deployment of one or more supported Dell CSI drivers Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\nOne Linux-based system, with Internet access, will be used to create the bundle. This involves the user invoking a script that utilizes docker to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker to restore container images from file and push them to a registry If one Linux system has both Internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\nDependency Usage docker docker will be used to pull images from public image registries, tag them, and push them to a private registry.\nRequired on both the system building the offline bundle as well as the system preparing for installation. Tested version is docker 18.09+ Executing the Installer To perform an offline installation of a Helm chart, the following steps should be performed:\nBuild an offline bundle. Unpack the offline bundle and prepare for installation. Perform a Helm installation. Build the Offline Bundle Copy the offline-installer.sh script to a local Linux system using curl or wget:\ncurl https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh --output offline-installer.sh or\nwget -O offline-installer.sh https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh Set the file as executable.\nchmod +x offline-installer.sh Build the bundle by providing the Helm chart name as the argument. Below is a sample output that may be different on your machine.\n./offline-installer.sh -c dell/karavi-observability * * Adding Helm repository https://dell.github.io/helm-charts * * Downloading Helm chart dell/karavi-observability to directory /home/user/offline-karavi-observability-bundle/helm-original * * Downloading and saving Docker images dellemc/csm-topology:v1.6.0 dellemc/csm-metrics-powerflex:v1.6.0 dellemc/csm-metrics-powerstore:v1.6.0 dellemc/csm-metrics-powerscale:v1.3.0 dellemc/csm-metrics-powermax:v1.1.0 otel/opentelemetry-collector:0.42.0 nginxinc/nginx-unprivileged:1.20 * * Compressing offline-karavi-observability-bundle.tar.gz Unpack the Offline Bundle Copy the bundle file to another Linux system that has access to the internal Docker registry and that can install the Helm chart. From that Linux system, unpack the bundle.\ntar -xzf offline-karavi-observability-bundle.tar.gz Change directory into the new directory created from unpacking the bundle:\ncd offline-karavi-observability-bundle Prepare the bundle by providing the internal Docker registry URL. Below is a sample output that may be different on your machine.\n./offline-installer.sh -p \u003cmy-registry\u003e:5000 * * Loading, tagging, and pushing Docker images to registry \u003cmy-registry\u003e:5000/ dellemc/csm-topology:v1.6.0 -\u003e \u003cmy-registry\u003e:5000/csm-topology:v1.6.0 dellemc/csm-metrics-powerflex:v1.6.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerflex:v1.6.0 dellemc/csm-metrics-powerstore:v1.6.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerstore:v1.6.0 dellemc/csm-metrics-powerscale:v1.3.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerscale:v1.3.0 dellemc/csm-metrics-powermax:v1.1.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powermax:v1.1.0 otel/opentelemetry-collector:0.42.0 -\u003e \u003cmy-registry\u003e:5000/opentelemetry-collector:0.42.0 nginxinc/nginx-unprivileged:1.20 -\u003e \u003cmy-registry\u003e:5000/nginx-unprivileged:1.20 Perform Helm installation Change directory to helm which contains the updated Helm chart directory:\ncd helm Install necessary cert-manager CustomResourceDefinitions provided:\nkubectl apply --validate=false -f cert-manager.crds.yaml Copy the CSI Driver Secret(s)\nCopy the CSI Driver Secret from the namespace where CSI Driver is installed to the namespace where CSM for Observability is to be installed.\nCSI Driver for PowerFlex:\nkubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default vxflexos-config, please use the following command to copy secret:\nkubectl get secret [VXFLEXOS-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG]/name: vxflexos-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerFlex, perform these steps:\nkubectl get configmap vxflexos-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default vxflexos-config-params, please use the following command to copy configmap:\nkubectl get configmap [VXFLEXOS-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG-PARAMS]/name: vxflexos-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - kubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for PowerStore:\nkubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default powerstore-config, please use the following command to copy secret:\nkubectl get secret [POWERSTORE-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERSTORE-CONFIG]/name: powerstore-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for PowerScale:\nkubectl get secret isilon-creds -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default isilon-creds, please use the following command to copy secret:\nkubectl get secret [ISILON-CREDS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CREDS]/name: isilon-creds/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerScale, perform these steps:\nkubectl get configmap isilon-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default isilon-config-params, please use the following command to copy configmap:\nkubectl get configmap [ISILON-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CONFIG-PARAMS]/name: isilon-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - kubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: isilon-karavi-authorization-config/' | sed 's/name: proxy-server-root-certificate/name: isilon-proxy-server-root-certificate/' | sed 's/name: proxy-authz-tokens/name: isilon-proxy-authz-tokens/' | kubectl create -f - CSI Driver for PowerMax:\nCopy the configmap from the CSI Driver for Dell PowerMax namespace to the CSM namespace. Note: Observability for PowerMax works only with CSI PowerMax driver with Proxy in StandAlone mode.\nkubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy configmap:\nkubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-REVERSEPROXY-CONFIG]/name: powermax-reverseproxy-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the secrets from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nfor secret in $(kubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy secrets:\nfor secret in $(kubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If CSM for Authorization is enabled for CSI PowerMax, perform these steps:\nkubectl get configmap powermax-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-config-params, use the following command to copy the configmap:\nkubectl get configmap [POWERMAX-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-CONFIG-PARAMS]/name: powermax-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - kubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: powermax-karavi-authorization-config/' | sed 's/name: proxy-server-root-certificate/name: powermax-proxy-server-root-certificate/' | sed 's/name: proxy-authz-tokens/name: powermax-proxy-authz-tokens/' | kubectl create -f - After the images have been made available and the Helm chart configuration is updated, follow the instructions within the Helm chart’s repository to complete the installation.\nNote:\nOptionally, you could provide your own configurations. A sample values.yaml file is located here. The default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured. If CSM for Authorization is enabled for CSI PowerScale, the karaviMetricsPowerscale.authorization parameters must be properly configured. If CSM for Authorization is enabled for CSI PowerMax, the karaviMetricsPowerMax.authorization parameters must be properly configured. helm install -n install-namespace app-name karavi-observability NAME: app-name LAST DEPLOYED: Fri Nov 6 08:48:13 2020 NAMESPACE: install-namespace STATUS: deployed REVISION: 1 TEST SUITE: None ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Offline Installer\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Offline …","ref":"/csm-docs/v1/observability/deployment/offline/","tags":"","title":"Offline Installer"},{"body":"The following instructions can be followed when a Helm chart will be installed in an environment that does not have an internet connection and will be unable to download the Helm chart and related Docker images.\nPrerequisites Helm 3.3 The deployment of one or more supported Dell CSI drivers Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\nOne Linux-based system, with internet access, will be used to create the bundle. This involves the user invoking a script that utilizes docker to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker to restore container images from file and push them to a registry If one Linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\nDependency Usage docker docker will be used to pull images from public image registries, tag them, and push them to a private registry.\nRequired on both the system building the offline bundle as well as the system preparing for installation. Tested version is docker 18.09+ Executing the Installer To perform an offline installation of a Helm chart, the following steps should be performed:\nBuild an offline bundle. Unpack the offline bundle and prepare for installation. Perform a Helm installation. Build the Offline Bundle Copy the offline-installer.sh script to a local Linux system using curl or wget:\ncurl https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh --output offline-installer.sh or\nwget -O offline-installer.sh https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh Set the file as executable.\nchmod +x offline-installer.sh Build the bundle by providing the Helm chart name as the argument. Below is a sample output that may be different on your machine.\n./offline-installer.sh -c dell/karavi-observability * * Adding Helm repository https://dell.github.io/helm-charts * * Downloading Helm chart dell/karavi-observability to directory /home/user/offline-karavi-observability-bundle/helm-original * * Downloading and saving Docker images dellemc/csm-topology:v1.5.0 dellemc/csm-metrics-powerflex:v1.5.0 dellemc/csm-metrics-powerstore:v1.5.0 dellemc/csm-metrics-powerscale:v1.2.0 dellemc/csm-metrics-powermax:v1.0.0 otel/opentelemetry-collector:0.42.0 nginxinc/nginx-unprivileged:1.20 * * Compressing offline-karavi-observability-bundle.tar.gz Unpack the Offline Bundle Copy the bundle file to another Linux system that has access to the internal Docker registry and that can install the Helm chart. From that Linux system, unpack the bundle.\ntar -xzf offline-karavi-observability-bundle.tar.gz Change directory into the new directory created from unpacking the bundle:\ncd offline-karavi-observability-bundle Prepare the bundle by providing the internal Docker registry URL. Below is a sample output that may be different on your machine.\n./offline-installer.sh -p \u003cmy-registry\u003e:5000 * * Loading, tagging, and pushing Docker images to registry \u003cmy-registry\u003e:5000/ dellemc/csm-topology:v1.5.0 -\u003e \u003cmy-registry\u003e:5000/csm-topology:v1.5.0 dellemc/csm-metrics-powerflex:v1.5.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerflex:v1.5.0 dellemc/csm-metrics-powerstore:v1.5.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerstore:v1.5.0 dellemc/csm-metrics-powerscale:v1.2.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerscale:v1.2.0 dellemc/csm-metrics-powermax:v1.0.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerscale:v1.0.0 otel/opentelemetry-collector:0.42.0 -\u003e \u003cmy-registry\u003e:5000/opentelemetry-collector:0.42.0 nginxinc/nginx-unprivileged:1.20 -\u003e \u003cmy-registry\u003e:5000/nginx-unprivileged:1.20 Perform Helm installation Change directory to helm which contains the updated Helm chart directory:\ncd helm Install necessary cert-manager CustomResourceDefinitions provided:\nkubectl apply --validate=false -f cert-manager.crds.yaml Copy the CSI Driver Secret(s)\nCopy the CSI Driver Secret from the namespace where CSI Driver is installed to the namespace where CSM for Observability is to be installed.\nCSI Driver for PowerFlex:\nkubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default vxflexos-config, please use the following command to copy secret:\nkubectl get secret [VXFLEXOS-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG]/name: vxflexos-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerFlex, perform these steps:\nkubectl get configmap vxflexos-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default vxflexos-config-params, please use the following command to copy configmap:\nkubectl get configmap [VXFLEXOS-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG-PARAMS]/name: vxflexos-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - kubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for PowerStore:\nkubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default powerstore-config, please use the following command to copy secret:\nkubectl get secret [POWERSTORE-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERSTORE-CONFIG]/name: powerstore-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for PowerScale:\nkubectl get secret isilon-creds -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default isilon-creds, please use the following command to copy secret:\nkubectl get secret [ISILON-CREDS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CREDS]/name: isilon-creds/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerScale, perform these steps:\nkubectl get configmap isilon-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default isilon-config-params, please use the following command to copy configmap:\nkubectl get configmap [ISILON-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CONFIG-PARAMS]/name: isilon-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - kubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: isilon-karavi-authorization-config/' | sed 's/name: proxy-server-root-certificate/name: isilon-proxy-server-root-certificate/' | sed 's/name: proxy-authz-tokens/name: isilon-proxy-authz-tokens/' | kubectl create -f - CSI Driver for PowerMax:\nCopy the configmap from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nNote: Observability for PowerMax works only with CSI PowerMax driver with Proxy in StandAlone mode.\nkubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy configmap:\nkubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-REVERSEPROXY-CONFIG]/name: powermax-reverseproxy-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the secrets from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nfor secret in $(kubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy secrets:\nfor secret in $(kubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If CSM for Authorization is enabled for CSI PowerMax, perform these steps:\nkubectl get configmap powermax-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-config-params, please use the following command to copy configmap:\nkubectl get configmap [POWERMAX-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-CONFIG-PARAMS]/name: powermax-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - kubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: powermax-karavi-authorization-config/' | sed 's/name: proxy-server-root-certificate/name: powermax-proxy-server-root-certificate/' | sed 's/name: proxy-authz-tokens/name: powermax-proxy-authz-tokens/' | kubectl create -f - Now that the required images have been made available and the Helm chart’s configuration updated with references to the internal registry location, installation can proceed by following the instructions that are documented within the Helm chart’s repository.\nNote:\nOptionally, you could provide your own configurations. A sample values.yaml file is located here. The default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured. If CSM for Authorization is enabled for CSI PowerScale, the karaviMetricsPowerscale.authorization parameters must be properly configured. If CSM for Authorization is enabled for CSI PowerMax, the karaviMetricsPowerMax.authorization parameters must be properly configured. helm install -n install-namespace app-name karavi-observability NAME: app-name LAST DEPLOYED: Fri Nov 6 08:48:13 2020 NAMESPACE: install-namespace STATUS: deployed REVISION: 1 TEST SUITE: None ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Offline Installer\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Offline …","ref":"/csm-docs/v2/observability/deployment/offline/","tags":"","title":"Offline Installer"},{"body":"The following instructions can be followed when a Helm chart will be installed in an environment that does not have an internet connection and will be unable to download the Helm chart and related Docker images.\nPrerequisites Helm 3.3 The deployment of one or more supported Dell CSI drivers Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\nOne Linux-based system, with internet access, will be used to create the bundle. This involves the user invoking a script that utilizes docker to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker to restore container images from file and push them to a registry If one Linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\nDependency Usage docker docker will be used to pull images from public image registries, tag them, and push them to a private registry.\nRequired on both the system building the offline bundle as well as the system preparing for installation. Tested version is docker 18.09+ Executing the Installer To perform an offline installation of a Helm chart, the following steps should be performed:\nBuild an offline bundle. Unpack the offline bundle and prepare for installation. Perform a Helm installation. Build the Offline Bundle Copy the offline-installer.sh script to a local Linux system using curl or wget:\n[user@anothersystem /home/user]# curl https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh --output offline-installer.sh or\n[user@anothersystem /home/user]# wget -O offline-installer.sh https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh Set the file as executable.\n[user@anothersystem /home/user]# chmod +x offline-installer.sh Build the bundle by providing the Helm chart name as the argument. Below is a sample output that may be different on your machine.\n[user@anothersystem /home/user]# ./offline-installer.sh -c dell/karavi-observability * * Adding Helm repository https://dell.github.io/helm-charts * * Downloading Helm chart dell/karavi-observability to directory /home/user/offline-karavi-observability-bundle/helm-original * * Downloading and saving Docker images dellemc/csm-topology:v1.5.0 dellemc/csm-metrics-powerflex:v1.5.0 dellemc/csm-metrics-powerstore:v1.5.0 dellemc/csm-metrics-powerscale:v1.2.0 dellemc/csm-metrics-powermax:v1.0.0 otel/opentelemetry-collector:0.42.0 nginxinc/nginx-unprivileged:1.20 * * Compressing offline-karavi-observability-bundle.tar.gz Unpack the Offline Bundle Copy the bundle file to another Linux system that has access to the internal Docker registry and that can install the Helm chart. From that Linux system, unpack the bundle.\n[user@anothersystem /home/user]# tar -xzf offline-karavi-observability-bundle.tar.gz Change directory into the new directory created from unpacking the bundle:\n[user@anothersystem /home/user]# cd offline-karavi-observability-bundle Prepare the bundle by providing the internal Docker registry URL. Below is a sample output that may be different on your machine.\n[user@anothersystem /home/user/offline-karavi-observability-bundle]# ./offline-installer.sh -p \u003cmy-registry\u003e:5000 * * Loading, tagging, and pushing Docker images to registry \u003cmy-registry\u003e:5000/ dellemc/csm-topology:v1.5.0 -\u003e \u003cmy-registry\u003e:5000/csm-topology:v1.5.0 dellemc/csm-metrics-powerflex:v1.5.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerflex:v1.5.0 dellemc/csm-metrics-powerstore:v1.5.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerstore:v1.5.0 dellemc/csm-metrics-powerscale:v1.2.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerscale:v1.2.0 dellemc/csm-metrics-powermax:v1.0.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerscale:v1.0.0 otel/opentelemetry-collector:0.42.0 -\u003e \u003cmy-registry\u003e:5000/opentelemetry-collector:0.42.0 nginxinc/nginx-unprivileged:1.20 -\u003e \u003cmy-registry\u003e:5000/nginx-unprivileged:1.20 Perform Helm installation Change directory to helm which contains the updated Helm chart directory:\n[user@anothersystem /home/user/offline-karavi-observability-bundle]# cd helm Install necessary cert-manager CustomResourceDefinitions provided:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl apply --validate=false -f cert-manager.crds.yaml Copy the CSI Driver Secret(s)\nCopy the CSI Driver Secret from the namespace where CSI Driver is installed to the namespace where CSM for Observability is to be installed.\nCSI Driver for PowerFlex:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default vxflexos-config, please use the following command to copy secret:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret [VXFLEXOS-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG]/name: vxflexos-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerFlex, perform these steps:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get configmap vxflexos-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default vxflexos-config-params, please use the following command to copy configmap:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get configmap [VXFLEXOS-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [VXFLEXOS-CONFIG-PARAMS]/name: vxflexos-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - [user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for PowerStore:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default powerstore-config, please use the following command to copy secret:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret [POWERSTORE-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERSTORE-CONFIG]/name: powerstore-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for PowerScale:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret isilon-creds -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver secret name is not the default isilon-creds, please use the following command to copy secret:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret [ISILON-CREDS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CREDS]/name: isilon-creds/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerScale, perform these steps:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get configmap isilon-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default isilon-config-params, please use the following command to copy configmap:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get configmap [ISILON-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [ISILON-CONFIG-PARAMS]/name: isilon-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - [user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: isilon-karavi-authorization-config/' | sed 's/name: proxy-server-root-certificate/name: isilon-proxy-server-root-certificate/' | sed 's/name: proxy-authz-tokens/name: isilon-proxy-authz-tokens/' | kubectl create -f - CSI Driver for PowerMax:\nCopy the configmap from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nNote: Observability for PowerMax works only with CSI PowerMax driver with Proxy in StandAlone mode.\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy configmap:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-REVERSEPROXY-CONFIG]/name: powermax-reverseproxy-config/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Copy the secrets from the CSI Driver for Dell PowerMax namespace to the CSM namespace.\nfor secret in $(kubectl get configmap powermax-reverseproxy-config -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If the CSI driver configmap name is not the default powermax-reverseproxy-config, please use the following command to copy secrets:\nfor secret in $(kubectl get configmap [POWERMAX-REVERSEPROXY-CONFIG] -n [CSI_DRIVER_NAMESPACE] -o jsonpath=\"{.data.config\\.yaml}\" | grep arrayCredentialSecret | awk 'BEGIN{FS=\":\"}{print $2}' | uniq) do kubectl get secret $secret -n [CSI_DRIVER_NAMESPACE] -o yaml | sed \"s/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/\" | kubectl create -f - done If CSM for Authorization is enabled for CSI PowerMax, perform these steps:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get configmap powermax-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If the CSI driver configmap name is not the default powermax-config-params, please use the following command to copy configmap:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get configmap [POWERMAX-CONFIG-PARAMS] -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/name: [POWERMAX-CONFIG-PARAMS]/name: powermax-config-params/' | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - [user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | sed 's/name: karavi-authorization-config/name: powermax-karavi-authorization-config/' | sed 's/name: proxy-server-root-certificate/name: powermax-proxy-server-root-certificate/' | sed 's/name: proxy-authz-tokens/name: powermax-proxy-authz-tokens/' | kubectl create -f - Now that the required images have been made available and the Helm chart’s configuration updated with references to the internal registry location, installation can proceed by following the instructions that are documented within the Helm chart’s repository.\nNote:\nOptionally, you could provide your own configurations. A sample values.yaml file is located here. The default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured. If CSM for Authorization is enabled for CSI PowerScale, the karaviMetricsPowerscale.authorization parameters must be properly configured. If CSM for Authorization is enabled for CSI PowerMax, the karaviMetricsPowermax.authorization parameters must be properly configured. [user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# helm install -n install-namespace app-name karavi-observability NAME: app-name LAST DEPLOYED: Fri Nov 6 08:48:13 2020 NAMESPACE: install-namespace STATUS: deployed REVISION: 1 TEST SUITE: None ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Offline Installer\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Offline …","ref":"/csm-docs/v3/observability/deployment/offline/","tags":"","title":"Offline Installer"},{"body":"Users can install the Dell CSI Operator via Operatorhub.io on Kubernetes. The following outlines the process to do so:\nSteps\nSearch dell in the storage category in Operatorhub.io. Click Dell Operator. Check the desired version is selected and click Install. Follow the provided instructions. Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","categories":"","description":"Installing the Dell CSI Operator via OperatorHub.io","excerpt":"Installing the Dell CSI Operator via OperatorHub.io","ref":"/csm-docs/v1/csidriver/partners/operator/","tags":"","title":"OperatorHub.io"},{"body":"Users can install the Dell CSI Operator via Operatorhub.io on Kubernetes. The following outlines the process to do so:\nSteps\nSearch dell in the storage category in Operatorhub.io. Click Dell Operator. Check the desired version is selected and click Install. Follow the provided instructions. Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","categories":"","description":"Installing the Dell CSI Operator via OperatorHub.io","excerpt":"Installing the Dell CSI Operator via OperatorHub.io","ref":"/csm-docs/v2/csidriver/partners/operator/","tags":"","title":"OperatorHub.io"},{"body":"Users can install the Dell CSI Operator via Operatorhub.io on Kubernetes. The following outlines the process to do so:\nSteps\nSearch dell in the storage category in Operatorhub.io. Click Dell Operator. Check the desired version is selected and click Install. Follow the provided instructions. Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","categories":"","description":"Installing the Dell CSI Operator via OperatorHub.io","excerpt":"Installing the Dell CSI Operator via OperatorHub.io","ref":"/csm-docs/v3/csidriver/partners/operator/","tags":"","title":"OperatorHub.io"},{"body":"The Dell CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\nType “Dell” in the OperatorHub section under Operators, to get the list of available Dell CSI Operators. Check the version you want to install from the list, you can check the details by clicking it. Once selected, click “Install” to proceed with the installation process. You can verify the list of available operators by selecting the “Installed Operator” section. Select the Dell CSI Operator for further details. Install CSI Drivers via Operator Steps\nSelect the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected. After clicking the “Create CSIUnity” option in the above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters. You can check the driver installed and node and controller pods running in the Pods section under Workloads. ","categories":"","description":"Installing the certified Dell CSI Operator on OpenShift\n","excerpt":"Installing the certified Dell CSI Operator on OpenShift\n","ref":"/csm-docs/v1/csidriver/partners/redhat/","tags":"","title":"Red Hat OpenShift"},{"body":"The Dell CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\nType “Dell” in the OperatorHub section under Operators, to get the list of available Dell CSI Operators. Check the version you want to install from the list, you can check the details by clicking it. Once selected, click “Install” to proceed with the installation process. You can verify the list of available operators by selecting the “Installed Operator” section. Select the Dell CSI Operator for further details. Install CSI Drivers via Operator Steps\nSelect the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected. After clicking the “Create CSIUnity” option in the above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters. You can check the driver installed and node and controller pods running in the Pods section under Workloads. ","categories":"","description":"Installing the certified Dell CSI Operator on OpenShift\n","excerpt":"Installing the certified Dell CSI Operator on OpenShift\n","ref":"/csm-docs/v2/csidriver/partners/redhat/","tags":"","title":"Red Hat OpenShift"},{"body":"The Dell CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\nType “Dell” in the OperatorHub section under Operators, to get the list of available Dell CSI Operators. Check the version you want to install from the list, you can check the details by clicking it. Once selected, click “Install” to proceed with the installation process. You can verify the list of available operators by selecting the “Installed Operator” section. Select the Dell CSI Operator for further details. Install CSI Drivers via Operator Steps\nSelect the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected. After clicking the “Create CSIUnity” option in the above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters. You can check the driver installed and node and controller pods running in the Pods section under Workloads. ","categories":"","description":"Installing the certified Dell CSI Operator on OpenShift\n","excerpt":"Installing the certified Dell CSI Operator on OpenShift\n","ref":"/csm-docs/v3/csidriver/partners/redhat/","tags":"","title":"Red Hat OpenShift"},{"body":" Can CSM Operator manage existing drivers installed using Helm charts or the Dell CSI Operator? Why do some of the Custom Resource fields show up as invalid or unsupported in the OperatorHub GUI? How can I view detailed logs for the CSM Operator? My Dell CSI Driver install failed. How do I fix it? My CSM Replication install fails to validate replication prechecks with ’no such host’. Can CSM Operator manage existing drivers installed using Helm charts or the Dell CSI Operator? The Dell CSM Operator is unable to manage any existing driver installed using Helm charts or the Dell CSI Operator. If you already have installed one of the Dell CSI driver in your cluster and want to use the CSM operator based deployment, uninstall the driver and then redeploy the driver via Dell CSM Operator\nWhy do some of the Custom Resource fields show up as invalid or unsupported in the OperatorHub GUI? The Dell CSM Operator is not fully compliant with the OperatorHub React UI elements. Due to this, some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSM Operator.\nHow can I view detailed logs for the CSM Operator? Detailed logs of the CSM Operator can be displayed using the following command:\nkubectl logs \u003ccsm-operator-controller-podname\u003e -n \u003cnamespace\u003e My Dell CSI Driver install failed. How do I fix it? Describe the current state by issuing: kubectl describe csm \u003ccustom-resource-name\u003e -n \u003cnamespace\u003e\nIn the output refer to the status and events section. If status shows pods that are in the failed state, refer to the CSI Driver Troubleshooting guide.\nExample:\nStatus: Controller Status: Available: 0 Desired: 2 Failed: 2 Node Status: Available: 0 Desired: 2 Failed: 2 State: Failed Events Warning Updated 67s (x15 over 2m4s) csm (combined from similar events): at 1646848059520359167 Pod error details ControllerError: ErrImagePull= pull access denied for dellem/csi-isilon, repository does not exist or may require 'docker login': denied: requested access to the resource is denied, Daemonseterror: ErrImagePull= pull access denied for dellem/csi-isilon, repository does not exist or may require 'docker login': denied: requested access to the resource is denied The above event shows dellem/csi-isilon does not exist, to resolve this user can kubectl edit the csm and update to correct image.\nTo get details of driver installation: kubectl logs \u003cdell-csm-operator-controller-manager-pod\u003e -n dell-csm-operator.\nTypical reasons for errors:\nIncorrect driver version Incorrect driver type Incorrect driver Spec env, args for containers Incorrect RBAC permissions My CSM Replication install fails to validate replication prechecks with ’no such host'. In replication environments that utilize more than one cluster, and utilize FQDNs to reference API endpoints, it is highly recommended that the DNS be configured to resolve requests involving the FQDN to the appropriate cluster.\nIf for some reason it is not possible to configure the DNS, the /etc/hosts file should be updated to map the FQDN to the appropriate IP. This change will need to be made to the /etc/hosts file on:\nThe bastion node(s) (or wherever repctl is used). Either the CSM Operator Deployment or ClusterServiceVersion custom resource if using an Operator Lifecycle Manager (such as with an OperatorHub install). Both dell-replication-controller-manager deployments. To update the ClusterServiceVersion, execute the command below, replacing the fields for the remote cluster’s FQDN and IP.\nkubectl patch clusterserviceversions.operators.coreos.com -n \u003coperator-namespace\u003e dell-csm-operator-certified.v1.3.0 \\ --type=json -p='[{\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/hostAliases\", \"value\": [{\"ip\":\"\u003cremote-IP\u003e\",\"hostnames\":[\"\u003cremote-FQDN\u003e\"]}]}]' To update the dell-replication-controller-manager deployment, execute the command below, replacing the fields for the remote cluster’s FQDN and IP. Make sure to update the deployment on both the primary and disaster recovery clusters.\nkubectl patch deployment -n dell-replication-controller dell-replication-controller-manager \\ -p '{\"spec\":{\"template\":{\"spec\":{\"hostAliases\":[{\"hostnames\":[\"\u003cremote-FQDN\u003e\"],\"ip\":\"\u003cremote-IP\u003e\"}]}}}}' ","categories":"","description":"Troubleshooting guide  for Dell CSM Operator\n","excerpt":"Troubleshooting guide  for Dell CSM Operator\n","ref":"/csm-docs/docs/deployment/csmoperator/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":" Can CSM Operator manage existing drivers installed using Helm charts or the Dell CSI Operator? Why does some of the Custom Resource fields show up as invalid or unsupported in the OperatorHub GUI? How can I view detailed logs for the CSM Operator? My Dell CSI Driver install failed. How do I fix it? My CSM Replication install fails to validate replication prechecks with ’no such host’. Can CSM Operator manage existing drivers installed using Helm charts or the Dell CSI Operator? The Dell CSM Operator is unable to manage any existing driver installed using Helm charts or the Dell CSI Operator. If you already have installed one of the Dell CSI driver in your cluster and want to use the CSM operator based deployment, uninstall the driver and then redeploy the driver via Dell CSM Operator\nWhy do some of the Custom Resource fields show up as invalid or unsupported in the OperatorHub GUI? The Dell CSM Operator is not fully compliant with the OperatorHub React UI elements. Due to this, some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSM Operator.\nHow can I view detailed logs for the CSM Operator? Detailed logs of the CSM Operator can be displayed using the following command:\nkubectl logs \u003ccsm-operator-controller-podname\u003e -n \u003cnamespace\u003e My Dell CSI Driver install failed. How do I fix it? Describe the current state by issuing: kubectl describe csm \u003ccustom-resource-name\u003e -n \u003cnamespace\u003e\nIn the output refer to the status and events section. If status shows pods that are in the failed state, refer to the CSI Driver Troubleshooting guide.\nExample:\nStatus: Controller Status: Available: 0 Desired: 2 Failed: 2 Node Status: Available: 0 Desired: 2 Failed: 2 State: Failed Events Warning Updated 67s (x15 over 2m4s) csm (combined from similar events): at 1646848059520359167 Pod error details ControllerError: ErrImagePull= pull access denied for dellem/csi-isilon, repository does not exist or may require 'docker login': denied: requested access to the resource is denied, Daemonseterror: ErrImagePull= pull access denied for dellem/csi-isilon, repository does not exist or may require 'docker login': denied: requested access to the resource is denied The above event shows dellem/csi-isilon does not exist, to resolve this user can kubectl edit the csm and update to correct image.\nTo get details of driver installation: kubectl logs \u003cdell-csm-operator-controller-manager-pod\u003e -n dell-csm-operator.\nTypical reasons for errors:\nIncorrect driver version Incorrect driver type Incorrect driver Spec env, args for containers Incorrect RBAC permissions My CSM Replication install fails to validate replication prechecks with ’no such host'. In replication environments that utilize more than one cluster, and utilize FQDNs to reference API endpoints, it is highly recommended that the DNS be configured to resolve requests involving the FQDN to the appropriate cluster.\nIf for some reason it is not possible to configure the DNS, the /etc/hosts file should be updated to map the FQDN to the appropriate IP. This change will need to be made to the /etc/hosts file on:\nThe bastion node(s) (or wherever repctl is used). Either the CSM Operator Deployment or ClusterServiceVersion custom resource if using an Operator Lifecycle Manager (such as with an OperatorHub install). Both dell-replication-controller-manager deployments. To update the ClusterServiceVersion, execute the command below, replacing the fields for the remote cluster’s FQDN and IP.\nkubectl patch clusterserviceversions.operators.coreos.com -n \u003coperator-namespace\u003e dell-csm-operator-certified.v1.3.0 \\ --type=json -p='[{\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/hostAliases\", \"value\": [{\"ip\":\"\u003cremote-IP\u003e\",\"hostnames\":[\"\u003cremote-FQDN\u003e\"]}]}]' To update the dell-replication-controller-manager deployment, execute the command below, replacing the fields for the remote cluster’s FQDN and IP. Make sure to update the deployment on both the primary and disaster recovery clusters.\nkubectl patch deployment -n dell-replication-controller dell-replication-controller-manager \\ -p '{\"spec\":{\"template\":{\"spec\":{\"hostAliases\":[{\"hostnames\":[\"\u003cremote-FQDN\u003e\"],\"ip\":\"\u003cremote-IP\u003e\"}]}}}}' ","categories":"","description":"Troubleshooting guide  for Dell CSM Operator\n","excerpt":"Troubleshooting guide  for Dell CSM Operator\n","ref":"/csm-docs/v1/deployment/csmoperator/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":" Can CSM Operator manage existing drivers installed using Helm charts or the Dell CSI Operator? Why does some of the Custom Resource fields show up as invalid or unsupported in the OperatorHub GUI? How can I view detailed logs for the CSM Operator? My Dell CSI Driver install failed. How do I fix it? Can CSM Operator manage existing drivers installed using Helm charts or the Dell CSI Operator? The Dell CSM Operator is unable to manage any existing driver installed using Helm charts or the Dell CSI Operator. If you already have installed one of the Dell CSI driver in your cluster and want to use the CSM operator based deployment, uninstall the driver and then redeploy the driver via Dell CSM Operator\nWhy does some of the Custom Resource fields show up as invalid or unsupported in the OperatorHub GUI? The Dell CSM Operator is not fully compliant with the OperatorHub React UI elements.Due to this, some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSM Operator.\nHow can I view detailed logs for the CSM Operator? Detailed logs of the CSM Operator can be displayed using the following command:\nkubectl logs \u003ccsm-operator-controller-podname\u003e -n \u003cnamespace\u003e My Dell CSI Driver install failed. How do I fix it? Describe the current state by issuing: kubectl describe csm \u003ccustom-resource-name\u003e -n \u003cnamespace\u003e\nIn the output refer to the status and events section. If status shows pods that are in the failed state, refer to the CSI Driver Troubleshooting guide.\nExample:\nStatus: Controller Status: Available: 0 Desired: 2 Failed: 2 Node Status: Available: 0 Desired: 2 Failed: 2 State: Failed Events Warning Updated 67s (x15 over 2m4s) csm (combined from similar events): at 1646848059520359167 Pod error details ControllerError: ErrImagePull= pull access denied for dellem/csi-isilon, repository does not exist or may require 'docker login': denied: requested access to the resource is denied, Daemonseterror: ErrImagePull= pull access denied for dellem/csi-isilon, repository does not exist or may require 'docker login': denied: requested access to the resource is denied The above event shows dellem/csi-isilon does not exist, to resolve this user can kubectl edit the csm and update to correct image.\nTo get details of driver installation: kubectl logs \u003cdell-csm-operator-controller-manager-pod\u003e -n dell-csm-operator.\nTypical reasons for errors:\nIncorrect driver version Incorrect driver type Incorrect driver Spec env, args for containers Incorrect RBAC permissions ","categories":"","description":"Troubleshooting guide  for Dell CSM Operator\n","excerpt":"Troubleshooting guide  for Dell CSM Operator\n","ref":"/csm-docs/v2/deployment/csmoperator/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":" Can CSM Operator manage existing drivers installed using Helm charts or the Dell CSI Operator? Why does some of the Custom Resource fields show up as invalid or unsupported in the OperatorHub GUI? How can I view detailed logs for the CSM Operator? My Dell CSI Driver install failed. How do I fix it? Can CSM Operator manage existing drivers installed using Helm charts or the Dell CSI Operator? The Dell CSM Operator is unable to manage any existing driver installed using Helm charts or the Dell CSI Operator. If you already have installed one of the Dell CSI driver in your cluster and want to use the CSM operator based deployment, uninstall the driver and then redeploy the driver via Dell CSM Operator\nWhy does some of the Custom Resource fields show up as invalid or unsupported in the OperatorHub GUI? The Dell CSM Operator is not fully compliant with the OperatorHub React UI elements.Due to this, some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSM Operator.\nHow can I view detailed logs for the CSM Operator? Detailed logs of the CSM Operator can be displayed using the following command:\nkubectl logs \u003ccsm-operator-controller-podname\u003e -n \u003cnamespace\u003e My Dell CSI Driver install failed. How do I fix it? Describe the current state by issuing: kubectl describe csm \u003ccustom-resource-name\u003e -n \u003cnamespace\u003e\nIn the output refer to the status and events section. If status shows pods that are in the failed state, refer to the CSI Driver Troubleshooting guide.\nExample:\nStatus: Controller Status: Available: 0 Desired: 2 Failed: 2 Node Status: Available: 0 Desired: 2 Failed: 2 State: Failed Events Warning Updated 67s (x15 over 2m4s) csm (combined from similar events): at 1646848059520359167 Pod error details ControllerError: ErrImagePull= pull access denied for dellem/csi-isilon, repository does not exist or may require 'docker login': denied: requested access to the resource is denied, Daemonseterror: ErrImagePull= pull access denied for dellem/csi-isilon, repository does not exist or may require 'docker login': denied: requested access to the resource is denied The above event shows dellem/csi-isilon does not exist, to resolve this user can kubectl edit the csm and update to correct image.\nTo get details of driver installation: kubectl logs \u003cdell-csm-operator-controller-manager-pod\u003e -n dell-csm-operator.\nTypical reasons for errors:\nIncorrect driver version Incorrect driver type Incorrect driver Spec env, args for containers Incorrect RBAC permissions ","categories":"","description":"Troubleshooting guide  for Dell CSM Operator\n","excerpt":"Troubleshooting guide  for Dell CSM Operator\n","ref":"/csm-docs/v3/deployment/csmoperator/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Uninstall COSI driver installed via Helm To uninstall a driver use a helm uninstall command:\nhelm uninstall dell-cosi --namespace dell-cosi ","categories":"","description":"Methods to uninstall Dell COSI Driver","excerpt":"Methods to uninstall Dell COSI Driver","ref":"/csm-docs/docs/cosidriver/uninstallation/","tags":"","title":"Uninstallation"},{"body":"Uninstall a CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the driver:\n./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e For usage information:\n./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u003cnamespace\u003e Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u003chelm release\u003e Name to register with helm, default value will match the driver name -h Help Uninstall a CSI driver installed via Dell CSM Operator For uninstalling any CSI drivers deployed by the Dell CSM Operator, refer to instructions here\n","categories":"","description":"Methods to uninstall Dell CSI Driver","excerpt":"Methods to uninstall Dell CSI Driver","ref":"/csm-docs/docs/csidriver/uninstall/","tags":"","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Observability.\nUninstall the CSM for Observability Helm Chart The command below removes all the Kubernetes components associated with the chart.\nhelm delete karavi-observability --namespace [CSM_NAMESPACE] You may also want to uninstall the CRDs created for cert-manager.\nkubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.10.0/cert-manager.crds.yaml ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Uninstallation\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Uninstallation\n","ref":"/csm-docs/docs/observability/uninstall/","tags":"","title":"Uninstallation"},{"body":"Uninstall a CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the driver:\n./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e For usage information:\n./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u003cnamespace\u003e Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u003chelm release\u003e Name to register with helm, default value will match the driver name -h Help Uninstall a CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed by the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall the driver installed via the operator, delete the Custom Resource(CR)\n#Replace driver-type, driver-name and driver-namespace with their respective values\nkubectl delete \u003cdriver-type\u003e/\u003cdriver-name\u003e -n \u003cdriver-namespace\u003e ","categories":"","description":"Methods to uninstall Dell CSI Driver","excerpt":"Methods to uninstall Dell CSI Driver","ref":"/csm-docs/v1/csidriver/uninstall/","tags":"","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Observability.\nUninstall the CSM for Observability Helm Chart The command below removes all the Kubernetes components associated with the chart.\nhelm delete karavi-observability --namespace [CSM_NAMESPACE] You may also want to uninstall the CRDs created for cert-manager.\nkubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.10.0/cert-manager.crds.yaml ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Uninstallation\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Uninstallation\n","ref":"/csm-docs/v1/observability/uninstall/","tags":"","title":"Uninstallation"},{"body":"Uninstall a CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the driver:\n./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e For usage information:\n./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u003cnamespace\u003e Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u003chelm release\u003e Name to register with helm, default value will match the driver name -h Help Uninstall a CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed by the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall the driver installed via the operator, delete the Custom Resource(CR)\n#Replace driver-type, driver-name and driver-namespace with their respective values\nkubectl delete \u003cdriver-type\u003e/\u003cdriver-name\u003e -n \u003cdriver-namespace\u003e ","categories":"","description":"Methods to uninstall Dell CSI Driver","excerpt":"Methods to uninstall Dell CSI Driver","ref":"/csm-docs/v2/csidriver/uninstall/","tags":"","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Observability.\nUninstall the CSM for Observability Helm Chart The command below removes all the Kubernetes components associated with the chart.\nhelm delete karavi-observability --namespace [CSM_NAMESPACE] You may also want to uninstall the CRDs created for cert-manager.\nkubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.10.0/cert-manager.crds.yaml ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Uninstallation\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Uninstallation\n","ref":"/csm-docs/v2/observability/uninstall/","tags":"","title":"Uninstallation"},{"body":"Uninstall a CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the driver:\n./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e For usage information:\n[dell-csi-helm-installer]# ./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u003cnamespace\u003e Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u003chelm release\u003e Name to register with helm, default value will match the driver name -h Help Uninstall a CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed by the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall the driver installed via the operator, delete the Custom Resource(CR)\n# Replace driver-type, driver-name and driver-namespace with their respective values kubectl delete \u003cdriver-type\u003e/\u003cdriver-name\u003e -n \u003cdriver-namespace\u003e ","categories":"","description":"Methods to uninstall Dell CSI Driver","excerpt":"Methods to uninstall Dell CSI Driver","ref":"/csm-docs/v3/csidriver/uninstall/","tags":"","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Observability.\nUninstall the CSM for Observability Helm Chart The command below removes all the Kubernetes components associated with the chart.\n$ helm delete karavi-observability --namespace [CSM_NAMESPACE] You may also want to uninstall the CRDs created for cert-manager.\n$ kubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.10.0/cert-manager.crds.yaml ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Uninstallation\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Uninstallation\n","ref":"/csm-docs/v3/observability/uninstall/","tags":"","title":"Uninstallation"},{"body":" The CSM Authorization RPM will be deprecated in a future release. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nThis section outlines the upgrade steps for Container Storage Modules (CSM) for Authorization. The upgrade of CSM for Authorization is handled in 2 parts:\nUpgrading the CSM for Authorization proxy server Upgrading the Dell CSI drivers with CSM for Authorization enabled Upgrading CSM for Authorization proxy server Obtain the latest single binary installer RPM by following one of our two options here.\nTo update the rpm package on the system, run the below command from within the extracted folder:\nsh install_karavi_auth.sh --upgrade As an option, on version 1.6.0, the Nodeports for the ingress controller can be specified:\nsh install_karavi_auth.sh --upgrade --traefik_web_port \u003cweb port number\u003e --traefik_websecure_port \u003cwebsecure port number\u003e Ex.:\nsh install_karavi_auth.sh --upgrade --traefik_web_port 30001 --traefik_websecure_port 30002 To verify that the new version of the rpm is installed and K3s has been updated, run the below commands:\nrpm -qa | grep karavi k3s kubectl version Note: The above steps manage install and upgrade of all dependencies that are required by the CSM for Authorization proxy server.\nUpgrading Dell CSI Driver(s) with CSM for Authorization enabled Given a setup where the CSM for Authorization proxy server is already upgraded to the latest version, follow the upgrade instructions for the applicable CSI Driver(s) to upgrade the driver and the CSM for Authorization sidecar\nUpgrade PowerFlex CSI driver Upgrade PowerMax CSI driver Upgrade PowerScale CSI driver Rollback This section outlines the rollback steps for Container Storage Modules (CSM) for Authorization.\nRollback CSM for Authorization proxy server To rollback the rpm package on the system, run the below command:\nrpm -Uvh --oldpackage karavi-authorization-\u003cold_version\u003e.x86_64.rpm --nopreun --nopostun ","categories":"","description":"Upgrade Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization\n","excerpt":"Upgrade Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/authorization/upgrade/","tags":"","title":"Upgrade"},{"body":"","categories":"","description":"Upgrade Dell CSI Drivers","excerpt":"Upgrade Dell CSI Drivers","ref":"/csm-docs/docs/csidriver/upgradation/","tags":["upgrade","csi-driver"],"title":"Upgrade"},{"body":"This section outlines the upgrade steps for Container Storage Modules (CSM) for Observability. CSM for Observability upgrade can be achieved in one of two ways:\nHelm Chart Upgrade Online Installer Upgrade Helm Chart Upgrade CSM for Observability Helm upgrade supports Helm, Online Installer, and Offline Installer deployments.\nTo upgrade an existing Helm installation of CSM for Observability to the latest release, download the latest Helm charts.\nhelm repo update Check if the latest Helm chart version is available:\nhelm search repo dell NAME CHART VERSION APP VERSION DESCRIPTION dell/karavi-observability 1.7.0 1.7.0 CSM for Observability is part of the [Container... Note: If using cert-manager CustomResourceDefinitions older than v1.5.3, delete the old CRDs and install v1.5.3 of the CRDs prior to upgrade. See Prerequisites for location of CRDs.\nUpgrade to the latest CSM for Observability release:\nUpgrade Helm and Online Installer deployments:\nhelm upgrade --version $latest_chart_version --values values.yaml karavi-observability dell/karavi-observability -n $namespace Upgrade Offline Installer deployment:\nhelm upgrade --version $latest_chart_version karavi-observability dell/karavi-observability -n $namespace The configuration section lists all the parameters that can be configured using the values.yaml file.\nOnline Installer Upgrade CSM for Observability online installer upgrade can be used if the initial deployment was performed using the Online Installer or Helm.\nChange to the installer directory:\ncd karavi-observability/installer Update values.yaml file as needed. Configuration options are outlined in the Helm chart deployment section.\nExecute the ./karavi-observability-install.sh script:\n./karavi-observability-install.sh upgrade --namespace $namespace --values myvalues.yaml --version $latest_chart_version --------------------------------------------------------------------------------- \u003e Upgrading Karavi Observability in namespace karavi on 1.27 --------------------------------------------------------------------------------- | |- Karavi Observability is installed. Upgrade can continue Success | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying helm version Success | |- Upgrading CertManager CRDs Success | |- Updating helm repositories Success | |- Upgrading Karavi Observability helm chart Success | |- Waiting for pods in namespace karavi to be ready Success Offline Installer Upgrade Assuming that you have already installed the Karavi Observability Helm Chart by offline installer and meet its installation requirement. These instructions can be followed when a Helm chart was installed and will be upgraded in an environment that does not have an Internet connection and will be unable to download the Helm chart and related Docker images.\nBuild the Offline Bundle Follow Offline Karavi Observability Helm Chart Installer to build the latest bundle.\nUnpack the Offline Bundle Follow Offline Karavi Observability Helm Chart Installer, copy and unpack the Offline Bundle to another Linux system, and push Docker images to the internal Docker registry.\nPerform Helm upgrade\nChange directory to helm which contains the updated Helm chart directory:\ncd helm Install necessary cert-manager CustomResourceDefinitions provided.\nkubectl apply --validate=false -f cert-manager.crds.yaml (Optional) Enable Karavi Observability for PowerFlex/PowerScale to use an existing instance of Karavi Authorization for accessing the REST API for the given storage systems. Note: Assuming that if the Karavi Observability’s Authorization has been enabled in the phase of Offline Karavi Observability Helm Chart Installer, the Authorization Secrets/Configmap have been copied to the Karavi Observability namespace. A sample configuration values.yaml file is located here. In your own configuration values.yaml, you need to enable PowerFlex/PowerScale Authorization, and provide the location of the sidecar-proxy Docker image and URL of the Karavi Authorization proxyHost address.\nAfter the images have been made available and the Helm chart configuration is updated, follow the instructions within the Helm chart’s repository to complete the installation. Note: Assuming that Your Secrets from CSI Drivers have been copied to the Karavi Observability namespace during the steps of Offline Karavi Observability Helm Chart Installer Optionally, you could provide your own configurations. A sample values.yaml file is located here.\nhelm upgrade -n install-namespace app-name karavi-observability NAME: app-name LAST DEPLOYED: Wed Aug 17 14:44:04 2022 NAMESPACE: install-namespace STATUS: deployed REVISION: 1 TEST SUITE: None ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Upgrade\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Upgrade\n","ref":"/csm-docs/docs/observability/upgrade/","tags":"","title":"Upgrade"},{"body":"CSM for Resiliency can be upgraded as part of the Dell CSI driver upgrade process. The drivers can be upgraded either by a helm chart or by the Dell CSM Operator. Currently, only Helm chart upgrade is supported for CSM for Resiliency.\nFor information on the PowerFlex CSI driver upgrade process, see PowerFlex CSI Driver.\nFor information on the Unity XT CSI driver upgrade process, see Unity XT CSI Driver.\nFor information on the PowerScale CSI driver upgrade process, see PowerScale CSI Driver.\nFor information on the PowerStore CSI driver upgrade process, see PowerStore CSI Driver.\nHelm Chart Upgrade To upgrade CSM for Resiliency with the driver, the following steps are required.\nNote: These steps refer to the values file and csi-install.sh script that were used during initial installation of the Dell CSI driver.\nSteps\nUpdate the podmon.image value in the values files to reference the new podmon image. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade ","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency upgrade\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency upgrade\n","ref":"/csm-docs/docs/resiliency/upgrade/","tags":"","title":"Upgrade"},{"body":" The CSM Authorization RPM will be deprecated in a future release. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nThis section outlines the upgrade steps for Container Storage Modules (CSM) for Authorization. The upgrade of CSM for Authorization is handled in 2 parts:\nUpgrading the CSM for Authorization proxy server Upgrading the Dell CSI drivers with CSM for Authorization enabled Upgrading CSM for Authorization proxy server Obtain the latest single binary installer RPM by following one of our two options here.\nTo update the rpm package on the system, run the below command from within the extracted folder:\nsh install_karavi_auth.sh --upgrade As an option, on version 1.6.0, the Nodeports for the ingress controller can be specified:\nsh install_karavi_auth.sh --upgrade --traefik_web_port \u003cweb port number\u003e --traefik_websecure_port \u003cwebsecure port number\u003e Ex.:\nsh install_karavi_auth.sh --upgrade --traefik_web_port 30001 --traefik_websecure_port 30002 To verify that the new version of the rpm is installed and K3s has been updated, run the below commands:\nrpm -qa | grep karavi k3s kubectl version Note: The above steps manage install and upgrade of all dependencies that are required by the CSM for Authorization proxy server.\nUpgrading Dell CSI Driver(s) with CSM for Authorization enabled Given a setup where the CSM for Authorization proxy server is already upgraded to the latest version, follow the upgrade instructions for the applicable CSI Driver(s) to upgrade the driver and the CSM for Authorization sidecar\nUpgrade PowerFlex CSI driver Upgrade PowerMax CSI driver Upgrade PowerScale CSI driver Rollback This section outlines the rollback steps for Container Storage Modules (CSM) for Authorization.\nRollback CSM for Authorization proxy server To rollback the rpm package on the system, run the below command:\nrpm -Uvh --oldpackage karavi-authorization-\u003cold_version\u003e.x86_64.rpm --nopreun --nopostun ","categories":"","description":"Upgrade Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization\n","excerpt":"Upgrade Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v1/authorization/upgrade/","tags":"","title":"Upgrade"},{"body":"","categories":"","description":"Upgrade Dell CSI Drivers","excerpt":"Upgrade Dell CSI Drivers","ref":"/csm-docs/v1/csidriver/upgradation/","tags":["upgrade","csi-driver"],"title":"Upgrade"},{"body":"This section outlines the upgrade steps for Container Storage Modules (CSM) for Observability. CSM for Observability upgrade can be achieved in one of two ways:\nHelm Chart Upgrade Online Installer Upgrade Helm Chart Upgrade CSM for Observability Helm upgrade supports Helm, Online Installer, and Offline Installer deployments.\nTo upgrade an existing Helm installation of CSM for Observability to the latest release, download the latest Helm charts.\nhelm repo update Check if the latest Helm chart version is available:\nhelm search repo dell NAME CHART VERSION APP VERSION DESCRIPTION dell/karavi-observability 1.6.0 1.6.0 CSM for Observability is part of the [Container... Note: If using cert-manager CustomResourceDefinitions older than v1.5.3, delete the old CRDs and install v1.5.3 of the CRDs prior to upgrade. See Prerequisites for location of CRDs.\nUpgrade to the latest CSM for Observability release:\nUpgrade Helm and Online Installer deployments:\nhelm upgrade --version $latest_chart_version --values values.yaml karavi-observability dell/karavi-observability -n $namespace Upgrade Offline Installer deployment:\nhelm upgrade --version $latest_chart_version karavi-observability dell/karavi-observability -n $namespace The configuration section lists all the parameters that can be configured using the values.yaml file.\nOnline Installer Upgrade CSM for Observability online installer upgrade can be used if the initial deployment was performed using the Online Installer or Helm.\nChange to the installer directory:\ncd karavi-observability/installer Update values.yaml file as needed. Configuration options are outlined in the Helm chart deployment section.\nExecute the ./karavi-observability-install.sh script:\n./karavi-observability-install.sh upgrade --namespace $namespace --values myvalues.yaml --version $latest_chart_version --------------------------------------------------------------------------------- \u003e Upgrading Karavi Observability in namespace karavi on 1.27 --------------------------------------------------------------------------------- | |- Karavi Observability is installed. Upgrade can continue Success | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying helm version Success | |- Upgrading CertManager CRDs Success | |- Updating helm repositories Success | |- Upgrading Karavi Observability helm chart Success | |- Waiting for pods in namespace karavi to be ready Success Offline Installer Upgrade Assuming that you have already installed the Karavi Observability Helm Chart by offline installer and meet its installation requirement. These instructions can be followed when a Helm chart was installed and will be upgraded in an environment that does not have an Internet connection and will be unable to download the Helm chart and related Docker images.\nBuild the Offline Bundle Follow Offline Karavi Observability Helm Chart Installer to build the latest bundle.\nUnpack the Offline Bundle Follow Offline Karavi Observability Helm Chart Installer, copy and unpack the Offline Bundle to another Linux system, and push Docker images to the internal Docker registry.\nPerform Helm upgrade\nChange directory to helm which contains the updated Helm chart directory:\ncd helm Install necessary cert-manager CustomResourceDefinitions provided.\nkubectl apply --validate=false -f cert-manager.crds.yaml (Optional) Enable Karavi Observability for PowerFlex/PowerScale to use an existing instance of Karavi Authorization for accessing the REST API for the given storage systems. Note: Assuming that if the Karavi Observability’s Authorization has been enabled in the phase of Offline Karavi Observability Helm Chart Installer, the Authorization Secrets/Configmap have been copied to the Karavi Observability namespace. A sample configuration values.yaml file is located here. In your own configuration values.yaml, you need to enable PowerFlex/PowerScale Authorization, and provide the location of the sidecar-proxy Docker image and URL of the Karavi Authorization proxyHost address.\nAfter the images have been made available and the Helm chart configuration is updated, follow the instructions within the Helm chart’s repository to complete the installation. Note: Assuming that Your Secrets from CSI Drivers have been copied to the Karavi Observability namespace during the steps of Offline Karavi Observability Helm Chart Installer Optionally, you could provide your own configurations. A sample values.yaml file is located here.\nhelm upgrade -n install-namespace app-name karavi-observability NAME: app-name LAST DEPLOYED: Wed Aug 17 14:44:04 2022 NAMESPACE: install-namespace STATUS: deployed REVISION: 1 TEST SUITE: None ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Upgrade\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Upgrade\n","ref":"/csm-docs/v1/observability/upgrade/","tags":"","title":"Upgrade"},{"body":"CSM for Resiliency can be upgraded as part of the Dell CSI driver upgrade process. The drivers can be upgraded either by a helm chart or by the Dell CSI Operator. Currently, only Helm chart upgrade is supported for CSM for Resiliency.\nFor information on the PowerFlex CSI driver upgrade process, see PowerFlex CSI Driver.\nFor information on the Unity XT CSI driver upgrade process, see Unity XT CSI Driver.\nFor information on the PowerScale CSI driver upgrade process, see PowerScale CSI Driver.\nFor information on the PowerStore CSI driver upgrade process, see PowerStore CSI Driver.\nHelm Chart Upgrade To upgrade CSM for Resiliency with the driver, the following steps are required.\nNote: These steps refer to the values file and csi-install.sh script that were used during initial installation of the Dell CSI driver.\nSteps\nUpdate the podmon.image value in the values files to reference the new podmon image. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade ","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency upgrade\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency upgrade\n","ref":"/csm-docs/v1/resiliency/upgrade/","tags":"","title":"Upgrade"},{"body":"This section outlines the upgrade steps for Container Storage Modules (CSM) for Authorization. The upgrade of CSM for Authorization is handled in 2 parts:\nUpgrading the CSM for Authorization proxy server Upgrading the Dell CSI drivers with CSM for Authorization enabled Upgrading CSM for Authorization proxy server Obtain the latest single binary installer RPM by following one of our two options here.\nTo update the rpm package on the system, run the below command from within the extracted folder:\nsh install_karavi_auth.sh --upgrade As an option, on version 1.6.0, the Nodeports for the ingress controller can be specified:\nsh install_karavi_auth.sh --upgrade --traefik_web_port \u003cweb port number\u003e --traefik_websecure_port \u003cwebsecure port number\u003e Ex.:\nsh install_karavi_auth.sh --upgrade --traefik_web_port 30001 --traefik_websecure_port 30002 To verify that the new version of the rpm is installed and K3s has been updated, run the below commands:\nrpm -qa | grep karavi k3s kubectl version Note: The above steps manage install and upgrade of all dependencies that are required by the CSM for Authorization proxy server.\nUpgrading Dell CSI Driver(s) with CSM for Authorization enabled Given a setup where the CSM for Authorization proxy server is already upgraded to the latest version, follow the upgrade instructions for the applicable CSI Driver(s) to upgrade the driver and the CSM for Authorization sidecar\nUpgrade PowerFlex CSI driver Upgrade PowerMax CSI driver Upgrade PowerScale CSI driver Rollback This section outlines the rollback steps for Container Storage Modules (CSM) for Authorization.\nRollback CSM for Authorization proxy server To rollback the rpm package on the system, run the below command:\nrpm -Uvh --oldpackage karavi-authorization-\u003cold_version\u003e.x86_64.rpm --nopreun --nopostun ","categories":"","description":"Upgrade Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization\n","excerpt":"Upgrade Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v2/authorization/upgrade/","tags":"","title":"Upgrade"},{"body":"","categories":"","description":"Upgrade Dell CSI Drivers","excerpt":"Upgrade Dell CSI Drivers","ref":"/csm-docs/v2/csidriver/upgradation/","tags":["upgrade","csi-driver"],"title":"Upgrade"},{"body":"This section outlines the upgrade steps for Container Storage Modules (CSM) for Observability. CSM for Observability upgrade can be achieved in one of two ways:\nHelm Chart Upgrade Online Installer Upgrade Helm Chart Upgrade CSM for Observability Helm upgrade supports Helm, Online Installer, and Offline Installer deployments.\nTo upgrade an existing Helm installation of CSM for Observability to the latest release, download the latest Helm charts.\nhelm repo update Check if the latest Helm chart version is available:\nhelm search repo dell NAME CHART VERSION APP VERSION DESCRIPTION dell/karavi-observability 1.5.0 1.5.0 CSM for Observability is part of the [Container... Note: If using cert-manager CustomResourceDefinitions older than v1.5.3, delete the old CRDs and install v1.5.3 of the CRDs prior to upgrade. See Prerequisites for location of CRDs.\nUpgrade to the latest CSM for Observability release:\nUpgrade Helm and Online Installer deployments:\nhelm upgrade --version $latest_chart_version --values values.yaml karavi-observability dell/karavi-observability -n $namespace Upgrade Offline Installer deployment:\nhelm upgrade --version $latest_chart_version karavi-observability dell/karavi-observability -n $namespace The configuration section lists all the parameters that can be configured using the values.yaml file.\nOnline Installer Upgrade CSM for Observability online installer upgrade can be used if the initial deployment was performed using the Online Installer or Helm.\nChange to the installer directory:\ncd karavi-observability/installer Update values.yaml file as needed. Configuration options are outlined in the Helm chart deployment section.\nExecute the ./karavi-observability-install.sh script:\n./karavi-observability-install.sh upgrade --namespace $namespace --values myvalues.yaml --version $latest_chart_version --------------------------------------------------------------------------------- \u003e Upgrading Karavi Observability in namespace karavi on 1.21 --------------------------------------------------------------------------------- | |- Karavi Observability is installed. Upgrade can continue Success | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying helm version Success | |- Upgrading CertManager CRDs Success | |- Updating helm repositories Success | |- Upgrading Karavi Observability helm chart Success | |- Waiting for pods in namespace karavi to be ready Success Offline Installer Upgrade Assuming that you have already installed the Karavi Observability Helm Chart by offline installer and meet its installation requirement. These instructions can be followed when a Helm chart was installed and will be upgraded in an environment that does not have an internet connection and will be unable to download the Helm chart and related Docker images.\nBuild the Offline Bundle Follow Offline Karavi Observability Helm Chart Installer to build the latest bundle.\nUnpack the Offline Bundle Follow Offline Karavi Observability Helm Chart Installer, copy and unpack the Offline Bundle to another Linux system, and push Docker images to the internal Docker registry.\nPerform Helm upgrade\nChange directory to helm which contains the updated Helm chart directory:\ncd helm Install necessary cert-manager CustomResourceDefinitions provided.\nkubectl apply --validate=false -f cert-manager.crds.yaml (Optional) Enable Karavi Observability for PowerFlex/PowerScale to use an existing instance of Karavi Authorization for accessing the REST API for the given storage systems.\nNote: Assuming that if the Karavi Observability’s Authorization has been enabled in the phase of Offline Karavi Observability Helm Chart Installer, the Authorization Secrets/Configmap have been copied to the Karavi Observability namespace.\nA sample configuration values.yaml file is located here.\nIn your own configuration values.yaml, you need to enable PowerFlex/PowerScale Authorization, and provide the location of the sidecar-proxy Docker image and URL of the Karavi Authorization proxyHost address.\nNow that the required images have been made available and the Helm chart’s configuration updated with references to the internal registry location, installation can proceed by following the instructions that are documented within the Helm chart’s repository.\nNote: Assuming that Your Secrets from CSI Drivers have been copied to the Karavi Observability namespace in the phase of Offline Karavi Observability Helm Chart Installer\nOptionally, you could provide your own configurations. A sample values.yaml file is located here.\nhelm upgrade -n install-namespace app-name karavi-observability NAME: app-name LAST DEPLOYED: Wed Aug 17 14:44:04 2022 NAMESPACE: install-namespace STATUS: deployed REVISION: 1 TEST SUITE: None ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Upgrade\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Upgrade\n","ref":"/csm-docs/v2/observability/upgrade/","tags":"","title":"Upgrade"},{"body":"CSM for Resiliency can be upgraded as part of the Dell CSI driver upgrade process. The drivers can be upgraded either by a helm chart or by the Dell CSI Operator. Currently, only Helm chart upgrade is supported for CSM for Resiliency.\nFor information on the PowerFlex CSI driver upgrade process, see PowerFlex CSI Driver.\nFor information on the Unity XT CSI driver upgrade process, see Unity XT CSI Driver.\nFor information on the PowerScale CSI driver upgrade process, see PowerScale CSI Driver.\nFor information on the PowerStore CSI driver upgrade process, see PowerStore CSI Driver.\nHelm Chart Upgrade To upgrade CSM for Resiliency with the driver, the following steps are required.\nNote: These steps refer to the values file and csi-install.sh script that were used during initial installation of the Dell CSI driver.\nSteps\nUpdate the podmon.image value in the values files to reference the new podmon image. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade ","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency upgrade\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency upgrade\n","ref":"/csm-docs/v2/resiliency/upgrade/","tags":"","title":"Upgrade"},{"body":" The CSM Authorization RPM is no longer actively maintained or supported. It will be deprecated in CSM 2.0. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nThis section outlines the upgrade steps for Container Storage Modules (CSM) for Authorization. The upgrade of CSM for Authorization is handled in 2 parts:\nUpgrading the CSM for Authorization proxy server Upgrading the Dell CSI drivers with CSM for Authorization enabled Upgrading CSM for Authorization proxy server Obtain the latest single binary installer RPM by following one of our two options here.\nTo update the rpm package on the system, run the below command from within the extracted folder:\nsh install_karavi_auth.sh --upgrade As an option, on version 1.6.0, the Nodeports for the ingress controller can be specified:\nsh install_karavi_auth.sh --upgrade --traefik_web_port \u003cweb port number\u003e --traefik_websecure_port \u003cwebsecure port number\u003e Ex.: sh install_karavi_auth.sh --upgrade --traefik_web_port 30001 --traefik_websecure_port 30002 To verify that the new version of the rpm is installed and K3s has been updated, run the below commands:\nrpm -qa | grep karavi k3s kubectl version Note: The above steps manage install and upgrade of all dependencies that are required by the CSM for Authorization proxy server.\nUpgrading Dell CSI Driver(s) with CSM for Authorization enabled Given a setup where the CSM for Authorization proxy server is already upgraded to the latest version, follow the upgrade instructions for the applicable CSI Driver(s) to upgrade the driver and the CSM for Authorization sidecar\nUpgrade PowerFlex CSI driver Upgrade PowerMax CSI driver Upgrade PowerScale CSI driver Rollback This section outlines the rollback steps for Container Storage Modules (CSM) for Authorization.\nRollback CSM for Authorization proxy server To rollback the rpm package on the system, run the below command:\nrpm -Uvh --oldpackage karavi-authorization-\u003cold_version\u003e.x86_64.rpm --nopreun --nopostun ","categories":"","description":"Upgrade Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization\n","excerpt":"Upgrade Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v3/authorization/upgrade/","tags":"","title":"Upgrade"},{"body":"","categories":"","description":"Upgrade Dell CSI Drivers","excerpt":"Upgrade Dell CSI Drivers","ref":"/csm-docs/v3/csidriver/upgradation/","tags":["upgrade","csi-driver"],"title":"Upgrade"},{"body":"This section outlines the upgrade steps for Container Storage Modules (CSM) for Observability. CSM for Observability upgrade can be achieved in one of two ways:\nHelm Chart Upgrade Online Installer Upgrade Helm Chart Upgrade CSM for Observability Helm upgrade supports Helm, Online Installer, and Offline Installer deployments.\nTo upgrade an existing Helm installation of CSM for Observability to the latest release, download the latest Helm charts.\nhelm repo update Check if the latest Helm chart version is available:\nhelm search repo dell NAME CHART VERSION APP VERSION DESCRIPTION dell/karavi-observability 1.5.0 1.5.0 CSM for Observability is part of the [Container... Note: If using cert-manager CustomResourceDefinitions older than v1.5.3, delete the old CRDs and install v1.5.3 of the CRDs prior to upgrade. See Prerequisites for location of CRDs.\nUpgrade to the latest CSM for Observability release:\nUpgrade Helm and Online Installer deployments: $ helm upgrade --version $latest_chart_version --values values.yaml karavi-observability dell/karavi-observability -n $namespace Upgrade Offline Installer deployment: $ helm upgrade --version $latest_chart_version karavi-observability dell/karavi-observability -n $namespace The configuration section lists all the parameters that can be configured using the values.yaml file.\nOnline Installer Upgrade CSM for Observability online installer upgrade can be used if the initial deployment was performed using the Online Installer or Helm.\nChange to the installer directory:\n[user@system /home/user]# cd karavi-observability/installer Update values.yaml file as needed. Configuration options are outlined in the Helm chart deployment section.\nExecute the ./karavi-observability-install.sh script:\n[user@system /home/user/karavi-observability/installer]# ./karavi-observability-install.sh upgrade --namespace $namespace --values myvalues.yaml --version $latest_chart_version --------------------------------------------------------------------------------- \u003e Upgrading Karavi Observability in namespace karavi on 1.21 --------------------------------------------------------------------------------- | |- Karavi Observability is installed. Upgrade can continue Success | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying helm version Success | |- Upgrading CertManager CRDs Success | |- Updating helm repositories Success | |- Upgrading Karavi Observability helm chart Success | |- Waiting for pods in namespace karavi to be ready Success Offline Installer Upgrade Assuming that you have already installed the Karavi Observability Helm Chart by offline installer and meet its installation requirement. These instructions can be followed when a Helm chart was installed and will be upgraded in an environment that does not have an internet connection and will be unable to download the Helm chart and related Docker images.\nBuild the Offline Bundle Follow Offline Karavi Observability Helm Chart Installer to build the latest bundle.\nUnpack the Offline Bundle Follow Offline Karavi Observability Helm Chart Installer, copy and unpack the Offline Bundle to another Linux system, and push Docker images to the internal Docker registry.\nPerform Helm upgrade\nChange directory to helm which contains the updated Helm chart directory:\n[user@anothersystem /home/user/offline-karavi-observability-bundle]# cd helm Install necessary cert-manager CustomResourceDefinitions provided.\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl apply --validate=false -f cert-manager.crds.yaml (Optional) Enable Karavi Observability for PowerFlex/PowerScale to use an existing instance of Karavi Authorization for accessing the REST API for the given storage systems.\nNote: Assuming that if the Karavi Observability’s Authorization has been enabled in the phase of Offline Karavi Observability Helm Chart Installer, the Authorization Secrets/Configmap have been copied to the Karavi Observability namespace.\nA sample configuration values.yaml file is located here.\nIn your own configuration values.yaml, you need to enable PowerFlex/PowerScale Authorization, and provide the location of the sidecar-proxy Docker image and URL of the Karavi Authorization proxyHost address.\nNow that the required images have been made available and the Helm chart’s configuration updated with references to the internal registry location, installation can proceed by following the instructions that are documented within the Helm chart’s repository.\nNote: Assuming that Your Secrets from CSI Drivers have been copied to the Karavi Observability namespace in the phase of Offline Karavi Observability Helm Chart Installer\nOptionally, you could provide your own configurations. A sample values.yaml file is located here.\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# helm upgrade -n install-namespace app-name karavi-observability NAME: app-name LAST DEPLOYED: Wed Aug 17 14:44:04 2022 NAMESPACE: install-namespace STATUS: deployed REVISION: 1 TEST SUITE: None ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability Upgrade\n","excerpt":"Dell Container Storage Modules (CSM) for Observability Upgrade\n","ref":"/csm-docs/v3/observability/upgrade/","tags":"","title":"Upgrade"},{"body":"CSM for Resiliency can be upgraded as part of the Dell CSI driver upgrade process. The drivers can be upgraded either by a helm chart or by the Dell CSI Operator. Currently, only Helm chart upgrade is supported for CSM for Resiliency.\nFor information on the PowerFlex CSI driver upgrade process, see PowerFlex CSI Driver.\nFor information on the Unity XT CSI driver upgrade process, see Unity XT CSI Driver.\nFor information on the PowerScale CSI driver upgrade process, see PowerScale CSI Driver.\nFor information on the PowerStore CSI driver upgrade process, see PowerStore CSI Driver.\nHelm Chart Upgrade To upgrade CSM for Resiliency with the driver, the following steps are required.\nNote: These steps refer to the values file and csi-install.sh script that were used during initial installation of the Dell CSI driver.\nSteps\nUpdate the podmon.image value in the values files to reference the new podmon image. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade. ","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency upgrade\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency upgrade\n","ref":"/csm-docs/v3/resiliency/upgrade/","tags":"","title":"Upgrade"},{"body":"After Application Mobility is installed, the dellctl CLI can be used to register clusters and manage backups and restores of applications. These examples also provide references for using the Application Mobility Custom Resource Definitions (CRDs) to define Custom Resources (CRs) as an alternative to using the dellctl CLI.\nBackup and Restore an Application This example details the steps when an application in namespace demo1 is being backed up and then later restored to either the same cluster or another cluster. In this sample, both Application Mobility and Velero are installed in the application-mobility namespace.\nIf Velero is not installed in the default velero namespace and dellctl is being used, set this environment variable to the namespace where it is installed:\nexport VELERO_NAMESPACE=application-mobility On the source cluster, create a Backup by providing a name and the included namespace where the application is installed. The application and its data will be available in the object store bucket and can be restored at a later time.\nUsing dellctl:\ndellctl backup create backup1 --include-namespaces demo1 --namespace application-mobility Using Backup Custom Resource:\napiVersion: mobility.storage.dell.com/v1alpha1 kind: Backup metadata: name: backup1 namespace: application-mobility spec: includedNamespaces: [demo1] datamover: Restic clones: [] Monitor the backup status until it is marked as Completed.\nUsing dellctl:\ndellctl backup get --namespace application-mobility Using kubectl:\nkubectl describe backups.mobility.storage.dell.com/backup1 -n application-mobility If the Storage Class name on the target cluster is different than the Storage Class name on the source cluster where the backup was created, a mapping between source and target Storage Class names must be defined. See Changing PV/PVC Storage Classes.\nThe application and its data can be restored on either the same cluster or another cluster by referring to the backup name and providing an optional mapping of the original namespace to the target namespace.\nUsing dellctl:\ndellctl restore create restore1 --from-backup backup1 \\ --namespace-mappings \"demo1:restorens1\" --namespace application-mobility Using Restore Custom Resource:\napiVersion: mobility.storage.dell.com/v1alpha1 kind: Restore metadata: name: restore1 namespace: application-mobility spec: backupName: backup1 namespaceMapping: \"demo1\" : \"restorens1\" Monitor the restore status until it is marked as Completed.\nUsing dellctl:\ndellctl restore get --namespace application-mobility Using kubectl:\nkubectl describe restores.mobility.storage.dell.com/restore1 -n application-mobility Clone an Application This example details the steps when an application in namespace demo1 is cloned from a source cluster to a target cluster in a single operation. In this sample, both Application Mobility and Velero are installed in the application-mobility namespace.\nIf Velero is not installed in the default velero namespace and dellctl is being used, set this environment variable to the namespace where it is installed:\nexport VELERO_NAMESPACE=application-mobility Register the target cluster if using dellctl\ndellctl cluster add -n targetcluster -u \u003ckube-system-namespace-uuid\u003e -f ~/kubeconfigs/target-cluster-kubeconfig If the Storage Class name on the target cluster is different than the Storage Class name on the source cluster where the backup was created, a mapping between source and target Storage Class names must be defined. See Changing PV/PVC Storage Classes.\nCreate a Backup by providing a name, the included namespace where the application is installed, and the target cluster and namespace mapping where the application will be restored.\nUsing dellctl:\ndellctl backup create backup1 --include-namespaces demo1 --clones \"targetcluster/demo1:restore-ns2\" \\ --namespace application-mobility Using Backup Custom Resource:\napiVersion: mobility.storage.dell.com/v1alpha1 kind: Backup metadata: name: backup1 namespace: application-mobility spec: includedNamespaces: [demo1] datamover: Restic clones: - namespaceMapping: \"demo1\": \"restore-ns2\" restoreOnceAvailable: true targetCluster: targetcluster Monitor the restore status on the target cluster until it is marked as Completed.\nUsing dellctl:\ndellctl restore get --namespace application-mobility Using kubectl:\nkubectl get restores.mobility.storage.dell.com -n application-mobility kubectl describe restores.mobility.storage.dell.com/\u003crestore-name\u003e -n application-mobility Changing PV/PVC Storage Classes Create a ConfigMap on the target cluster in the same namespace where Application Mobility is installed. The data field must contain a mapping of source Storage Class name to target Storage Class name. See Velero’s documentation for Changing PV/PVC Storage Classes for additional details.\napiVersion: v1 kind: ConfigMap metadata: name: change-storage-class-config namespace: \u003capplication-mobility-namespace\u003e labels: velero.io/plugin-config: \"\" velero.io/change-storage-class: RestoreItemAction data: \u003csource-storage-class-name\u003e: \u003ctarget-storage-class-name\u003e ","categories":"","description":"Use Cases\n","excerpt":"Use Cases\n","ref":"/csm-docs/docs/applicationmobility/use_cases/","tags":"","title":"Use Cases"},{"body":"After Application Mobility is installed, the dellctl CLI can be used to register clusters and manage backups and restores of applications. These examples also provide references for using the Application Mobility Custom Resource Definitions (CRDs) to define Custom Resources (CRs) as an alternative to using the dellctl CLI.\nBackup and Restore an Application This example details the steps when an application in namespace demo1 is being backed up and then later restored to either the same cluster or another cluster. In this sample, both Application Mobility and Velero are installed in the application-mobility namespace.\nIf Velero is not installed in the default velero namespace and dellctl is being used, set this environment variable to the namespace where it is installed:\nexport VELERO_NAMESPACE=application-mobility On the source cluster, create a Backup by providing a name and the included namespace where the application is installed. The application and its data will be available in the object store bucket and can be restored at a later time.\nUsing dellctl:\ndellctl backup create backup1 --include-namespaces demo1 --namespace application-mobility Using Backup Custom Resource:\napiVersion: mobility.storage.dell.com/v1alpha1 kind: Backup metadata: name: backup1 namespace: application-mobility spec: includedNamespaces: [demo1] datamover: Restic clones: [] Monitor the backup status until it is marked as Completed.\nUsing dellctl:\ndellctl backup get --namespace application-mobility Using kubectl:\nkubectl describe backups.mobility.storage.dell.com/backup1 -n application-mobility If the Storage Class name on the target cluster is different than the Storage Class name on the source cluster where the backup was created, a mapping between source and target Storage Class names must be defined. See Changing PV/PVC Storage Classes.\nThe application and its data can be restored on either the same cluster or another cluster by referring to the backup name and providing an optional mapping of the original namespace to the target namespace.\nUsing dellctl:\ndellctl restore create restore1 --from-backup backup1 \\ --namespace-mappings \"demo1:restorens1\" --namespace application-mobility Using Restore Custom Resource:\napiVersion: mobility.storage.dell.com/v1alpha1 kind: Restore metadata: name: restore1 namespace: application-mobility spec: backupName: backup1 namespaceMapping: \"demo1\" : \"restorens1\" Monitor the restore status until it is marked as Completed.\nUsing dellctl:\ndellctl restore get --namespace application-mobility Using kubectl:\nkubectl describe restores.mobility.storage.dell.com/restore1 -n application-mobility Clone an Application This example details the steps when an application in namespace demo1 is cloned from a source cluster to a target cluster in a single operation. In this sample, both Application Mobility and Velero are installed in the application-mobility namespace.\nIf Velero is not installed in the default velero namespace and dellctl is being used, set this environment variable to the namespace where it is installed:\nexport VELERO_NAMESPACE=application-mobility Register the target cluster if using dellctl\ndellctl cluster add -n targetcluster -u \u003ckube-system-namespace-uuid\u003e -f ~/kubeconfigs/target-cluster-kubeconfig If the Storage Class name on the target cluster is different than the Storage Class name on the source cluster where the backup was created, a mapping between source and target Storage Class names must be defined. See Changing PV/PVC Storage Classes.\nCreate a Backup by providing a name, the included namespace where the application is installed, and the target cluster and namespace mapping where the application will be restored.\nUsing dellctl:\ndellctl backup create backup1 --include-namespaces demo1 --clones \"targetcluster/demo1:restore-ns2\" \\ --namespace application-mobility Using Backup Custom Resource:\napiVersion: mobility.storage.dell.com/v1alpha1 kind: Backup metadata: name: backup1 namespace: application-mobility spec: includedNamespaces: [demo1] datamover: Restic clones: - namespaceMapping: \"demo1\": \"restore-ns2\" restoreOnceAvailable: true targetCluster: targetcluster Monitor the restore status on the target cluster until it is marked as Completed.\nUsing dellctl:\ndellctl restore get --namespace application-mobility Using kubectl:\nkubectl get restores.mobility.storage.dell.com -n application-mobility kubectl describe restores.mobility.storage.dell.com/\u003crestore-name\u003e -n application-mobility Changing PV/PVC Storage Classes Create a ConfigMap on the target cluster in the same namespace where Application Mobility is installed. The data field must contain a mapping of source Storage Class name to target Storage Class name. See Velero’s documentation for Changing PV/PVC Storage Classes for additional details.\napiVersion: v1 kind: ConfigMap metadata: name: change-storage-class-config namespace: \u003capplication-mobility-namespace\u003e labels: velero.io/plugin-config: \"\" velero.io/change-storage-class: RestoreItemAction data: \u003csource-storage-class-name\u003e: \u003ctarget-storage-class-name\u003e ","categories":"","description":"Use Cases\n","excerpt":"Use Cases\n","ref":"/csm-docs/v1/applicationmobility/use_cases/","tags":"","title":"Use Cases"},{"body":"After Application Mobility is installed, the dellctl CLI can be used to register clusters and manage backups and restores of applications. These examples also provide references for using the Application Mobility Custom Resource Definitions (CRDs) to define Custom Resources (CRs) as an alternative to using the dellctl CLI.\nBackup and Restore an Application This example details the steps when an application in namespace demo1 is being backed up and then later restored to either the same cluster or another cluster. In this sample, both Application Mobility and Velero are installed in the application-mobility namespace.\nIf Velero is not installed in the default velero namespace and dellctl is being used, set this environment variable to the namespace where it is installed:\nexport VELERO_NAMESPACE=application-mobility On the source cluster, create a Backup by providing a name and the included namespace where the application is installed. The application and its data will be available in the object store bucket and can be restored at a later time.\nUsing dellctl:\ndellctl backup create backup1 --include-namespaces demo1 --namespace application-mobility Using Backup Custom Resource:\napiVersion: mobility.storage.dell.com/v1alpha1 kind: Backup metadata: name: backup1 namespace: application-mobility spec: includedNamespaces: [demo1] datamover: Restic clones: [] Monitor the backup status until it is marked as Completed.\nUsing dellctl:\ndellctl backup get --namespace application-mobility Using kubectl:\nkubectl describe backups.mobility.storage.dell.com/backup1 -n application-mobility If the Storage Class name on the target cluster is different than the Storage Class name on the source cluster where the backup was created, a mapping between source and target Storage Class names must be defined. See Changing PV/PVC Storage Classes.\nThe application and its data can be restored on either the same cluster or another cluster by referring to the backup name and providing an optional mapping of the original namespace to the target namespace.\nUsing dellctl:\ndellctl restore create restore1 --from-backup backup1 \\ --namespace-mappings \"demo1:restorens1\" --namespace application-mobility Using Restore Custom Resource:\napiVersion: mobility.storage.dell.com/v1alpha1 kind: Restore metadata: name: restore1 namespace: application-mobility spec: backupName: backup1 namespaceMapping: \"demo1\" : \"restorens1\" Monitor the restore status until it is marked as Completed.\nUsing dellctl:\ndellctl restore get --namespace application-mobility Using kubectl:\nkubectl describe restores.mobility.storage.dell.com/restore1 -n application-mobility Clone an Application This example details the steps when an application in namespace demo1 is cloned from a source cluster to a target cluster in a single operation. In this sample, both Application Mobility and Velero are installed in the application-mobility namespace.\nIf Velero is not installed in the default velero namespace and dellctl is being used, set this environment variable to the namespace where it is installed:\nexport VELERO_NAMESPACE=application-mobility Register the target cluster if using dellctl\ndellctl cluster add -n targetcluster -u \u003ckube-system-namespace-uuid\u003e -f ~/kubeconfigs/target-cluster-kubeconfig If the Storage Class name on the target cluster is different than the Storage Class name on the source cluster where the backup was created, a mapping between source and target Storage Class names must be defined. See Changing PV/PVC Storage Classes.\nCreate a Backup by providing a name, the included namespace where the application is installed, and the target cluster and namespace mapping where the application will be restored.\nUsing dellctl:\ndellctl backup create backup1 --include-namespaces demo1 --clones \"targetcluster/demo1:restore-ns2\" \\ --namespace application-mobility Using Backup Custom Resource:\napiVersion: mobility.storage.dell.com/v1alpha1 kind: Backup metadata: name: backup1 namespace: application-mobility spec: includedNamespaces: [demo1] datamover: Restic clones: - namespaceMapping: \"demo1\": \"restore-ns2\" restoreOnceAvailable: true targetCluster: targetcluster Monitor the restore status on the target cluster until it is marked as Completed.\nUsing dellctl:\ndellctl restore get --namespace application-mobility Using kubectl:\nkubectl get restores.mobility.storage.dell.com -n application-mobility kubectl describe restores.mobility.storage.dell.com/\u003crestore-name\u003e -n application-mobility Changing PV/PVC Storage Classes Create a ConfigMap on the target cluster in the same namespace where Application Mobility is installed. The data field must contain a mapping of source Storage Class name to target Storage Class name. See Velero’s documentation for Changing PV/PVC Storage Classes for additional details.\napiVersion: v1 kind: ConfigMap metadata: name: change-storage-class-config namespace: \u003capplication-mobility-namespace\u003e labels: velero.io/plugin-config: \"\" velero.io/change-storage-class: RestoreItemAction data: \u003csource-storage-class-name\u003e: \u003ctarget-storage-class-name\u003e ","categories":"","description":"Use Cases\n","excerpt":"Use Cases\n","ref":"/csm-docs/v2/applicationmobility/use_cases/","tags":"","title":"Use Cases"},{"body":"After Application Mobility is installed, the dellctl CLI can be used to register clusters and manage backups and restores of applications. These examples also provide references for using the Application Mobility Custom Resource Definitions (CRDs) to define Custom Resources (CRs) as an alternative to using the dellctl CLI.\nBackup and Restore an Application This example details the steps when an application in namespace demo1 is being backed up and then later restored to either the same cluster or another cluster. In this sample, both Application Mobility and Velero are installed in the application-mobility namespace.\nIf Velero is not installed in the default velero namespace and dellctl is being used, set this environment variable to the namespace where it is installed:\nexport VELERO_NAMESPACE=application-mobility On the source cluster, create a Backup by providing a name and the included namespace where the application is installed. The application and its data will be available in the object store bucket and can be restored at a later time.\nUsing dellctl:\ndellctl backup create backup1 --include-namespaces demo1 --namespace application-mobility Using Backup Custom Resource:\napiVersion: mobility.storage.dell.com/v1alpha1 kind: Backup metadata: name: backup1 namespace: application-mobility spec: includedNamespaces: [demo1] datamover: Restic clones: [] Monitor the backup status until it is marked as Completed.\nUsing dellctl:\ndellctl backup get --namespace application-mobility Using kubectl:\nkubectl describe backups.mobility.storage.dell.com/backup1 -n application-mobility If the Storage Class name on the target cluster is different than the Storage Class name on the source cluster where the backup was created, a mapping between source and target Storage Class names must be defined. See Changing PV/PVC Storage Classes.\nThe application and its data can be restored on either the same cluster or another cluster by referring to the backup name and providing an optional mapping of the original namespace to the target namespace.\nUsing dellctl:\ndellctl restore create restore1 --from-backup backup1 \\ --namespace-mappings \"demo1:restorens1\" --namespace application-mobility Using Restore Custom Resource:\napiVersion: mobility.storage.dell.com/v1alpha1 kind: Restore metadata: name: restore1 namespace: application-mobility spec: backupName: backup1 namespaceMapping: \"demo1\" : \"restorens1\" Monitor the restore status until it is marked as Completed.\nUsing dellctl:\ndellctl restore get --namespace application-mobility Using kubectl:\nkubectl describe restores.mobility.storage.dell.com/restore1 -n application-mobility Clone an Application This example details the steps when an application in namespace demo1 is cloned from a source cluster to a target cluster in a single operation. In this sample, both Application Mobility and Velero are installed in the application-mobility namespace.\nIf Velero is not installed in the default velero namespace and dellctl is being used, set this environment variable to the namespace where it is installed:\nexport VELERO_NAMESPACE=application-mobility Register the target cluster if using dellctl\ndellctl cluster add -n targetcluster -u \u003ckube-system-namespace-uuid\u003e -f ~/kubeconfigs/target-cluster-kubeconfig If the Storage Class name on the target cluster is different than the Storage Class name on the source cluster where the backup was created, a mapping between source and target Storage Class names must be defined. See Changing PV/PVC Storage Classes.\nCreate a Backup by providing a name, the included namespace where the application is installed, and the target cluster and namespace mapping where the application will be restored.\nUsing dellctl:\ndellctl backup create backup1 --include-namespaces demo1 --clones \"targetcluster/demo1:restore-ns2\" \\ --namespace application-mobility Using Backup Custom Resource:\napiVersion: mobility.storage.dell.com/v1alpha1 kind: Backup metadata: name: backup1 namespace: application-mobility spec: includedNamespaces: [demo1] datamover: Restic clones: - namespaceMapping: \"demo1\": \"restore-ns2\" restoreOnceAvailable: true targetCluster: targetcluster Monitor the restore status on the target cluster until it is marked as Completed.\nUsing dellctl:\ndellctl restore get --namespace application-mobility Using kubectl:\nkubectl get restores.mobility.storage.dell.com -n application-mobility kubectl describe restores.mobility.storage.dell.com/\u003crestore-name\u003e -n application-mobility Changing PV/PVC Storage Classes Create a ConfigMap on the target cluster in the same namespace where Application Mobility is installed. The data field must contain a mapping of source Storage Class name to target Storage Class name. See Velero’s documentation for Changing PV/PVC Storage Classes for additional details.\napiVersion: v1 kind: ConfigMap metadata: name: change-storage-class-config namespace: \u003capplication-mobility-namespace\u003e labels: velero.io/plugin-config: \"\" velero.io/change-storage-class: RestoreItemAction data: \u003csource-storage-class-name\u003e: \u003ctarget-storage-class-name\u003e ","categories":"","description":"Use Cases\n","excerpt":"Use Cases\n","ref":"/csm-docs/v3/applicationmobility/use_cases/","tags":"","title":"Use Cases"},{"body":"Vault Server Installation If there is already a Vault server available, skip to Minimum Server Configuration.\nIf there is no Vault server available to use with Encryption, it can be installed in many ways following Hashicorp Vault documentation.\nFor testing environment, however, a simple deployment suggested in this section may suffice. It creates a standalone server with in-memory (non-persistent) storage, running in a Docker container.\nNOTE: With in-memory storage, the encryption keys are permanently destroyed upon the server termination.\nGenerate TLS certificates for server and client Create server CA private key and certificate:\nopenssl req -x509 -sha256 -days 365 -newkey rsa:2048 -nodes \\ -subj \"/CN=Vault Root CA\" \\ -keyout server-ca.key \\ -out server-ca.crt Create server private key and CSR:\nopenssl req -newkey rsa:2048 -nodes \\ -subj \"/CN=vault-demo-server\" \\ -keyout server.key \\ -out server.csr Create server certificate signed by the CA:\nReplace \u003cexternal IP\u003e with an IP address by which Encryption can reach the Vault server. This may be the address of the Docker host where the Vault server will be running. The same address should be used for vault_addr in vault-client-conf.\ncat \u003e cert.ext \u003c\u003cEOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE subjectAltName = @alt_names [alt_names] DNS.1 = vault-demo-server IP.1 = 127.0.0.1 IP.2 = \u003cexternal IP\u003e EOF openssl x509 -req \\ -CA server-ca.crt -CAkey server-ca.key \\ -in server.csr \\ -out server.crt \\ -days 365 \\ -extfile cert.ext \\ -CAcreateserial cat server-ca.crt \u003e\u003e server.crt Create client CA private key and certificate:\nopenssl req -x509 -sha256 -days 365 -newkey rsa:2048 -nodes \\ -subj \"/CN=Client Root CA\" \\ -keyout client-ca.key \\ -out client-ca.crt Create client private key and CSR:\nopenssl req -newkey rsa:2048 -nodes \\ -subj \"/CN=vault-client\" \\ -keyout client.key \\ -out client.csr Create client certificate signed by the CA:\ncat \u003e cert.ext \u003c\u003cEOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE subjectAltName = @alt_names [alt_names] DNS.1 = vault-client IP.1 = 127.0.0.1 EOF openssl x509 -req \\ -CA client-ca.crt -CAkey client-ca.key \\ -in client.csr \\ -out client.crt \\ -days 365 \\ -extfile cert.ext \\ -CAcreateserial cat client-ca.crt \u003e\u003e client.crt Create server hcl file cat \u003eserver.hcl \u003c\u003cEOF storage \"inmem\" {} listener \"tcp\" { address = \"0.0.0.0:8400\" tls_disable = \"false\" tls_cipher_suites = \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" tls_min_version = \"tls12\" tls_cert_file = \"/var/vault/server.crt\" tls_key_file = \"/var/vault/server.key\" tls_client_ca_file = \"/var/vault/client-ca.crt\" tls_require_and_verify_client_cert = \"true\" } disable_mlock = true api_addr = \"http://127.0.0.1:8200\" ui = true EOF Start Vault Server Variable CONF_DIR below refers to the directory containing files server.crt, server.key, client-ca.crt and server.hcl.\nVOL_DIR=\"$CONF_DIR\" VOL_DIR_D=\"/var/vault\" ROOT_TOKEN=\"DemoRootToken\" VAULT_IMG=\"vault:1.9.3\" docker run --rm -d \\ --name=\"vault-server\" \\ -p 8200:8200 -p 8400:8400 \\ -v $VOL_DIR:$VOL_DIR_D -w $VOL_DIR_D \\ -e VAULT_DEV_ROOT_TOKEN_ID=$ROOT_TOKEN \\ -e VAULT_ADDR=\"http://127.0.0.1:8200\" \\ -e VAULT_TOKEN=$ROOT_TOKEN \\ $VAULT_IMG \\ sh -c 'vault server -dev -dev-listen-address 0.0.0.0:8200 -config=server.hcl' Minimum Server Configuration NOTE: this configuration is a bare minimum to support Encryption and is not intended for use in production environment. Refer to the Hashicorp Vault documentation for recommended configuration options.\nIf a test instance of Vault is used, the vault commands below can be executed in the Vault server container shell. To enter the shell, run docker exec -it vault-server sh. After completing the configuration process, exit the shell by typing exit.\nAlternatively, you can download the vault binary and run it anywhere. It will require two environment variables to communicate with the Vault server:\nVAULT_ADDR - URL similar to http://127.0.0.1:8200. You may need to change the address in the URL to the address of the Docker host where the server is running. VAULT_TOKEN - Authentication token, e.g. the root token DemoRootToken used in the test instance of Vault. Enable Key/Value secret engine vault secrets enable -version=2 -path=dea-keys/ kv vault write /dea-keys/config cas_required=true max_versions=1 Key/Value secret engine is used to store encryption keys. Each encryption key is represented by a key-value entry.\nEnable AppRole authentication vault auth enable approle Create a role vault write auth/approle/role/dea-role \\ secret_id_ttl=28d \\ token_num_uses=0 \\ token_ttl=1h \\ token_max_ttl=1h \\ token_explicit_max_ttl=10d \\ secret_id_num_uses=0 TTL values here are chosen arbitrarily and can be changed to desired values.\nCreate and assign a token policy to the role vault policy write dea-policy - \u003c\u003cEOF path \"dea-keys/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } path \"dea-keys/config\" { capabilities = [\"read\"] } EOF vault write auth/approle/role/dea-role token_policies=\"dea-policy\" The role needs full access to the Key/Value secret engine to perform encryption key operations. The role only needs read access to the Key/Value secret engine config endpoint for configuration validation.\nSet role ID and secret ID to the role Role ID and secret ID are normally generated by Vault. To get the generated values, run these commands:\nvault read auth/approle/role/dea-role/role-id vault write -f auth/approle/role/dea-role/secret-id In test environment, pre-defined role ID and secret ID can be set, for example demo-role-id and demo-secret-id:\nvault write auth/approle/role/dea-role/role-id role_id=demo-role-id vault write auth/approle/role/dea-role/custom-secret-id secret_id=demo-secret-id Secret ID has an expiration time after which it becomes invalid resulting in authorization failure. The expiration time for new secret IDs can be set in secret_id_ttl parameter when the role is created or later on using vault write auth/approle/role/dea-role/secret-id-ttl secret_id_ttl=24h.\nToken TTL Considerations Effective client token TTL is determined by the Vault server based on multiple factors which are described in the Vault documentation.\nWith the default server settings, role level values control TTL in this way:\ntoken_explicit_max_ttl=2h - limits the client token TTL to 2 hours since it was originally issues as a result of login. This is a hard limit.\ntoken_ttl=30m - sets the default client token TTL to 30 minutes. 30 minutes are counted from the login time and from any following token renewal. The client token will only be able to renew 3 times before reaching it total allowed TTL of 2 hours.\nExisting role values can be changed using vault write auth/approle/role/dea-role token_ttl=30m token_explicit_max_ttl=2h.\nSelecting too short TTL values will result in excessive overhead in Encryption to remain authenticated to the Vault server.\n","categories":"","description":"Configuration requirements for Vault server\n","excerpt":"Configuration requirements for Vault server\n","ref":"/csm-docs/docs/secure/encryption/vault/","tags":"","title":"Vault Configuration"},{"body":"Vault Server Installation If there is already a Vault server available, skip to Minimum Server Configuration.\nIf there is no Vault server available to use with Encryption, it can be installed in many ways following Hashicorp Vault documentation.\nFor testing environment, however, a simple deployment suggested in this section may suffice. It creates a standalone server with in-memory (non-persistent) storage, running in a Docker container.\nNOTE: With in-memory storage, the encryption keys are permanently destroyed upon the server termination.\nGenerate TLS certificates for server and client Create server CA private key and certificate:\nopenssl req -x509 -sha256 -days 365 -newkey rsa:2048 -nodes \\ -subj \"/CN=Vault Root CA\" \\ -keyout server-ca.key \\ -out server-ca.crt Create server private key and CSR:\nopenssl req -newkey rsa:2048 -nodes \\ -subj \"/CN=vault-demo-server\" \\ -keyout server.key \\ -out server.csr Create server certificate signed by the CA:\nReplace \u003cexternal IP\u003e with an IP address by which Encryption can reach the Vault server. This may be the address of the Docker host where the Vault server will be running. The same address should be used for vault_addr in vault-client-conf.\ncat \u003e cert.ext \u003c\u003cEOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE subjectAltName = @alt_names [alt_names] DNS.1 = vault-demo-server IP.1 = 127.0.0.1 IP.2 = \u003cexternal IP\u003e EOF openssl x509 -req \\ -CA server-ca.crt -CAkey server-ca.key \\ -in server.csr \\ -out server.crt \\ -days 365 \\ -extfile cert.ext \\ -CAcreateserial cat server-ca.crt \u003e\u003e server.crt Create client CA private key and certificate:\nopenssl req -x509 -sha256 -days 365 -newkey rsa:2048 -nodes \\ -subj \"/CN=Client Root CA\" \\ -keyout client-ca.key \\ -out client-ca.crt Create client private key and CSR:\nopenssl req -newkey rsa:2048 -nodes \\ -subj \"/CN=vault-client\" \\ -keyout client.key \\ -out client.csr Create client certificate signed by the CA:\ncat \u003e cert.ext \u003c\u003cEOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE subjectAltName = @alt_names [alt_names] DNS.1 = vault-client IP.1 = 127.0.0.1 EOF openssl x509 -req \\ -CA client-ca.crt -CAkey client-ca.key \\ -in client.csr \\ -out client.crt \\ -days 365 \\ -extfile cert.ext \\ -CAcreateserial cat client-ca.crt \u003e\u003e client.crt Create server hcl file cat \u003eserver.hcl \u003c\u003cEOF storage \"inmem\" {} listener \"tcp\" { address = \"0.0.0.0:8400\" tls_disable = \"false\" tls_cipher_suites = \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" tls_min_version = \"tls12\" tls_cert_file = \"/var/vault/server.crt\" tls_key_file = \"/var/vault/server.key\" tls_client_ca_file = \"/var/vault/client-ca.crt\" tls_require_and_verify_client_cert = \"true\" } disable_mlock = true api_addr = \"http://127.0.0.1:8200\" ui = true EOF Start Vault Server Variable CONF_DIR below refers to the directory containing files server.crt, server.key, client-ca.crt and server.hcl.\nVOL_DIR=\"$CONF_DIR\" VOL_DIR_D=\"/var/vault\" ROOT_TOKEN=\"DemoRootToken\" VAULT_IMG=\"vault:1.9.3\" docker run --rm -d \\ --name=\"vault-server\" \\ -p 8200:8200 -p 8400:8400 \\ -v $VOL_DIR:$VOL_DIR_D -w $VOL_DIR_D \\ -e VAULT_DEV_ROOT_TOKEN_ID=$ROOT_TOKEN \\ -e VAULT_ADDR=\"http://127.0.0.1:8200\" \\ -e VAULT_TOKEN=$ROOT_TOKEN \\ $VAULT_IMG \\ sh -c 'vault server -dev -dev-listen-address 0.0.0.0:8200 -config=server.hcl' Minimum Server Configuration NOTE: this configuration is a bare minimum to support Encryption and is not intended for use in production environment. Refer to the Hashicorp Vault documentation for recommended configuration options.\nIf a test instance of Vault is used, the vault commands below can be executed in the Vault server container shell. To enter the shell, run docker exec -it vault-server sh. After completing the configuration process, exit the shell by typing exit.\nAlternatively, you can download the vault binary and run it anywhere. It will require two environment variables to communicate with the Vault server:\nVAULT_ADDR - URL similar to http://127.0.0.1:8200. You may need to change the address in the URL to the address of the Docker host where the server is running. VAULT_TOKEN - Authentication token, e.g. the root token DemoRootToken used in the test instance of Vault. Enable Key/Value secret engine vault secrets enable -version=2 -path=dea-keys/ kv vault write /dea-keys/config cas_required=true max_versions=1 Key/Value secret engine is used to store encryption keys. Each encryption key is represented by a key-value entry.\nEnable AppRole authentication vault auth enable approle Create a role vault write auth/approle/role/dea-role \\ secret_id_ttl=28d \\ token_num_uses=0 \\ token_ttl=1h \\ token_max_ttl=1h \\ token_explicit_max_ttl=10d \\ secret_id_num_uses=0 TTL values here are chosen arbitrarily and can be changed to desired values.\nCreate and assign a token policy to the role vault policy write dea-policy - \u003c\u003cEOF path \"dea-keys/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } path \"dea-keys/config\" { capabilities = [\"read\"] } EOF vault write auth/approle/role/dea-role token_policies=\"dea-policy\" The role needs full access to the Key/Value secret engine to perform encryption key operations. The role only needs read access to the Key/Value secret engine config endpoint for configuration validation.\nSet role ID and secret ID to the role Role ID and secret ID are normally generated by Vault. To get the generated values, run these commands:\nvault read auth/approle/role/dea-role/role-id vault write -f auth/approle/role/dea-role/secret-id In test environment, pre-defined role ID and secret ID can be set, for example demo-role-id and demo-secret-id:\nvault write auth/approle/role/dea-role/role-id role_id=demo-role-id vault write auth/approle/role/dea-role/custom-secret-id secret_id=demo-secret-id Secret ID has an expiration time after which it becomes invalid resulting in authorization failure. The expiration time for new secret IDs can be set in secret_id_ttl parameter when the role is created or later on using vault write auth/approle/role/dea-role/secret-id-ttl secret_id_ttl=24h.\nToken TTL Considerations Effective client token TTL is determined by the Vault server based on multiple factors which are described in the Vault documentation.\nWith the default server settings, role level values control TTL in this way:\ntoken_explicit_max_ttl=2h - limits the client token TTL to 2 hours since it was originally issues as a result of login. This is a hard limit.\ntoken_ttl=30m - sets the default client token TTL to 30 minutes. 30 minutes are counted from the login time and from any following token renewal. The client token will only be able to renew 3 times before reaching it total allowed TTL of 2 hours.\nExisting role values can be changed using vault write auth/approle/role/dea-role token_ttl=30m token_explicit_max_ttl=2h.\nSelecting too short TTL values will result in excessive overhead in Encryption to remain authenticated to the Vault server.\n","categories":"","description":"Configuration requirements for Vault server\n","excerpt":"Configuration requirements for Vault server\n","ref":"/csm-docs/v1/secure/encryption/vault/","tags":"","title":"Vault Configuration"},{"body":"Vault Server Installation If there is already a Vault server available, skip to Minimum Server Configuration.\nIf there is no Vault server available to use with Encryption, it can be installed in many ways following Hashicorp Vault documentation.\nFor testing environment, however, a simple deployment suggested in this section may suffice. It creates a standalone server with in-memory (non-persistent) storage, running in a Docker container.\nNOTE: With in-memory storage, the encryption keys are permanently destroyed upon the server termination.\nGenerate TLS certificates for server and client Create server CA private key and certificate:\nopenssl req -x509 -sha256 -days 365 -newkey rsa:2048 -nodes \\ -subj \"/CN=Vault Root CA\" \\ -keyout server-ca.key \\ -out server-ca.crt Create server private key and CSR:\nopenssl req -newkey rsa:2048 -nodes \\ -subj \"/CN=vault-demo-server\" \\ -keyout server.key \\ -out server.csr Create server certificate signed by the CA:\nReplace \u003cexternal IP\u003e with an IP address by which Encryption can reach the Vault server. This may be the address of the Docker host where the Vault server will be running. The same address should be used for vault_addr in vault-client-conf.\ncat \u003e cert.ext \u003c\u003cEOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE subjectAltName = @alt_names [alt_names] DNS.1 = vault-demo-server IP.1 = 127.0.0.1 IP.2 = \u003cexternal IP\u003e EOF openssl x509 -req \\ -CA server-ca.crt -CAkey server-ca.key \\ -in server.csr \\ -out server.crt \\ -days 365 \\ -extfile cert.ext \\ -CAcreateserial cat server-ca.crt \u003e\u003e server.crt Create client CA private key and certificate:\nopenssl req -x509 -sha256 -days 365 -newkey rsa:2048 -nodes \\ -subj \"/CN=Client Root CA\" \\ -keyout client-ca.key \\ -out client-ca.crt Create client private key and CSR:\nopenssl req -newkey rsa:2048 -nodes \\ -subj \"/CN=vault-client\" \\ -keyout client.key \\ -out client.csr Create client certificate signed by the CA:\ncat \u003e cert.ext \u003c\u003cEOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE subjectAltName = @alt_names [alt_names] DNS.1 = vault-client IP.1 = 127.0.0.1 EOF openssl x509 -req \\ -CA client-ca.crt -CAkey client-ca.key \\ -in client.csr \\ -out client.crt \\ -days 365 \\ -extfile cert.ext \\ -CAcreateserial cat client-ca.crt \u003e\u003e client.crt Create server hcl file cat \u003eserver.hcl \u003c\u003cEOF storage \"inmem\" {} listener \"tcp\" { address = \"0.0.0.0:8400\" tls_disable = \"false\" tls_cipher_suites = \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" tls_min_version = \"tls12\" tls_cert_file = \"/var/vault/server.crt\" tls_key_file = \"/var/vault/server.key\" tls_client_ca_file = \"/var/vault/client-ca.crt\" tls_require_and_verify_client_cert = \"true\" } disable_mlock = true api_addr = \"http://127.0.0.1:8200\" ui = true EOF Start Vault Server Variable CONF_DIR below refers to the directory containing files server.crt, server.key, client-ca.crt and server.hcl.\nVOL_DIR=\"$CONF_DIR\" VOL_DIR_D=\"/var/vault\" ROOT_TOKEN=\"DemoRootToken\" VAULT_IMG=\"vault:1.9.3\" docker run --rm -d \\ --name=\"vault-server\" \\ -p 8200:8200 -p 8400:8400 \\ -v $VOL_DIR:$VOL_DIR_D -w $VOL_DIR_D \\ -e VAULT_DEV_ROOT_TOKEN_ID=$ROOT_TOKEN \\ -e VAULT_ADDR=\"http://127.0.0.1:8200\" \\ -e VAULT_TOKEN=$ROOT_TOKEN \\ $VAULT_IMG \\ sh -c 'vault server -dev -dev-listen-address 0.0.0.0:8200 -config=server.hcl' Minimum Server Configuration NOTE: this configuration is a bare minimum to support Encryption and is not intended for use in production environment. Refer to the Hashicorp Vault documentation for recommended configuration options.\nIf a test instance of Vault is used, the vault commands below can be executed in the Vault server container shell. To enter the shell, run docker exec -it vault-server sh. After completing the configuration process, exit the shell by typing exit.\nAlternatively, you can download the vault binary and run it anywhere. It will require two environment variables to communicate with the Vault server:\nVAULT_ADDR - URL similar to http://127.0.0.1:8200. You may need to change the address in the URL to the address of the Docker host where the server is running. VAULT_TOKEN - Authentication token, e.g. the root token DemoRootToken used in the test instance of Vault. Enable Key/Value secret engine vault secrets enable -version=2 -path=dea-keys/ kv vault write /dea-keys/config cas_required=true max_versions=1 Key/Value secret engine is used to store encryption keys. Each encryption key is represented by a key-value entry.\nEnable AppRole authentication vault auth enable approle Create a role vault write auth/approle/role/dea-role \\ secret_id_ttl=28d \\ token_num_uses=0 \\ token_ttl=1h \\ token_max_ttl=1h \\ token_explicit_max_ttl=10d \\ secret_id_num_uses=0 TTL values here are chosen arbitrarily and can be changed to desired values.\nCreate and assign a token policy to the role vault policy write dea-policy - \u003c\u003cEOF path \"dea-keys/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } path \"dea-keys/config\" { capabilities = [\"read\"] } EOF vault write auth/approle/role/dea-role token_policies=\"dea-policy\" The role needs full access to the Key/Value secret engine to perform encryption key operations. The role only needs read access to the Key/Value secret engine config endpoint for configuration validation.\nSet role ID and secret ID to the role Role ID and secret ID are normally generated by Vault. To get the generated values, run these commands:\nvault read auth/approle/role/dea-role/role-id vault write -f auth/approle/role/dea-role/secret-id In test environment, pre-defined role ID and secret ID can be set, for example demo-role-id and demo-secret-id:\nvault write auth/approle/role/dea-role/role-id role_id=demo-role-id vault write auth/approle/role/dea-role/custom-secret-id secret_id=demo-secret-id Secret ID has an expiration time after which it becomes invalid resulting in authorization failure. The expiration time for new secret IDs can be set in secret_id_ttl parameter when the role is created or later on using vault write auth/approle/role/dea-role/secret-id-ttl secret_id_ttl=24h.\nToken TTL Considerations Effective client token TTL is determined by the Vault server based on multiple factors which are described in the Vault documentation.\nWith the default server settings, role level values control TTL in this way:\ntoken_explicit_max_ttl=2h - limits the client token TTL to 2 hours since it was originally issues as a result of login. This is a hard limit.\ntoken_ttl=30m - sets the default client token TTL to 30 minutes. 30 minutes are counted from the login time and from any following token renewal. The client token will only be able to renew 3 times before reaching it total allowed TTL of 2 hours.\nExisting role values can be changed using vault write auth/approle/role/dea-role token_ttl=30m token_explicit_max_ttl=2h.\nSelecting too short TTL values will result in excessive overhead in Encryption to remain authenticated to the Vault server.\n","categories":"","description":"Configuration requirements for Vault server\n","excerpt":"Configuration requirements for Vault server\n","ref":"/csm-docs/v2/secure/encryption/vault/","tags":"","title":"Vault Configuration"},{"body":"Vault Server Installation If there is already a Vault server available, skip to Minimum Server Configuration.\nIf there is no Vault server available to use with Encryption, it can be installed in many ways following Hashicorp Vault documentation.\nFor testing environment, however, a simple deployment suggested in this section may suffice. It creates a standalone server with in-memory (non-persistent) storage, running in a Docker container.\nNOTE: With in-memory storage, the encryption keys are permanently destroyed upon the server termination.\nGenerate TLS certificates for server and client Create server CA private key and certificate:\nopenssl req -x509 -sha256 -days 365 -newkey rsa:2048 -nodes \\ -subj \"/CN=Vault Root CA\" \\ -keyout server-ca.key \\ -out server-ca.crt Create server private key and CSR:\nopenssl req -newkey rsa:2048 -nodes \\ -subj \"/CN=vault-demo-server\" \\ -keyout server.key \\ -out server.csr Create server certificate signed by the CA:\nReplace \u003cexternal IP\u003e with an IP address by which Encryption can reach the Vault server. This may be the address of the Docker host where the Vault server will be running. The same address should be used for vault_addr in vault-client-conf.\ncat \u003e cert.ext \u003c\u003cEOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE subjectAltName = @alt_names [alt_names] DNS.1 = vault-demo-server IP.1 = 127.0.0.1 IP.2 = \u003cexternal IP\u003e EOF openssl x509 -req \\ -CA server-ca.crt -CAkey server-ca.key \\ -in server.csr \\ -out server.crt \\ -days 365 \\ -extfile cert.ext \\ -CAcreateserial cat server-ca.crt \u003e\u003e server.crt Create client CA private key and certificate:\nopenssl req -x509 -sha256 -days 365 -newkey rsa:2048 -nodes \\ -subj \"/CN=Client Root CA\" \\ -keyout client-ca.key \\ -out client-ca.crt Create client private key and CSR:\nopenssl req -newkey rsa:2048 -nodes \\ -subj \"/CN=vault-client\" \\ -keyout client.key \\ -out client.csr Create client certificate signed by the CA:\ncat \u003e cert.ext \u003c\u003cEOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE subjectAltName = @alt_names [alt_names] DNS.1 = vault-client IP.1 = 127.0.0.1 EOF openssl x509 -req \\ -CA client-ca.crt -CAkey client-ca.key \\ -in client.csr \\ -out client.crt \\ -days 365 \\ -extfile cert.ext \\ -CAcreateserial cat client-ca.crt \u003e\u003e client.crt Create server hcl file cat \u003eserver.hcl \u003c\u003cEOF storage \"inmem\" {} listener \"tcp\" { address = \"0.0.0.0:8400\" tls_disable = \"false\" tls_cipher_suites = \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" tls_min_version = \"tls12\" tls_cert_file = \"/var/vault/server.crt\" tls_key_file = \"/var/vault/server.key\" tls_client_ca_file = \"/var/vault/client-ca.crt\" tls_require_and_verify_client_cert = \"true\" } disable_mlock = true api_addr = \"http://127.0.0.1:8200\" ui = true EOF Start Vault Server Variable CONF_DIR below refers to the directory containing files server.crt, server.key, client-ca.crt and server.hcl.\nVOL_DIR=\"$CONF_DIR\" VOL_DIR_D=\"/var/vault\" ROOT_TOKEN=\"DemoRootToken\" VAULT_IMG=\"vault:1.9.3\" docker run --rm -d \\ --name=\"vault-server\" \\ -p 8200:8200 -p 8400:8400 \\ -v $VOL_DIR:$VOL_DIR_D -w $VOL_DIR_D \\ -e VAULT_DEV_ROOT_TOKEN_ID=$ROOT_TOKEN \\ -e VAULT_ADDR=\"http://127.0.0.1:8200\" \\ -e VAULT_TOKEN=$ROOT_TOKEN \\ $VAULT_IMG \\ sh -c 'vault server -dev -dev-listen-address 0.0.0.0:8200 -config=server.hcl' Minimum Server Configuration NOTE: this configuration is a bare minimum to support Encryption and is not intended for use in production environment. Refer to the Hashicorp Vault documentation for recommended configuration options.\nIf a test instance of Vault is used, the vault commands below can be executed in the Vault server container shell. To enter the shell, run docker exec -it vault-server sh. After completing the configuration process, exit the shell by typing exit.\nAlternatively, you can download the vault binary and run it anywhere. It will require two environment variables to communicate with the Vault server:\nVAULT_ADDR - URL similar to http://127.0.0.1:8200. You may need to change the address in the URL to the address of the Docker host where the server is running. VAULT_TOKEN - Authentication token, e.g. the root token DemoRootToken used in the test instance of Vault. Enable Key/Value secret engine vault secrets enable -version=2 -path=dea-keys/ kv vault write /dea-keys/config cas_required=true max_versions=1 Key/Value secret engine is used to store encryption keys. Each encryption key is represented by a key-value entry.\nEnable AppRole authentication vault auth enable approle Create a role vault write auth/approle/role/dea-role \\ secret_id_ttl=28d \\ token_num_uses=0 \\ token_ttl=1h \\ token_max_ttl=1h \\ token_explicit_max_ttl=10d \\ secret_id_num_uses=0 TTL values here are chosen arbitrarily and can be changed to desired values.\nCreate and assign a token policy to the role vault policy write dea-policy - \u003c\u003cEOF path \"dea-keys/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } path \"dea-keys/config\" { capabilities = [\"read\"] } EOF vault write auth/approle/role/dea-role token_policies=\"dea-policy\" The role needs full access to the Key/Value secret engine to perform encryption key operations. The role only needs read access to the Key/Value secret engine config endpoint for configuration validation.\nSet role ID and secret ID to the role Role ID and secret ID are normally generated by Vault. To get the generated values, run these commands:\nvault read auth/approle/role/dea-role/role-id vault write -f auth/approle/role/dea-role/secret-id In test environment, pre-defined role ID and secret ID can be set, for example demo-role-id and demo-secret-id:\nvault write auth/approle/role/dea-role/role-id role_id=demo-role-id vault write auth/approle/role/dea-role/custom-secret-id secret_id=demo-secret-id Secret ID has an expiration time after which it becomes invalid resulting in authorization failure. The expiration time for new secret IDs can be set in secret_id_ttl parameter when the role is created or later on using vault write auth/approle/role/dea-role/secret-id-ttl secret_id_ttl=24h.\nToken TTL Considerations Effective client token TTL is determined by the Vault server based on multiple factors which are described in the Vault documentation.\nWith the default server settings, role level values control TTL in this way:\ntoken_explicit_max_ttl=2h - limits the client token TTL to 2 hours since it was originally issues as a result of login. This is a hard limit.\ntoken_ttl=30m - sets the default client token TTL to 30 minutes. 30 minutes are counted from the login time and from any following token renewal. The client token will only be able to renew 3 times before reaching it total allowed TTL of 2 hours.\nExisting role values can be changed using vault write auth/approle/role/dea-role token_ttl=30m token_explicit_max_ttl=2h.\nSelecting too short TTL values will result in excessive overhead in Encryption to remain authenticated to the Vault server.\n","categories":"","description":"Configuration requirements for Vault server\n","excerpt":"Configuration requirements for Vault server\n","ref":"/csm-docs/v3/secure/encryption/vault/","tags":"","title":"Vault Configuration"},{"body":"Container Storage Modules (CSM) for Authorization is part of the open-source suite of Kubernetes storage enablers for Dell products.\nCSM for Authorization provides storage and Kubernetes administrators the ability to apply RBAC for Dell CSI Drivers. It does this by deploying a proxy between the CSI driver and the storage system to enforce role-based access and usage rules.\nStorage administrators of compatible storage platforms will be able to apply quota and RBAC rules that instantly and automatically restrict cluster tenants usage of storage resources. Users of storage through CSM for Authorization do not need to have storage admin root credentials to access the storage system.\nKubernetes administrators will have an interface to create, delete, and manage roles/groups that storage rules may be applied. Administrators and/or users may then generate authentication tokens that may be used by tenants to use storage with proper access policies being automatically enforced.\nThe following diagram shows a high-level overview of CSM for Authorization with a tenant-app that is using a CSI driver to perform storage operations through the CSM for Authorization proxy-server to access the a Dell storage system. All requests from the CSI driver will contain the token for the given tenant that was granted by the Storage Administrator.\nCSM for Authorization Capabilities Feature PowerFlex PowerMax PowerScale Unity XT PowerStore Ability to set storage quota limits to ensure k8s tenants are not overconsuming storage Yes Yes No (natively supported) No No Ability to create access control policies to ensure k8s tenant clusters are not accessing storage that does not belong to them Yes Yes No (natively supported) No No Ability to shield storage credentials from Kubernetes administrators ensuring credentials are only handled by storage admins Yes Yes Yes No No NOTE: PowerScale OneFS implements its own form of Role-Based Access Control (RBAC). CSM for Authorization does not enforce any role-based restrictions for PowerScale. To configure RBAC for PowerScale, refer to the PowerScale OneFS documentation.\nSupported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.26, 1.27, 1.28 Supported Storage Platforms PowerMax PowerFlex PowerScale Storage Array PowerMax 2500/8500 PowerMaxOS 10 (6079) , PowerMaxOS 10.0.1 (6079) , PowerMaxOS 10.1 (6079)\nPowerMax 2000/8000 - 5978.711.xxx, 5978.479.xxx,\nUnisphere 10.0, 10.0.1, 10.1 3.6.x, 4.0.x, 4.5 OneFS 9.5.0.x (x \u003e= 5) Supported CSI Drivers CSM for Authorization supports the following CSI drivers and versions. Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerFlex csi-powerflex v2.0 + CSI Driver for Dell PowerMax csi-powermax v2.0 + CSI Driver for Dell PowerScale csi-powerscale v2.0 + NOTE: If the deployed CSI driver has a number of controller pods equal to the number of schedulable nodes in your cluster, CSM for Authorization may not be able to inject properly into the driver’s controller pod. To resolve this, please refer to our troubleshooting guide on the topic.\nAuthorization Components Support Matrix CSM for Authorization consists of 2 components - The authorization sidecar, bundled with the driver, communicates with the Authorization proxy server to validate access to Storage platforms. The authorization sidecar is backward compatible with older Authorization proxy server versions. However, it is highly recommended to have the Authorization proxy server and sidecar installed from the same release of CSM.\nRoles and Responsibilities The CSM for Authorization CLI can be executed in the context of the following roles:\nStorage Administrators Kubernetes Tenant Administrators Storage Administrators Storage Administrators can perform the following operations within CSM for Authorization\nTenant Management (create, get, list, delete, bind roles, unbind roles) Token Management (generate, revoke) Storage System Management (create, get, list, update, delete) Storage Access Roles Management (assign to a storage system with an optional quota) Tenant Administrators Tenants of CSM for Authorization can use the token provided by the Storage Administrators in their storage requests.\nWorkflow Tenant Admin requests storage from a Storage Admin. Storage Admin uses CSM Authorization CLI to:\na) Create a tenant resource.\nb) Create a role permitting desired storage access.\nc) Assign the role to the tenant and generate a token.\nStorage Admin returns a token to the Tenant Admin. Tenant Admin inputs the Token into their Kubernetes cluster as a Secret. Tenant Admin updates CSI driver with CSM Authorization sidecar module. ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/authorization/","tags":"","title":"Authorization"},{"body":"Container Storage Modules (CSM) for Authorization is part of the open-source suite of Kubernetes storage enablers for Dell products.\nCSM for Authorization provides storage and Kubernetes administrators the ability to apply RBAC for Dell CSI Drivers. It does this by deploying a proxy between the CSI driver and the storage system to enforce role-based access and usage rules.\nStorage administrators of compatible storage platforms will be able to apply quota and RBAC rules that instantly and automatically restrict cluster tenants usage of storage resources. Users of storage through CSM for Authorization do not need to have storage admin root credentials to access the storage system.\nKubernetes administrators will have an interface to create, delete, and manage roles/groups that storage rules may be applied. Administrators and/or users may then generate authentication tokens that may be used by tenants to use storage with proper access policies being automatically enforced.\nThe following diagram shows a high-level overview of CSM for Authorization with a tenant-app that is using a CSI driver to perform storage operations through the CSM for Authorization proxy-server to access the a Dell storage system. All requests from the CSI driver will contain the token for the given tenant that was granted by the Storage Administrator.\nCSM for Authorization Capabilities Feature PowerFlex PowerMax PowerScale Unity XT PowerStore Ability to set storage quota limits to ensure k8s tenants are not overconsuming storage Yes Yes No (natively supported) No No Ability to create access control policies to ensure k8s tenant clusters are not accessing storage that does not belong to them Yes Yes No (natively supported) No No Ability to shield storage credentials from Kubernetes administrators ensuring credentials are only handled by storage admins Yes Yes Yes No No NOTE: PowerScale OneFS implements its own form of Role-Based Access Control (RBAC). CSM for Authorization does not enforce any role-based restrictions for PowerScale. To configure RBAC for PowerScale, refer to the PowerScale OneFS documentation.\nSupported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.25, 1.26, 1.27 RHEL 7.x, 8.x CentOS 7.8, 7.9 Supported Storage Platforms PowerMax PowerFlex PowerScale Storage Array PowerMax 2500/8500 PowerMaxOS 10 (6079) , PowerMaxOS 10.0.1 (6079) PowerMax 2000/8000 - 5978.711.xxx, 5978.479.xxx,\nUnisphere 10.0, 10.0.1 3.5.x, 3.6.x OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4, 9.5 Supported CSI Drivers CSM for Authorization supports the following CSI drivers and versions. Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerFlex csi-powerflex v2.0 + CSI Driver for Dell PowerMax csi-powermax v2.0 + CSI Driver for Dell PowerScale csi-powerscale v2.0 + NOTE: If the deployed CSI driver has a number of controller pods equal to the number of schedulable nodes in your cluster, CSM for Authorization may not be able to inject properly into the driver’s controller pod. To resolve this, please refer to our troubleshooting guide on the topic.\nAuthorization Components Support Matrix CSM for Authorization consists of 2 components - the Authorization sidecar and the Authorization proxy server. It is important that the version of the Authorization sidecar image maps to a supported version of the Authorization proxy server.\nAuthorization Sidecar Image Tag Authorization Proxy Server Version dellemc/csm-authorization-sidecar:v1.0.0 v1.0.0, v1.1.0 dellemc/csm-authorization-sidecar:v1.2.0 v1.1.0, v1.2.0 dellemc/csm-authorization-sidecar:v1.3.0 v1.1.0, v1.2.0, v1.3.0 dellemc/csm-authorization-sidecar:v1.4.0 v1.1.0, v1.2.0, v1.3.0, v1.4.0 dellemc/csm-authorization-sidecar:v1.5.0 v1.1.0, v1.2.0, v1.3.0, v1.4.0, v1.5.0 dellemc/csm-authorization-sidecar:v1.5.1 v1.1.0, v1.2.0, v1.3.0, v1.4.0, v1.5.0, v1.5.1 dellemc/csm-authorization-sidecar:v1.6.0 v1.1.0, v1.2.0, v1.3.0, v1.4.0, v1.5.0, v1.5.1, v1.6.0 dellemc/csm-authorization-sidecar:v1.7.0 v1.1.0, v1.2.0, v1.3.0, v1.4.0, v1.5.0, v1.5.1, v1.6.0, v1.7.0 dellemc/csm-authorization-sidecar:v1.8.0 v1.1.0, v1.2.0, v1.3.0, v1.4.0, v1.5.0, v1.5.1, v1.6.0, v1.7.0, v1.8.0 Roles and Responsibilities The CSM for Authorization CLI can be executed in the context of the following roles:\nStorage Administrators Kubernetes Tenant Administrators Storage Administrators Storage Administrators can perform the following operations within CSM for Authorization\nTenant Management (create, get, list, delete, bind roles, unbind roles) Token Management (generate, revoke) Storage System Management (create, get, list, update, delete) Storage Access Roles Management (assign to a storage system with an optional quota) Tenant Administrators Tenants of CSM for Authorization can use the token provided by the Storage Administrators in their storage requests.\nWorkflow Tenant Admin requests storage from a Storage Admin. Storage Admin uses CSM Authorization CLI to:\na) Create a tenant resource.\nb) Create a role permitting desired storage access.\nc) Assign the role to the tenant and generate a token.\nStorage Admin returns a token to the Tenant Admin. Tenant Admin inputs the Token into their Kubernetes cluster as a Secret. Tenant Admin updates CSI driver with CSM Authorization sidecar module. ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v1/authorization/","tags":"","title":"Authorization"},{"body":"Container Storage Modules (CSM) for Authorization is part of the open-source suite of Kubernetes storage enablers for Dell products.\nCSM for Authorization provides storage and Kubernetes administrators the ability to apply RBAC for Dell CSI Drivers. It does this by deploying a proxy between the CSI driver and the storage system to enforce role-based access and usage rules.\nStorage administrators of compatible storage platforms will be able to apply quota and RBAC rules that instantly and automatically restrict cluster tenants usage of storage resources. Users of storage through CSM for Authorization do not need to have storage admin root credentials to access the storage system.\nKubernetes administrators will have an interface to create, delete, and manage roles/groups that storage rules may be applied. Administrators and/or users may then generate authentication tokens that may be used by tenants to use storage with proper access policies being automatically enforced.\nThe following diagram shows a high-level overview of CSM for Authorization with a tenant-app that is using a CSI driver to perform storage operations through the CSM for Authorization proxy-server to access the a Dell storage system. All requests from the CSI driver will contain the token for the given tenant that was granted by the Storage Administrator.\nCSM for Authorization Capabilities Feature PowerFlex PowerMax PowerScale Unity XT PowerStore Ability to set storage quota limits to ensure k8s tenants are not overconsuming storage Yes Yes No (natively supported) No No Ability to create access control policies to ensure k8s tenant clusters are not accessing storage that does not belong to them Yes Yes No (natively supported) No No Ability to shield storage credentials from Kubernetes administrators ensuring credentials are only handled by storage admins Yes Yes Yes No No NOTE: PowerScale OneFS implements its own form of Role-Based Access Control (RBAC). CSM for Authorization does not enforce any role-based restrictions for PowerScale. To configure RBAC for PowerScale, refer to the PowerScale OneFS documentation.\nSupported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.25, 1.26, 1.27 RHEL 7.x, 8.x CentOS 7.8, 7.9 Supported Storage Platforms PowerMax PowerFlex PowerScale Storage Array PowerMax 2500/8500 PowerMaxOS 10 (6079) , PowerMaxOS 10.0.1 (6079) PowerMax 2000/8000 - 5978.711.xxx, 5978.479.xxx,\nUnisphere 10.0, 10.0.1 3.5.x, 3.6.x OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4 Supported CSI Drivers CSM for Authorization supports the following CSI drivers and versions. Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerFlex csi-powerflex v2.0 + CSI Driver for Dell PowerMax csi-powermax v2.0 + CSI Driver for Dell PowerScale csi-powerscale v2.0 + NOTE: If the deployed CSI driver has a number of controller pods equal to the number of schedulable nodes in your cluster, CSM for Authorization may not be able to inject properly into the driver’s controller pod. To resolve this, please refer to our troubleshooting guide on the topic.\nAuthorization Components Support Matrix CSM for Authorization consists of 2 components - the Authorization sidecar and the Authorization proxy server. It is important that the version of the Authorization sidecar image maps to a supported version of the Authorization proxy server.\nAuthorization Sidecar Image Tag Authorization Proxy Server Version dellemc/csm-authorization-sidecar:v1.0.0 v1.0.0, v1.1.0 dellemc/csm-authorization-sidecar:v1.2.0 v1.1.0, v1.2.0 dellemc/csm-authorization-sidecar:v1.3.0 v1.1.0, v1.2.0, v1.3.0 dellemc/csm-authorization-sidecar:v1.4.0 v1.1.0, v1.2.0, v1.3.0, v1.4.0 dellemc/csm-authorization-sidecar:v1.5.0 v1.1.0, v1.2.0, v1.3.0, v1.4.0, v1.5.0 dellemc/csm-authorization-sidecar:v1.5.1 v1.1.0, v1.2.0, v1.3.0, v1.4.0, v1.5.0, v1.5.1 dellemc/csm-authorization-sidecar:v1.6.0 v1.1.0, v1.2.0, v1.3.0, v1.4.0, v1.5.0, v1.5.1, v1.6.0 dellemc/csm-authorization-sidecar:v1.7.0 v1.1.0, v1.2.0, v1.3.0, v1.4.0, v1.5.0, v1.5.1, v1.6.0, v1.7.0 Roles and Responsibilities The CSM for Authorization CLI can be executed in the context of the following roles:\nStorage Administrators Kubernetes Tenant Administrators Storage Administrators Storage Administrators can perform the following operations within CSM for Authorization\nTenant Management (create, get, list, delete, bind roles, unbind roles) Token Management (generate, revoke) Storage System Management (create, get, list, update, delete) Storage Access Roles Management (assign to a storage system with an optional quota) Tenant Administrators Tenants of CSM for Authorization can use the token provided by the Storage Administrators in their storage requests.\nWorkflow Tenant Admin requests storage from a Storage Admin. Storage Admin uses CSM Authorization CLI to:\na) Create a tenant resource.\nb) Create a role permitting desired storage access.\nc) Assign the role to the tenant and generate a token.\nStorage Admin returns a token to the Tenant Admin. Tenant Admin inputs the Token into their Kubernetes cluster as a Secret. Tenant Admin updates CSI driver with CSM Authorization sidecar module. ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v2/authorization/","tags":"","title":"Authorization"},{"body":"Container Storage Modules (CSM) for Authorization is part of the open-source suite of Kubernetes storage enablers for Dell products.\nCSM for Authorization provides storage and Kubernetes administrators the ability to apply RBAC for Dell CSI Drivers. It does this by deploying a proxy between the CSI driver and the storage system to enforce role-based access and usage rules.\nStorage administrators of compatible storage platforms will be able to apply quota and RBAC rules that instantly and automatically restrict cluster tenants usage of storage resources. Users of storage through CSM for Authorization do not need to have storage admin root credentials to access the storage system.\nKubernetes administrators will have an interface to create, delete, and manage roles/groups that storage rules may be applied. Administrators and/or users may then generate authentication tokens that may be used by tenants to use storage with proper access policies being automatically enforced.\nThe following diagram shows a high-level overview of CSM for Authorization with a tenant-app that is using a CSI driver to perform storage operations through the CSM for Authorization proxy-server to access the a Dell storage system. All requests from the CSI driver will contain the token for the given tenant that was granted by the Storage Administrator.\nCSM for Authorization Capabilities Feature PowerFlex PowerMax PowerScale Unity XT PowerStore Ability to set storage quota limits to ensure k8s tenants are not overconsuming storage Yes Yes No (natively supported) No No Ability to create access control policies to ensure k8s tenant clusters are not accessing storage that does not belong to them Yes Yes No (natively supported) No No Ability to shield storage credentials from Kubernetes administrators ensuring credentials are only handled by storage admins Yes Yes Yes No No NOTE: PowerScale OneFS implements its own form of Role-Based Access Control (RBAC). CSM for Authorization does not enforce any role-based restrictions for PowerScale. To configure RBAC for PowerScale, refer to the PowerScale OneFS documentation.\nSupported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.25, 1.26, 1.27 RHEL 7.x, 8.x CentOS 7.8, 7.9 Supported Storage Platforms PowerMax PowerFlex PowerScale Storage Array PowerMax 2000/8000 PowerMax 2500/8500 5978.479.479, 5978.711.711, 6079.xxx.xxx, Unisphere 10.0 3.5.x, 3.6.x OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4 Supported CSI Drivers CSM for Authorization supports the following CSI drivers and versions. Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerFlex csi-powerflex v2.0 + CSI Driver for Dell PowerMax csi-powermax v2.0 + CSI Driver for Dell PowerScale csi-powerscale v2.0 + NOTE: If the deployed CSI driver has a number of controller pods equal to the number of schedulable nodes in your cluster, CSM for Authorization may not be able to inject properly into the driver’s controller pod. To resolve this, please refer to our troubleshooting guide on the topic.\nAuthorization Components Support Matrix CSM for Authorization consists of 2 components - the Authorization sidecar and the Authorization proxy server. It is important that the version of the Authorization sidecar image maps to a supported version of the Authorization proxy server.\nAuthorization Sidecar Image Tag Authorization Proxy Server Version dellemc/csm-authorization-sidecar:v1.0.0 v1.0.0, v1.1.0 dellemc/csm-authorization-sidecar:v1.2.0 v1.1.0, v1.2.0 dellemc/csm-authorization-sidecar:v1.3.0 v1.1.0, v1.2.0, v1.3.0 dellemc/csm-authorization-sidecar:v1.4.0 v1.1.0, v1.2.0, v1.3.0, v1.4.0 dellemc/csm-authorization-sidecar:v1.5.0 v1.1.0, v1.2.0, v1.3.0, v1.4.0, v1.5.0 dellemc/csm-authorization-sidecar:v1.5.1 v1.1.0, v1.2.0, v1.3.0, v1.4.0, v1.5.0, v1.5.1 dellemc/csm-authorization-sidecar:v1.6.0 v1.1.0, v1.2.0, v1.3.0, v1.4.0, v1.5.0, v1.5.1, v1.6.0 Roles and Responsibilities The CSM for Authorization CLI can be executed in the context of the following roles:\nStorage Administrators Kubernetes Tenant Administrators Storage Administrators Storage Administrators can perform the following operations within CSM for Authorization\nTenant Management (create, get, list, delete, bind roles, unbind roles) Token Management (generate, revoke) Storage System Management (create, get, list, update, delete) Storage Access Roles Management (assign to a storage system with an optional quota) Tenant Administrators Tenants of CSM for Authorization can use the token provided by the Storage Administrators in their storage requests.\nWorkflow Tenant Admin requests storage from a Storage Admin. Storage Admin uses CSM Authorization CLI to:\na) Create a tenant resource.\nb) Create a role permitting desired storage access.\nc) Assign the role to the tenant and generate a token.\nStorage Admin returns a token to the Tenant Admin. Tenant Admin inputs the Token into their Kubernetes cluster as a Secret. Tenant Admin updates CSI driver with CSM Authorization sidecar module. ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v3/authorization/","tags":"","title":"Authorization"},{"body":" The CSM Authorization karavictl CLI is no longer actively maintained or supported. It will be deprecated in CSM 2.0.\nkaravictl is a command-line interface (CLI) used to interact with and manage your Container Storage Modules (CSM) Authorization deployment. This document outlines all karavictl commands, their intended use, options that can be provided to alter their execution, and expected output from those commands.\nIf you feel that something is unclear or missing in this document, please open up an issue.\nCommand Description karavictl karavictl is used to interact with CSM Authorization Server karavictl admin token Generate admin tokens karavictl cluster-info Display the state of resources within the cluster karavictl generate Generate resources for use with CSM karavictl generate token Generate tokens karavictl role Manage role karavictl role get Get role karavictl role list List roles karavictl role create Create one or more CSM roles karavictl role update Update the quota of one or more CSM roles karavictl role delete Delete role karavictl rolebinding Manage role bindings karavictl rolebinding create Create a rolebinding between role and tenant karavictl rolebinding delete Delete a rolebinding between role and tenant karavictl storage Manage storage systems karavictl storage get Get details on a registered storage system karavictl storage list List registered storage systems karavictl storage create Create and register a storage system karavictl storage update Update a registered storage system karavictl storage delete Delete a registered storage system karavictl tenant Manage tenants karavictl tenant create Create a tenant resource within CSM karavictl tenant get Get a tenant resource within CSM karavictl tenant list Lists tenant resources within CSM karavictl tenant revoke Get a tenant resource within CSM karavictl tenant delete Deletes a tenant resource within CSM karavictl tenant update Updates a tenant resource within CSM General Commands karavictl karavictl is used to interact with CSM Authorization Server\nSynopsis karavictl provides security, RBAC, and quota limits for accessing Dell storage products from Kubernetes clusters\nCommands admin Generate admin token for use with CSM Authorization cluster-info Display the state of resources within the cluster completion Generate the autocompletion script for the specified shell generate Generate resources for use with Karavi help Help about any command role Manage roles rolebinding Manage role bindings storage Manage storage systems tenant Manage tenants Options -h, --help Help for karavictl -f, --admin-token Path to admin token file; required for all commands except `admin token` and `cluster-info` --addr Address of the CSM Authorization Proxy Server; required for all commands except `admin token` and `cluster-info` --insecure Skip certificate validation of the CSM Authorization Proxy Server Output Outputs help text\nkaravictl admin token Generate admin tokens\nSynopsis Generate admin token for use with CSM Authorization commands. The tokens output in YAML format, which can be saved in a file.\nkaravictl admin token [flags] Required Flags -n, --name Name of the admin Optional Flags -h, --help Help for token -s, --jwt-signing-secret Specify JWT signing secret, or omit to use stdin --refresh-token-expiration Expiration time of the refresh token, e.g. 48h (default 720h0m0s) --access-token-expiration Expiration time of the access token, e.g. 1m30s (default 1m0s) Output $ karavictl admin token --name admin --access-token-expiration 30s --refresh-token-expiration 120m $ Enter JWT Signing Secret: *********** { \"Access\": \u003cACCESS-TOKEN\u003e, \"Refresh\": \u003cREFRESH-TOKEN\u003e } Alternatively, one can supply JWT signing secret with command.\n$ karavictl admin token --name admin --jwt-signing-secret secret --access-token-expiration 30s --refresh-token-expiration 120m { \"Access\": \u003cACCESS-TOKEN\u003e, \"Refresh\": \u003cREFRESH-TOKEN\u003e } karavictl cluster-info Display the state of resources within the cluster\nSynopsis Prints table of resources within the cluster, including their readiness\nkaravictl cluster-info [flags] Optional Flags -h, --help Help for cluster-info -w, --watch Watch for changes Output karavictl cluster-info NAME READY UP-TO-DATE AVAILABLE AGE tenant-service 1/1 1 1 59m redis-primary 1/1 1 1 59m proxy-server 1/1 1 1 59m redis-commander 1/1 1 1 59m storage-service 1/1 1 1 59m role-service 1/1 1 1 59m karavictl generate Generate resources for use with CSM\nSynopsis Generates resources for use with CSM\nkaravictl generate [flags] Optional Flags -h, --help Help for generate karavictl generate token Generate tokens\nSynopsis Generate tokens for use with the CSI Driver when in proxy mode The tokens are output as a Kubernetes Secret resource, so the results may be piped directly to kubectl:\nExample:\nkaravictl generate token --tenant Alice --admin-token admintoken.yaml --addr csm-authorization.host.com | kubectl apply -f - Required Flags -t, --tenant Name of the tenant -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for token --insecure Skip certificate validation of the CSM Authorization Proxy Server --access-token-expiration Expiration time of the access token, e.g. 1m30s (default 1m0s) --refresh-token-expiration Expiration time of the refresh token, e.g. 48h (default 720h0m0s) Output karavictl generate token --tenant Alice --admin-token admintoken.yaml --addr csm-authorization.host.com apiVersion: v1 data: access: \u003cACCESS-TOKEN\u003e refresh: \u003cREFRESH-TOKEN\u003e kind: Secret metadata: creationTimestamp: null name: proxy-authz-tokens type: Opaque Usually, you will want to pipe the output to kubectl to apply the secret\nkaravictl generate token --tenant Alice --admin-token admintoken.yaml --addr csm-authorization.host.com | kubectl apply -f - The token is read once when the driver pods are started and is not dynamically updated. If you are applying a new token in an existing driver installation, restart the driver pods for the new token to take effect.\nkubectl -n \u003cdriver-namespace\u003e rollout restart deploy/\u003cdriver\u003e-controller kubectl -n \u003cdriver-namespace\u003e rollout restart ds/\u003cdriver\u003e-node karavictl role Manage roles\nSynopsis Manage roles\nkaravictl role [flags] Options -h, --help Help for role karavictl role get Get role\nSynopsis Get role\nkaravictl role get [flags] Required Flags -n, --name Name of the role -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for get --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl role get CSISilver --admin-token admintoken.yaml --addr csm-authorization.host.com { \"Name\": \"CSISilver\", \"StorageSystem\": \"3000000000011111\", \"PoolQuotas\": [ { \"Pool\": \"mypool\", \"Quota\": \"16 GB\" } ] } karavictl role list List roles\nSynopsis List roles\nkaravictl role list [flags] Required Flags -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for list --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl role list --admin-token admintoken.yaml --addr csm-authorization.host.com { \"CSIGold\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 32000000 } ] } ], \"CSISilver\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 16000000 } ] } ] } karavictl role create Create one or more CSM roles\nSynopsis Creates one or more CSM roles\nkaravictl role create [flags] Required Flags --role Role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for create --insecure Skip certificate validation of the CSM Authorization Proxy Server NOTE:\nSetting the quota to 0 will not enforce storage quota Output karavictl role create --role=role-name=system-type=000000000001=mypool=200000000 --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the creation occurred.\nkaravictl role update Update the quota of one or more CSM roles\nSynopsis Updates the quota of one or more CSM roles\nkaravictl role update [flags] Required Flags --role Role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for update --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl role update --role=role-name=system-type=000000000001=mypool=400000000 --admin-token admintoken.yaml On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the update occurred.\nkaravictl role delete Delete role\nSynopsis Delete role\nkaravictl role delete [flags] Required Flags --role Role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl role delete --name CSISilver --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the deletion occurred.\nkaravictl rolebinding Manage role bindings\nSynopsis Management for role bindings\nkaravictl rolebinding [flags] Options -h, --help help for rolebinding karavictl rolebinding create Create a rolebinding between role and tenant\nSynopsis Creates a rolebinding between role and tenant\nkaravictl rolebinding create [flags] Required Flags -r, --role Role name -t, --tenant Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for create --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl rolebinding create --role CSISilver --tenant Alice --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the rolebinding creation occurred.\nkaravictl rolebinding delete Delete a rolebinding between role and tenant\nSynopsis Deletes a rolebinding between role and tenant\nkaravictl rolebinding delete [flags] Required Flags -r, --role Role name -t, --tenant Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl rolebinding delete --role CSISilver --tenant Alice --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output.\nStorage Commands karavictl storage Manage storage systems\nSynopsis Manages storage systems\nkaravictl storage [flags] Options -h, --help Help for storage karavictl storage get Get details on a registered storage system.\nSynopsis Gets details on a registered storage system.\nkaravictl storage get [flags] Required Flags -s, --system-id System identifier (default \"systemid\") -t, --type Type of storage system (\"powerflex\", \"powermax\", \"powerscale\") -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl storage get --type powerflex --system-id 3000000000011111 --admin-token admintoken.yaml --addr csm-authorization.host.com { \"User\": \"admin\", \"Password\": \"(omitted)\", \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true } karavictl storage list List registered storage systems.\nSynopsis Lists registered storage systems.\nkaravictl storage list [flags] Required Flags -t, --type Type of storage system (\"powerflex\", \"powermax\", \"powerscale\") -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl storage list --admin-token admintoken.yaml --addr csm-authorization.host.com { \"storage\": { \"powerflex\": { \"3000000000011111\": { \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true, \"Password\": \"(omitted)\", \"User\": \"admin\" } } } } karavictl storage create Create and register a storage system.\nSynopsis Creates and registers a storage system.\nkaravictl storage create [flags] Required Flags -e, --endpoint Endpoint of REST API gateway -p, --password Password (default \"****\") -s, --system-id System identifier (default \"systemid\") -t, --type Type of storage system (\"powerflex\", \"powermax\") -u, --user Username (default \"admin\") -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete -a, --array-insecure Skip certificate validation of the storage array --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl storage create --endpoint https://1.1.1.1 --insecure --array-insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the creation occurred.\nkaravictl storage update Update a registered storage system.\nSynopsis Updates a registered storage system.\nkaravictl storage update [flags] Required Flags -e, --endpoint Endpoint of REST API gateway -p, --pass Password (default \"****\") -s, --system-id System identifier (default \"systemid\") -t, --type Type of storage system (\"powerflex\", \"powermax\") -u, --user Username (default \"admin\") -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete -a, --array-insecure Skip certificate validation of the storage array --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl storage update --endpoint https://1.1.1.1 --insecure --array-insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the update occurred.\nkaravictl storage delete Delete a registered storage system.\nSynopsis Deletes a registered storage system.\nkaravictl storage delete [flags] Required Flags -s, --system-id System identifier (default \"systemid\") -t, --type Type of storage system (\"powerflex\", \"powermax\") -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl storage delete --type powerflex --system-id 3000000000011111 --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the deletion occurred.\nTenant Commands karavictl tenant Manage tenants\nSynopsis Management for tenants\nkaravictl tenant [flags] Options -h, --help help for tenant karavictl tenant create Create a tenant resource within CSM\nSynopsis Creates a tenant resource within CSM\nkaravictl tenant create [flags] Required Flags -n, --name Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete -a, --approvesdc To allow/deny SDC approval requests (default true | This flag is only applicable to PowerFlex. This flag will Approve/Deny a tenant's SDC request) --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant create --name Alice --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the creation occurred.\nkaravictl tenant get Get a tenant resource within CSM\nSynopsis Gets a tenant resource and its assigned roles within CSM\nkaravictl tenant get [flags] Required Flags -n, --name Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant get --name Alice --admin-token admintoken.yaml --addr csm-authorization.host.com { \"name\": \"Alice\" \"roles\": \"role-1,role-2\" } karavictl tenant list Lists tenant resources within CSM\nSynopsis Lists tenant resources within CSM\nkaravictl tenant list [flags] Required Flags -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant list --admin-token admintoken.yaml --addr csm-authorization.host.com { \"tenants\": [ { \"name\": \"Alice\" } ] } karavictl tenant revoke Revokes access for a tenant\nSynopsis Revokes access to storage resources for a tenant\nkaravictl tenant revoke [flags] Required Flags -n, --name Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -c, --cancel Cancel a previous tenant revocation -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant revoke --name Alice --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output.\nkaravictl tenant delete Deletes a tenant resource within CSM\nSynopsis Deletes a tenant resource within CSM\nkaravictl tenant delete [flags] Required Flags -n, --name Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant delete --name Alice --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the deletion occurred.\nkaravictl tenant update Updates a tenant’s resource within CSM\nSynopsis Updates a tenant resource within CSM\nkaravictl tenant update [flags] Required Flags -n, --name Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete -a, --approvesdc To allow/deny SDC approval requests (default true | This flag is only applicable to PowerFlex. This flag will Approve/Deny a tenant's SDC request) --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant update --name Alice --approvesdc=false --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the update was persisted.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization CLI\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/authorization/cli/","tags":"","title":"CLI"},{"body":" The CSM Authorization karavictl CLI is no longer actively maintained or supported. It will be deprecated in CSM 2.0.\nkaravictl is a command-line interface (CLI) used to interact with and manage your Container Storage Modules (CSM) Authorization deployment. This document outlines all karavictl commands, their intended use, options that can be provided to alter their execution, and expected output from those commands.\nIf you feel that something is unclear or missing in this document, please open up an issue.\nCommand Description karavictl karavictl is used to interact with CSM Authorization Server karavictl admin token Generate admin tokens karavictl cluster-info Display the state of resources within the cluster karavictl generate Generate resources for use with CSM karavictl generate token Generate tokens karavictl role Manage role karavictl role get Get role karavictl role list List roles karavictl role create Create one or more CSM roles karavictl role update Update the quota of one or more CSM roles karavictl role delete Delete role karavictl rolebinding Manage role bindings karavictl rolebinding create Create a rolebinding between role and tenant karavictl rolebinding delete Delete a rolebinding between role and tenant karavictl storage Manage storage systems karavictl storage get Get details on a registered storage system karavictl storage list List registered storage systems karavictl storage create Create and register a storage system karavictl storage update Update a registered storage system karavictl storage delete Delete a registered storage system karavictl tenant Manage tenants karavictl tenant create Create a tenant resource within CSM karavictl tenant get Get a tenant resource within CSM karavictl tenant list Lists tenant resources within CSM karavictl tenant revoke Get a tenant resource within CSM karavictl tenant delete Deletes a tenant resource within CSM karavictl tenant update Updates a tenant resource within CSM General Commands karavictl karavictl is used to interact with CSM Authorization Server\nSynopsis karavictl provides security, RBAC, and quota limits for accessing Dell storage products from Kubernetes clusters\nCommands admin Generate admin token for use with CSM Authorization cluster-info Display the state of resources within the cluster completion Generate the autocompletion script for the specified shell generate Generate resources for use with Karavi help Help about any command role Manage roles rolebinding Manage role bindings storage Manage storage systems tenant Manage tenants Options -h, --help Help for karavictl -f, --admin-token Path to admin token file; required for all commands except `admin token` and `cluster-info` --addr Address of the CSM Authorization Proxy Server; required for all commands except `admin token` and `cluster-info` --insecure Skip certificate validation of the CSM Authorization Proxy Server Output Outputs help text\nkaravictl admin token Generate admin tokens\nSynopsis Generate admin token for use with CSM Authorization commands. The tokens output in YAML format, which can be saved in a file.\nkaravictl admin token [flags] Required Flags -n, --name Name of the admin Optional Flags -h, --help Help for token -s, --jwt-signing-secret Specify JWT signing secret, or omit to use stdin --refresh-token-expiration Expiration time of the refresh token, e.g. 48h (default 720h0m0s) --access-token-expiration Expiration time of the access token, e.g. 1m30s (default 1m0s) Output $ karavictl admin token --name admin --access-token-expiration 30s --refresh-token-expiration 120m $ Enter JWT Signing Secret: *********** { \"Access\": \u003cACCESS-TOKEN\u003e, \"Refresh\": \u003cREFRESH-TOKEN\u003e } Alternatively, one can supply JWT signing secret with command.\n$ karavictl admin token --name admin --jwt-signing-secret secret --access-token-expiration 30s --refresh-token-expiration 120m { \"Access\": \u003cACCESS-TOKEN\u003e, \"Refresh\": \u003cREFRESH-TOKEN\u003e } karavictl cluster-info Display the state of resources within the cluster\nSynopsis Prints table of resources within the cluster, including their readiness\nkaravictl cluster-info [flags] Optional Flags -h, --help Help for cluster-info -w, --watch Watch for changes Output karavictl cluster-info NAME READY UP-TO-DATE AVAILABLE AGE tenant-service 1/1 1 1 59m redis-primary 1/1 1 1 59m proxy-server 1/1 1 1 59m redis-commander 1/1 1 1 59m storage-service 1/1 1 1 59m role-service 1/1 1 1 59m karavictl generate Generate resources for use with CSM\nSynopsis Generates resources for use with CSM\nkaravictl generate [flags] Optional Flags -h, --help Help for generate karavictl generate token Generate tokens\nSynopsis Generate tokens for use with the CSI Driver when in proxy mode The tokens are output as a Kubernetes Secret resource, so the results may be piped directly to kubectl:\nExample:\nkaravictl generate token --tenant Alice --admin-token admintoken.yaml --addr csm-authorization.host.com | kubectl apply -f - Required Flags -t, --tenant Name of the tenant -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for token --insecure Skip certificate validation of the CSM Authorization Proxy Server --access-token-expiration Expiration time of the access token, e.g. 1m30s (default 1m0s) --refresh-token-expiration Expiration time of the refresh token, e.g. 48h (default 720h0m0s) Output karavictl generate token --tenant Alice --admin-token admintoken.yaml --addr csm-authorization.host.com apiVersion: v1 data: access: \u003cACCESS-TOKEN\u003e refresh: \u003cREFRESH-TOKEN\u003e kind: Secret metadata: creationTimestamp: null name: proxy-authz-tokens type: Opaque Usually, you will want to pipe the output to kubectl to apply the secret\nkaravictl generate token --tenant Alice --admin-token admintoken.yaml --addr csm-authorization.host.com | kubectl apply -f - karavictl role Manage roles\nSynopsis Manage roles\nkaravictl role [flags] Options -h, --help Help for role karavictl role get Get role\nSynopsis Get role\nkaravictl role get [flags] Required Flags -n, --name Name of the role -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for get --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl role get CSISilver --admin-token admintoken.yaml --addr csm-authorization.host.com { \"Name\": \"CSISilver\", \"StorageSystem\": \"3000000000011111\", \"PoolQuotas\": [ { \"Pool\": \"mypool\", \"Quota\": \"16 GB\" } ] } karavictl role list List roles\nSynopsis List roles\nkaravictl role list [flags] Required Flags -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for list --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl role list --admin-token admintoken.yaml --addr csm-authorization.host.com { \"CSIGold\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 32000000 } ] } ], \"CSISilver\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 16000000 } ] } ] } karavictl role create Create one or more CSM roles\nSynopsis Creates one or more CSM roles\nkaravictl role create [flags] Required Flags --role Role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for create --insecure Skip certificate validation of the CSM Authorization Proxy Server NOTE:\nSetting the quota to 0 will not enforce storage quota Output karavictl role create --role=role-name=system-type=000000000001=mypool=200000000 --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the creation occurred.\nkaravictl role update Update the quota of one or more CSM roles\nSynopsis Updates the quota of one or more CSM roles\nkaravictl role update [flags] Required Flags --role Role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for update --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl role update --role=role-name=system-type=000000000001=mypool=400000000 --admin-token admintoken.yaml On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the update occurred.\nkaravictl role delete Delete role\nSynopsis Delete role\nkaravictl role delete [flags] Required Flags --role Role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl role delete --name CSISilver --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the deletion occurred.\nkaravictl rolebinding Manage role bindings\nSynopsis Management for role bindings\nkaravictl rolebinding [flags] Options -h, --help help for rolebinding karavictl rolebinding create Create a rolebinding between role and tenant\nSynopsis Creates a rolebinding between role and tenant\nkaravictl rolebinding create [flags] Required Flags -r, --role Role name -t, --tenant Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for create --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl rolebinding create --role CSISilver --tenant Alice --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the rolebinding creation occurred.\nkaravictl rolebinding delete Delete a rolebinding between role and tenant\nSynopsis Deletes a rolebinding between role and tenant\nkaravictl rolebinding delete [flags] Required Flags -r, --role Role name -t, --tenant Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl rolebinding delete --role CSISilver --tenant Alice --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output.\nStorage Commands karavictl storage Manage storage systems\nSynopsis Manages storage systems\nkaravictl storage [flags] Options -h, --help Help for storage karavictl storage get Get details on a registered storage system.\nSynopsis Gets details on a registered storage system.\nkaravictl storage get [flags] Required Flags -s, --system-id System identifier (default \"systemid\") -t, --type Type of storage system (\"powerflex\", \"powermax\", \"powerscale\") -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl storage get --type powerflex --system-id 3000000000011111 --admin-token admintoken.yaml --addr csm-authorization.host.com { \"User\": \"admin\", \"Password\": \"(omitted)\", \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true } karavictl storage list List registered storage systems.\nSynopsis Lists registered storage systems.\nkaravictl storage list [flags] Required Flags -t, --type Type of storage system (\"powerflex\", \"powermax\", \"powerscale\") -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl storage list --admin-token admintoken.yaml --addr csm-authorization.host.com { \"storage\": { \"powerflex\": { \"3000000000011111\": { \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true, \"Password\": \"(omitted)\", \"User\": \"admin\" } } } } karavictl storage create Create and register a storage system.\nSynopsis Creates and registers a storage system.\nkaravictl storage create [flags] Required Flags -e, --endpoint Endpoint of REST API gateway -p, --password Password (default \"****\") -s, --system-id System identifier (default \"systemid\") -t, --type Type of storage system (\"powerflex\", \"powermax\") -u, --user Username (default \"admin\") -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete -a, --array-insecure Skip certificate validation of the storage array --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl storage create --endpoint https://1.1.1.1 --insecure --array-insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the creation occurred.\nkaravictl storage update Update a registered storage system.\nSynopsis Updates a registered storage system.\nkaravictl storage update [flags] Required Flags -e, --endpoint Endpoint of REST API gateway -p, --pass Password (default \"****\") -s, --system-id System identifier (default \"systemid\") -t, --type Type of storage system (\"powerflex\", \"powermax\") -u, --user Username (default \"admin\") -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete -a, --array-insecure Skip certificate validation of the storage array --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl storage update --endpoint https://1.1.1.1 --insecure --array-insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the update occurred.\nkaravictl storage delete Delete a registered storage system.\nSynopsis Deletes a registered storage system.\nkaravictl storage delete [flags] Required Flags -s, --system-id System identifier (default \"systemid\") -t, --type Type of storage system (\"powerflex\", \"powermax\") -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl storage delete --type powerflex --system-id 3000000000011111 --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the deletion occurred.\nTenant Commands karavictl tenant Manage tenants\nSynopsis Management for tenants\nkaravictl tenant [flags] Options -h, --help help for tenant karavictl tenant create Create a tenant resource within CSM\nSynopsis Creates a tenant resource within CSM\nkaravictl tenant create [flags] Required Flags -n, --name Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete -a, --approvesdc To allow/deny SDC approval requests (default true | This flag is only applicable to PowerFlex. This flag will Approve/Deny a tenant's SDC request) --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant create --name Alice --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the creation occurred.\nkaravictl tenant get Get a tenant resource within CSM\nSynopsis Gets a tenant resource and its assigned roles within CSM\nkaravictl tenant get [flags] Required Flags -n, --name Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant get --name Alice --admin-token admintoken.yaml --addr csm-authorization.host.com { \"name\": \"Alice\" \"roles\": \"role-1,role-2\" } karavictl tenant list Lists tenant resources within CSM\nSynopsis Lists tenant resources within CSM\nkaravictl tenant list [flags] Required Flags -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant list --admin-token admintoken.yaml --addr csm-authorization.host.com { \"tenants\": [ { \"name\": \"Alice\" } ] } karavictl tenant revoke Revokes access for a tenant\nSynopsis Revokes access to storage resources for a tenant\nkaravictl tenant revoke [flags] Required Flags -n, --name Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -c, --cancel Cancel a previous tenant revocation -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant revoke --name Alice --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output.\nkaravictl tenant delete Deletes a tenant resource within CSM\nSynopsis Deletes a tenant resource within CSM\nkaravictl tenant delete [flags] Required Flags -n, --name Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant delete --name Alice --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the deletion occurred.\nkaravictl tenant update Updates a tenant’s resource within CSM\nSynopsis Updates a tenant resource within CSM\nkaravictl tenant update [flags] Required Flags -n, --name Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete -a, --approvesdc To allow/deny SDC approval requests (default true | This flag is only applicable to PowerFlex. This flag will Approve/Deny a tenant's SDC request) --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant update --name Alice --approvesdc=false --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the update was persisted.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization CLI\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v1/authorization/cli/","tags":"","title":"CLI"},{"body":"karavictl is a command-line interface (CLI) used to interact with and manage your Container Storage Modules (CSM) Authorization deployment. This document outlines all karavictl commands, their intended use, options that can be provided to alter their execution, and expected output from those commands.\nIf you feel that something is unclear or missing in this document, please open up an issue.\nCommand Description karavictl karavictl is used to interact with CSM Authorization Server karavictl admin token Generate admin tokens karavictl cluster-info Display the state of resources within the cluster karavictl generate Generate resources for use with CSM karavictl generate token Generate tokens karavictl role Manage role karavictl role get Get role karavictl role list List roles karavictl role create Create one or more CSM roles karavictl role update Update the quota of one or more CSM roles karavictl role delete Delete role karavictl rolebinding Manage role bindings karavictl rolebinding create Create a rolebinding between role and tenant karavictl rolebinding delete Delete a rolebinding between role and tenant karavictl storage Manage storage systems karavictl storage get Get details on a registered storage system karavictl storage list List registered storage systems karavictl storage create Create and register a storage system karavictl storage update Update a registered storage system karavictl storage delete Delete a registered storage system karavictl tenant Manage tenants karavictl tenant create Create a tenant resource within CSM karavictl tenant get Get a tenant resource within CSM karavictl tenant list Lists tenant resources within CSM karavictl tenant revoke Get a tenant resource within CSM karavictl tenant delete Deletes a tenant resource within CSM karavictl tenant update Updates a tenant resource within CSM General Commands karavictl karavictl is used to interact with CSM Authorization Server\nSynopsis karavictl provides security, RBAC, and quota limits for accessing Dell storage products from Kubernetes clusters\nCommands admin Generate admin token for use with CSM Authorization cluster-info Display the state of resources within the cluster completion Generate the autocompletion script for the specified shell generate Generate resources for use with Karavi help Help about any command role Manage roles rolebinding Manage role bindings storage Manage storage systems tenant Manage tenants Options -h, --help Help for karavictl -f, --admin-token Path to admin token file; required for all commands except `admin token` and `cluster-info` --addr Address of the CSM Authorization Proxy Server; required for all commands except `admin token` and `cluster-info` --insecure Skip certificate validation of the CSM Authorization Proxy Server Output Outputs help text\nkaravictl admin token Generate admin tokens\nSynopsis Generate admin token for use with CSM Authorization commands. The tokens output in YAML format, which can be saved in a file.\nkaravictl admin token [flags] Required Flags -n, --name Name of the admin Optional Flags -h, --help Help for token -s, --jwt-signing-secret Specify JWT signing secret, or omit to use stdin --refresh-token-expiration Expiration time of the refresh token, e.g. 48h (default 720h0m0s) --access-token-expiration Expiration time of the access token, e.g. 1m30s (default 1m0s) Output $ karavictl admin token --name admin --access-token-expiration 30s --refresh-token-expiration 120m $ Enter JWT Signing Secret: *********** { \"Access\": \u003cACCESS-TOKEN\u003e, \"Refresh\": \u003cREFRESH-TOKEN\u003e } Alternatively, one can supply JWT signing secret with command.\n$ karavictl admin token --name admin --jwt-signing-secret secret --access-token-expiration 30s --refresh-token-expiration 120m { \"Access\": \u003cACCESS-TOKEN\u003e, \"Refresh\": \u003cREFRESH-TOKEN\u003e } karavictl cluster-info Display the state of resources within the cluster\nSynopsis Prints table of resources within the cluster, including their readiness\nkaravictl cluster-info [flags] Optional Flags -h, --help Help for cluster-info -w, --watch Watch for changes Output karavictl cluster-info NAME READY UP-TO-DATE AVAILABLE AGE tenant-service 1/1 1 1 59m redis-primary 1/1 1 1 59m proxy-server 1/1 1 1 59m redis-commander 1/1 1 1 59m storage-service 1/1 1 1 59m role-service 1/1 1 1 59m karavictl generate Generate resources for use with CSM\nSynopsis Generates resources for use with CSM\nkaravictl generate [flags] Optional Flags -h, --help Help for generate karavictl generate token Generate tokens\nSynopsis Generate tokens for use with the CSI Driver when in proxy mode The tokens are output as a Kubernetes Secret resource, so the results may be piped directly to kubectl:\nExample:\nkaravictl generate token --tenant Alice --admin-token admintoken.yaml --addr csm-authorization.host.com | kubectl apply -f - Required Flags -t, --tenant Name of the tenant -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for token --insecure Skip certificate validation of the CSM Authorization Proxy Server --access-token-expiration Expiration time of the access token, e.g. 1m30s (default 1m0s) --refresh-token-expiration Expiration time of the refresh token, e.g. 48h (default 720h0m0s) Output karavictl generate token --tenant Alice --admin-token admintoken.yaml --addr csm-authorization.host.com apiVersion: v1 data: access: \u003cACCESS-TOKEN\u003e refresh: \u003cREFRESH-TOKEN\u003e kind: Secret metadata: creationTimestamp: null name: proxy-authz-tokens type: Opaque Usually, you will want to pipe the output to kubectl to apply the secret\nkaravictl generate token --tenant Alice --admin-token admintoken.yaml --addr csm-authorization.host.com | kubectl apply -f - karavictl role Manage roles\nSynopsis Manage roles\nkaravictl role [flags] Options -h, --help Help for role karavictl role get Get role\nSynopsis Get role\nkaravictl role get [flags] Required Flags -n, --name Name of the role -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for get --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl role get CSISilver --admin-token admintoken.yaml --addr csm-authorization.host.com { \"Name\": \"CSISilver\", \"StorageSystem\": \"3000000000011111\", \"PoolQuotas\": [ { \"Pool\": \"mypool\", \"Quota\": \"16 GB\" } ] } karavictl role list List roles\nSynopsis List roles\nkaravictl role list [flags] Required Flags -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for list --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl role list --admin-token admintoken.yaml --addr csm-authorization.host.com { \"CSIGold\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 32000000 } ] } ], \"CSISilver\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 16000000 } ] } ] } karavictl role create Create one or more CSM roles\nSynopsis Creates one or more CSM roles\nkaravictl role create [flags] Required Flags --role Role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for create --insecure Skip certificate validation of the CSM Authorization Proxy Server NOTE:\nSetting the quota to 0 will not enforce storage quota Output karavictl role create --role=role-name=system-type=000000000001=mypool=200000000 --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the creation occurred.\nkaravictl role update Update the quota of one or more CSM roles\nSynopsis Updates the quota of one or more CSM roles\nkaravictl role update [flags] Required Flags --role Role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for update --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl role update --role=role-name=system-type=000000000001=mypool=400000000 --admin-token admintoken.yaml On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the update occurred.\nkaravictl role delete Delete role\nSynopsis Delete role\nkaravictl role delete [flags] Required Flags --role Role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl role delete --name CSISilver --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the deletion occurred.\nkaravictl rolebinding Manage role bindings\nSynopsis Management for role bindings\nkaravictl rolebinding [flags] Options -h, --help help for rolebinding karavictl rolebinding create Create a rolebinding between role and tenant\nSynopsis Creates a rolebinding between role and tenant\nkaravictl rolebinding create [flags] Required Flags -r, --role Role name -t, --tenant Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for create --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl rolebinding create --role CSISilver --tenant Alice --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the rolebinding creation occurred.\nkaravictl rolebinding delete Delete a rolebinding between role and tenant\nSynopsis Deletes a rolebinding between role and tenant\nkaravictl rolebinding delete [flags] Required Flags -r, --role Role name -t, --tenant Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl rolebinding delete --role CSISilver --tenant Alice --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output.\nStorage Commands karavictl storage Manage storage systems\nSynopsis Manages storage systems\nkaravictl storage [flags] Options -h, --help Help for storage karavictl storage get Get details on a registered storage system.\nSynopsis Gets details on a registered storage system.\nkaravictl storage get [flags] Required Flags -s, --system-id System identifier (default \"systemid\") -t, --type Type of storage system (\"powerflex\", \"powermax\", \"powerscale\") -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl storage get --type powerflex --system-id 3000000000011111 --admin-token admintoken.yaml --addr csm-authorization.host.com { \"User\": \"admin\", \"Password\": \"(omitted)\", \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true } karavictl storage list List registered storage systems.\nSynopsis Lists registered storage systems.\nkaravictl storage list [flags] Required Flags -t, --type Type of storage system (\"powerflex\", \"powermax\", \"powerscale\") -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl storage list --admin-token admintoken.yaml --addr csm-authorization.host.com { \"storage\": { \"powerflex\": { \"3000000000011111\": { \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true, \"Password\": \"(omitted)\", \"User\": \"admin\" } } } } karavictl storage create Create and register a storage system.\nSynopsis Creates and registers a storage system.\nkaravictl storage create [flags] Required Flags -e, --endpoint Endpoint of REST API gateway -p, --password Password (default \"****\") -s, --system-id System identifier (default \"systemid\") -t, --type Type of storage system (\"powerflex\", \"powermax\") -u, --user Username (default \"admin\") -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete -a, --array-insecure Skip certificate validation of the storage array --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl storage create --endpoint https://1.1.1.1 --insecure --array-insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the creation occurred.\nkaravictl storage update Update a registered storage system.\nSynopsis Updates a registered storage system.\nkaravictl storage update [flags] Required Flags -e, --endpoint Endpoint of REST API gateway -p, --pass Password (default \"****\") -s, --system-id System identifier (default \"systemid\") -t, --type Type of storage system (\"powerflex\", \"powermax\") -u, --user Username (default \"admin\") -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete -a, --array-insecure Skip certificate validation of the storage array --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl storage update --endpoint https://1.1.1.1 --insecure --array-insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the update occurred.\nkaravictl storage delete Delete a registered storage system.\nSynopsis Deletes a registered storage system.\nkaravictl storage delete [flags] Required Flags -s, --system-id System identifier (default \"systemid\") -t, --type Type of storage system (\"powerflex\", \"powermax\") -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl storage delete --type powerflex --system-id 3000000000011111 --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the deletion occurred.\nTenant Commands karavictl tenant Manage tenants\nSynopsis Management for tenants\nkaravictl tenant [flags] Options -h, --help help for tenant karavictl tenant create Create a tenant resource within CSM\nSynopsis Creates a tenant resource within CSM\nkaravictl tenant create [flags] Required Flags -n, --name Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete -a, --approvesdc To allow/deny SDC approval requests (default true | This flag is only applicable to PowerFlex. This flag will Approve/Deny a tenant's SDC request) --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant create --name Alice --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the creation occurred.\nkaravictl tenant get Get a tenant resource within CSM\nSynopsis Gets a tenant resource and its assigned roles within CSM\nkaravictl tenant get [flags] Required Flags -n, --name Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant get --name Alice --admin-token admintoken.yaml --addr csm-authorization.host.com { \"name\": \"Alice\" \"roles\": \"role-1,role-2\" } karavictl tenant list Lists tenant resources within CSM\nSynopsis Lists tenant resources within CSM\nkaravictl tenant list [flags] Required Flags -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant list --admin-token admintoken.yaml --addr csm-authorization.host.com { \"tenants\": [ { \"name\": \"Alice\" } ] } karavictl tenant revoke Revokes access for a tenant\nSynopsis Revokes access to storage resources for a tenant\nkaravictl tenant revoke [flags] Required Flags -n, --name Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -c, --cancel Cancel a previous tenant revocation -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant revoke --name Alice --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output.\nkaravictl tenant delete Deletes a tenant resource within CSM\nSynopsis Deletes a tenant resource within CSM\nkaravictl tenant delete [flags] Required Flags -n, --name Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant delete --name Alice --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the deletion occurred.\nkaravictl tenant update Updates a tenant’s resource within CSM\nSynopsis Updates a tenant resource within CSM\nkaravictl tenant update [flags] Required Flags -n, --name Tenant name -f, --admin-token Path to admin token file --addr Address of the CSM Authorization Proxy Server Optional Flags -h, --help Help for delete -a, --approvesdc To allow/deny SDC approval requests (default true | This flag is only applicable to PowerFlex. This flag will Approve/Deny a tenant's SDC request) --insecure Skip certificate validation of the CSM Authorization Proxy Server Output karavictl tenant update --name Alice --approvesdc=false --admin-token admintoken.yaml --addr csm-authorization.host.com On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the update was persisted.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization CLI\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v2/authorization/cli/","tags":"","title":"CLI"},{"body":" The CSM Authorization karavictl CLI is no longer actively maintained or supported. It will be deprecated in CSM 2.0.\nkaravictl is a command-line interface (CLI) used to interact with and manage your Container Storage Modules (CSM) Authorization deployment. This document outlines all karavictl commands, their intended use, options that can be provided to alter their execution, and expected output from those commands.\nIf you feel that something is unclear or missing in this document, please open up an issue.\nCommand Description karavictl karavictl is used to interact with CSM Authorization Server karavictl cluster-info Display the state of resources within the cluster karavictl generate Generate resources for use with CSM karavictl generate token Generate tokens karavictl role Manage role karavictl role get Get role karavictl role list List roles karavictl role create Create one or more CSM roles karavictl role update Update the quota of one or more CSM roles karavictl role delete Delete role karavictl rolebinding Manage role bindings karavictl rolebinding create Create a rolebinding between role and tenant karavictl rolebinding delete Delete a rolebinding between role and tenant karavictl storage Manage storage systems karavictl storage get Get details on a registered storage system karavictl storage list List registered storage systems karavictl storage create Create and register a storage system karavictl storage update Update a registered storage system karavictl storage delete Delete a registered storage system karavictl tenant Manage tenants karavictl tenant create Create a tenant resource within CSM karavictl tenant get Get a tenant resource within CSM karavictl tenant list Lists tenant resources within CSM karavictl tenant revoke Get a tenant resource within CSM karavictl tenant delete Deletes a tenant resource within CSM karavictl tenant update Updates a tenant resource within CSM General Commands karavictl karavictl is used to interact with CSM Authorization Server\nSynopsis karavictl provides security, RBAC, and quota limits for accessing Dell storage products from Kubernetes clusters\nOptions --config string config file (default is $HOME/.karavictl.yaml) -h, --help help for karavictl -t, --toggle Help message for toggle Output Outputs help text\nkaravictl cluster-info Display the state of resources within the cluster\nSynopsis Prints table of resources within the cluster, including their readiness\nkaravictl cluster-info [flags] Options -h, --help help for cluster-info -w, --watch Watch for changes Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl cluster-info NAME READY UP-TO-DATE AVAILABLE AGE github-auth-provider 1/1 1 1 59m tenant-service 1/1 1 1 59m redis-primary 1/1 1 1 59m proxy-server 1/1 1 1 59m redis-commander 1/1 1 1 59m karavictl generate Generate resources for use with CSM\nSynopsis Generates resources for use with CSM\nkaravictl generate [flags] Options -h, --help help for generate Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\nkaravictl generate token Generate tokens\nSynopsis Generate tokens for use with the CSI Driver when in proxy mode The tokens are output as a Kubernetes Secret resource, so the results may be piped directly to kubectl:\nExample: karavictl generate token | kubectl apply -f -\nkaravictl generate token [flags] Options --addr string host:port address (default \"grpc.gatekeeper.cluster:443\") --from-config string File providing self-generated token information -h, --help help for token --tenant Tenant name --shared-secret string Shared secret for token signing Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl generate token --shared-secret supersecret apiVersion: v1 kind: Secret metadata: name: proxy-authz-tokens namespace: vxflexos type: Opaque data: access: \u003cACCESS-TOKEN\u003e refresh: \u003cREFRESH-TOKEN\u003e Usually, you will want to pipe the output to kubectl to apply the secret\n$ karavictl generate token --shared-secret supersecret | kubectl apply -f - Role Commands karavictl role Manage roles\nSynopsis Manage roles\nkaravictl role [flags] Options -h, --help help for role Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\nkaravictl role get Get role\nSynopsis Get role\nkaravictl role get [flags] Options -h, --help help for get --insecure insecure skip verify flag for Helm deployment --addr address of the container for Helm deployment (pod:port) Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role get CSISilver { \"Name\": \"CSISilver\", \"StorageSystem\": \"3000000000011111\", \"PoolQuotas\": [ { \"Pool\": \"mypool\", \"Quota\": \"16 GB\" } ] } karavictl role list List roles\nSynopsis List roles\nkaravictl role list [flags] Options -h, --help help for list --insecure insecure skip verify flag for Helm deployment --addr address of the container for Helm deployment (pod:port) Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role list { \"CSIGold\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 32000000 } ] } ], \"CSISilver\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 16000000 } ] } ] } karavictl role create Create one or more CSM roles\nSynopsis Creates one or more CSM roles\nkaravictl role create [flags] Options -f, --from-file string role data from a file --role strings role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e --insecure insecure skip verify flag for Helm deployment --addr address of the container for Helm deployment (pod:port) -h, --help help for create NOTE:\nFor PowerScale, set the quota to 0 as CSM for Authorization does not enforce quota limits. Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role create --from-file roles.json On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the creation occurred.\nAlternatively, you can create a role in-line using:\n$ karavictl role create --role=role-name=system-type=000000000001=mypool=200000000 karavictl role update Update the quota of one or more CSM roles\nSynopsis Updates the quota of one or more CSM roles\nkaravictl role update [flags] Options -f, --from-file string role data from a file --role strings role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e --insecure insecure skip verify flag for Helm deployment --addr address of the container for Helm deployment (pod:port) -h, --help help for update Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role update --from-file roles.json On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the update occurred.\nAlternatively, you can update existing roles in-line using:\n$ karavictl role update --role=role-name=system-type=000000000001=mypool=400000000 karavictl role delete Delete role\nSynopsis Delete role\nkaravictl role delete \u003crole-name\u003e [flags] Options -h, --help help for delete --insecure insecure skip verify flag for Helm deployment --addr address of the container for Helm deployment (pod:port) Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role delete CSISilver On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the deletion occurred.\nkaravictl rolebinding Manage role bindings\nSynopsis Management for role bindings\nkaravictl rolebinding [flags] Options -h, --help help for rolebinding Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\nkaravictl rolebinding create Create a rolebinding between role and tenant\nSynopsis Creates a rolebinding between role and tenant\nkaravictl rolebinding create [flags] Options -h, --help help for create -r, --role string Role name -t, --tenant string Tenant name --insecure boolean insecure skip verify flag for Helm deployment Options inherited from parent commands --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl rolebinding create --role CSISilver --tenant Alice On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the rolebinding creation occurred.\nkaravictl rolebinding delete Delete a rolebinding between role and tenant\nSynopsis Deletes a rolebinding between role and tenant\nkaravictl rolebinding delete [flags] Options -h, --help help for create -r, --role string Role name -t, --tenant string Tenant name --insecure boolean insecure skip verify flag for Helm deployment Options inherited from parent commands --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl rolebinding delete --role CSISilver --tenant Alice On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the rolebinding deletion occurred.\nStorage Commands karavictl storage Manage storage systems\nSynopsis Manages storage systems\nkaravictl storage [flags] Options -h, --help help for storage Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\nkaravictl storage get Get details on a registered storage system.\nSynopsis Gets details on a registered storage system.\nkaravictl storage get [flags] Options -h, --help help for get -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") --insecure insecure skip verify flag for Helm deployment --addr address of the container for Helm deployment (pod:port) Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage get --type powerflex --system-id 3000000000011111 { \"User\": \"admin\", \"Password\": \"(omitted)\", \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true } karavictl storage list List registered storage systems.\nSynopsis Lists registered storage systems.\nkaravictl storage list [flags] Options -h, --help help for list --insecure insecure skip verify flag for Helm deployment --addr address of the container for Helm deployment (pod:port) Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage list { \"storage\": { \"powerflex\": { \"3000000000011111\": { \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true, \"Password\": \"(omitted)\", \"User\": \"admin\" } } } } karavictl storage create Create and register a storage system.\nSynopsis Creates and registers a storage system.\nkaravictl storage create [flags] Options -e, --endpoint string Endpoint of REST API gateway -h, --help help for create -a, --array-insecure Array insecure skip verify -p, --password string Password (default \"****\") -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") -u, --user string Username (default \"admin\") --insecure insecure skip verify flag for Helm deployment --addr address of the container for Helm deployment (pod:port) Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage create --endpoint https://1.1.1.1 --insecure --array-insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the creation occurred.\nkaravictl storage update Update a registered storage system.\nSynopsis Updates a registered storage system.\nkaravictl storage update [flags] Options -e, --endpoint string Endpoint of REST API gateway -h, --help help for update -a, --array-insecure Array insecure skip verify -p, --pass string Password (default \"****\") -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") -u, --user string Username (default \"admin\") --insecure insecure skip verify flag for Helm deployment --addr address of the container for Helm deployment (pod:port) Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage update --endpoint https://1.1.1.1 --insecure --array-insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the update occurred.\nkaravictl storage delete Delete a registered storage system.\nSynopsis Deletes a registered storage system.\nkaravictl storage delete [flags] Options -h, --help help for delete -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") --insecure insecure skip verify flag for Helm deployment --addr address of the container for Helm deployment (pod:port) Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage delete --type powerflex --system-id 3000000000011111 On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the deletion occurred.\nTenant Commands karavictl tenant Manage tenants\nSynopsis Management for tenants\nkaravictl tenant [flags] Options -h, --help help for tenant Options inherited from parent commands --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\nkaravictl tenant create Create a tenant resource within CSM\nSynopsis Creates a tenant resource within CSM\nkaravictl tenant create [flags] Options -h, --help help for create -n, --name string Tenant name --insecure insecure skip verify flag for Helm deployment Options inherited from parent commands --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant create --name Alice On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the creation occurred.\nkaravictl tenant get Get a tenant resource within CSM\nSynopsis Gets a tenant resource and its assigned roles within CSM\nkaravictl tenant get [flags] Options -h, --help help for create -n, --name string Tenant name --insecure insecure skip verify flag for Helm deployment Options inherited from parent commands --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant get --name Alice { \"name\": \"Alice\" \"roles\": \"role-1,role-2\" } karavictl tenant list Lists tenant resources within CSM\nSynopsis Lists tenant resources within CSM\nkaravictl tenant list [flags] Options -h, --help help for create --insecure insecure skip verify flag for Helm deployment Options inherited from parent commands --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant list { \"tenants\": [ { \"name\": \"Alice\" } ] } karavictl tenant revoke Revokes access for a tenant\nSynopsis Revokes access to storage resources for a tenant\nkaravictl tenant revoke [flags] Options -h, --help help for create -n, --name string Tenant name --insecure insecure skip verify flag for Helm deployment Options inherited from parent commands --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant revoke --name Alice On success, there will be no output.\nkaravictl tenant delete Deletes a tenant resource within CSM\nSynopsis Deletes a tenant resource within CSM\nkaravictl tenant delete [flags] Options -h, --help help for create -n, --name string Tenant name --insecure insecure skip verify flag for Helm deployment Options inherited from parent commands --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant delete --name Alice On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the deletion occurred.\nkaravictl tenant update Updates a tenant’s resource within CSM\nSynopsis Updates a tenant resource within CSM\nkaravictl tenant update [flags] Options -h, --help help for create -n, --name string Tenant name --approvesdc boolean (Usage: --approvesdc=true/false | This flag is only applicable to PowerFlex. This flag will Approve/Deny a tenant's SDC request ) Options inherited from parent commands --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant update --name Alice --approvesdc=false On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the update was persisted.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization CLI\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v3/authorization/cli/","tags":"","title":"CLI"},{"body":"Disaster Recovery Workflows Once the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), users can exercise the general Disaster Recovery workflows.\nPlanned Migration to the target cluster/array This scenario is the typical choice when you want to try your disaster recovery plan or you need to switch activities from one site to another:\na. Execute “failover” action on selected ReplicationGroup using the cluster name\n./repctl --rg rg-id failover --target target-cluster-name b. Execute “reprotect” action on selected ReplicationGroup which will resume the replication from new “source”\n./repctl --rg rg-id reprotect --at new-source-cluster-name Unplanned Migration to the target cluster/array This scenario is the typical choice when a site goes down:\na. Execute “failover” action on selected ReplicationGroup using the cluster name\n./repctl --rg rg-id failover --target target-cluster-name --unplanned b. Execute “swap” action on selected ReplicationGroup which would swap personalities of R1 and R2 (only applicable for PowerMax driver)\n./repctl --rg rg-id swap --at target-cluster-name Note: Unplanned migration usually happens when the original “source” cluster is unavailable. The following action makes sense when the cluster is back.\nc. Execute “reprotect” action on selected ReplicationGroup which will resume the replication.\n./repctl --rg rg-id reprotect --at new-source-cluster-name NOTE: When users do Failover and Failback, the tests pods on the source cluster may go “CrashLoopOff” state since it will try to remount the same volume which is already mounted. To get around this problem, bring down the number of replicas to 0 and then after that is done, bring it up to 1.\n","categories":"","description":"Disaster Recovery Workflows\n","excerpt":"Disaster Recovery Workflows\n","ref":"/csm-docs/docs/replication/disaster-recovery/","tags":"","title":"Disaster Recovery"},{"body":"Disaster Recovery Workflows Once the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), users can exercise the general Disaster Recovery workflows.\nPlanned Migration to the target cluster/array This scenario is the typical choice when you want to try your disaster recovery plan or you need to switch activities from one site to another:\na. Execute “failover” action on selected ReplicationGroup using the cluster name\n./repctl --rg rg-id failover --target target-cluster-name b. Execute “reprotect” action on selected ReplicationGroup which will resume the replication from new “source”\n./repctl --rg rg-id reprotect --at new-source-cluster-name Unplanned Migration to the target cluster/array This scenario is the typical choice when a site goes down:\na. Execute “failover” action on selected ReplicationGroup using the cluster name\n./repctl --rg rg-id failover --target target-cluster-name --unplanned b. Execute “swap” action on selected ReplicationGroup which would swap personalities of R1 and R2 (only applicable for PowerMax driver)\n./repctl --rg rg-id swap --at target-cluster-name Note: Unplanned migration usually happens when the original “source” cluster is unavailable. The following action makes sense when the cluster is back.\nc. Execute “reprotect” action on selected ReplicationGroup which will resume the replication.\n./repctl --rg rg-id reprotect --at new-source-cluster-name NOTE: When users do Failover and Failback, the tests pods on the source cluster may go “CrashLoopOff” state since it will try to remount the same volume which is already mounted. To get around this problem, bring down the number of replicas to 0 and then after that is done, bring it up to 1.\n","categories":"","description":"Disaster Recovery Workflows\n","excerpt":"Disaster Recovery Workflows\n","ref":"/csm-docs/v1/replication/disaster-recovery/","tags":"","title":"Disaster Recovery"},{"body":"Disaster Recovery Workflows Once the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), users can exercise the general Disaster Recovery workflows.\nPlanned Migration to the target cluster/array This scenario is the typical choice when you want to try your disaster recovery plan or you need to switch activities from one site to another:\na. Execute “failover” action on selected ReplicationGroup using the cluster name\n./repctl --rg rg-id failover --target target-cluster-name b. Execute “reprotect” action on selected ReplicationGroup which will resume the replication from new “source”\n./repctl --rg rg-id reprotect --at new-source-cluster-name Unplanned Migration to the target cluster/array This scenario is the typical choice when a site goes down:\na. Execute “failover” action on selected ReplicationGroup using the cluster name\n./repctl --rg rg-id failover --target target-cluster-name --unplanned b. Execute “swap” action on selected ReplicationGroup which would swap personalities of R1 and R2 (only applicable for PowerMax driver)\n./repctl --rg rg-id swap --at target-cluster-name Note: Unplanned migration usually happens when the original “source” cluster is unavailable. The following action makes sense when the cluster is back.\nc. Execute “reprotect” action on selected ReplicationGroup which will resume the replication.\n./repctl --rg rg-id reprotect --at new-source-cluster-name NOTE: When users do Failover and Failback, the tests pods on the source cluster may go “CrashLoopOff” state since it will try to remount the same volume which is already mounted. To get around this problem, bring down the number of replicas to 0 and then after that is done, bring it up to 1.\n","categories":"","description":"Disaster Recovery Workflows\n","excerpt":"Disaster Recovery Workflows\n","ref":"/csm-docs/v2/replication/disaster-recovery/","tags":"","title":"Disaster Recovery"},{"body":"Disaster Recovery Workflows Once the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), users can exercise the general Disaster Recovery workflows.\nPlanned Migration to the target cluster/array This scenario is the typical choice when you want to try your disaster recovery plan or you need to switch activities from one site to another:\na. Execute \"failover\" action on selected ReplicationGroup using the cluster name ./repctl --rg rg-id failover --target target-cluster-name b. Execute \"reprotect\" action on selected ReplicationGroup which will resume the replication from new \"source\" ./repctl --rg rg-id reprotect --at new-source-cluster-name Unplanned Migration to the target cluster/array This scenario is the typical choice when a site goes down:\na. Execute \"failover\" action on selected ReplicationGroup using the cluster name ./repctl --rg rg-id failover --target target-cluster-name --unplanned b. Execute \"swap\" action on selected ReplicationGroup which would swap personalities of R1 and R2 (only applicable for PowerMax driver) ./repctl --rg rg-id swap --at target-cluster-name **Note:** Unplanned migration usually happens when the original \"source\" cluster is unavailable. The following action makes sense when the cluster is back. c. Execute \"reprotect\" action on selected ReplicationGroup which will resume the replication. ./repctl --rg rg-id reprotect --at new-source-cluster-name NOTE: When users do Failover and Failback, the tests pods on the source cluster may go “CrashLoopOff” state since it will try to remount the same volume which is already mounted. To get around this problem, bring down the number of replicas to 0 and then after that is done, bring it up to 1.\n","categories":"","description":"Disaster Recovery Workflows\n","excerpt":"Disaster Recovery Workflows\n","ref":"/csm-docs/v3/replication/disaster-recovery/","tags":"","title":"Disaster Recovery"},{"body":"ObjectScale Area Core Features Implementation level Status Details Provisioning Create Bucket Minimum Viable Product ✅ Done Bucket is created using default settings. Brownfield provisioning ✅ Done Bucket is created based on existing bucket in Object Storage Provisioner. Advanced provisioning 📝 Design draft Extra (non-default) parameters for bucket provisioning are controlled from the BucketClass. Delete Bucket Minimum Viable Product ✅ Done Bucket is deleted. Access Management Grant Bucket Access Minimum Viable Product ✅ Done Full access is granted for given bucket. Advanced permissions 📝 Design draft More control over permission is done through BucketAccessClass. Revoke Bucket Access Minimum Viable Product ✅ Done Access is revoked. ","categories":"","description":"Description of COSI Driver features","excerpt":"Description of COSI Driver features","ref":"/csm-docs/docs/cosidriver/features/","tags":"","title":"Features"},{"body":"","categories":"","description":"Description of CSI Driver features","excerpt":"Description of CSI Driver features","ref":"/csm-docs/docs/csidriver/features/","tags":["pod-deploy","csi-driver"],"title":"Features"},{"body":"ObjectScale Area Core Features Implementation level Status Details Provisioning Create Bucket Minimum Viable Product ✅ Done Bucket is created using default settings. Brownfield provisioning ✅ Done Bucket is created based on existing bucket in Object Storage Provisioner. Advanced provisioning 📝 Design draft Extra (non-default) parameters for bucket provisioning are controlled from the BucketClass. Delete Bucket Minimum Viable Product ✅ Done Bucket is deleted. Access Management Grant Bucket Access Minimum Viable Product ✅ Done Full access is granted for given bucket. Advanced permissions 📝 Design draft More control over permission is done through BucketAccessClass. Revoke Bucket Access Minimum Viable Product ✅ Done Access is revoked. ","categories":"","description":"Description of COSI Driver features","excerpt":"Description of COSI Driver features","ref":"/csm-docs/v1/cosidriver/features/","tags":"","title":"Features"},{"body":"","categories":"","description":"Description of CSI Driver features","excerpt":"Description of CSI Driver features","ref":"/csm-docs/v1/csidriver/features/","tags":["pod-deploy","csi-driver"],"title":"Features"},{"body":"","categories":"","description":"Description of CSI Driver features","excerpt":"Description of CSI Driver features","ref":"/csm-docs/v2/csidriver/features/","tags":["pod-deploy","csi-driver"],"title":"Features"},{"body":"","categories":"","description":"Description of CSI Driver features","excerpt":"Description of CSI Driver features","ref":"/csm-docs/v3/csidriver/features/","tags":["pod-deploy","csi-driver"],"title":"Features"},{"body":"","categories":"","description":"Process of installation","excerpt":"Process of installation","ref":"/csm-docs/docs/cosidriver/installation/","tags":"","title":"Installation"},{"body":"","categories":"","description":"Process of installation","excerpt":"Process of installation","ref":"/csm-docs/v1/cosidriver/installation/","tags":"","title":"Installation"},{"body":"Install Replication Walkthrough NOTE: These steps should not be used when installing using Dell CSM Operator.\nSet up repctl tool Before you begin, make sure you have the repctl tool available.\nYou can download a pre-built repctl binary from our Releases page.\nwget https://github.com/dell/csm-replication/releases/download/v1.7.1/repctl-linux-amd64 mv repctl-linux-amd64 repctl chmod +x repctl Alternately, if you want to build the binary yourself, you can follow these steps:\ngit clone -b v1.7.1 https://github.com/dell/csm-replication.git cd csm-replication/repctl make build Installation steps NOTE: The repctl commands only have to be run from one Kubernetes cluster. Repctl does the appropriate configuration on both clusters, when installing replication with it.\nYou can start using Container Storage Modules (CSM) for Replication with help from repctl using these simple steps:\nPrepare admin Kubernetes clusters configs Add admin configs as clusters to repctl: ./repctl cluster add -f \"/root/.kube/config-1\",\"/root/.kube/config-2\" -n \"cluster-1\",\"cluster-2\" NOTE: If using a single Kubernetes cluster in a stretched configuration there will be only one cluster.\nInstall replication controller and CRDs: ./repctl create -f ../deploy/replicationcrds.all.yaml ./repctl create -f ../deploy/controller.yaml NOTE: The controller will report that configmap is invalid. This is expected behavior. The message should disappear once you inject the kubeconfigs (next step).\n(Choose one) (More secure) Inject service accounts’ configs into clusters: ./repctl cluster inject --use-sa (Less secure) Inject admin configs into clusters: ./repctl cluster inject Modify csm-replication/repctl/examples/\u003cstorage\u003e_example_values.yaml config with replication information: NOTE: clusterID should match names you gave to clusters in step 2\nCreate replication storage classes using config: ./repctl create sc --from-config ./examples/\u003cstorage\u003e_example_values.yaml Install CSI driver for your chosen storage in source cluster and provision replicated volumes (optional) Create PVCs on target cluster from Replication Group: ./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false Note: all repctl output is saved in a repctl.log file in the current working directory and can be attached to any installation troubleshooting requests.\n","categories":"","description":"Installation of CSM for Replication using repctl","excerpt":"Installation of CSM for Replication using repctl","ref":"/csm-docs/docs/replication/deployment/install-repctl/","tags":"","title":"Installation using repctl"},{"body":"Install Replication Walkthrough NOTE: These steps should not be used when installing using Dell CSM Operator.\nSet up repctl tool Before you begin, make sure you have the repctl tool available.\nYou can download a pre-built repctl binary from our Releases page.\nwget https://github.com/dell/csm-replication/releases/download/v1.6.0/repctl-linux-amd64 mv repctl-linux-amd64 repctl chmod +x repctl Alternately, if you want to build the binary yourself, you can follow these steps:\ngit clone -b v1.6.0 https://github.com/dell/csm-replication.git cd csm-replication/repctl make build Installation steps NOTE: The repctl commands only have to be run from one Kubernetes cluster. Repctl does the appropriate configuration on both clusters, when installing replication with it.\nYou can start using Container Storage Modules (CSM) for Replication with help from repctl using these simple steps:\nPrepare admin Kubernetes clusters configs Add admin configs as clusters to repctl: ./repctl cluster add -f \"/root/.kube/config-1\",\"/root/.kube/config-2\" -n \"cluster-1\",\"cluster-2\" NOTE: If using a single Kubernetes cluster in a stretched configuration there will be only one cluster.\nInstall replication controller and CRDs: ./repctl create -f ../deploy/replicationcrds.all.yaml ./repctl create -f ../deploy/controller.yaml NOTE: The controller will report that configmap is invalid. This is expected behavior. The message should disappear once you inject the kubeconfigs (next step).\n(Choose one) (More secure) Inject service accounts’ configs into clusters: ./repctl cluster inject --use-sa (Less secure) Inject admin configs into clusters: ./repctl cluster inject Modify csm-replication/repctl/examples/\u003cstorage\u003e_example_values.yaml config with replication information: NOTE: clusterID should match names you gave to clusters in step 2\nCreate replication storage classes using config: ./repctl create sc --from-config ./examples/\u003cstorage\u003e_example_values.yaml Install CSI driver for your chosen storage in source cluster and provision replicated volumes (optional) Create PVCs on target cluster from Replication Group: ./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false Note: all repctl output is saved in a repctl.log file in the current working directory and can be attached to any installation troubleshooting requests.\n","categories":"","description":"Installation of CSM for Replication using repctl","excerpt":"Installation of CSM for Replication using repctl","ref":"/csm-docs/v1/replication/deployment/install-repctl/","tags":"","title":"Installation using repctl"},{"body":"Install Replication Walkthrough NOTE: These steps should not be used when installing using Dell CSM Operator.\nSet up repctl tool Before you begin, make sure you have the repctl tool available.\nYou can download a pre-built repctl binary from our Releases page.\nwget https://github.com/dell/csm-replication/releases/download/v1.5.0/repctl-linux-amd64 mv repctl-linux-amd64 repctl chmod +x repctl Alternately, if you want to build the binary yourself, you can follow these steps:\ngit clone -b v1.5.0 https://github.com/dell/csm-replication.git cd csm-replication/repctl make build Installation steps NOTE: The repctl commands only have to be run from one Kubernetes cluster. Repctl does the appropriate configuration on both clusters, when installing replication with it.\nYou can start using Container Storage Modules (CSM) for Replication with help from repctl using these simple steps:\nPrepare admin Kubernetes clusters configs Add admin configs as clusters to repctl: ./repctl cluster add -f \"/root/.kube/config-1\",\"/root/.kube/config-2\" -n \"cluster-1\",\"cluster-2\" NOTE: If using a single Kubernetes cluster in a stretched configuration there will be only one cluster.\nInstall replication controller and CRDs: ./repctl create -f ../deploy/replicationcrds.all.yaml ./repctl create -f ../deploy/controller.yaml NOTE: The controller will report that configmap is invalid. This is expected behavior. The message should disappear once you inject the kubeconfigs (next step).\n(Choose one) (More secure) Inject service accounts’ configs into clusters: ./repctl cluster inject --use-sa (Less secure) Inject admin configs into clusters: ./repctl cluster inject Modify csm-replication/repctl/examples/\u003cstorage\u003e_example_values.yaml config with replication information: NOTE: clusterID should match names you gave to clusters in step 2\nCreate replication storage classes using config: ./repctl create sc --from-config ./examples/\u003cstorage\u003e_example_values.yaml Install CSI driver for your chosen storage in source cluster and provision replicated volumes (optional) Create PVCs on target cluster from Replication Group: ./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false Note: all repctl output is saved in a repctl.log file in the current working directory and can be attached to any installation troubleshooting requests.\n","categories":"","description":"Installation of CSM for Replication using repctl","excerpt":"Installation of CSM for Replication using repctl","ref":"/csm-docs/v2/replication/deployment/install-repctl/","tags":"","title":"Installation using repctl"},{"body":"Install Replication Walkthrough NOTE: These steps should not be used when installing using Dell CSM Operator.\nSet up repctl tool Before you begin, make sure you have the repctl tool available.\nYou can download a pre-built repctl binary from our Releases page.\nwget https://github.com/dell/csm-replication/releases/download/v1.4.0/repctl-linux-amd64 mv repctl-linux-amd64 repctl chmod +x repctl Alternately, if you want to build the binary yourself, you can follow these steps:\ngit clone -b v1.4.0 https://github.com/dell/csm-replication.git cd csm-replication/repctl make build Installation steps NOTE: The repctl commands only have to be run from one Kubernetes cluster. Repctl does the appropriate configuration on both clusters, when installing replication with it.\nYou can start using Container Storage Modules (CSM) for Replication with help from repctl using these simple steps:\nPrepare admin Kubernetes clusters configs Add admin configs as clusters to repctl: ./repctl cluster add -f \"/root/.kube/config-1\",\"/root/.kube/config-2\" -n \"cluster-1\",\"cluster-2\" NOTE: If using a single Kubernetes cluster in a stretched configuration there will be only one cluster.\nInstall replication controller and CRDs: ./repctl create -f ../deploy/replicationcrds.all.yaml ./repctl create -f ../deploy/controller.yaml NOTE: The controller will report that configmap is invalid. This is expected behavior. The message should disappear once you inject the kubeconfigs (next step).\n(Choose one) (More secure) Inject service accounts’ configs into clusters: ./repctl cluster inject --use-sa (Less secure) Inject admin configs into clusters: ./repctl cluster inject Modify csm-replication/repctl/examples/\u003cstorage\u003e_example_values.yaml config with replication information: NOTE: clusterID should match names you gave to clusters in step 2\nCreate replication storage classes using config: ./repctl create sc --from-config ./examples/\u003cstorage\u003e_example_values.yaml Install CSI driver for your chosen storage in source cluster and provision replicated volumes (optional) Create PVCs on target cluster from Replication Group: ./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false Note: all repctl output is saved in a repctl.log file in the current working directory and can be attached to any installation troubleshooting requests.\n","categories":"","description":"Installation of CSM for Replication using repctl","excerpt":"Installation of CSM for Replication using repctl","ref":"/csm-docs/v3/replication/deployment/install-repctl/","tags":"","title":"Installation using repctl"},{"body":"Install Replication Walkthrough NOTE: These steps should be repeated on all Kubernetes clusters where you want to configure replication.\ngit clone -b v1.7.1 https://github.com/dell/csm-replication.git cd csm-replication kubectl create ns dell-replication-controller # Download and modify the default values.yaml file if you wish to customize your deployment in any way wget -O myvalues.yaml https://raw.githubusercontent.com/dell/helm-charts/csm-replication-1.7.1/charts/csm-replication/values.yaml bash scripts/install.sh --values ./myvalues.yaml Note: Current installation method allows you to specify custom \u003cFQDN\u003e:\u003cIP\u003e entries to be appended to controller’s /etc/hosts file. It can be useful if controller is being deployed in private environment where DNS is not set up properly, but kubernetes clusters use FQDN as API server’s address. The feature can be enabled by modifying values.yaml.\n- ip: \"10.10.10.10\" hostnames: - \"foo.bar\" - ip: \"10.10.10.11\" hostnames: - \"foo.baz\" This script will do the following:\nInstall DellCSIReplicationGroup CRD in your cluster Install dell-replication-controller After the installation ConfigMap will consist of only the logLevel field, to add the rest configuration to the cluster do the following:\nUpdate the configuration in deploy/config.yaml after going through the guide here Run the following commands to update and complete the installation cd csm-replication kubectl create configmap dell-replication-controller-config --namespace dell-replication-controller --from-file deploy/config.yaml -o yaml --dry-run | kubectl apply -f - ","categories":"","description":"Installation of CSM for Replication using script (Helm chart)","excerpt":"Installation of CSM for Replication using script (Helm chart)","ref":"/csm-docs/docs/replication/deployment/install-script/","tags":"","title":"Installation using script"},{"body":"Install Replication Walkthrough NOTE: These steps should be repeated on all Kubernetes clusters where you want to configure replication.\ngit clone -b v1.6.0 https://github.com/dell/csm-replication.git cd csm-replication kubectl create ns dell-replication-controller # Download and modify the default values.yaml file if you wish to customize your deployment in any way wget -O myvalues.yaml https://raw.githubusercontent.com/dell/helm-charts/csm-replication-1.6.0/charts/csm-replication/values.yaml bash scripts/install.sh --values ./myvalues.yaml Note: Current installation method allows you to specify custom \u003cFQDN\u003e:\u003cIP\u003e entries to be appended to controller’s /etc/hosts file. It can be useful if controller is being deployed in private environment where DNS is not set up properly, but kubernetes clusters use FQDN as API server’s address. The feature can be enabled by modifying values.yaml.\n- ip: \"10.10.10.10\" hostnames: - \"foo.bar\" - ip: \"10.10.10.11\" hostnames: - \"foo.baz\" This script will do the following:\nInstall DellCSIReplicationGroup CRD in your cluster Install dell-replication-controller After the installation ConfigMap will consist of only the logLevel field, to add the rest configuration to the cluster do the following:\nUpdate the configuration in deploy/config.yaml after going through the guide here Run the following commands to update and complete the installation cd csm-replication kubectl create configmap dell-replication-controller-config --namespace dell-replication-controller --from-file deploy/config.yaml -o yaml --dry-run | kubectl apply -f - ","categories":"","description":"Installation of CSM for Replication using script (Helm chart)","excerpt":"Installation of CSM for Replication using script (Helm chart)","ref":"/csm-docs/v1/replication/deployment/install-script/","tags":"","title":"Installation using script"},{"body":"Install Replication Walkthrough NOTE: These steps should be repeated on all Kubernetes clusters where you want to configure replication.\ngit clone -b v1.5.0 https://github.com/dell/csm-replication.git cd csm-replication kubectl create ns dell-replication-controller # Copy and modify values.yaml file if you wish to customize your deployment in any way cp ./helm/csm-replication/values.yaml ./myvalues.yaml bash scripts/install.sh --values ./myvalues.yaml Note: Current installation method allows you to specify custom \u003cFQDN\u003e:\u003cIP\u003e entries to be appended to controller’s /etc/hosts file. It can be useful if controller is being deployed in private environment where DNS is not set up properly, but kubernetes clusters use FQDN as API server’s address. The feature can be enabled by modifying values.yaml.\n- ip: \"10.10.10.10\" hostnames: - \"foo.bar\" - ip: \"10.10.10.11\" hostnames: - \"foo.baz\" This script will do the following:\nInstall DellCSIReplicationGroup CRD in your cluster Install dell-replication-controller After the installation ConfigMap will consist of only the logLevel field, to add the rest configuration to the cluster do the following:\nUpdate the configuration in deploy/config.yaml after going through the guide here Run the following commands to update and complete the installation cd csm-replication kubectl create configmap dell-replication-controller-config --namespace dell-replication-controller --from-file deploy/config.yaml -o yaml --dry-run | kubectl apply -f - ","categories":"","description":"Installation of CSM for Replication using script (Helm chart)","excerpt":"Installation of CSM for Replication using script (Helm chart)","ref":"/csm-docs/v2/replication/deployment/install-script/","tags":"","title":"Installation using script"},{"body":"Install Replication Walkthrough NOTE: These steps should be repeated on all Kubernetes clusters where you want to configure replication.\ngit clone -b v1.4.0 https://github.com/dell/csm-replication.git cd csm-replication kubectl create ns dell-replication-controller # Copy and modify values.yaml file if you wish to customize your deployment in any way cp ./helm/csm-replication/values.yaml ./myvalues.yaml bash scripts/install.sh --values ./myvalues.yaml Note: Current installation method allows you to specify custom \u003cFQDN\u003e:\u003cIP\u003e entries to be appended to controller’s /etc/hosts file. It can be useful if controller is being deployed in private environment where DNS is not set up properly, but kubernetes clusters use FQDN as API server’s address. The feature can be enabled by modifying values.yaml.\n- ip: \"10.10.10.10\" hostnames: - \"foo.bar\" - ip: \"10.10.10.11\" hostnames: - \"foo.baz\" This script will do the following:\nInstall DellCSIReplicationGroup CRD in your cluster Install dell-replication-controller After the installation ConfigMap will consist of only the logLevel field, to add the rest configuration to the cluster do the following:\nUpdate the configuration in deploy/config.yaml after going through the guide here Run the following commands to update and complete the installation cd csm-replication kubectl create configmap dell-replication-controller-config --namespace dell-replication-controller --from-file deploy/config.yaml -o yaml --dry-run | kubectl apply -f - ","categories":"","description":"Installation of CSM for Replication using script (Helm chart)","excerpt":"Installation of CSM for Replication using script (Helm chart)","ref":"/csm-docs/v3/replication/deployment/install-script/","tags":"","title":"Installation using script"},{"body":"","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) Policies\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) Policies\n","ref":"/csm-docs/docs/references/policies/","tags":"","title":"Policies"},{"body":"","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) Policies\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) Policies\n","ref":"/csm-docs/v1/references/policies/","tags":"","title":"Policies"},{"body":"","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) Policies\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) Policies\n","ref":"/csm-docs/v2/references/policies/","tags":"","title":"Policies"},{"body":"","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) Policies\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) Policies\n","ref":"/csm-docs/v3/references/policies/","tags":"","title":"Policies"},{"body":"Rekey Controller Installation The CSM Encryption Rekey CRD Controller is an optional component that, if installed, allows encrypted volumes rekeying in a Kubernetes cluster. The Rekey Controller can be installed via the Dell Helm charts repository.\nDell Helm charts can be added with the command helm repo add dell https://dell.github.io/helm-charts.\nKubeconfig Secret A secret with kubeconfig must be created with the name cluster-kube-config. Here is an example:\nkubectl create secret generic cluster-kube-config --from-file=config=/root/.kube/config Helm Chart Values The Rekey Controller Helm chart defines these values:\n# Rekey controller image name. image: \"dellemc/csm-encryption-rekey-controller:v0.2.0\" # Rekey controller image pull policy. # Allowed values: # Always: Always pull the image. # IfNotPresent: Only pull the image if it does not already exist on the node. # Never: Never pull the image. imagePullPolicy: IfNotPresent # logLevel: Log level of the rekey controller. # Allowed values: \"error\", \"warning\", \"info\", \"debug\", \"trace\". logLevel: \"info\" # This value is required and must match encryption.pluginName value # of the corresponding Dell CSI driver. provisioner: # This value is required and must match encryption.apiPort value # of the corresponding Dell CSI driver. port: Parameter Description Required Default image Rekey controller image name. No “dellemc/csm-encryption-rekey-controller:v0.2.0” imagePullPolicy Rekey controller image pull policy. No “IfNotPresent” logLevel Log level of the rekey controller. No “info” provisioner This value is required and must match encryption.pluginName value of the corresponding Dell CSI driver. Yes port This value is required and must match encryption.apiPort value of the corresponding Dell CSI driver. Yes Deployment Copy the chart’s values.yaml to a local file and adjust the values in the local file for the current cluster. Deploy the controller using a command similar to this:\nhelm install --values local-values.yaml rekey-controller dell/csm-encryption-rekey-controller A rekey-controller pod should now be up and running.\nRekey Usage Rekeying is initiated and monitored via Kubernetes custom resources of type rekeys.encryption.storage.dell.com. This can be done directly using kubectl or in a more user-friendly way using dellctl. Creation of a rekey resource for a PV will kick off a rekey process on this PV. The rekey resource will contain the result of the operation. Refer to Rekey Status for possible status values.\nRekey with dellctl If dellctl CLI is installed, rekeying an encrypted volume is simple. For example, to rekey a PV with the name k8s-112a5d41bc use a command like this:\ndellctl encryption rekey myrekey k8s-112a5d41bc INFO rekey request \"myrekey\" submitted successfully for persistent volume \"k8s-112a5d41bc\".\rINFO Run 'dellctl encryption rekey-status myrekey' for more details. Then to check the status of the newly created rekey with the name myrekey use this command:\ndellctl encryption rekey-status myrekey INFO Status of rekey request myrekey = completed Rekey with kubectl Create a cluster-scoped rekey resource to rekey an encrypted volume. For example, to rekey a PV with the name k8s-09a76734f use a command like this:\nkubectl create -f - \u003c\u003cEOF apiVersion: \"encryption.storage.dell.com/v1alpha1\" kind: \"Rekey\" metadata: name: \"example-rekey\" spec: persistentVolumeName: \"k8s-029a76734f\" EOF Once the rekey resource has been created, after some time, the status of the rekey can be inspected through the status.phase field of the rekey resource.\necho $(kubectl get rekey example-rekey -o jsonpath='{.status.phase}') completed Rekey Status The status.phase field of a rekey resource can have these values:\nValue Description initialized The request has been received by the Rekey Controller. started The request is being processed by the Encryption driver. completed The request successfully completed and the volume is protected by a new key. rejected The rekey process has not started, a non-existent or not encrypted PV in the request is a common reason. failed The rekey process has failed, possibly due to unreachable Encryption driver or an error response from the driver. unknown The request was sent to the Encryption driver, but no response was received. It is still possible that the rekey succeeded and the volume key has changed. Cleanup Remove old rekey resources just like any other resource, using kubectl delete.\n","categories":"","description":"Rekey Configuration and Usage\n","excerpt":"Rekey Configuration and Usage\n","ref":"/csm-docs/docs/secure/encryption/rekey/","tags":"","title":"Rekey Configuration"},{"body":"Rekey Controller Installation The CSM Encryption Rekey CRD Controller is an optional component that, if installed, allows encrypted volumes rekeying in a Kubernetes cluster. The Rekey Controller can be installed via the Dell Helm charts repository.\nDell Helm charts can be added with the command helm repo add dell https://dell.github.io/helm-charts.\nKubeconfig Secret A secret with kubeconfig must be created with the name cluster-kube-config. Here is an example:\nkubectl create secret generic cluster-kube-config --from-file=config=/root/.kube/config Helm Chart Values The Rekey Controller Helm chart defines these values:\n# Rekey controller image name. image: \"dellemc/csm-encryption-rekey-controller:v0.2.0\" # Rekey controller image pull policy. # Allowed values: # Always: Always pull the image. # IfNotPresent: Only pull the image if it does not already exist on the node. # Never: Never pull the image. imagePullPolicy: IfNotPresent # logLevel: Log level of the rekey controller. # Allowed values: \"error\", \"warning\", \"info\", \"debug\", \"trace\". logLevel: \"info\" # This value is required and must match encryption.pluginName value # of the corresponding Dell CSI driver. provisioner: # This value is required and must match encryption.apiPort value # of the corresponding Dell CSI driver. port: Parameter Description Required Default image Rekey controller image name. No “dellemc/csm-encryption-rekey-controller:v0.2.0” imagePullPolicy Rekey controller image pull policy. No “IfNotPresent” logLevel Log level of the rekey controller. No “info” provisioner This value is required and must match encryption.pluginName value of the corresponding Dell CSI driver. Yes port This value is required and must match encryption.apiPort value of the corresponding Dell CSI driver. Yes Deployment Copy the chart’s values.yaml to a local file and adjust the values in the local file for the current cluster. Deploy the controller using a command similar to this:\nhelm install --values local-values.yaml rekey-controller dell/csm-encryption-rekey-controller A rekey-controller pod should now be up and running.\nRekey Usage Rekeying is initiated and monitored via Kubernetes custom resources of type rekeys.encryption.storage.dell.com. This can be done directly using kubectl or in a more user-friendly way using dellctl. Creation of a rekey resource for a PV will kick off a rekey process on this PV. The rekey resource will contain the result of the operation. Refer to Rekey Status for possible status values.\nRekey with dellctl If dellctl CLI is installed, rekeying an encrypted volume is simple. For example, to rekey a PV with the name k8s-112a5d41bc use a command like this:\ndellctl encryption rekey myrekey k8s-112a5d41bc INFO rekey request \"myrekey\" submitted successfully for persistent volume \"k8s-112a5d41bc\". INFO Run 'dellctl encryption rekey-status myrekey' for more details. Then to check the status of the newly created rekey with the name myrekey use this command:\ndellctl encryption rekey-status myrekey INFO Status of rekey request myrekey = completed Rekey with kubectl Create a cluster-scoped rekey resource to rekey an encrypted volume. For example, to rekey a PV with the name k8s-09a76734f use a command like this:\nkubectl create -f - \u003c\u003cEOF apiVersion: \"encryption.storage.dell.com/v1alpha1\" kind: \"Rekey\" metadata: name: \"example-rekey\" spec: persistentVolumeName: \"k8s-029a76734f\" EOF Once the rekey resource has been created, after some time, the status of the rekey can be inspected through the status.phase field of the rekey resource.\necho $(kubectl get rekey example-rekey -o jsonpath='{.status.phase}') completed Rekey Status The status.phase field of a rekey resource can have these values:\nValue Description initialized The request has been received by the Rekey Controller. started The request is being processed by the Encryption driver. completed The request successfully completed and the volume is protected by a new key. rejected The rekey process has not started, a non-existent or not encrypted PV in the request is a common reason. failed The rekey process has failed, possibly due to unreachable Encryption driver or an error response from the driver. unknown The request was sent to the Encryption driver, but no response was received. It is still possible that the rekey succeeded and the volume key has changed. Cleanup Remove old rekey resources just like any other resource, using kubectl delete.\n","categories":"","description":"Rekey Configuration and Usage\n","excerpt":"Rekey Configuration and Usage\n","ref":"/csm-docs/v1/secure/encryption/rekey/","tags":"","title":"Rekey Configuration"},{"body":"Rekey Controller Installation The CSM Encryption Rekey CRD Controller is an optional component that, if installed, allows encrypted volumes rekeying in a Kubernetes cluster. The Rekey Controller can be installed via the Dell Helm charts repository.\nDell Helm charts can be added with the command helm repo add dell https://dell.github.io/helm-charts.\nKubeconfig Secret A secret with kubeconfig must be created with the name cluster-kube-config. Here is an example:\nkubectl create secret generic cluster-kube-config --from-file=config=/root/.kube/config Helm Chart Values The Rekey Controller Helm chart defines these values:\n# Rekey controller image name. image: \"dellemc/csm-encryption-rekey-controller:v0.2.0\" # Rekey controller image pull policy. # Allowed values: # Always: Always pull the image. # IfNotPresent: Only pull the image if it does not already exist on the node. # Never: Never pull the image. imagePullPolicy: IfNotPresent # logLevel: Log level of the rekey controller. # Allowed values: \"error\", \"warning\", \"info\", \"debug\", \"trace\". logLevel: \"info\" # This value is required and must match encryption.pluginName value # of the corresponding Dell CSI driver. provisioner: # This value is required and must match encryption.apiPort value # of the corresponding Dell CSI driver. port: Parameter Description Required Default image Rekey controller image name. No “dellemc/csm-encryption-rekey-controller:v0.2.0” imagePullPolicy Rekey controller image pull policy. No “IfNotPresent” logLevel Log level of the rekey controller. No “info” provisioner This value is required and must match encryption.pluginName value of the corresponding Dell CSI driver. Yes port This value is required and must match encryption.apiPort value of the corresponding Dell CSI driver. Yes Deployment Copy the chart’s values.yaml to a local file and adjust the values in the local file for the current cluster. Deploy the controller using a command similar to this:\nhelm install --values local-values.yaml rekey-controller dell/csm-encryption-rekey-controller A rekey-controller pod should now be up and running.\nRekey Usage Rekeying is initiated and monitored via Kubernetes custom resources of type rekeys.encryption.storage.dell.com. This can be done directly using kubectl or in a more user-friendly way using dellctl. Creation of a rekey resource for a PV will kick off a rekey process on this PV. The rekey resource will contain the result of the operation. Refer to Rekey Status for possible status values.\nRekey with dellctl If dellctl CLI is installed, rekeying an encrypted volume is simple. For example, to rekey a PV with the name k8s-112a5d41bc use a command like this:\ndellctl encryption rekey myrekey k8s-112a5d41bc INFO rekey request \"myrekey\" submitted successfully for persistent volume \"k8s-112a5d41bc\". INFO Run 'dellctl encryption rekey-status myrekey' for more details. Then to check the status of the newly created rekey with the name myrekey use this command:\ndellctl encryption rekey-status myrekey INFO Status of rekey request myrekey = completed Rekey with kubectl Create a cluster-scoped rekey resource to rekey an encrypted volume. For example, to rekey a PV with the name k8s-09a76734f use a command like this:\nkubectl create -f - \u003c\u003cEOF apiVersion: \"encryption.storage.dell.com/v1alpha1\" kind: \"Rekey\" metadata: name: \"example-rekey\" spec: persistentVolumeName: \"k8s-029a76734f\" EOF Once the rekey resource has been created, after some time, the status of the rekey can be inspected through the status.phase field of the rekey resource.\necho $(kubectl get rekey example-rekey -o jsonpath='{.status.phase}') completed Rekey Status The status.phase field of a rekey resource can have these values:\nValue Description initialized The request has been received by the Rekey Controller. started The request is being processed by the Encryption driver. completed The request successfully completed and the volume is protected by a new key. rejected The rekey process has not started, a non-existent or not encrypted PV in the request is a common reason. failed The rekey process has failed, possibly due to unreachable Encryption driver or an error response from the driver. unknown The request was sent to the Encryption driver, but no response was received. It is still possible that the rekey succeeded and the volume key has changed. Cleanup Remove old rekey resources just like any other resource, using kubectl delete.\n","categories":"","description":"Rekey Configuration and Usage\n","excerpt":"Rekey Configuration and Usage\n","ref":"/csm-docs/v2/secure/encryption/rekey/","tags":"","title":"Rekey Configuration"},{"body":"Rekey Controller Installation The CSM Encryption Rekey CRD Controller is an optional component that, if installed, allows encrypted volumes rekeying in a Kubernetes cluster. The Rekey Controller can be installed via the Dell Helm charts repository.\nDell Helm charts can be added with the command helm repo add dell https://dell.github.io/helm-charts.\nKubeconfig Secret A secret with kubeconfig must be created with the name cluster-kube-config. Here is an example:\nkubectl create secret generic cluster-kube-config --from-file=config=/root/.kube/config Helm Chart Values The Rekey Controller Helm chart defines these values:\n# Rekey controller image name. image: \"dellemc/csm-encryption-rekey-controller:v0.2.0\" # Rekey controller image pull policy. # Allowed values: # Always: Always pull the image. # IfNotPresent: Only pull the image if it does not already exist on the node. # Never: Never pull the image. imagePullPolicy: IfNotPresent # logLevel: Log level of the rekey controller. # Allowed values: \"error\", \"warning\", \"info\", \"debug\", \"trace\". logLevel: \"info\" # This value is required and must match encryption.pluginName value # of the corresponding Dell CSI driver. provisioner: # This value is required and must match encryption.apiPort value # of the corresponding Dell CSI driver. port: Parameter Description Required Default image Rekey controller image name. No “dellemc/csm-encryption-rekey-controller:v0.2.0” imagePullPolicy Rekey controller image pull policy. No “IfNotPresent” logLevel Log level of the rekey controller. No “info” provisioner This value is required and must match encryption.pluginName value of the corresponding Dell CSI driver. Yes port This value is required and must match encryption.apiPort value of the corresponding Dell CSI driver. Yes Deployment Copy the chart’s values.yaml to a local file and adjust the values in the local file for the current cluster. Deploy the controller using a command similar to this:\nhelm install --values local-values.yaml rekey-controller dell/csm-encryption-rekey-controller A rekey-controller pod should now be up and running.\nRekey Usage Rekeying is initiated and monitored via Kubernetes custom resources of type rekeys.encryption.storage.dell.com. This can be done directly using kubectl or in a more user-friendly way using dellctl. Creation of a rekey resource for a PV will kick off a rekey process on this PV. The rekey resource will contain the result of the operation. Refer to Rekey Status for possible status values.\nRekey with dellctl If dellctl CLI is installed, rekeying an encrypted volume is simple. For example, to rekey a PV with the name k8s-112a5d41bc use a command like this:\n$ dellctl encryption rekey myrekey k8s-112a5d41bc INFO rekey request \"myrekey\" submitted successfully for persistent volume \"k8s-112a5d41bc\". INFO Run 'dellctl encryption rekey-status myrekey' for more details. Then to check the status of the newly created rekey with the name myrekey use this command:\n$ dellctl encryption rekey-status myrekey INFO Status of rekey request myrekey = completed Rekey with kubectl Create a cluster-scoped rekey resource to rekey an encrypted volume. For example, to rekey a PV with the name k8s-09a76734f use a command like this:\nkubectl create -f - \u003c\u003cEOF apiVersion: \"encryption.storage.dell.com/v1alpha1\" kind: \"Rekey\" metadata: name: \"example-rekey\" spec: persistentVolumeName: \"k8s-029a76734f\" EOF Once the rekey resource has been created, after some time, the status of the rekey can be inspected through the status.phase field of the rekey resource.\n$ echo $(kubectl get rekey example-rekey -o jsonpath='{.status.phase}') completed Rekey Status The status.phase field of a rekey resource can have these values:\nValue Description initialized The request has been received by the Rekey Controller. started The request is being processed by the Encryption driver. completed The request successfully completed and the volume is protected by a new key. rejected The rekey process has not started, a non-existent or not encrypted PV in the request is a common reason. failed The rekey process has failed, possibly due to unreachable Encryption driver or an error response from the driver. unknown The request was sent to the Encryption driver, but no response was received. It is still possible that the rekey succeeded and the volume key has changed. Cleanup Remove old rekey resources just like any other resource, using kubectl delete.\n","categories":"","description":"Rekey Configuration and Usage\n","excerpt":"Rekey Configuration and Usage\n","ref":"/csm-docs/v3/secure/encryption/rekey/","tags":"","title":"Rekey Configuration"},{"body":"Frequently Asked Questions How can I diagnose an issue with Application Mobility? How can I view logs? How can I debug and troubleshoot issues with Kubernetes? Why are there error logs about a license? How can I diagnose an issue with Application Mobility? Once you have attempted to install Application Mobility to your Kubernetes or OpenShift cluster, the first step in troubleshooting is locating the problem.\nGet information on the state of your Pods.\nkubectl get pods -n $namespace Get verbose output of the current state of a Pod.\nkubectl describe pod -n $namespace $pod How can I view logs? View pod container logs. Output logs to a file for further debugging.\nkubectl logs -n $namespace $pod $container kubectl logs -n $namespace $pod $container \u003e $logFileName How can I debug and troubleshoot issues with Kubernetes? To debug your application that may not be behaving correctly, please reference Kubernetes troubleshooting applications guide.\nFor tips on debugging your cluster, please see this troubleshooting guide.\nWhy are there error logs about a license? Application Mobility requires a license in order to function. See the Deployment instructions for steps to request a license.\nThere will be errors in the logs about the license for these cases:\nLicense does not exist License is not valid for the current Kubernetes cluster License has expired ","categories":"","description":"Troubleshooting\n","excerpt":"Troubleshooting\n","ref":"/csm-docs/docs/applicationmobility/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Frequently Asked Questions Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? How can I diagnose an issue with Container Storage Modules (CSM) for Observability? How can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? How can I debug and troubleshoot issues with Kubernetes? How can I troubleshoot latency problems with CSM for Observability? Why does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage? Why do I see FailedMount warnings when describing pods in my cluster? Why do I see ‘Failed calling webhook’ error when reinstalling CSM for Observability? Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? This issue can arise when the topology service manifest is updated to expose the service as NodePort and a client makes a request to the service. Karavi-toplogy is configured with a self-signed or custom certificate and when a client does not recognize a server’s certificate, it shows an error and pings the server(karavi-topology) with the error. You would see the issue when accessing the service through a browser or curl:\nBrowser experience A user who tries to connect to karavi-topology on any browser may receive an error/warning message about the certificate. The message may vary depending on the browser. For instance, in Internet Explorer, you’ll see:\nThere is a problem with this website's security certificate. The security certificate presented by this website was not issued by a trusted certificate authority While this certificate problem may indicate an attempt to fool you or intercept data you send to the server, see resolution on how to fix it\nCurl experience A user who tries to connect to karavi-topology by using curl may receive the following warning or error message:\ncurl -v https://\u003ckaravi-topology-cluster-IP\u003e:\u003cport?/query * Trying ***********... * TCP_NODELAY set * Connected to *********** (***********) port 31433 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /etc/ssl/certs/ca-certificates.crt CApath: /etc/ssl/certs * TLSv1.3 (OUT), TLS handshake, Client hello (1): * TLSv1.3 (IN), TLS handshake, Server hello (2): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Unknown (8): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Certificate (11): * TLSv1.3 (OUT), TLS alert, Server hello (2): * SSL certificate problem: unable to get local issuer certificate * stopped the pause stream! * Closing connection 0 curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: https://curl.haxx.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. Kubernetes Admin experience Due to the error above, the client pings the topology server with a TLS handshake error which is logged in karavi-topology pod. For instance,\nkubectl logs -n powerflex karavi-topology-5d4669d6dd-trzxw 2021/04/27 09:38:28 Set DriverNames to [csi-vxflexos.dellemc.com] 2021/04/28 07:15:05 http: TLS handshake error from 10.42.0.0:58450: local error: tls: bad record MAC 2021/04/28 07:16:14 http: TLS handshake error from 10.42.0.0:55311: local error: tls: bad record MAC Resolution To resolve this issue, we need to configure the client to be aware of the karavi-topology certificate (this includes all custom SSL certificate that are not issued from a trusted Certificate Authority (CA))\nGet a copy of the certificate used by karavi-topology If we supplied a custom certificate during installing karavi-topology, we can simply open the .crt and copy the text. However, if it was assigned by cert-manager, you can get a copy of the certificate by running the following kubectl command on the clusters.\nkubectl -n \u003cnamespace\u003e get secret karavi-topology-tls -o jsonpath='{.data.tls\\.crt}' | base64 -d -----BEGIN CERTIFICATE----- RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe -----END CERTIFICATE----- Configure your client to accept the above certificate A workaround on most browsers is to accept the karavi-topology certificate by clicking Continue to this website (not recommended). This will make all other successive communication to not cause any certificate error. Anyhow, you will need to read the documentation for your specific client to configure the above certificate. For Grafana, here are two ways to configure the karavi-topology datasource to use the above certificate:\nDeploy certificate with new Grafana instance Please follow the steps in Sample Grafana Deployment but attach the certificate to your `grafana-values.yaml` before deploying. The file should look like: # grafana-values.yaml image: repository: grafana/grafana tag: 7.3.0 sha: \"\" pullPolicy: IfNotPresent service: type: NodePort ## Administrator credentials when not using an existing Secret adminUser: admin adminPassword: admin ## Pass the plugins you want installed as a list. ## plugins: - grafana-simple-json-datasource - briangann-datatable-panel - grafana-piechart-panel ## Configure grafana datasources ## ref: http://docs.grafana.org/administration/provisioning/#datasources ## datasources: datasources.yaml: apiVersion: 1 datasources: - name: Karavi-Topology type: grafana-simple-json-datasource access: proxy url: 'https://karavi-topology:8443' isDefault: null version: 1 editable: true jsonData: tlsAuthWithCACert: true secureJsonData: tlsCaCert: | -----BEGIN CERTIFICATE----- RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe -----END CERTIFICATE----- - name: Prometheus type: prometheus access: proxy url: 'http://prometheus:9090' isDefault: null version: 1 editable: true testFramework: enabled: false sidecar: datasources: enabled: true dashboards: enabled: true ## Additional grafana server ConfigMap mounts ## Defines additional mounts with ConfigMap. ConfigMap must be manually created in the namespace. extraConfigmapMounts: [] Add certificate to an existing Grafana instance - This only happens if you configure jsonData to not skip tls verification. If this is the case, you'll need to re-deploy grafana as shown above or, form Grafana UI, edit Karavi-Topology datasource to use the certificate. To do the latter: Visit your Grafana UI on a browser Navigate to setting and go to Data Sources Click on Karavi-Topology Ensure that Skip TLS Verify is already off Switch on With CA Cert Copy the above certificate into the TLS Auth Details text box that appears Click Save \u0026 Test and validate that everything is working fine when a green bar showing Data source is working appears How can I diagnose an issue with CSM for Observability? Once you have attempted to install CSM for Observability to your Kubernetes or OpenShift cluster, the first step in troubleshooting is locating the problem.\nGet information on the state of your Pods.\nkubectl get pods -n $namespace Get verbose output of the current state of a Pod.\nkubectl describe pod -n $namespace $pod How can I view logs? View pod container logs. Output logs to a file for further debugging.\nkubectl logs -n $namespace $pod $container kubectl logs -n $namespace $pod $container \u003e $logFileName More information for viewing logs can be found here.\nHow can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? The ServiceMonitor allows us to define how a set of services should be monitored by Prometheus. Please see our prometheus documentation for creating a ServiceMonitor.\nHow can I debug and troubleshoot issues with Kubernetes? To debug your application that may not be behaving correctly, please reference Kubernetes troubleshooting applications guide.\nFor tips on debugging your cluster, please see this troubleshooting guide.\nHow can I troubleshoot latency problems with CSM for Observability? CSM for Observability is instrumented to report trace data to Zipkin. Please see Tracing for more information on enabling tracing for CSM for Observability.\nWhy does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage? Check the pods in the CSM for Observability namespace. If the pod starting with ‘karavi-observability-cert-manager-cainjector-*’ is in ‘CrashLoopBackOff’ or ‘Error\" stage with a number of restarts, check if the logs for that pod show the below error:\nkubectl logs -n $namespace $cert-manager-cainjector-podname error registering secret controller: no matches for kind \"MutatingWebhookConfiguration\" in version \"admissionregistration.k8s.io/v1beta1\" If the Kubernetes cluster version is 1.22.2 (or higher), this error is due to an incompatible cert-manager version. Please upgrade to the latest CSM for Observability release (v1.0.1 or higher).\nWhy do I see FailedMount warnings when describing pods in my cluster? The warning can arise when a self-signed certificate for otel-collector is issued. It takes a few minutes or less for the signed certificate to generate and be consumed in the namespace. Once the certificate is consumed, the FailedMount warnings are resolved and the containers start properly.\nkubectl describe pod -n $namespace $pod MountVolume.SetUp failed for volume \"tls-secret\" : secret \"otel-collector-tls\" not found Unable to attach or mount volumes: unmounted volumes=[tls-secret], unattached volumes=[vxflexos-config-params vxflexos-config tls-secret karavi-metrics-powerflex-configmap kube-api-access-4fqgl karavi-authorization-config proxy-server-root-certificate]: timed out waiting for the condition Why do I see ‘Failed calling webhook’ error when reinstalling CSM for Observability? This warning can occur when a user uninstalls Observability by deleting the Kubernetes namespace before properly cleaning up by running helm delete on the Observability Helm installation. This results in the credential manager failing to properly integrate with Observability on future installations. The user may see the following error in the module pods upon reinstallation:\nError: INSTALLATION FAILED: failed to create resource: Internal error occurred: failed calling webhook \"webhook.cert-manager.io\": failed to call webhook: Post \"https://karavi-observability-cert-manager-webhook.karavi-observability.svc:443/mutate?timeout=10s\": dial tcp 10.106.44.80:443: connect: connection refused To resolve this, leave the CSM namespace in place after a failed installation, and run the below command:\nhelm delete karavi-observability --namespace [CSM_NAMESPACE] Then delete the namespace kubectl delete ns [CSM_NAMESPACE]. Wait until namespace is fully deleted, recreate the namespace, and reinstall Observability again.\n","categories":"","description":"Troubleshooting guide\n","excerpt":"Troubleshooting guide\n","ref":"/csm-docs/docs/observability/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate troubleshooting. If you experience a problem with CSM for Resiliency it is important you provide us with as much information as possible so that we can diagnose the issue and improve CSM for Resiliency. Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate sending us the logs and other information needed to diagnose a problem.\nMonitoring Protected Pods and Node Status There are two tools for monitoring the status of protected pods and nodes.\nThe mon.sh script displays the following information every 5 seconds:\nThe date and time. A list of the nodes and their status. A list of the taints applied to each node. A list of the leases in the CSI driver’s namespace. (Edit the script to change the CSI driver namespace if necessary. It defaults to vxflexos as the driver namespace.) A list of the CSI driver pods and their status (defaults to vxflexos namespace.) A list of the protected pods and their status. (Edit the script if you do not use the default podmon label key.) For systems with many protected pods, the monx.sh may provide a more usable output format. It displays the following fields every 5 seconds:\nThe date and time. A list of the nodes and their status. A list of the taints applied to each node. A summary for each node hosting protected pods of the number of pods in various states such as the Running, Creating, and Error states. (Edit the script if you do not use the default podmon label key.) A list of the protected pods not in the Running state. Collecting Logs If you have a problem with CSM for Resiliency it’s best to collect the logs to help with diagnosis. This tool can also be used to collect logs to submit as part of an issue to help us diagnose. Please use the collect_logs.sh. Type “collect_logs.sh –help” for help on the arguments.\nThe script collects the following information:\nA list of the driver pods. A list of the protected pods. The podmon container logs for each of the driver pods. The driver container logs for each of the driver pods. For each namespace containing protected pods, the recent events logged in that namespace. After successful execution of the script, it will deposit a file similar to driver.logs.20210319_1407.tgz in the current directory. Please submit that file with any issues.\nActions to take during failure to clean pod resources completely The node-podmon cleanup algorithm purposefully will not remove the node taint until all the protected volumes have been cleaned up from the node. This works well if the node fault lasts long enough that controller-podmon can evacuate all the protected pods from the node. However, if the failure is short-lived, and controller-podmon does not clean up all the protected pods on the node, or if for some reason node-podmon cannot clean a pod completely, the taint is left on the node, and manual intervention is required. The required intervention is for the operator to reboot the node, which will ensure that no zombie pods survive. Upon seeing the reboot, node-podmon will then remove the taint.\n","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency - Troubleshooting\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency - Troubleshooting\n","ref":"/csm-docs/docs/resiliency/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Frequently Asked Questions How can I diagnose an issue with Application Mobility? How can I view logs? How can I debug and troubleshoot issues with Kubernetes? Why are there error logs about a license? How can I diagnose an issue with Application Mobility? Once you have attempted to install Application Mobility to your Kubernetes or OpenShift cluster, the first step in troubleshooting is locating the problem.\nGet information on the state of your Pods.\nkubectl get pods -n $namespace Get verbose output of the current state of a Pod.\nkubectl describe pod -n $namespace $pod How can I view logs? View pod container logs. Output logs to a file for further debugging.\nkubectl logs -n $namespace $pod $container kubectl logs -n $namespace $pod $container \u003e $logFileName How can I debug and troubleshoot issues with Kubernetes? To debug your application that may not be behaving correctly, please reference Kubernetes troubleshooting applications guide.\nFor tips on debugging your cluster, please see this troubleshooting guide.\nWhy are there error logs about a license? Application Mobility requires a license in order to function. See the Deployment instructions for steps to request a license.\nThere will be errors in the logs about the license for these cases:\nLicense does not exist License is not valid for the current Kubernetes cluster License has expired ","categories":"","description":"Troubleshooting\n","excerpt":"Troubleshooting\n","ref":"/csm-docs/v1/applicationmobility/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Frequently Asked Questions Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? How can I diagnose an issue with Container Storage Modules (CSM) for Observability? How can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? How can I debug and troubleshoot issues with Kubernetes? How can I troubleshoot latency problems with CSM for Observability? Why does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage? Why do I see FailedMount warnings when describing pods in my cluster? Why do I see ‘Failed calling webhook’ error when reinstalling CSM for Observability? Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? This issue can arise when the topology service manifest is updated to expose the service as NodePort and a client makes a request to the service. Karavi-toplogy is configured with a self-signed or custom certificate and when a client does not recognize a server’s certificate, it shows an error and pings the server(karavi-topology) with the error. You would see the issue when accessing the service through a browser or curl:\nBrowser experience A user who tries to connect to karavi-topology on any browser may receive an error/warning message about the certificate. The message may vary depending on the browser. For instance, in Internet Explorer, you’ll see:\nThere is a problem with this website's security certificate. The security certificate presented by this website was not issued by a trusted certificate authority While this certificate problem may indicate an attempt to fool you or intercept data you send to the server, see resolution on how to fix it\nCurl experience A user who tries to connect to karavi-topology by using curl may receive the following warning or error message:\ncurl -v https://\u003ckaravi-topology-cluster-IP\u003e:\u003cport?/query * Trying ***********... * TCP_NODELAY set * Connected to *********** (***********) port 31433 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /etc/ssl/certs/ca-certificates.crt CApath: /etc/ssl/certs * TLSv1.3 (OUT), TLS handshake, Client hello (1): * TLSv1.3 (IN), TLS handshake, Server hello (2): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Unknown (8): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Certificate (11): * TLSv1.3 (OUT), TLS alert, Server hello (2): * SSL certificate problem: unable to get local issuer certificate * stopped the pause stream! * Closing connection 0 curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: https://curl.haxx.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. Kubernetes Admin experience Due to the error above, the client pings the topology server with a TLS handshake error which is logged in karavi-topology pod. For instance,\nkubectl logs -n powerflex karavi-topology-5d4669d6dd-trzxw 2021/04/27 09:38:28 Set DriverNames to [csi-vxflexos.dellemc.com] 2021/04/28 07:15:05 http: TLS handshake error from 10.42.0.0:58450: local error: tls: bad record MAC 2021/04/28 07:16:14 http: TLS handshake error from 10.42.0.0:55311: local error: tls: bad record MAC Resolution To resolve this issue, we need to configure the client to be aware of the karavi-topology certificate (this includes all custom SSL certificate that are not issued from a trusted Certificate Authority (CA))\nGet a copy of the certificate used by karavi-topology If we supplied a custom certificate during installing karavi-topology, we can simply open the .crt and copy the text. However, if it was assigned by cert-manager, you can get a copy of the certificate by running the following kubectl command on the clusters.\nkubectl -n \u003cnamespace\u003e get secret karavi-topology-tls -o jsonpath='{.data.tls\\.crt}' | base64 -d -----BEGIN CERTIFICATE----- RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe -----END CERTIFICATE----- Configure your client to accept the above certificate A workaround on most browsers is to accept the karavi-topology certificate by clicking Continue to this website (not recommended). This will make all other successive communication to not cause any certificate error. Anyhow, you will need to read the documentation for your specific client to configure the above certificate. For Grafana, here are two ways to configure the karavi-topology datasource to use the above certificate:\nDeploy certificate with new Grafana instance Please follow the steps in Sample Grafana Deployment but attach the certificate to your `grafana-values.yaml` before deploying. The file should look like: # grafana-values.yaml image: repository: grafana/grafana tag: 7.3.0 sha: \"\" pullPolicy: IfNotPresent service: type: NodePort ## Administrator credentials when not using an existing Secret adminUser: admin adminPassword: admin ## Pass the plugins you want installed as a list. ## plugins: - grafana-simple-json-datasource - briangann-datatable-panel - grafana-piechart-panel ## Configure grafana datasources ## ref: http://docs.grafana.org/administration/provisioning/#datasources ## datasources: datasources.yaml: apiVersion: 1 datasources: - name: Karavi-Topology type: grafana-simple-json-datasource access: proxy url: 'https://karavi-topology:8443' isDefault: null version: 1 editable: true jsonData: tlsAuthWithCACert: true secureJsonData: tlsCaCert: | -----BEGIN CERTIFICATE----- RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe -----END CERTIFICATE----- - name: Prometheus type: prometheus access: proxy url: 'http://prometheus:9090' isDefault: null version: 1 editable: true testFramework: enabled: false sidecar: datasources: enabled: true dashboards: enabled: true ## Additional grafana server ConfigMap mounts ## Defines additional mounts with ConfigMap. ConfigMap must be manually created in the namespace. extraConfigmapMounts: [] Add certificate to an existing Grafana instance - This only happens if you configure jsonData to not skip tls verification. If this is the case, you'll need to re-deploy grafana as shown above or, form Grafana UI, edit Karavi-Topology datasource to use the certificate. To do the latter: Visit your Grafana UI on a browser Navigate to setting and go to Data Sources Click on Karavi-Topology Ensure that Skip TLS Verify is already off Switch on With CA Cert Copy the above certificate into the TLS Auth Details text box that appears Click Save \u0026 Test and validate that everything is working fine when a green bar showing Data source is working appears How can I diagnose an issue with CSM for Observability? Once you have attempted to install CSM for Observability to your Kubernetes or OpenShift cluster, the first step in troubleshooting is locating the problem.\nGet information on the state of your Pods.\nkubectl get pods -n $namespace Get verbose output of the current state of a Pod.\nkubectl describe pod -n $namespace $pod How can I view logs? View pod container logs. Output logs to a file for further debugging.\nkubectl logs -n $namespace $pod $container kubectl logs -n $namespace $pod $container \u003e $logFileName More information for viewing logs can be found here.\nHow can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? The ServiceMonitor allows us to define how a set of services should be monitored by Prometheus. Please see our prometheus documentation for creating a ServiceMonitor.\nHow can I debug and troubleshoot issues with Kubernetes? To debug your application that may not be behaving correctly, please reference Kubernetes troubleshooting applications guide.\nFor tips on debugging your cluster, please see this troubleshooting guide.\nHow can I troubleshoot latency problems with CSM for Observability? CSM for Observability is instrumented to report trace data to Zipkin. Please see Tracing for more information on enabling tracing for CSM for Observability.\nWhy does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage? Check the pods in the CSM for Observability namespace. If the pod starting with ‘karavi-observability-cert-manager-cainjector-*’ is in ‘CrashLoopBackOff’ or ‘Error\" stage with a number of restarts, check if the logs for that pod show the below error:\nkubectl logs -n $namespace $cert-manager-cainjector-podname error registering secret controller: no matches for kind \"MutatingWebhookConfiguration\" in version \"admissionregistration.k8s.io/v1beta1\" If the Kubernetes cluster version is 1.22.2 (or higher), this error is due to an incompatible cert-manager version. Please upgrade to the latest CSM for Observability release (v1.0.1 or higher).\nWhy do I see FailedMount warnings when describing pods in my cluster? The warning can arise when a self-signed certificate for otel-collector is issued. It takes a few minutes or less for the signed certificate to generate and be consumed in the namespace. Once the certificate is consumed, the FailedMount warnings are resolved and the containers start properly.\nkubectl describe pod -n $namespace $pod MountVolume.SetUp failed for volume \"tls-secret\" : secret \"otel-collector-tls\" not found Unable to attach or mount volumes: unmounted volumes=[tls-secret], unattached volumes=[vxflexos-config-params vxflexos-config tls-secret karavi-metrics-powerflex-configmap kube-api-access-4fqgl karavi-authorization-config proxy-server-root-certificate]: timed out waiting for the condition Why do I see ‘Failed calling webhook’ error when reinstalling CSM for Observability? This warning can occur when a user uninstalls Observability by deleting the Kubernetes namespace before properly cleaning up by running helm delete on the Observability Helm installation. This results in the credential manager failing to properly integrate with Observability on future installations. The user may see the following error in the module pods upon reinstallation:\nError: INSTALLATION FAILED: failed to create resource: Internal error occurred: failed calling webhook \"webhook.cert-manager.io\": failed to call webhook: Post \"https://karavi-observability-cert-manager-webhook.karavi-observability.svc:443/mutate?timeout=10s\": dial tcp 10.106.44.80:443: connect: connection refused To resolve this, leave the CSM namespace in place after a failed installation, and run the below command:\nhelm delete karavi-observability --namespace [CSM_NAMESPACE] Then delete the namespace kubectl delete ns [CSM_NAMESPACE]. Wait until namespace is fully deleted, recreate the namespace, and reinstall Observability again.\nOther issues and workarounds Symptoms Prevention, Resolution or Workaround karavi-metrics pod crashes for all the supported platforms whenever there are PVs without claim in the cluster Work around is to create PVCs using the PVs which are not in bound state or delete the PVs without claims ","categories":"","description":"Troubleshooting guide\n","excerpt":"Troubleshooting guide\n","ref":"/csm-docs/v1/observability/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate troubleshooting. If you experience a problem with CSM for Resiliency it is important you provide us with as much information as possible so that we can diagnose the issue and improve CSM for Resiliency. Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate sending us the logs and other information needed to diagnose a problem.\nMonitoring Protected Pods and Node Status There are two tools for monitoring the status of protected pods and nodes.\nThe mon.sh script displays the following information every 5 seconds:\nThe date and time. A list of the nodes and their status. A list of the taints applied to each node. A list of the leases in the CSI driver’s namespace. (Edit the script to change the CSI driver namespace if necessary. It defaults to vxflexos as the driver namespace.) A list of the CSI driver pods and their status (defaults to vxflexos namespace.) A list of the protected pods and their status. (Edit the script if you do not use the default podmon label key.) For systems with many protected pods, the monx.sh may provide a more usable output format. It displays the following fields every 5 seconds:\nThe date and time. A list of the nodes and their status. A list of the taints applied to each node. A summary for each node hosting protected pods of the number of pods in various states such as the Running, Creating, and Error states. (Edit the script if you do not use the default podmon label key.) A list of the protected pods not in the Running state. Collecting Logs If you have a problem with CSM for Resiliency it’s best to collect the logs to help with diagnosis. This tool can also be used to collect logs to submit as part of an issue to help us diagnose. Please use the collect_logs.sh. Type “collect_logs.sh –help” for help on the arguments.\nThe script collects the following information:\nA list of the driver pods. A list of the protected pods. The podmon container logs for each of the driver pods. The driver container logs for each of the driver pods. For each namespace containing protected pods, the recent events logged in that namespace. After successful execution of the script, it will deposit a file similar to driver.logs.20210319_1407.tgz in the current directory. Please submit that file with any issues.\nActions to take during failure to clean pod resources completely The node-podmon cleanup algorithm purposefully will not remove the node taint until all the protected volumes have been cleaned up from the node. This works well if the node fault lasts long enough that controller-podmon can evacuate all the protected pods from the node. However, if the failure is short-lived, and controller-podmon does not clean up all the protected pods on the node, or if for some reason node-podmon cannot clean a pod completely, the taint is left on the node, and manual intervention is required. The required intervention is for the operator to reboot the node, which will ensure that no zombie pods survive. Upon seeing the reboot, node-podmon will then remove the taint.\n","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency - Troubleshooting\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency - Troubleshooting\n","ref":"/csm-docs/v1/resiliency/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Frequently Asked Questions How can I diagnose an issue with Application Mobility? How can I view logs? How can I debug and troubleshoot issues with Kubernetes? Why are there error logs about a license? How can I diagnose an issue with Application Mobility? Once you have attempted to install Application Mobility to your Kubernetes or OpenShift cluster, the first step in troubleshooting is locating the problem.\nGet information on the state of your Pods.\nkubectl get pods -n $namespace Get verbose output of the current state of a Pod.\nkubectl describe pod -n $namespace $pod How can I view logs? View pod container logs. Output logs to a file for further debugging.\nkubectl logs -n $namespace $pod $container kubectl logs -n $namespace $pod $container \u003e $logFileName How can I debug and troubleshoot issues with Kubernetes? To debug your application that may not be behaving correctly, please reference Kubernetes troubleshooting applications guide.\nFor tips on debugging your cluster, please see this troubleshooting guide.\nWhy are there error logs about a license? Application Mobility requires a license in order to function. See the Deployment instructions for steps to request a license.\nThere will be errors in the logs about the license for these cases:\nLicense does not exist License is not valid for the current Kubernetes cluster License has expired ","categories":"","description":"Troubleshooting\n","excerpt":"Troubleshooting\n","ref":"/csm-docs/v2/applicationmobility/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Frequently Asked Questions Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? How can I diagnose an issue with Container Storage Modules (CSM) for Observability? How can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? How can I debug and troubleshoot issues with Kubernetes? How can I troubleshoot latency problems with CSM for Observability? Why does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage? Why do I see FailedMount warnings when describing pods in my cluster? Why do I see ‘Failed calling webhook’ error when reinstalling CSM for Observability? Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? This issue can arise when the topology service manifest is updated to expose the service as NodePort and a client makes a request to the service. Karavi-toplogy is configured with a self-signed or custom certificate and when a client does not recognize a server’s certificate, it shows an error and pings the server(karavi-topology) with the error. You would see the issue when accessing the service through a browser or curl:\nBrowser experience A user who tries to connect to karavi-topology on any browser may receive an error/warning message about the certificate. The message may vary depending on the browser. For instance, in Internet Explorer, you’ll see:\nThere is a problem with this website's security certificate. The security certificate presented by this website was not issued by a trusted certificate authority While this certificate problem may indicate an attempt to fool you or intercept data you send to the server, see resolution on how to fix it\nCurl experience A user who tries to connect to karavi-topology by using curl may receive the following warning or error message:\ncurl -v https://\u003ckaravi-topology-cluster-IP\u003e:\u003cport?/query * Trying ***********... * TCP_NODELAY set * Connected to *********** (***********) port 31433 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /etc/ssl/certs/ca-certificates.crt CApath: /etc/ssl/certs * TLSv1.3 (OUT), TLS handshake, Client hello (1): * TLSv1.3 (IN), TLS handshake, Server hello (2): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Unknown (8): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Certificate (11): * TLSv1.3 (OUT), TLS alert, Server hello (2): * SSL certificate problem: unable to get local issuer certificate * stopped the pause stream! * Closing connection 0 curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: https://curl.haxx.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. Kubernetes Admin experience Due to the error above, the client pings the topology server with a TLS handshake error which is logged in karavi-topology pod. For instance,\nkubectl logs -n powerflex karavi-topology-5d4669d6dd-trzxw 2021/04/27 09:38:28 Set DriverNames to [csi-vxflexos.dellemc.com] 2021/04/28 07:15:05 http: TLS handshake error from 10.42.0.0:58450: local error: tls: bad record MAC 2021/04/28 07:16:14 http: TLS handshake error from 10.42.0.0:55311: local error: tls: bad record MAC Resolution To resolve this issue, we need to configure the client to be aware of the karavi-topology certificate (this includes all custom SSL certificate that are not issued from a trusted Certificate Authority (CA))\nGet a copy of the certificate used by karavi-topology If we supplied a custom certificate during installing karavi-topology, we can simply open the .crt and copy the text. However, if it was assigned by cert-manager, you can get a copy of the certificate by running the following kubectl command on the clusters.\nkubectl -n \u003cnamespace\u003e get secret karavi-topology-tls -o jsonpath='{.data.tls\\.crt}' | base64 -d -----BEGIN CERTIFICATE----- RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe -----END CERTIFICATE----- Configure your client to accept the above certificate A workaround on most browsers is to accept the karavi-topology certificate by clicking Continue to this website (not recommended). This will make all other successive communication to not cause any certificate error. Anyhow, you will need to read the documentation for your specific client to configure the above certificate. For Grafana, here are two ways to configure the karavi-topology datasource to use the above certificate:\nDeploy certificate with new Grafana instance Please follow the steps in Sample Grafana Deployment but attach the certificate to your `grafana-values.yaml` before deploying. The file should look like: # grafana-values.yaml image: repository: grafana/grafana tag: 7.3.0 sha: \"\" pullPolicy: IfNotPresent service: type: NodePort ## Administrator credentials when not using an existing Secret adminUser: admin adminPassword: admin ## Pass the plugins you want installed as a list. ## plugins: - grafana-simple-json-datasource - briangann-datatable-panel - grafana-piechart-panel ## Configure grafana datasources ## ref: http://docs.grafana.org/administration/provisioning/#datasources ## datasources: datasources.yaml: apiVersion: 1 datasources: - name: Karavi-Topology type: grafana-simple-json-datasource access: proxy url: 'https://karavi-topology:8443' isDefault: null version: 1 editable: true jsonData: tlsAuthWithCACert: true secureJsonData: tlsCaCert: | -----BEGIN CERTIFICATE----- RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe -----END CERTIFICATE----- - name: Prometheus type: prometheus access: proxy url: 'http://prometheus:9090' isDefault: null version: 1 editable: true testFramework: enabled: false sidecar: datasources: enabled: true dashboards: enabled: true ## Additional grafana server ConfigMap mounts ## Defines additional mounts with ConfigMap. ConfigMap must be manually created in the namespace. extraConfigmapMounts: [] Add certificate to an existing Grafana instance - This only happens if you configure jsonData to not skip tls verification. If this is the case, you'll need to re-deploy grafana as shown above or, form Grafana UI, edit Karavi-Topology datasource to use the certificate. To do the latter: Visit your Grafana UI on a browser Navigate to setting and go to Data Sources Click on Karavi-Topology Ensure that Skip TLS Verify is already off Switch on With CA Cert Copy the above certificate into the TLS Auth Details text box that appears Click Save \u0026 Test and validate that everything is working fine when a green bar showing Data source is working appears How can I diagnose an issue with CSM for Observability? Once you have attempted to install CSM for Observability to your Kubernetes or OpenShift cluster, the first step in troubleshooting is locating the problem.\nGet information on the state of your Pods.\nkubectl get pods -n $namespace Get verbose output of the current state of a Pod.\nkubectl describe pod -n $namespace $pod How can I view logs? View pod container logs. Output logs to a file for further debugging.\nkubectl logs -n $namespace $pod $container kubectl logs -n $namespace $pod $container \u003e $logFileName More information for viewing logs can be found here.\nHow can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? The ServiceMonitor allows us to define how a set of services should be monitored by Prometheus. Please see our prometheus documentation for creating a ServiceMonitor.\nHow can I debug and troubleshoot issues with Kubernetes? To debug your application that may not be behaving correctly, please reference Kubernetes troubleshooting applications guide.\nFor tips on debugging your cluster, please see this troubleshooting guide.\nHow can I troubleshoot latency problems with CSM for Observability? CSM for Observability is instrumented to report trace data to Zipkin. Please see Tracing for more information on enabling tracing for CSM for Observability.\nWhy does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage? Check the pods in the CSM for Observability namespace. If the pod starting with ‘karavi-observability-cert-manager-cainjector-*’ is in ‘CrashLoopBackOff’ or ‘Error\" stage with a number of restarts, check if the logs for that pod show the below error:\nkubectl logs -n $namespace $cert-manager-cainjector-podname error registering secret controller: no matches for kind \"MutatingWebhookConfiguration\" in version \"admissionregistration.k8s.io/v1beta1\" If the Kubernetes cluster version is 1.22.2 (or higher), this error is due to an incompatible cert-manager version. Please upgrade to the latest CSM for Observability release (v1.0.1 or higher).\nWhy do I see FailedMount warnings when describing pods in my cluster? The warning can arise when a self-signed certificate for otel-collector is issued. It takes a few minutes or less for the signed certificate to generate and be consumed in the namespace. Once the certificate is consumed, the FailedMount warnings are resolved and the containers start properly.\nkubectl describe pod -n $namespace $pod MountVolume.SetUp failed for volume \"tls-secret\" : secret \"otel-collector-tls\" not found Unable to attach or mount volumes: unmounted volumes=[tls-secret], unattached volumes=[vxflexos-config-params vxflexos-config tls-secret karavi-metrics-powerflex-configmap kube-api-access-4fqgl karavi-authorization-config proxy-server-root-certificate]: timed out waiting for the condition Why do I see ‘Failed calling webhook’ error when reinstalling CSM for Observability? This warning can occur when a user uninstalls Observability by deleting the Kubernetes namespace before properly cleaning up by running helm delete on the Observability Helm installation. This results in the credential manager failing to properly integrate with Observability on future installations. The user may see the following error in the module pods upon reinstallation:\nError: INSTALLATION FAILED: failed to create resource: Internal error occurred: failed calling webhook \"webhook.cert-manager.io\": failed to call webhook: Post \"https://karavi-observability-cert-manager-webhook.karavi-observability.svc:443/mutate?timeout=10s\": dial tcp 10.106.44.80:443: connect: connection refused To resolve this, leave the CSM namespace in place after a failed installation, and run the below command:\nhelm delete karavi-observability --namespace [CSM_NAMESPACE] Then delete the namespace kubectl delete ns [CSM_NAMESPACE]. Wait until namespace is fully deleted, recreate the namespace, and reinstall Observability again.\nOther issues and workarounds Symptoms Prevention, Resolution or Workaround karavi-metrics pod crashes for all the supported platforms whenever there are PVs without claim in the cluster Work around is to create PVCs using the PVs which are not in bound state or delete the PVs without claims ","categories":"","description":"Troubleshooting guide\n","excerpt":"Troubleshooting guide\n","ref":"/csm-docs/v2/observability/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate troubleshooting. If you experience a problem with CSM for Resiliency it is important you provide us with as much information as possible so that we can diagnose the issue and improve CSM for Resiliency. Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate sending us the logs and other information needed to diagnose a problem.\nMonitoring Protected Pods and Node Status There are two tools for monitoring the status of protected pods and nodes.\nThe mon.sh script displays the following information every 5 seconds:\nThe date and time. A list of the nodes and their status. A list of the taints applied to each node. A list of the leases in the CSI driver’s namespace. (Edit the script to change the CSI driver namespace if necessary. It defaults to vxflexos as the driver namespace.) A list of the CSI driver pods and their status (defaults to vxflexos namespace.) A list of the protected pods and their status. (Edit the script if you do not use the default podmon label key.) For systems with many protected pods, the monx.sh may provide a more usable output format. It displays the following fields every 5 seconds:\nThe date and time. A list of the nodes and their status. A list of the taints applied to each node. A summary for each node hosting protected pods of the number of pods in various states such as the Running, Creating, and Error states. (Edit the script if you do not use the default podmon label key.) A list of the protected pods not in the Running state. Collecting Logs If you have a problem with CSM for Resiliency it’s best to collect the logs to help with diagnosis. This tool can also be used to collect logs to submit as part of an issue to help us diagnose. Please use the collect_logs.sh. Type “collect_logs.sh –help” for help on the arguments.\nThe script collects the following information:\nA list of the driver pods. A list of the protected pods. The podmon container logs for each of the driver pods. The driver container logs for each of the driver pods. For each namespace containing protected pods, the recent events logged in that namespace. After successful execution of the script, it will deposit a file similar to driver.logs.20210319_1407.tgz in the current directory. Please submit that file with any issues.\nActions to take during failure to clean pod resources completely The node-podmon cleanup algorithm purposefully will not remove the node taint until all the protected volumes have been cleaned up from the node. This works well if the node fault lasts long enough that controller-podmon can evacuate all the protected pods from the node. However, if the failure is short-lived, and controller-podmon does not clean up all the protected pods on the node, or if for some reason node-podmon cannot clean a pod completely, the taint is left on the node, and manual intervention is required. The required intervention is for the operator to reboot the node, which will ensure that no zombie pods survive. Upon seeing the reboot, node-podmon will then remove the taint.\n","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency - Troubleshooting\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency - Troubleshooting\n","ref":"/csm-docs/v2/resiliency/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Frequently Asked Questions How can I diagnose an issue with Application Mobility? How can I view logs? How can I debug and troubleshoot issues with Kubernetes? Why are there error logs about a license? How can I diagnose an issue with Application Mobility? Once you have attempted to install Application Mobility to your Kubernetes or OpenShift cluster, the first step in troubleshooting is locating the problem.\nGet information on the state of your Pods.\nkubectl get pods -n $namespace Get verbose output of the current state of a Pod.\nkubectl describe pod -n $namespace $pod How can I view logs? View pod container logs. Output logs to a file for further debugging.\nkubectl logs -n $namespace $pod $container kubectl logs -n $namespace $pod $container \u003e $logFileName How can I debug and troubleshoot issues with Kubernetes? To debug your application that may not be behaving correctly, please reference Kubernetes troubleshooting applications guide.\nFor tips on debugging your cluster, please see this troubleshooting guide.\nWhy are there error logs about a license? Application Mobility requires a license in order to function. See the Deployment instructions for steps to request a license.\nThere will be errors in the logs about the license for these cases:\nLicense does not exist License is not valid for the current Kubernetes cluster License has expired ","categories":"","description":"Troubleshooting\n","excerpt":"Troubleshooting\n","ref":"/csm-docs/v3/applicationmobility/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Frequently Asked Questions Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? How can I diagnose an issue with Container Storage Modules (CSM) for Observability? How can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? How can I debug and troubleshoot issues with Kubernetes? How can I troubleshoot latency problems with CSM for Observability? Why does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage? Why do I see FailedMount warnings when describing pods in my cluster? Why do I see ‘Failed calling webhook’ error when reinstalling CSM for Observability? Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? This issue can arise when the topology service manifest is updated to expose the service as NodePort and a client makes a request to the service. Karavi-toplogy is configured with a self-signed or custom certificate and when a client does not recognize a server’s certificate, it shows an error and pings the server(karavi-topology) with the error. You would see the issue when accessing the service through a browser or curl:\nBrowser experience A user who tries to connect to karavi-topology on any browser may receive an error/warning message about the certificate. The message may vary depending on the browser. For instance, in Internet Explorer, you’ll see:\nThere is a problem with this website's security certificate. The security certificate presented by this website was not issued by a trusted certificate authority While this certificate problem may indicate an attempt to fool you or intercept data you send to the server, see resolution on how to fix it\nCurl experience A user who tries to connect to karavi-topology by using curl may receive the following warning or error message:\n[root@:~]$ curl -v https://\u003ckaravi-topology-cluster-IP\u003e:\u003cport?/query * Trying ***********... * TCP_NODELAY set * Connected to *********** (***********) port 31433 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /etc/ssl/certs/ca-certificates.crt CApath: /etc/ssl/certs * TLSv1.3 (OUT), TLS handshake, Client hello (1): * TLSv1.3 (IN), TLS handshake, Server hello (2): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Unknown (8): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Certificate (11): * TLSv1.3 (OUT), TLS alert, Server hello (2): * SSL certificate problem: unable to get local issuer certificate * stopped the pause stream! * Closing connection 0 curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: https://curl.haxx.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. Kubernetes Admin experience Due to the error above, the client pings the topology server with a TLS handshake error which is logged in karavi-topology pod. For instance,\n[root@:~]$ kubectl logs -n powerflex karavi-topology-5d4669d6dd-trzxw 2021/04/27 09:38:28 Set DriverNames to [csi-vxflexos.dellemc.com] 2021/04/28 07:15:05 http: TLS handshake error from 10.42.0.0:58450: local error: tls: bad record MAC 2021/04/28 07:16:14 http: TLS handshake error from 10.42.0.0:55311: local error: tls: bad record MAC Resolution To resolve this issue, we need to configure the client to be aware of the karavi-topology certificate (this includes all custom SSL certificate that are not issued from a trusted Certificate Authority (CA))\nGet a copy of the certificate used by karavi-topology If we supplied a custom certificate during installing karavi-topology, we can simply open the .crt and copy the text. However, if it was assigned by cert-manager, you can get a copy of the certificate by running the following kubectl command on the clusters.\n[root@:~]$ kubectl -n \u003cnamespace\u003e get secret karavi-topology-tls -o jsonpath='{.data.tls\\.crt}' | base64 -d -----BEGIN CERTIFICATE----- RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe -----END CERTIFICATE----- Configure your client to accept the above certificate A workaround on most browsers is to accept the karavi-topology certificate by clicking Continue to this website (not recommended). This will make all other successive communication to not cause any certificate error. Anyhow, you will need to read the documentation for your specific client to configure the above certificate. For Grafana, here are two ways to configure the karavi-topology datasource to use the above certificate:\nDeploy certificate with new Grafana instance Please follow the steps in Sample Grafana Deployment but attach the certificate to your `grafana-values.yaml` before deploying. The file should look like: # grafana-values.yaml image: repository: grafana/grafana tag: 7.3.0 sha: \"\" pullPolicy: IfNotPresent service: type: NodePort ## Administrator credentials when not using an existing Secret adminUser: admin adminPassword: admin ## Pass the plugins you want installed as a list. ## plugins: - grafana-simple-json-datasource - briangann-datatable-panel - grafana-piechart-panel ## Configure grafana datasources ## ref: http://docs.grafana.org/administration/provisioning/#datasources ## datasources: datasources.yaml: apiVersion: 1 datasources: - name: Karavi-Topology type: grafana-simple-json-datasource access: proxy url: 'https://karavi-topology:8443' isDefault: null version: 1 editable: true jsonData: tlsAuthWithCACert: true secureJsonData: tlsCaCert: | -----BEGIN CERTIFICATE----- RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe -----END CERTIFICATE----- - name: Prometheus type: prometheus access: proxy url: 'http://prometheus:9090' isDefault: null version: 1 editable: true testFramework: enabled: false sidecar: datasources: enabled: true dashboards: enabled: true ## Additional grafana server ConfigMap mounts ## Defines additional mounts with ConfigMap. ConfigMap must be manually created in the namespace. extraConfigmapMounts: [] Add certificate to an existing Grafana instance - This only happens if you configure jsonData to not skip tls verification. If this is the case, you'll need to re-deploy grafana as shown above or, form Grafana UI, edit Karavi-Topology datasource to use the certificate. To do the latter: Visit your Grafana UI on a browser Navigate to setting and go to Data Sources Click on Karavi-Topology Ensure that Skip TLS Verify is already off Switch on With CA Cert Copy the above certificate into the TLS Auth Details text box that appears Click Save \u0026 Test and validate that everything is working fine when a green bar showing Data source is working appears How can I diagnose an issue with CSM for Observability? Once you have attempted to install CSM for Observability to your Kubernetes or OpenShift cluster, the first step in troubleshooting is locating the problem.\nGet information on the state of your Pods.\nkubectl get pods -n $namespace Get verbose output of the current state of a Pod.\nkubectl describe pod -n $namespace $pod How can I view logs? View pod container logs. Output logs to a file for further debugging.\nkubectl logs -n $namespace $pod $container kubectl logs -n $namespace $pod $container \u003e $logFileName More information for viewing logs can be found here.\nHow can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? The ServiceMonitor allows us to define how a set of services should be monitored by Prometheus. Please see our prometheus documentation for creating a ServiceMonitor.\nHow can I debug and troubleshoot issues with Kubernetes? To debug your application that may not be behaving correctly, please reference Kubernetes troubleshooting applications guide.\nFor tips on debugging your cluster, please see this troubleshooting guide.\nHow can I troubleshoot latency problems with CSM for Observability? CSM for Observability is instrumented to report trace data to Zipkin. Please see Tracing for more information on enabling tracing for CSM for Observability.\nWhy does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage? Check the pods in the CSM for Observability namespace. If the pod starting with ‘karavi-observability-cert-manager-cainjector-*’ is in ‘CrashLoopBackOff’ or ‘Error\" stage with a number of restarts, check if the logs for that pod show the below error:\nkubectl logs -n $namespace $cert-manager-cainjector-podname error registering secret controller: no matches for kind \"MutatingWebhookConfiguration\" in version \"admissionregistration.k8s.io/v1beta1\" If the Kubernetes cluster version is 1.22.2 (or higher), this error is due to an incompatible cert-manager version. Please upgrade to the latest CSM for Observability release (v1.0.1 or higher).\nWhy do I see FailedMount warnings when describing pods in my cluster? The warning can arise when a self-signed certificate for otel-collector is issued. It takes a few minutes or less for the signed certificate to generate and be consumed in the namespace. Once the certificate is consumed, the FailedMount warnings are resolved and the containers start properly.\n[root@:~]$ kubectl describe pod -n $namespace $pod MountVolume.SetUp failed for volume \"tls-secret\" : secret \"otel-collector-tls\" not found Unable to attach or mount volumes: unmounted volumes=[tls-secret], unattached volumes=[vxflexos-config-params vxflexos-config tls-secret karavi-metrics-powerflex-configmap kube-api-access-4fqgl karavi-authorization-config proxy-server-root-certificate]: timed out waiting for the condition Why do I see ‘Failed calling webhook’ error when reinstalling CSM for Observability? This warning can occur when a user uninstalls Observability by deleting the Kubernetes namespace before properly cleaning up by running helm delete on the Observability Helm installation. This results in the credential manager failing to properly integrate with Observability on future installations. The user may see the following error in the module pods upon reinstallation:\nError: INSTALLATION FAILED: failed to create resource: Internal error occurred: failed calling webhook \"webhook.cert-manager.io\": failed to call webhook: Post \"https://karavi-observability-cert-manager-webhook.karavi-observability.svc:443/mutate?timeout=10s\": dial tcp 10.106.44.80:443: connect: connection refused To resolve this, leave the CSM namespace in place after a failed installation, and run the below command:\nhelm delete karavi-observability --namespace [CSM_NAMESPACE]\nThen delete the namespace kubectl delete ns [CSM_NAMESPACE]. Wait until namespace is fully deleted, recreate the namespace, and reinstall Observability again.\nOther issues and workarounds Symptoms Prevention, Resolution or Workaround karavi-metrics pod crashes for all the supported platforms whenever there are PVs without claim in the cluster Work around is to create PVCs using the PVs which are not in bound state or delete the PVs without claims ","categories":"","description":"Troubleshooting guide\n","excerpt":"Troubleshooting guide\n","ref":"/csm-docs/v3/observability/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate troubleshooting. If you experience a problem with CSM for Resiliency it is important you provide us with as much information as possible so that we can diagnose the issue and improve CSM for Resiliency. Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate sending us the logs and other information needed to diagnose a problem.\nMonitoring Protected Pods and Node Status There are two tools for monitoring the status of protected pods and nodes.\nThe mon.sh script displays the following information every 5 seconds:\nThe date and time. A list of the nodes and their status. A list of the taints applied to each node. A list of the leases in the CSI driver’s namespace. (Edit the script to change the CSI driver namespace if necessary. It defaults to vxflexos as the driver namespace.) A list of the CSI driver pods and their status (defaults to vxflexos namespace.) A list of the protected pods and their status. (Edit the script if you do not use the default podmon label key.) For systems with many protected pods, the monx.sh may provide a more usable output format. It displays the following fields every 5 seconds:\nThe date and time. A list of the nodes and their status. A list of the taints applied to each node. A summary for each node hosting protected pods of the number of pods in various states such as the Running, Creating, and Error states. (Edit the script if you do not use the default podmon label key.) A list of the protected pods not in the Running state. Collecting Logs If you have a problem with CSM for Resiliency it’s best to collect the logs to help with diagnosis. This tool can also be used to collect logs to submit as part of an issue to help us diagnose. Please use the collect_logs.sh. Type “collect_logs.sh –help” for help on the arguments.\nThe script collects the following information:\nA list of the driver pods. A list of the protected pods. The podmon container logs for each of the driver pods. The driver container logs for each of the driver pods. For each namespace containing protected pods, the recent events logged in that namespace. After successful execution of the script, it will deposit a file similar to driver.logs.20210319_1407.tgz in the current directory. Please submit that file with any issues.\nActions to take during failure to clean pod resources completely The node-podmon cleanup algorithm purposefully will not remove the node taint until all the protected volumes have been cleaned up from the node. This works well if the node fault lasts long enough that controller-podmon can evacuate all the protected pods from the node. However, if the failure is short-lived, and controller-podmon does not clean up all the protected pods on the node, or if for some reason node-podmon cannot clean a pod completely, the taint is left on the node, and manual intervention is required. The required intervention is for the operator to reboot the node, which will ensure that no zombie pods survive. Upon seeing the reboot, node-podmon will then remove the taint.\n","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency - Troubleshooting\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency - Troubleshooting\n","ref":"/csm-docs/v3/resiliency/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":" The Dell CSI Operator is no longer actively maintained or supported. Dell CSI Operator has been replaced with Dell CSM Operator. If you are currently using Dell CSI Operator, refer to the operator migration documentation to migrate from Dell CSI Operator to Dell CSM Operator.\n","categories":"","description":"Installation of CSI drivers using Dell CSI Operator\n","excerpt":"Installation of CSI drivers using Dell CSI Operator\n","ref":"/csm-docs/docs/csidriver/installation/operator/","tags":"","title":"CSI Driver installation using Dell CSI Operator"},{"body":" The Dell CSI Operator is no longer actively maintained or supported. Dell CSI Operator has been replaced with Dell CSM Operator. If you are currently using Dell CSI Operator, refer to the operator migration documentation to migrate from Dell CSI Operator to Dell CSM Operator.\nThe Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using the OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\nPrerequisites (Optional) Volume Snapshot Requirements On Upstream Kubernetes clusters, ensure that to install\nVolumeSnapshot CRDs - Install v1 VolumeSnapshot CRDs External Volume Snapshot Controller For detailed snapshot setup procedure, click here.\nNOTE: That step can be skipped with OpenShift.\nInstallation Dell CSI Operator has been tested and qualified with\nUpstream Kubernetes or OpenShift (see supported versions) Before you begin If you have installed an old version of the dell-csi-operator which was available with the name CSI Operator, please refer to this section before continuing.\nFull list of CSI Drivers and versions supported by the Dell CSI Operator CSI Driver Version ConfigVersion Kubernetes Version OpenShift Version CSI PowerMax 2.5.0 v2.5.0 1.23, 1.24, 1.25 4.10, 4.10 EUS, 4.11 CSI PowerMax 2.6.0 v2.6.0 1.24, 1.25, 1.26 4.10, 4.10 EUS, 4.11 CSI PowerMax 2.7.0 v2.7.0 1.25, 1.26, 1.27 4.11, 4.12, 4.12 EUS CSI PowerFlex 2.5.0 v2.5.0 1.23, 1.24, 1.25 4.10, 4.10 EUS, 4.11 CSI PowerFlex 2.6.0 v2.6.0 1.24, 1.25, 1.26 4.10, 4.10 EUS, 4.11 CSI PowerFlex 2.7.0 v2.7.0 1.25, 1.26, 1.27 4.11, 4.12 EUS, 4.12 CSI PowerScale 2.5.0 v2.5.0 1.23, 1.24, 1.25 4.10, 4.10 EUS, 4.11 CSI PowerScale 2.6.0 v2.6.0 1.24, 1.25, 1.26 4.10, 4.10 EUS, 4.11 CSI PowerScale 2.7.0 v2.7.0 1.25, 1.26, 1.27 4.11, 4.12, 4.12 EUS CSI Unity XT 2.5.0 v2.5.0 1.23, 1.24, 1.25 4.10, 4.10 EUS, 4.11 CSI Unity XT 2.6.0 v2.6.0 1.24, 1.25, 1.26 4.10, 4.10 EUS, 4.11 CSI Unity XT 2.7.0 v2.7.0 1.25, 1.26, 1.27 4.11, 4.12, 4.12 EUS CSI PowerStore 2.5.0 v2.5.0 1.23, 1.24, 1.25 4.10, 4.10 EUS, 4.11 CSI PowerStore 2.6.0 v2.6.0 1.24, 1.25, 1.26 4.10, 4.10 EUS, 4.11 CSI PowerStore 2.7.0 v2.7.0 1.25, 1.26, 1.27 4.11, 4.12, 4.12 EUS Dell CSI Operator can be installed via OLM (Operator Lifecycle Manager) and manual installation.\nInstallation Using Operator Lifecycle Manager dell-csi-operator can be installed using Operator Lifecycle Manager (OLM) on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the Operator to -\nAutomatic - If you want the Operator to be automatically installed or upgraded (once an upgrade becomes available) Manual - If you want a Cluster Administrator to manually review and approve the InstallPlan for installation/upgrades NOTE: The recommended version of OLM for upstream Kubernetes is v0.18.3.\nPre-Requisite for installation with OLM Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM.\n#Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM\ngit clone -b v1.12.0 https://github.com/dell/dell-csi-operator.git cd dell-csi-operator tar -czf config.tar.gz driverconfig/ kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u003coperator-namespace\u003e Upstream Kubernetes For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page. Red Hat OpenShift Clusters For installing via OpenShift with the Operator, go to the OpenShift page. Manual Installation Steps Skip step 1 for “offline bundle installation” and continue using the workspace created by untar of dell-csi-operator-bundle.tar.gz.\nClone and checkout the required dell-csi-operator version using git clone -b v1.12.0 https://github.com/dell/dell-csi-operator.git. cd dell-csi-operator Run bash scripts/install.sh to install the operator. Run the command oc get pods -n dell-csi-operator to validate the installation. If completed successfully, you should be able to see the operator-related pod in the ‘dell-csi-operator’ namespace. Custom Resource Definitions As part of the Dell CSI Operator installation, a CRD representing each driver installation is also installed.\nList of CRDs which are installed in API Group storage.dell.com\ncsipowermax csiunity csivxflexos csiisilon csipowerstore csipowermaxrevproxy For installation of the supported drivers, a CustomResource has to be created in your cluster.\nPre-Requisites for installation of the CSI Drivers Pre-requisites for upstream Kubernetes Clusters On upstream Kubernetes clusters, make sure to install\nVolumeSnapshot CRDs On clusters running v1.25,v1.26 \u0026 v1.27, make sure to install v1 VolumeSnapshot CRDs External Volume Snapshot Controller with the correct version Pre-requisites for Red Hat OpenShift Clusters iSCSI If you are installing a CSI driver which is going to use iSCSI as the transport protocol, please follow the following instructions.\nIn Red Hat OpenShift clusters, you can create a MachineConfig object using the console or oc to ensure that the iSCSI daemon starts on all the Red Hat CoreOS nodes. Here is an example of a MachineConfig object:\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: 99-iscsid labels: machineconfiguration.openshift.io/role: worker spec: config: ignition: version: 3.2.0 systemd: units: - name: \"iscsid.service\" enabled: true Once the MachineConfig object has been deployed, CoreOS will ensure that iscsid.service starts automatically.\nAlternatively, you can check the status of the iSCSI service by entering the following command on each worker node in the cluster:\nsudo systemctl status iscsid The service should be up and running (i.e. should be active state).\nIf the iscsid.service is not running, then perform the following steps on each worker node in the cluster\nLogin to worker nodes and check if the file /etc/iscsi/initiatorname.iscsi has been created properly If the file doesn’t exist or it doesn’t contain a valid ISCSI IQN, then make sure it exists with valid entries Ensure that iscsid service is running - Enable sudo systemctl enable iscsid \u0026 restart sudo systemctl restart iscsid iscsid if necessary. Note: If your worker nodes are running Red Hat CoreOS, make sure that automatic ISCSI login at boot is configured. Please contact RedHat for more details. MultiPath If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\nuser_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0 Use the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: workers-multipath-conf-default labels: machineconfiguration.openshift.io/role: worker spec: config: ignition: version: 3.2.0 storage: files: - contents: source: data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0K verification: {} filesystem: root mode: 400 path: /etc/multipath.conf After deploying thisMachineConfig object, CoreOS will start multipath service automatically.\nAlternatively, you can check the status of the multipath service by entering the following command in each worker nodes.\nsudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally, you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer to official documentation of the multipath configuration.\nInstalling CSI Driver via Operator CSI Drivers can be installed by creating a CustomResource object in your cluster.\nSample manifest files for each driver CustomResourceDefintion have been provided in the samples folder to help with the installation of the drivers. These files follow the naming convention\n{driver name}_{driver version}_k8s_{k8 version}.yaml Or\n{driver name}_{driver version}_ops_{OpenShift version}.yaml For e.g.\nsamples/powermax_v270_k8s_127.yaml* \u003c- To install CSI PowerMax driver v2.7.0 on a Kubernetes 1.27 cluster samples/powermax_v270_ops_412.yaml* \u003c- To install CSI PowerMax driver v2.7.0 on an OpenShift 4.12 cluster Copy the correct sample file and edit the mandatory \u0026 any optional parameters specific to your driver installation by following the instructions here\nNOTE: A detailed explanation of the various mandatory and optional fields in the CustomResource is available here. Please make sure to read through and understand the various fields.\nRun the following command to install the CSI driver.\nkubectl create -f \u003cdriver-manifest.yaml\u003e Note: If you are using an OLM based installation, the example manifests are available in the OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the installation Once the driver Custom Resource has been created, you can verify the installation\nCheck if Driver CR got created successfully\nFor e.g. – If you installed the PowerMax driver\nkubectl get csipowermax -n \u003cdriver-namespace\u003e Check the status of the Custom Resource to verify if the driver installation was successful\nIf the driver-namespace was set to test-powermax, and the name of the driver is powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the details of the Custom Resource.\nNote: If the state of the CustomResource is Running then all the driver pods have been successfully installed. If the state is SuccessFul, then it means the driver deployment was successful but some driver pods may not be in a Running state. Please refer to the Troubleshooting section here if you encounter any issues during installation.\nUpdate CSI Drivers The CSI Drivers installed by the Dell CSI Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include –\nModifying the installation directly via kubectl edit\nkubectl get \u003cdriver-object\u003e -n \u003cdriver-namespace\u003e For example - If the Unity XT driver is installed then run this command to get the object name of kind CSIUnity. #Replace driver-namespace with the namespace where the Unity XT driver is installed\nkubectl get csiunity -n \u003cdriver-namespace\u003e use the object name in kubectl edit command.\nkubectl edit \u003cdriver-object\u003e/\u003cobject-name\u003e -n \u003cdriver-namespace\u003e For example - If the object name is CSIUnity. #Replace object-name with the object name of kind CSIUnity\nkubectl edit csiunity/\u003cobject-name\u003e -n \u003cdriver-namespace\u003e and modify the installation. The usual fields to edit are the version of drivers, sidecars and the environment variables.\nModify the API object in place via kubectl patch command. For example if you want to patch the deployment to have two replicas for Unity XT driver then run this command to get the deployment\nkubectl get deployments -n \u003cdriver-namespace\u003e to patch the deployment with your patch object inline run this command. #Replace deployment with the name of the deployment\nkubectl patch deploy/\u003cdeployment\u003e -n \u003cdriver-namespace\u003e -p '{\"spec\":{\"replicas\": 2}}' to patch the deployment with your patch file run this command. #Replace deployment with the name of the deployment\nkubectl patch deployment \u003cdeployment\u003e --patch-file patch-file.yaml To create patch file or edit deployments, refer here for driver version \u0026 environment variables and here for version of side-cars. The latest versions of drivers could have additional environment variables or sidecars.\nThe below notes explain some of the general items to take care of.\nNOTES:\nIf you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required. driver: configVersion: v2.7.0 Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, we will have to modify the below block while upgrading the driver.To get the volume health state add external-health-monitor sidecar in the sidecar section and valueunder controller set to true and the value under node set to true as shown below:\ni. Add controller and node section as below: controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" dnsPolicy: ClusterFirstWithHostNet node: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" ii. Update the sidecar versions and add external-health-monitor sidecar if you want to enable health monitor of CSI volumes from Controller plugin: sideCars: - args: - --volume-name-prefix=csiunity - --default-fstype=ext4 image: k8s.gcr.io/sig-storage/csi-provisioner:v3.4.0 imagePullPolicy: IfNotPresent name: provisioner - args: - --snapshot-name-prefix=csiunitysnap image: k8s.gcr.io/sig-storage/csi-snapshotter:v6.2.1 imagePullPolicy: IfNotPresent name: snapshotter - args: - --monitor-interval=60s image: gcr.io/k8s-staging-sig-storage/csi-external-health-monitor-controller:v0.8.0 imagePullPolicy: IfNotPresent name: external-health-monitor - image: k8s.gcr.io/sig-storage/csi-attacher:v4.2.0 imagePullPolicy: IfNotPresent name: attacher - image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.6.3 imagePullPolicy: IfNotPresent name: registrar - image: k8s.gcr.io/sig-storage/csi-resizer:v1.7.0 imagePullPolicy: IfNotPresent name: resizer Configmap needs to be created with command kubectl create -f configmap.yaml using following yaml file. kind: ConfigMap metadata: name: unity-config-params namespace: unity data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"info\" ALLOW_RWO_MULTIPOD_ACCESS: \"false\" MAX_UNITY_VOLUMES_PER_NODE: \"0\" SYNC_NODE_INFO_TIME_INTERVAL: \"15\" TENANT_NAME: \"\" NOTE: Replicas in the driver CR file should not be greater than or equal to the number of worker nodes when upgrading the driver. If the Replicas count is not less than the worker node count, some of the driver controller pods would land in a pending state, and upgrade will not be successful. Driver controller pods go in a pending state because they have anti-affinity to each other and cannot be scheduled on nodes where there is a driver controller pod already running. Refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity for more details.\nNOTE: Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures.\nNOTE: From v1.4.0 onwards, Dell CSI Operator does not support the creation of StorageClass and VolumeSnapshotClass objects. Although these fields are still present in the various driver CustomResourceDefinitions, they would be ignored by the operator. These fields will be removed from the CustomResourceDefinitions in a future release. If StorageClass and VolumeSnapshotClass need to be retained, you should upgrade the driver as per the recommended way noted above. StorageClass and VolumeSnapshotClass would not be retained on driver uninstallation.\nSupported modifications Changing environment variable values for driver Adding (supported) environment variables Updating the image of the driver Limitations The Dell CSI Operator can’t manage any existing driver installed using Helm charts. If you already have installed one of the Dell CSI drivers in your cluster and want to use the operator based deployment, uninstall the driver and then redeploy the driver following the installation procedure described. The Dell CSI Operator is not fully compliant with the OperatorHub React UI elements and some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSI Operator Custom Resource Specification Each CSI Driver installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.\nBelow is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - Refer full list of supported driver for finding out the appropriate config version here replicas - Number of replicas for controller plugin - Must be set to 1 for all drivers\ndnsPolicy - Determines the dnsPolicy for the node daemonset. Accepted values are Default, ClusterFirst, ClusterFirstWithHostNet, None common\nThis field is mandatory and is used to specify common properties for both controller and the node plugin\nimage - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values Optional fields controller - List of environment variables and values which are applicable only for controller\nnode - List of environment variables and values which are applicable only for node\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver\nforceUpdate\nBoolean value which can be set to true in order to force update the status of the CSI Driver\ntolerations List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet\nIt should be set separately in the controller and node sections if you want separate set of tolerations for them\nnodeSelector Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet\nfsGroupPolicy Defines which FS Group policy mode to be used, Supported modes: None, File and ReadWriteOnceWithFSType\nHere is a sample specification annotated with comments to explain each field\napiVersion: storage.dell.com/v1 kind: CSIPowerMax # Type of the driver metadata: name: test-powermax # Name of the driver namespace: test-powermax # Namespace where driver is installed spec: driver: # Used to specify configuration version configVersion: v3 # Refer the table containing the full list of supported drivers to find the appropriate config version replicas: 1 forceUpdate: false # Set to true in case you want to force an update of driver status common: # All common specification image: \"dellemc/csi-powermax:v1.4.0.000R\" #driver image for a particular release imagePullPolicy: IfNotPresent envs: - name: X_CSI_POWERMAX_ENDPOINT value: \"https://0.0.0.0:8443/\" - name: X_CSI_K8S_CLUSTER_PREFIX value: \"XYZ\" You can set the field replicas to a higher number than 1 for the latest driver versions.\nNote - The image field should point to the correct image tag for version of the driver you are installing.\nFor e.g. - If you wish to install v2.7.0 of the CSI PowerMax driver, use the image tag dellemc/csi-powermax:v2.7.0\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. The only exception to this is modifications requested by the documentation, for example, filling in blank IPs or other such system-specific data. Any modifications not specifically requested by the documentation should be only done after consulting with Dell support.\nModify the driver specification Choose the correct configVersion. Refer the table containing the full list of supported drivers and versions. Provide the namespace (in metadata section) where you want to install the driver. Provide a name (in metadata section) for the driver. This will be the name of the Custom Resource. Edit the values for mandatory configuration parameters specific to your installation. Edit/Add any values for optional configuration parameters to customize your installation. If you are installing the latest versions of the CSI drivers, the default number of replicas is set to 2. You can increase/decrease this value. StorageClass and VolumeSnapshotClass New Installations You should not provide any StorageClass or VolumeSnapshotClass details during driver installation. The sample files for all the drivers have been updated to reflect this change. Even if these details are there in the sample files, StorageClass or VolumeSnapshotClass will not be created.\nWhat happens to my existing StorageClass \u0026 VolumeSnapshotClass objects In case you are upgrading an existing driver installation by using kubectl edit or by patching the object in place, any existing objects will remain as is. If you added more objects as part of the upgrade, then this request will be ignored by the Operator. If you uninstall the older driver, then any StorageClass or VolumeSnapshotClass objects will be deleted. An uninstall and followed by an install of the driver would also result in StorageClass and VolumeSnapshotClass getting deleted and not getting created again. NOTE: For more information on pre-requisites and parameters, please refer to the sub-pages below for each driver.\nNOTE: Storage Classes and Volume Snapshot Classes would no longer be created during the installation of the driver via an operator from v1.4.0 and higher.\n","categories":"","description":"Installation of CSI drivers using Dell CSI Operator\n","excerpt":"Installation of CSI drivers using Dell CSI Operator\n","ref":"/csm-docs/v1/csidriver/installation/operator/","tags":"","title":"CSI Driver installation using Dell CSI Operator"},{"body":" The Dell CSI Operator is no longer actively maintained or supported. Dell CSI Operator has been replaced with Dell CSM Operator. If you are currently using Dell CSI Operator, refer to the operator migration documentation to migrate from Dell CSI Operator to Dell CSM Operator.\nThe Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using the OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\nPrerequisites (Optional) Volume Snapshot Requirements On Upstream Kubernetes clusters, ensure that to install\nVolumeSnapshot CRDs - Install v1 VolumeSnapshot CRDs External Volume Snapshot Controller For detailed snapshot setup procedure, click here.\nNOTE: That step can be skipped with OpenShift.\nInstallation Dell CSI Operator has been tested and qualified with\nUpstream Kubernetes or OpenShift (see supported versions) Before you begin If you have installed an old version of the dell-csi-operator which was available with the name CSI Operator, please refer to this section before continuing.\nFull list of CSI Drivers and versions supported by the Dell CSI Operator CSI Driver Version ConfigVersion Kubernetes Version OpenShift Version CSI PowerMax 2.5.0 v2.5.0 1.23, 1.24, 1.25 4.10, 4.10 EUS, 4.11 CSI PowerMax 2.6.0 v2.6.0 1.24, 1.25, 1.26 4.10, 4.10 EUS, 4.11 CSI PowerMax 2.7.0 v2.7.0 1.25, 1.26, 1.27 4.11, 4.12 EUS, 4.12 CSI PowerFlex 2.5.0 v2.5.0 1.23, 1.24, 1.25 4.10, 4.10 EUS, 4.11 CSI PowerFlex 2.6.0 v2.6.0 1.24, 1.25, 1.26 4.10, 4.10 EUS, 4.11 CSI PowerFlex 2.7.0 v2.7.0 1.25, 1.26, 1.27 4.11, 4.12 EUS, 4.12 CSI PowerScale 2.5.0 v2.5.0 1.23, 1.24, 1.25 4.10, 4.10 EUS, 4.11 CSI PowerScale 2.6.0 v2.6.0 1.24, 1.25, 1.26 4.10, 4.10 EUS, 4.11 CSI PowerScale 2.7.0 v2.7.0 1.25, 1.26, 1.27 4.11, 4.12, 4.12 EUS CSI Unity XT 2.5.0 v2.5.0 1.23, 1.24, 1.25 4.10, 4.10 EUS, 4.11 CSI Unity XT 2.6.0 v2.6.0 1.24, 1.25, 1.26 4.10, 4.10 EUS, 4.11 CSI Unity XT 2.7.0 v2.7.0 1.25, 1.26, 1.27 4.11, 4.12, 4.12 EUS CSI PowerStore 2.5.0 v2.5.0 1.23, 1.24, 1.25 4.10, 4.10 EUS, 4.11 CSI PowerStore 2.6.0 v2.6.0 1.24, 1.25, 1.26 4.10, 4.10 EUS, 4.11 CSI PowerStore 2.7.0 v2.7.0 1.25, 1.26, 1.27 4.11, 4.12, 4.12 EUS Dell CSI Operator can be installed via OLM (Operator Lifecycle Manager) and manual installation.\nInstallation Using Operator Lifecycle Manager dell-csi-operator can be installed using Operator Lifecycle Manager (OLM) on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the Operator to -\nAutomatic - If you want the Operator to be automatically installed or upgraded (once an upgrade becomes available) Manual - If you want a Cluster Administrator to manually review and approve the InstallPlan for installation/upgrades NOTE: The recommended version of OLM for upstream Kubernetes is v0.18.3.\nPre-Requisite for installation with OLM Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM.\n#Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM\ngit clone -b v1.12.0 https://github.com/dell/dell-csi-operator.git cd dell-csi-operator tar -czf config.tar.gz driverconfig/ kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u003coperator-namespace\u003e Upstream Kubernetes For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page. Red Hat OpenShift Clusters For installing via OpenShift with the Operator, go to the OpenShift page. Manual Installation Steps Skip step 1 for “offline bundle installation” and continue using the workspace created by untar of dell-csi-operator-bundle.tar.gz.\nClone and checkout the required dell-csi-operator version using git clone -b v1.12.0 https://github.com/dell/dell-csi-operator.git. cd dell-csi-operator Run bash scripts/install.sh to install the operator. Run the command oc get pods -n dell-csi-operator to validate the installation. If completed successfully, you should be able to see the operator-related pod in the ‘dell-csi-operator’ namespace. Custom Resource Definitions As part of the Dell CSI Operator installation, a CRD representing each driver installation is also installed.\nList of CRDs which are installed in API Group storage.dell.com\ncsipowermax csiunity csivxflexos csiisilon csipowerstore csipowermaxrevproxy For installation of the supported drivers, a CustomResource has to be created in your cluster.\nPre-Requisites for installation of the CSI Drivers Pre-requisites for upstream Kubernetes Clusters On upstream Kubernetes clusters, make sure to install\nVolumeSnapshot CRDs On clusters running v1.25,v1.26 \u0026 v1.27, make sure to install v1 VolumeSnapshot CRDs External Volume Snapshot Controller with the correct version Pre-requisites for Red Hat OpenShift Clusters iSCSI If you are installing a CSI driver which is going to use iSCSI as the transport protocol, please follow the following instructions.\nIn Red Hat OpenShift clusters, you can create a MachineConfig object using the console or oc to ensure that the iSCSI daemon starts on all the Red Hat CoreOS nodes. Here is an example of a MachineConfig object:\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: 99-iscsid labels: machineconfiguration.openshift.io/role: worker spec: config: ignition: version: 3.2.0 systemd: units: - name: \"iscsid.service\" enabled: true Once the MachineConfig object has been deployed, CoreOS will ensure that iscsid.service starts automatically.\nAlternatively, you can check the status of the iSCSI service by entering the following command on each worker node in the cluster:\nsudo systemctl status iscsid The service should be up and running (i.e. should be active state).\nIf the iscsid.service is not running, then perform the following steps on each worker node in the cluster\nLogin to worker nodes and check if the file /etc/iscsi/initiatorname.iscsi has been created properly If the file doesn’t exist or it doesn’t contain a valid ISCSI IQN, then make sure it exists with valid entries Ensure that iscsid service is running - Enable sudo systemctl enable iscsid \u0026 restart sudo systemctl restart iscsid iscsid if necessary. Note: If your worker nodes are running Red Hat CoreOS, make sure that automatic ISCSI login at boot is configured. Please contact RedHat for more details. MultiPath If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\nuser_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0 Use the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: workers-multipath-conf-default labels: machineconfiguration.openshift.io/role: worker spec: config: ignition: version: 3.2.0 storage: files: - contents: source: data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0K verification: {} filesystem: root mode: 400 path: /etc/multipath.conf After deploying thisMachineConfig object, CoreOS will start multipath service automatically.\nAlternatively, you can check the status of the multipath service by entering the following command in each worker nodes.\nsudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally, you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer to official documentation of the multipath configuration.\nInstalling CSI Driver via Operator CSI Drivers can be installed by creating a CustomResource object in your cluster.\nSample manifest files for each driver CustomResourceDefintion have been provided in the samples folder to help with the installation of the drivers. These files follow the naming convention\n{driver name}_{driver version}_k8s_{k8 version}.yaml Or\n{driver name}_{driver version}_ops_{OpenShift version}.yaml For e.g.\nsamples/powermax_v270_k8s_126.yaml* \u003c- To install CSI PowerMax driver v2.7.0 on a Kubernetes 1.26 cluster samples/powermax_v270_ops_411.yaml* \u003c- To install CSI PowerMax driver v2.7.0 on an OpenShift 4.11 cluster Copy the correct sample file and edit the mandatory \u0026 any optional parameters specific to your driver installation by following the instructions here\nNOTE: A detailed explanation of the various mandatory and optional fields in the CustomResource is available here. Please make sure to read through and understand the various fields.\nRun the following command to install the CSI driver.\nkubectl create -f \u003cdriver-manifest.yaml\u003e Note: If you are using an OLM based installation, the example manifests are available in the OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the installation Once the driver Custom Resource has been created, you can verify the installation\nCheck if Driver CR got created successfully\nFor e.g. – If you installed the PowerMax driver\nkubectl get csipowermax -n \u003cdriver-namespace\u003e Check the status of the Custom Resource to verify if the driver installation was successful\nIf the driver-namespace was set to test-powermax, and the name of the driver is powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the details of the Custom Resource.\nNote: If the state of the CustomResource is Running then all the driver pods have been successfully installed. If the state is SuccessFul, then it means the driver deployment was successful but some driver pods may not be in a Running state. Please refer to the Troubleshooting section here if you encounter any issues during installation.\nUpdate CSI Drivers The CSI Drivers installed by the Dell CSI Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include –\nModifying the installation directly via kubectl edit\nkubectl get \u003cdriver-object\u003e -n \u003cdriver-namespace\u003e For example - If the Unity XT driver is installed then run this command to get the object name of kind CSIUnity. #Replace driver-namespace with the namespace where the Unity XT driver is installed\nkubectl get csiunity -n \u003cdriver-namespace\u003e use the object name in kubectl edit command.\nkubectl edit \u003cdriver-object\u003e/\u003cobject-name\u003e -n \u003cdriver-namespace\u003e For example - If the object name is CSIUnity. #Replace object-name with the object name of kind CSIUnity\nkubectl edit csiunity/\u003cobject-name\u003e -n \u003cdriver-namespace\u003e and modify the installation. The usual fields to edit are the version of drivers, sidecars and the environment variables.\nModify the API object in place via kubectl patch command. For example if you want to patch the deployment to have two replicas for Unity XT driver then run this command to get the deployment\nkubectl get deployments -n \u003cdriver-namespace\u003e to patch the deployment with your patch object inline run this command. #Replace deployment with the name of the deployment\nkubectl patch deploy/\u003cdeployment\u003e -n \u003cdriver-namespace\u003e -p '{\"spec\":{\"replicas\": 2}}' to patch the deployment with your patch file run this command. #Replace deployment with the name of the deployment\nkubectl patch deployment \u003cdeployment\u003e --patch-file patch-file.yaml To create patch file or edit deployments, refer here for driver version \u0026 environment variables and here for version of side-cars. The latest versions of drivers could have additional environment variables or sidecars.\nThe below notes explain some of the general items to take care of.\nNOTES:\nIf you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required. driver: configVersion: v2.7.0 Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, we will have to modify the below block while upgrading the driver.To get the volume health state add external-health-monitor sidecar in the sidecar section and valueunder controller set to true and the value under node set to true as shown below:\ni. Add controller and node section as below: controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" dnsPolicy: ClusterFirstWithHostNet node: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" ii. Update the sidecar versions and add external-health-monitor sidecar if you want to enable health monitor of CSI volumes from Controller plugin: sideCars: - args: - --volume-name-prefix=csiunity - --default-fstype=ext4 image: k8s.gcr.io/sig-storage/csi-provisioner:v3.4.0 imagePullPolicy: IfNotPresent name: provisioner - args: - --snapshot-name-prefix=csiunitysnap image: k8s.gcr.io/sig-storage/csi-snapshotter:v6.2.1 imagePullPolicy: IfNotPresent name: snapshotter - args: - --monitor-interval=60s image: gcr.io/k8s-staging-sig-storage/csi-external-health-monitor-controller:v0.8.0 imagePullPolicy: IfNotPresent name: external-health-monitor - image: k8s.gcr.io/sig-storage/csi-attacher:v4.2.0 imagePullPolicy: IfNotPresent name: attacher - image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.6.3 imagePullPolicy: IfNotPresent name: registrar - image: k8s.gcr.io/sig-storage/csi-resizer:v1.7.0 imagePullPolicy: IfNotPresent name: resizer Configmap needs to be created with command kubectl create -f configmap.yaml using following yaml file. kind: ConfigMap metadata: name: unity-config-params namespace: unity data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"info\" ALLOW_RWO_MULTIPOD_ACCESS: \"false\" MAX_UNITY_VOLUMES_PER_NODE: \"0\" SYNC_NODE_INFO_TIME_INTERVAL: \"15\" TENANT_NAME: \"\" NOTE: Replicas in the driver CR file should not be greater than or equal to the number of worker nodes when upgrading the driver. If the Replicas count is not less than the worker node count, some of the driver controller pods would land in a pending state, and upgrade will not be successful. Driver controller pods go in a pending state because they have anti-affinity to each other and cannot be scheduled on nodes where there is a driver controller pod already running. Refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity for more details.\nNOTE: Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures.\nNOTE: From v1.4.0 onwards, Dell CSI Operator does not support the creation of StorageClass and VolumeSnapshotClass objects. Although these fields are still present in the various driver CustomResourceDefinitions, they would be ignored by the operator. These fields will be removed from the CustomResourceDefinitions in a future release. If StorageClass and VolumeSnapshotClass need to be retained, you should upgrade the driver as per the recommended way noted above. StorageClass and VolumeSnapshotClass would not be retained on driver uninstallation.\nSupported modifications Changing environment variable values for driver Adding (supported) environment variables Updating the image of the driver Limitations The Dell CSI Operator can’t manage any existing driver installed using Helm charts. If you already have installed one of the Dell CSI drivers in your cluster and want to use the operator based deployment, uninstall the driver and then redeploy the driver following the installation procedure described. The Dell CSI Operator is not fully compliant with the OperatorHub React UI elements and some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSI Operator Custom Resource Specification Each CSI Driver installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.\nBelow is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - Refer full list of supported driver for finding out the appropriate config version here replicas - Number of replicas for controller plugin - Must be set to 1 for all drivers\ndnsPolicy - Determines the dnsPolicy for the node daemonset. Accepted values are Default, ClusterFirst, ClusterFirstWithHostNet, None common\nThis field is mandatory and is used to specify common properties for both controller and the node plugin\nimage - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values Optional fields controller - List of environment variables and values which are applicable only for controller\nnode - List of environment variables and values which are applicable only for node\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver\nforceUpdate\nBoolean value which can be set to true in order to force update the status of the CSI Driver\ntolerations List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet\nIt should be set separately in the controller and node sections if you want separate set of tolerations for them\nnodeSelector Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet\nfsGroupPolicy Defines which FS Group policy mode to be used, Supported modes: None, File and ReadWriteOnceWithFSType\nHere is a sample specification annotated with comments to explain each field\napiVersion: storage.dell.com/v1 kind: CSIPowerMax # Type of the driver metadata: name: test-powermax # Name of the driver namespace: test-powermax # Namespace where driver is installed spec: driver: # Used to specify configuration version configVersion: v3 # Refer the table containing the full list of supported drivers to find the appropriate config version replicas: 1 forceUpdate: false # Set to true in case you want to force an update of driver status common: # All common specification image: \"dellemc/csi-powermax:v1.4.0.000R\" #driver image for a particular release imagePullPolicy: IfNotPresent envs: - name: X_CSI_POWERMAX_ENDPOINT value: \"https://0.0.0.0:8443/\" - name: X_CSI_K8S_CLUSTER_PREFIX value: \"XYZ\" You can set the field replicas to a higher number than 1 for the latest driver versions.\nNote - The image field should point to the correct image tag for version of the driver you are installing.\nFor e.g. - If you wish to install v2.7.0 of the CSI PowerMax driver, use the image tag dellemc/csi-powermax:v2.7.0\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. The only exception to this is modifications requested by the documentation, for example, filling in blank IPs or other such system-specific data. Any modifications not specifically requested by the documentation should be only done after consulting with Dell support.\nModify the driver specification Choose the correct configVersion. Refer the table containing the full list of supported drivers and versions. Provide the namespace (in metadata section) where you want to install the driver. Provide a name (in metadata section) for the driver. This will be the name of the Custom Resource. Edit the values for mandatory configuration parameters specific to your installation. Edit/Add any values for optional configuration parameters to customize your installation. If you are installing the latest versions of the CSI drivers, the default number of replicas is set to 2. You can increase/decrease this value. StorageClass and VolumeSnapshotClass New Installations You should not provide any StorageClass or VolumeSnapshotClass details during driver installation. The sample files for all the drivers have been updated to reflect this change. Even if these details are there in the sample files, StorageClass or VolumeSnapshotClass will not be created.\nWhat happens to my existing StorageClass \u0026 VolumeSnapshotClass objects In case you are upgrading an existing driver installation by using kubectl edit or by patching the object in place, any existing objects will remain as is. If you added more objects as part of the upgrade, then this request will be ignored by the Operator. If you uninstall the older driver, then any StorageClass or VolumeSnapshotClass objects will be deleted. An uninstall and followed by an install of the driver would also result in StorageClass and VolumeSnapshotClass getting deleted and not getting created again. NOTE: For more information on pre-requisites and parameters, please refer to the sub-pages below for each driver.\nNOTE: Storage Classes and Volume Snapshot Classes would no longer be created during the installation of the driver via an operator from v1.4.0 and higher.\n","categories":"","description":"Installation of CSI drivers using Dell CSI Operator\n","excerpt":"Installation of CSI drivers using Dell CSI Operator\n","ref":"/csm-docs/v2/csidriver/installation/operator/","tags":"","title":"CSI Driver installation using Dell CSI Operator"},{"body":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using the OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\nPrerequisites Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here:v6.2.1\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\nA common snapshot controller A CSI external-snapshotter sidecar The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v6.2.1\nNOTE:\nThe CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration. Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\nIt is recommended to use 6.2.1 version of snapshotter/snapshot-controller. Installation Dell CSI Operator has been tested and qualified with\nUpstream Kubernetes or OpenShift (see supported versions) Before you begin If you have installed an old version of the dell-csi-operator which was available with the name CSI Operator, please refer to this section before continuing.\nFull list of CSI Drivers and versions supported by the Dell CSI Operator CSI Driver Version ConfigVersion Kubernetes Version OpenShift Version CSI PowerMax 2.4.0 v2.4.0 1.22, 1.23, 1.24 4.9, 4.10, 4.10 EUS CSI PowerMax 2.5.0 v2.5.0 1.23, 1.24, 1.25 4.10, 4.10 EUS, 4.11 CSI PowerMax 2.6.0 v2.6.0 1.24, 1.25, 1.26 4.10, 4.10 EUS, 4.11 CSI PowerFlex 2.4.0 v2.4.0 1.22, 1.23, 1.24 4.9, 4.10, 4.10 EUS CSI PowerFlex 2.5.0 v2.5.0 1.23, 1.24, 1.25 4.10, 4.10 EUS, 4.11 CSI PowerFlex 2.6.0 v2.6.0 1.24, 1.25, 1.26 4.10, 4.10 EUS, 4.11 CSI PowerScale 2.4.0 v2.4.0 1.22, 1.23, 1.24 4.9, 4.10, 4.10 EUS CSI PowerScale 2.5.0 v2.5.0 1.23, 1.24, 1.25 4.10, 4.10 EUS, 4.11 CSI PowerScale 2.6.0 v2.6.0 1.24, 1.25, 1.26 4.10, 4.10 EUS, 4.11 CSI Unity XT 2.4.0 v2.4.0 1.22, 1.23, 1.24 4.9, 4.10, 4.10 EUS CSI Unity XT 2.5.0 v2.5.0 1.23, 1.24, 1.25 4.10, 4.10 EUS, 4.11 CSI Unity XT 2.6.0 v2.6.0 1.24, 1.25, 1.26 4.10, 4.10 EUS, 4.11 CSI PowerStore 2.4.0 v2.4.0 1.22, 1.23, 1.24 4.9, 4.10, 4.10 EUS CSI PowerStore 2.5.0 v2.5.0 1.23, 1.24, 1.25 4.10, 4.10 EUS, 4.11 CSI PowerStore 2.6.0 v2.6.0 1.24, 1.25, 1.26 4.10, 4.10 EUS, 4.11 Dell CSI Operator can be installed via OLM (Operator Lifecycle Manager) and manual installation.\nInstallation Using Operator Lifecycle Manager dell-csi-operator can be installed using Operator Lifecycle Manager (OLM) on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the Operator to -\nAutomatic - If you want the Operator to be automatically installed or upgraded (once an upgrade becomes available) Manual - If you want a Cluster Administrator to manually review and approve the InstallPlan for installation/upgrades NOTE: The recommended version of OLM for upstream Kubernetes is v0.18.3.\nPre-Requisite for installation with OLM Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM.\n$ git clone -b v1.11.0 https://github.com/dell/dell-csi-operator.git $ cd dell-csi-operator $ tar -czf config.tar.gz driverconfig/ # Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM $ kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u003coperator-namespace\u003e Upstream Kubernetes For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page. Red Hat OpenShift Clusters For installing via OpenShift with the Operator, go to the OpenShift page. Manual Installation Steps Skip step 1 for “offline bundle installation” and continue using the workspace created by untar of dell-csi-operator-bundle.tar.gz.\nClone and checkout the required dell-csi-operator version using git clone -b v1.11.0 https://github.com/dell/dell-csi-operator.git. cd dell-csi-operator Run bash scripts/install.sh to install the operator. Run the command oc get pods -n dell-csi-operator to validate the installation. If completed successfully, you should be able to see the operator-related pod in the ‘dell-csi-operator’ namespace. Custom Resource Definitions As part of the Dell CSI Operator installation, a CRD representing each driver installation is also installed.\nList of CRDs which are installed in API Group storage.dell.com\ncsipowermax csiunity csivxflexos csiisilon csipowerstore csipowermaxrevproxy For installation of the supported drivers, a CustomResource has to be created in your cluster.\nPre-Requisites for installation of the CSI Drivers Pre-requisites for upstream Kubernetes Clusters On upstream Kubernetes clusters, make sure to install\nVolumeSnapshot CRDs On clusters running v1.24,v1.25 \u0026 v1.26, make sure to install v1 VolumeSnapshot CRDs External Volume Snapshot Controller with the correct version Pre-requisites for Red Hat OpenShift Clusters iSCSI If you are installing a CSI driver which is going to use iSCSI as the transport protocol, please follow the following instructions.\nIn Red Hat OpenShift clusters, you can create a MachineConfig object using the console or oc to ensure that the iSCSI daemon starts on all the Red Hat CoreOS nodes. Here is an example of a MachineConfig object:\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: 99-iscsid labels: machineconfiguration.openshift.io/role: worker spec: config: ignition: version: 3.2.0 systemd: units: - name: \"iscsid.service\" enabled: true Once the MachineConfig object has been deployed, CoreOS will ensure that iscsid.service starts automatically.\nAlternatively, you can check the status of the iSCSI service by entering the following command on each worker node in the cluster:\nsudo systemctl status iscsid\nThe service should be up and running (i.e. should be active state).\nIf the iscsid.service is not running, then perform the following steps on each worker node in the cluster\nLogin to worker nodes and check if the file /etc/iscsi/initiatorname.iscsi has been created properly If the file doesn’t exist or it doesn’t contain a valid ISCSI IQN, then make sure it exists with valid entries Ensure that iscsid service is running - Enable sudo systemctl enable iscsid \u0026 restart sudo systemctl restart iscsid iscsid if necessary. Note: If your worker nodes are running Red Hat CoreOS, make sure that automatic ISCSI login at boot is configured. Please contact RedHat for more details. MultiPath If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\necho 'defaults { user_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0\nUse the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: workers-multipath-conf-default labels: machineconfiguration.openshift.io/role: worker spec: config: ignition: version: 3.2.0 storage: files: - contents: source: data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0K verification: {} filesystem: root mode: 400 path: /etc/multipath.conf After deploying thisMachineConfig object, CoreOS will start multipath service automatically.\nAlternatively, you can check the status of the multipath service by entering the following command in each worker nodes.\nsudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally, you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer to official documentation of the multipath configuration.\nInstalling CSI Driver via Operator CSI Drivers can be installed by creating a CustomResource object in your cluster.\nSample manifest files for each driver CustomResourceDefintion have been provided in the samples folder to help with the installation of the drivers. These files follow the naming convention\n{driver name}_{driver version}_k8s_{k8 version}.yaml Or\n{driver name}_{driver version}_ops_{OpenShift version}.yaml For e.g.\nsamples/powermax_v260_k8s_126.yaml* \u003c- To install CSI PowerMax driver v2.6.0 on a Kubernetes 1.26 cluster samples/powermax_v260_ops_411.yaml* \u003c- To install CSI PowerMax driver v2.6.0 on an OpenShift 4.11 cluster Copy the correct sample file and edit the mandatory \u0026 any optional parameters specific to your driver installation by following the instructions here\nNOTE: A detailed explanation of the various mandatory and optional fields in the CustomResource is available here. Please make sure to read through and understand the various fields.\nRun the following command to install the CSI driver.\nkubectl create -f \u003cdriver-manifest.yaml\u003e Note: If you are using an OLM based installation, the example manifests are available in the OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the installation Once the driver Custom Resource has been created, you can verify the installation\nCheck if Driver CR got created successfully\nFor e.g. – If you installed the PowerMax driver\n$ kubectl get csipowermax -n \u003cdriver-namespace\u003e Check the status of the Custom Resource to verify if the driver installation was successful\nIf the driver-namespace was set to test-powermax, and the name of the driver is powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the details of the Custom Resource.\nNote: If the state of the CustomResource is Running then all the driver pods have been successfully installed. If the state is SuccessFul, then it means the driver deployment was successful but some driver pods may not be in a Running state. Please refer to the Troubleshooting section here if you encounter any issues during installation.\nUpdate CSI Drivers The CSI Drivers installed by the Dell CSI Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include –\nModifying the installation directly via kubectl edit\n$ kubectl get \u003cdriver-object\u003e -n \u003cdriver-namespace\u003e For example - If the Unity XT driver is installed then run this command to get the object name of kind CSIUnity.\n# Replace driver-namespace with the namespace where the Unity XT driver is installed $ kubectl get csiunity -n \u003cdriver-namespace\u003e use the object name in kubectl edit command.\n$ kubectl edit \u003cdriver-object\u003e/\u003cobject-name\u003e -n \u003cdriver-namespace\u003e For example - If the object name is CSIUnity.\n# Replace object-name with the object name of kind CSIUnity $ kubectl edit csiunity/\u003cobject-name\u003e -n \u003cdriver-namespace\u003e and modify the installation. The usual fields to edit are the version of drivers, sidecars and the environment variables.\nModify the API object in place via kubectl patch command. For example if you want to patch the deployment to have two replicas for Unity XT driver then run this command to get the deployment\n$ kubectl get deployments -n \u003cdriver-namespace\u003e to patch the deployment with your patch object inline run this command.\n# Replace deployment with the name of the deployment $ kubectl patch deploy/\u003cdeployment\u003e -n \u003cdriver-namespace\u003e -p '{\"spec\":{\"replicas\": 2}}' to patch the deployment with your patch file run this command.\n# Replace deployment with the name of the deployment kubectl patch deployment \u003cdeployment\u003e --patch-file patch-file.yaml To create patch file or edit deployments, refer here for driver version \u0026 environment variables and here for version of side-cars. The latest versions of drivers could have additional environment variables or sidecars.\nThe below notes explain some of the general items to take care of.\nNOTES:\nIf you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required. driver: configVersion: v2.6.0 Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, we will have to modify the below block while upgrading the driver.To get the volume health state add external-health-monitor sidecar in the sidecar section and valueunder controller set to true and the value under node set to true as shown below:\ni. Add controller and node section as below: controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" dnsPolicy: ClusterFirstWithHostNet node: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" ii. Update the sidecar versions and add external-health-monitor sidecar if you want to enable health monitor of CSI volumes from Controller plugin: sideCars: - args: - --volume-name-prefix=csiunity - --default-fstype=ext4 image: k8s.gcr.io/sig-storage/csi-provisioner:v3.4.0 imagePullPolicy: IfNotPresent name: provisioner - args: - --snapshot-name-prefix=csiunitysnap image: k8s.gcr.io/sig-storage/csi-snapshotter:v6.2.1 imagePullPolicy: IfNotPresent name: snapshotter - args: - --monitor-interval=60s image: gcr.io/k8s-staging-sig-storage/csi-external-health-monitor-controller:v0.8.0 imagePullPolicy: IfNotPresent name: external-health-monitor - image: k8s.gcr.io/sig-storage/csi-attacher:v4.2.0 imagePullPolicy: IfNotPresent name: attacher - image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.6.3 imagePullPolicy: IfNotPresent name: registrar - image: k8s.gcr.io/sig-storage/csi-resizer:v1.7.0 imagePullPolicy: IfNotPresent name: resizer Configmap needs to be created with command kubectl create -f configmap.yaml using following yaml file. kind: ConfigMap metadata: name: unity-config-params namespace: test-unity data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"info\" ALLOW_RWO_MULTIPOD_ACCESS: \"false\" MAX_UNITY_VOLUMES_PER_NODE: \"0\" SYNC_NODE_INFO_TIME_INTERVAL: \"15\" TENANT_NAME: \"\" NOTE: Replicas in the driver CR file should not be greater than or equal to the number of worker nodes when upgrading the driver. If the Replicas count is not less than the worker node count, some of the driver controller pods would land in a pending state, and upgrade will not be successful. Driver controller pods go in a pending state because they have anti-affinity to each other and cannot be scheduled on nodes where there is a driver controller pod already running. Refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity for more details.\nNOTE: Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures.\nNOTE: From v1.4.0 onwards, Dell CSI Operator does not support the creation of StorageClass and VolumeSnapshotClass objects. Although these fields are still present in the various driver CustomResourceDefinitions, they would be ignored by the operator. These fields will be removed from the CustomResourceDefinitions in a future release. If StorageClass and VolumeSnapshotClass need to be retained, you should upgrade the driver as per the recommended way noted above. StorageClass and VolumeSnapshotClass would not be retained on driver uninstallation.\nSupported modifications Changing environment variable values for driver Adding (supported) environment variables Updating the image of the driver Limitations The Dell CSI Operator can’t manage any existing driver installed using Helm charts. If you already have installed one of the Dell CSI drivers in your cluster and want to use the operator based deployment, uninstall the driver and then redeploy the driver following the installation procedure described. The Dell CSI Operator is not fully compliant with the OperatorHub React UI elements and some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSI Operator Custom Resource Specification Each CSI Driver installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.\nBelow is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - Refer full list of supported driver for finding out the appropriate config version here replicas - Number of replicas for controller plugin - Must be set to 1 for all drivers\ndnsPolicy - Determines the dnsPolicy for the node daemonset. Accepted values are Default, ClusterFirst, ClusterFirstWithHostNet, None common\nThis field is mandatory and is used to specify common properties for both controller and the node plugin\nimage - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values Optional fields controller - List of environment variables and values which are applicable only for controller\nnode - List of environment variables and values which are applicable only for node\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver\nforceUpdate\nBoolean value which can be set to true in order to force update the status of the CSI Driver\ntolerations List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet\nIt should be set separately in the controller and node sections if you want separate set of tolerations for them\nnodeSelector Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet\nfsGroupPolicy Defines which FS Group policy mode to be used, Supported modes: None, File and ReadWriteOnceWithFSType\nHere is a sample specification annotated with comments to explain each field\napiVersion: storage.dell.com/v1 kind: CSIPowerMax # Type of the driver metadata: name: test-powermax # Name of the driver namespace: test-powermax # Namespace where driver is installed spec: driver: # Used to specify configuration version configVersion: v3 # Refer the table containing the full list of supported drivers to find the appropriate config version replicas: 1 forceUpdate: false # Set to true in case you want to force an update of driver status common: # All common specification image: \"dellemc/csi-powermax:v1.4.0.000R\" #driver image for a particular release imagePullPolicy: IfNotPresent envs: - name: X_CSI_POWERMAX_ENDPOINT value: \"https://0.0.0.0:8443/\" - name: X_CSI_K8S_CLUSTER_PREFIX value: \"XYZ\" You can set the field replicas to a higher number than 1 for the latest driver versions.\nNote - The image field should point to the correct image tag for version of the driver you are installing.\nFor e.g. - If you wish to install v2.6.0 of the CSI PowerMax driver, use the image tag dellemc/csi-powermax:v2.6.0\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. The only exception to this is modifications requested by the documentation, for example, filling in blank IPs or other such system-specific data. Any modifications not specifically requested by the documentation should be only done after consulting with Dell support.\nModify the driver specification Choose the correct configVersion. Refer the table containing the full list of supported drivers and versions. Provide the namespace (in metadata section) where you want to install the driver. Provide a name (in metadata section) for the driver. This will be the name of the Custom Resource. Edit the values for mandatory configuration parameters specific to your installation. Edit/Add any values for optional configuration parameters to customize your installation. If you are installing the latest versions of the CSI drivers, the default number of replicas is set to 2. You can increase/decrease this value. StorageClass and VolumeSnapshotClass New Installations You should not provide any StorageClass or VolumeSnapshotClass details during driver installation. The sample files for all the drivers have been updated to reflect this change. Even if these details are there in the sample files, StorageClass or VolumeSnapshotClass will not be created.\nWhat happens to my existing StorageClass \u0026 VolumeSnapshotClass objects In case you are upgrading an existing driver installation by using kubectl edit or by patching the object in place, any existing objects will remain as is. If you added more objects as part of the upgrade, then this request will be ignored by the Operator. If you uninstall the older driver, then any StorageClass or VolumeSnapshotClass objects will be deleted. An uninstall and followed by an install of the driver would also result in StorageClass and VolumeSnapshotClass getting deleted and not getting created again. NOTE: For more information on pre-requisites and parameters, please refer to the sub-pages below for each driver.\nNOTE: Storage Classes and Volume Snapshot Classes would no longer be created during the installation of the driver via an operator from v1.4.0 and higher.\n","categories":"","description":"Installation of CSI drivers using Dell CSI Operator\n","excerpt":"Installation of CSI drivers using Dell CSI Operator\n","ref":"/csm-docs/v3/csidriver/installation/operator/","tags":"","title":"CSI Driver installation using Dell CSI Operator"},{"body":"One of the goals of high availability is to eliminate single points of failure in a storage system. In Kubernetes, this can mean that a single PV represents multiple read/write enabled volumes on different arrays, located at reasonable distances with both the volumes in sync with each other. If one of the volumes goes down, there will still be another volume available for read and write. This kind of high availability can be achieved by using SRDF Metro replication mode, supported only by PowerMax arrays.\nSRDF Metro Architecture In SRDF metro configurations:\nR2 devices are Read/Write accessible to application hosts. Application host can write to both the R1 and R2 sides of the device pair. R2 devices assume the same external device identity(geometry, device WWN) as the R1 devices. All the above characteristic makes SRDF metro best suited for the scenarios in which high availability of data is desired. With respect to Kubernetes, the SRDF metro mode works in single cluster scenarios. In the metro, both the arrays—arrays with SRDF metro link setup between them—involved in the replication are managed by the same csi-powermax driver. The replication is triggered by creating a volume using a StorageClass with metro-related parameters. The driver on receiving the metro-related parameters in the CreateVolume call creates a metro replicated volume and the details about both the volumes are returned in the volume context to the Kubernetes cluster. So, the PV created in the process represents a pair of metro replicated volumes. When a PV, representing a pair of metro replicated volumes, is claimed by a pod, the host treats each of the volumes represented by the single PV as a separate data path. The switching between the paths, to read and write the data, is managed by the multipath driver. The switching happens automatically, as configured by the user—in round-robin fashion or otherwise—or it can happen if one of the paths goes down. For details on Linux multipath driver setup, click here.\nThe creation of volumes in SRDF metro mode doesn’t involve the replication sidecar or the common controller, nor does it cause the creation of any replication related custom resources; it just needs a csi-powermax driver that implements the CreateVolume gRPC endpoint with SRDF metro capability for it to work.\nUsage The metro replicated volumes are created just like the normal volumes, but the StorageClass contains some extra parameters related to metro replication. A StorageClass to create metro replicated volumes may look as follows:\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: storage-class-metro provisioner: driver.dellemc.com parameters: SRP: 'SRP_1' SYMID: '000000000001' ServiceLevel: 'Bronze' replication.storage.dell.com/IsReplicationEnabled: 'true' replication.storage.dell.com/RdfGroup: '7' # Optional for Auto SRDF group replication.storage.dell.com/RdfMode: 'METRO' replication.storage.dell.com/RemoteRDFGroup: '7' # Optional for Auto SRDF group replication.storage.dell.com/RemoteSYMID: '000000000002' replication.storage.dell.com/RemoteServiceLevel: 'Bronze' reclaimPolicy: Delete volumeBindingMode: Immediate NOTE: Different namespaces can share the same RDF group for creating volumes.\nSnapshots on SRDF Metro volumes A snapshot can be created on either of the volumes in the metro volume pair depending on the parameters in the VolumeSnapshotClass. The snapshots are by default created on the volumes on the R1 side of the SRDF metro pair, but if a Symmetrix ID is specified in the VolumeSnapshotClass parameters, the driver creates the snapshot on the specified array; the specified array can either be the R1 or the R2 array. A VolumeSnapshotClass with symmetrix ID specified in parameters may look as follows:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: sample-snapclass driver: driver.dellemc.com deletionPolicy: Delete parameters: SYMID: '000000000001' ","categories":"","description":"High Availability support for CSI PowerMax\n","excerpt":"High Availability support for CSI PowerMax\n","ref":"/csm-docs/docs/replication/high-availability/","tags":"","title":"High Availability"},{"body":"One of the goals of high availability is to eliminate single points of failure in a storage system. In Kubernetes, this can mean that a single PV represents multiple read/write enabled volumes on different arrays, located at reasonable distances with both the volumes in sync with each other. If one of the volumes goes down, there will still be another volume available for read and write. This kind of high availability can be achieved by using SRDF Metro replication mode, supported only by PowerMax arrays.\nSRDF Metro Architecture In SRDF metro configurations:\nR2 devices are Read/Write accessible to application hosts. Application host can write to both the R1 and R2 sides of the device pair. R2 devices assume the same external device identity(geometry, device WWN) as the R1 devices. All the above characteristic makes SRDF metro best suited for the scenarios in which high availability of data is desired. With respect to Kubernetes, the SRDF metro mode works in single cluster scenarios. In the metro, both the arrays—arrays with SRDF metro link setup between them—involved in the replication are managed by the same csi-powermax driver. The replication is triggered by creating a volume using a StorageClass with metro-related parameters. The driver on receiving the metro-related parameters in the CreateVolume call creates a metro replicated volume and the details about both the volumes are returned in the volume context to the Kubernetes cluster. So, the PV created in the process represents a pair of metro replicated volumes. When a PV, representing a pair of metro replicated volumes, is claimed by a pod, the host treats each of the volumes represented by the single PV as a separate data path. The switching between the paths, to read and write the data, is managed by the multipath driver. The switching happens automatically, as configured by the user—in round-robin fashion or otherwise—or it can happen if one of the paths goes down. For details on Linux multipath driver setup, click here.\nThe creation of volumes in SRDF metro mode doesn’t involve the replication sidecar or the common controller, nor does it cause the creation of any replication related custom resources; it just needs a csi-powermax driver that implements the CreateVolume gRPC endpoint with SRDF metro capability for it to work.\nUsage The metro replicated volumes are created just like the normal volumes, but the StorageClass contains some extra parameters related to metro replication. A StorageClass to create metro replicated volumes may look as follows:\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: storage-class-metro provisioner: driver.dellemc.com parameters: SRP: 'SRP_1' SYMID: '000000000001' ServiceLevel: 'Bronze' replication.storage.dell.com/IsReplicationEnabled: 'true' replication.storage.dell.com/RdfGroup: '7' # Optional for Auto SRDF group replication.storage.dell.com/RdfMode: 'METRO' replication.storage.dell.com/RemoteRDFGroup: '7' # Optional for Auto SRDF group replication.storage.dell.com/RemoteSYMID: '000000000002' replication.storage.dell.com/RemoteServiceLevel: 'Bronze' reclaimPolicy: Delete volumeBindingMode: Immediate NOTE: Different namespaces can share the same RDF group for creating volumes.\nSnapshots on SRDF Metro volumes A snapshot can be created on either of the volumes in the metro volume pair depending on the parameters in the VolumeSnapshotClass. The snapshots are by default created on the volumes on the R1 side of the SRDF metro pair, but if a Symmetrix ID is specified in the VolumeSnapshotClass parameters, the driver creates the snapshot on the specified array; the specified array can either be the R1 or the R2 array. A VolumeSnapshotClass with symmetrix ID specified in parameters may look as follows:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: sample-snapclass driver: driver.dellemc.com deletionPolicy: Delete parameters: SYMID: '000000000001' ","categories":"","description":"High Availability support for CSI PowerMax\n","excerpt":"High Availability support for CSI PowerMax\n","ref":"/csm-docs/v1/replication/high-availability/","tags":"","title":"High Availability"},{"body":"One of the goals of high availability is to eliminate single points of failure in a storage system. In Kubernetes, this can mean that a single PV represents multiple read/write enabled volumes on different arrays, located at reasonable distances with both the volumes in sync with each other. If one of the volumes goes down, there will still be another volume available for read and write. This kind of high availability can be achieved by using SRDF Metro replication mode, supported only by PowerMax arrays.\nSRDF Metro Architecture In SRDF metro configurations:\nR2 devices are Read/Write accessible to application hosts. Application host can write to both the R1 and R2 sides of the device pair. R2 devices assume the same external device identity(geometry, device WWN) as the R1 devices. All the above characteristic makes SRDF metro best suited for the scenarios in which high availability of data is desired. With respect to Kubernetes, the SRDF metro mode works in single cluster scenarios. In the metro, both the arrays—arrays with SRDF metro link setup between them—involved in the replication are managed by the same csi-powermax driver. The replication is triggered by creating a volume using a StorageClass with metro-related parameters. The driver on receiving the metro-related parameters in the CreateVolume call creates a metro replicated volume and the details about both the volumes are returned in the volume context to the Kubernetes cluster. So, the PV created in the process represents a pair of metro replicated volumes. When a PV, representing a pair of metro replicated volumes, is claimed by a pod, the host treats each of the volumes represented by the single PV as a separate data path. The switching between the paths, to read and write the data, is managed by the multipath driver. The switching happens automatically, as configured by the user—in round-robin fashion or otherwise—or it can happen if one of the paths goes down. For details on Linux multipath driver setup, click here.\nThe creation of volumes in SRDF metro mode doesn’t involve the replication sidecar or the common controller, nor does it cause the creation of any replication related custom resources; it just needs a csi-powermax driver that implements the CreateVolume gRPC endpoint with SRDF metro capability for it to work.\nUsage The metro replicated volumes are created just like the normal volumes, but the StorageClass contains some extra parameters related to metro replication. A StorageClass to create metro replicated volumes may look as follows:\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: storage-class-metro provisioner: driver.dellemc.com parameters: SRP: 'SRP_1' SYMID: '000000000001' ServiceLevel: 'Bronze' replication.storage.dell.com/IsReplicationEnabled: 'true' replication.storage.dell.com/RdfGroup: '7' # Optional for Auto SRDF group replication.storage.dell.com/RdfMode: 'METRO' replication.storage.dell.com/RemoteRDFGroup: '7' # Optional for Auto SRDF group replication.storage.dell.com/RemoteSYMID: '000000000002' replication.storage.dell.com/RemoteServiceLevel: 'Bronze' reclaimPolicy: Delete volumeBindingMode: Immediate NOTE: Different namespaces can share the same RDF group for creating volumes.\nSnapshots on SRDF Metro volumes A snapshot can be created on either of the volumes in the metro volume pair depending on the parameters in the VolumeSnapshotClass. The snapshots are by default created on the volumes on the R1 side of the SRDF metro pair, but if a Symmetrix ID is specified in the VolumeSnapshotClass parameters, the driver creates the snapshot on the specified array; the specified array can either be the R1 or the R2 array. A VolumeSnapshotClass with symmetrix ID specified in parameters may look as follows:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: sample-snapclass driver: driver.dellemc.com deletionPolicy: Delete parameters: SYMID: '000000000001' ","categories":"","description":"High Availability support for CSI PowerMax\n","excerpt":"High Availability support for CSI PowerMax\n","ref":"/csm-docs/v2/replication/high-availability/","tags":"","title":"High Availability"},{"body":"One of the goals of high availability is to eliminate single points of failure in a storage system. In Kubernetes, this can mean that a single PV represents multiple read/write enabled volumes on different arrays, located at reasonable distances with both the volumes in sync with each other. If one of the volumes goes down, there will still be another volume available for read and write. This kind of high availability can be achieved by using SRDF Metro replication mode, supported only by PowerMax arrays.\nSRDF Metro Architecture In SRDF metro configurations:\nR2 devices are Read/Write accessible to application hosts. Application host can write to both the R1 and R2 sides of the device pair. R2 devices assume the same external device identity(geometry, device WWN) as the R1 devices. All the above characteristic makes SRDF metro best suited for the scenarios in which high availability of data is desired. With respect to Kubernetes, the SRDF metro mode works in single cluster scenarios. In the metro, both the arrays—arrays with SRDF metro link setup between them—involved in the replication are managed by the same csi-powermax driver. The replication is triggered by creating a volume using a StorageClass with metro-related parameters. The driver on receiving the metro-related parameters in the CreateVolume call creates a metro replicated volume and the details about both the volumes are returned in the volume context to the Kubernetes cluster. So, the PV created in the process represents a pair of metro replicated volumes. When a PV, representing a pair of metro replicated volumes, is claimed by a pod, the host treats each of the volumes represented by the single PV as a separate data path. The switching between the paths, to read and write the data, is managed by the multipath driver. The switching happens automatically, as configured by the user—in round-robin fashion or otherwise—or it can happen if one of the paths goes down. For details on Linux multipath driver setup, click here.\nThe creation of volumes in SRDF metro mode doesn’t involve the replication sidecar or the common controller, nor does it cause the creation of any replication related custom resources; it just needs a csi-powermax driver that implements the CreateVolume gRPC endpoint with SRDF metro capability for it to work.\nUsage The metro replicated volumes are created just like the normal volumes, but the StorageClass contains some extra parameters related to metro replication. A StorageClass to create metro replicated volumes may look as follows:\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: storage-class-metro provisioner: driver.dellemc.com parameters: SRP: 'SRP_1' SYMID: '000000000001' ServiceLevel: 'Bronze' replication.storage.dell.com/IsReplicationEnabled: 'true' replication.storage.dell.com/RdfGroup: '7' # Optional for Auto SRDF group replication.storage.dell.com/RdfMode: 'METRO' replication.storage.dell.com/RemoteRDFGroup: '7' # Optional for Auto SRDF group replication.storage.dell.com/RemoteSYMID: '000000000002' replication.storage.dell.com/RemoteServiceLevel: 'Bronze' reclaimPolicy: Delete volumeBindingMode: Immediate NOTE: Different namespaces can share the same RDF group for creating volumes.\nSnapshots on SRDF Metro volumes A snapshot can be created on either of the volumes in the metro volume pair depending on the parameters in the VolumeSnapshotClass. The snapshots are by default created on the volumes on the R1 side of the SRDF metro pair, but if a Symmetrix ID is specified in the VolumeSnapshotClass parameters, the driver creates the snapshot on the specified array; the specified array can either be the R1 or the R2 array. A VolumeSnapshotClass with symmetrix ID specified in parameters may look as follows:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: sample-snapclass driver: driver.dellemc.com deletionPolicy: Delete parameters: SYMID: '000000000001' ","categories":"","description":"High Availability support for CSI PowerMax\n","excerpt":"High Availability support for CSI PowerMax\n","ref":"/csm-docs/v3/replication/high-availability/","tags":"","title":"High Availability"},{"body":"Container Storage Modules (CSM) for Observability is part of the open-source suite of Kubernetes storage enablers for Dell products.\nIt is an OpenTelemetry agent that collects array-level metrics for Dell storage so they can be exported into a Prometheus database. With CSM for Observability, you will gain visibility not only on the capacity of the volumes/file shares you manage with Dell CSM CSI (Container Storage Interface) drivers but also their performance in terms of bandwidth, IOPS, and response time.\nThanks to pre-packaged Grafana dashboards, you will be able to go through these metrics history and see the topology between a Kubernetes PV (Persistent Volume) and its translation as a LUN or file share in the backend array. This module also allows Kubernetes admins to collect array level metrics to check the overall capacity and performance directly from the Prometheus/Grafana tools rather than interfacing directly with the storage system itself.\nMetrics data is collected and pushed to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. SSL certificates for TLS between nodes are handled by cert-manager.\nCSM for Observability is composed of several services, each residing in its own GitHub repository, that can be installed following one of the four deployments we support here. Contributions can be made to this repository or any of the CSM for Observability repositories listed below.\nName Repository Description Metrics for PowerFlex CSM Metrics for PowerFlex Metrics for PowerFlex captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerFlex. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics, so they can be visualized in Grafana. Please visit the repository for more information. Metrics for PowerStore CSM Metrics for PowerStore Metrics for PowerStore captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerStore. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics, so they can be visualized in Grafana. Please visit the repository for more information. Metrics for PowerScale CSM Metrics for PowerScale Metrics for PowerScale captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerScale. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics, so they can be visualized in Grafana. Please visit the repository for more information. Metrics for PowerMax CSM Metrics for PowerMax Metrics for PowerMax captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerMax. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics, so they can be visualized in Grafana. Please visit the repository for more information. Volume Topology CSM Topology Topology provides Kubernetes administrators with the topology data related to containerized storage that is provisioned by a CSI (Container Storage Interface) Driver for Dell storage products. The Topology service is enabled by default as part of the CSM for Observability Helm Chart values file. Please visit the repository for more information. CSM for Observability Capabilities CSM for Observability provides the following capabilities:\nCapability PowerMax PowerFlex Unity XT PowerScale PowerStore Collect and expose Volume Metrics via the OpenTelemetry Collector yes yes no yes yes Collect and expose File System Metrics via the OpenTelemetry Collector no no no no yes Collect and expose export (k8s) node metrics via the OpenTelemetry Collector no yes no no no Collect and expose block storage metrics via the OpenTelemetry Collector yes yes no no yes Collect and expose file storage metrics via the OpenTelemetry Collector no no no yes yes Non-disruptive config changes yes yes no yes yes Non-disruptive log level changes yes yes no yes yes Grafana Dashboards for displaying metrics and topology data yes yes no yes yes Supported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.26, 1.27, 1.28 Red Hat OpenShift 4.13, 4.14 Rancher Kubernetes Engine yes Supported Storage Platforms PowerFlex PowerStore PowerScale PowerMax Storage Array 3.6.x, 4.0.x, 4.5 3.0, 3.2, 3.5 OneFS 9.3, 9.4, 9.5.0.x (x \u003e= 5) PowerMax 2000/8000 PowerMax 2500/8500 PowerMaxOS 10 (6079) , PowerMaxOS 10.0.1 (6079) , PowerMaxOS 10.1 (6079) PowerMax 2000/8000 - 5978.711.xxx, 5978.479.xxx Unisphere 10.0, 10.0.1, 10.1 Supported CSI Drivers CSM for Observability supports the following CSI drivers and versions. Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerFlex csi-powerflex v2.0 + CSI Driver for Dell PowerStore csi-powerstore v2.0 + CSI Driver for Dell PowerScale csi-powerscale v2.0 + CSI Driver for Dell PowerMax csi-powermax v2.5 + Topology Data CSM for Observability provides Kubernetes administrators with the topology data related to containerized storage. This topology data is visualized using Grafana: Field Description Namespace The namespace associated with the persistent volume claim Persistent Volume Claim The name of the persistent volume claim associated with the persistent volume Persistent Volume The name of the persistent volume Storage Class The storage class associated with the persistent volume Provisioned Size The provisioned size of the persistent volume Status The status of the persistent volume. “Released” indicates the persistent volume does not have a claim. “Bound” indicates the persistent volume has a claim Created The date the persistent volume was created Storage System The storage system ID or IP address the volume is associated with Protocol The storage system protocol type the volume/storage class is associated with Storage Pool The storage pool name the volume/storage class is associated with Storage System Volume Name The name of the volume on the storage system that is associated with the persistent volume TLS Encryption CSM for Observability deployment relies on cert-manager to manage SSL certificates that are used to encrypt communication between various components. When deploying CSM for Observability, cert-manager is installed and configured automatically. The cert-manager components listed below will be installed alongside CSM for Observability.\nComponent cert-manager cert-manager-cainjector cert-manager-webhook If desired you may provide your own certificate key pair to be used inside the cluster by providing the path to the certificate and key in the Helm chart config. If you do not provide a certificate, one will be generated for you on installation.\nNOTE: The certificate provided must be a CA certificate. This is to facilitate automated certificate rotation.\nViewing Logs Logs can be viewed by using the kubectl logs CLI command to output logs for a specific Pod or Deployment.\nFor example, the following script will capture logs of all Pods in the CSM namespace and save the output to one file per Pod.\n#!/bin/bash namespace=[CSM_NAMESPACE] for pod in $(kubectl get pods -n $namespace -o name); do logFileName=$(echo $pod | tr / -).txt kubectl logs -n $namespace $pod --all-containers \u003e $logFileName done ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability\n","excerpt":"Dell Container Storage Modules (CSM) for Observability\n","ref":"/csm-docs/docs/observability/","tags":"","title":"Observability"},{"body":"Container Storage Modules (CSM) for Observability is part of the open-source suite of Kubernetes storage enablers for Dell products.\nIt is an OpenTelemetry agent that collects array-level metrics for Dell storage so they can be exported into a Prometheus database. With CSM for Observability, you will gain visibility not only on the capacity of the volumes/file shares you manage with Dell CSM CSI (Container Storage Interface) drivers but also their performance in terms of bandwidth, IOPS, and response time.\nThanks to pre-packaged Grafana dashboards, you will be able to go through these metrics history and see the topology between a Kubernetes PV (Persistent Volume) and its translation as a LUN or file share in the backend array. This module also allows Kubernetes admins to collect array level metrics to check the overall capacity and performance directly from the Prometheus/Grafana tools rather than interfacing directly with the storage system itself.\nMetrics data is collected and pushed to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. SSL certificates for TLS between nodes are handled by cert-manager.\nCSM for Observability is composed of several services, each residing in its own GitHub repository, that can be installed following one of the four deployments we support here. Contributions can be made to this repository or any of the CSM for Observability repositories listed below.\nName Repository Description Metrics for PowerFlex CSM Metrics for PowerFlex Metrics for PowerFlex captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerFlex. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics, so they can be visualized in Grafana. Please visit the repository for more information. Metrics for PowerStore CSM Metrics for PowerStore Metrics for PowerStore captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerStore. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics, so they can be visualized in Grafana. Please visit the repository for more information. Metrics for PowerScale CSM Metrics for PowerScale Metrics for PowerScale captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerScale. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics, so they can be visualized in Grafana. Please visit the repository for more information. Metrics for PowerMax CSM Metrics for PowerMax Metrics for PowerMax captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerMax. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics, so they can be visualized in Grafana. Please visit the repository for more information. Volume Topology CSM Topology Topology provides Kubernetes administrators with the topology data related to containerized storage that is provisioned by a CSI (Container Storage Interface) Driver for Dell storage products. The Topology service is enabled by default as part of the CSM for Observability Helm Chart values file. Please visit the repository for more information. CSM for Observability Capabilities CSM for Observability provides the following capabilities:\nCapability PowerMax PowerFlex Unity XT PowerScale PowerStore Collect and expose Volume Metrics via the OpenTelemetry Collector yes yes no yes yes Collect and expose File System Metrics via the OpenTelemetry Collector no no no no yes Collect and expose export (k8s) node metrics via the OpenTelemetry Collector no yes no no no Collect and expose block storage metrics via the OpenTelemetry Collector yes yes no no yes Collect and expose file storage metrics via the OpenTelemetry Collector no no no yes yes Non-disruptive config changes yes yes no yes yes Non-disruptive log level changes yes yes no yes yes Grafana Dashboards for displaying metrics and topology data yes yes no yes yes Supported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.25, 1.26, 1.27 Red Hat OpenShift 4.11, 4.12, 4.13 Rancher Kubernetes Engine yes RHEL 7.x, 8.x CentOS 7.8, 7.9 Supported Storage Platforms PowerFlex PowerStore PowerScale PowerMax Storage Array 3.5.x, 3.6.x, 4.0 1.0.x, 2.0.x, 2.1.x, 3.0, 3.2, 3.5 OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4, 9.5 PowerMax 2000/8000 PowerMax 2500/8500 PowerMaxOS 10 (6079) , PowerMaxOS 10.0.1 (6079) PowerMax 2000/8000 - 5978.711.xxx, 5978.479.xxx Unisphere 10.0, 10.0.1 Supported CSI Drivers CSM for Observability supports the following CSI drivers and versions. Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerFlex csi-powerflex v2.0 + CSI Driver for Dell PowerStore csi-powerstore v2.0 + CSI Driver for Dell PowerScale csi-powerscale v2.0 + CSI Driver for Dell PowerMax csi-powermax v2.5 + Topology Data CSM for Observability provides Kubernetes administrators with the topology data related to containerized storage. This topology data is visualized using Grafana: Field Description Namespace The namespace associated with the persistent volume claim Persistent Volume Claim The name of the persistent volume claim associated with the persistent volume Persistent Volume The name of the persistent volume Storage Class The storage class associated with the persistent volume Provisioned Size The provisioned size of the persistent volume Status The status of the persistent volume. “Released” indicates the persistent volume does not have a claim. “Bound” indicates the persistent volume has a claim Created The date the persistent volume was created Storage System The storage system ID or IP address the volume is associated with Protocol The storage system protocol type the volume/storage class is associated with Storage Pool The storage pool name the volume/storage class is associated with Storage System Volume Name The name of the volume on the storage system that is associated with the persistent volume TLS Encryption CSM for Observability deployment relies on cert-manager to manage SSL certificates that are used to encrypt communication between various components. When deploying CSM for Observability, cert-manager is installed and configured automatically. The cert-manager components listed below will be installed alongside CSM for Observability.\nComponent cert-manager cert-manager-cainjector cert-manager-webhook If desired you may provide your own certificate key pair to be used inside the cluster by providing the path to the certificate and key in the Helm chart config. If you do not provide a certificate, one will be generated for you on installation.\nNOTE: The certificate provided must be a CA certificate. This is to facilitate automated certificate rotation.\nViewing Logs Logs can be viewed by using the kubectl logs CLI command to output logs for a specific Pod or Deployment.\nFor example, the following script will capture logs of all Pods in the CSM namespace and save the output to one file per Pod.\n#!/bin/bash namespace=[CSM_NAMESPACE] for pod in $(kubectl get pods -n $namespace -o name); do logFileName=$(echo $pod | tr / -).txt kubectl logs -n $namespace $pod --all-containers \u003e $logFileName done ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability\n","excerpt":"Dell Container Storage Modules (CSM) for Observability\n","ref":"/csm-docs/v1/observability/","tags":"","title":"Observability"},{"body":"Container Storage Modules (CSM) for Observability is part of the open-source suite of Kubernetes storage enablers for Dell products.\nIt is an OpenTelemetry agent that collects array-level metrics for Dell storage so they can be scraped into a Prometheus database. With CSM for Observability, you will gain visibility not only on the capacity of the volumes/file shares you manage with Dell CSM CSI (Container Storage Interface) drivers but also their performance in terms of bandwidth, IOPS, and response time.\nThanks to pre-packaged Grafana dashboards, you will be able to go through these metrics history and see the topology between a Kubernetes PV (Persistent Volume) and its translation as a LUN or file share in the backend array. This module also allows Kubernetes admins to collect array level metrics to check the overall capacity and performance directly from the Prometheus/Grafana tools rather than interfacing directly with the storage system itself.\nMetrics data is collected and pushed to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. SSL certificates for TLS between nodes are handled by cert-manager.\nCSM for Observability is composed of several services, each living in its own GitHub repository, that can be installed following one of the four deployments we support here. Contributions can be made to this repository or any of the CSM for Observability repositories listed below.\nName Repository Description Metrics for PowerFlex CSM Metrics for PowerFlex Metrics for PowerFlex captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerFlex. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics, so they can be visualized in Grafana. Please visit the repository for more information. Metrics for PowerStore CSM Metrics for PowerStore Metrics for PowerStore captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerStore. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics, so they can be visualized in Grafana. Please visit the repository for more information. Metrics for PowerScale CSM Metrics for PowerScale Metrics for PowerScale captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerScale. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics, so they can be visualized in Grafana. Please visit the repository for more information. Metrics for PowerMax CSM Metrics for PowerMax Metrics for PowerMax captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerMax. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics, so they can be visualized in Grafana. Please visit the repository for more information. Volume Topology CSM Topology Topology provides Kubernetes administrators with the topology data related to containerized storage that is provisioned by a CSI (Container Storage Interface) Driver for Dell storage products. The Topology service is enabled by default as part of the CSM for Observability Helm Chart values file. Please visit the repository for more information. CSM for Observability Capabilities CSM for Observability provides the following capabilities:\nCapability PowerMax PowerFlex Unity XT PowerScale PowerStore Collect and expose Volume Metrics via the OpenTelemetry Collector yes yes no yes yes Collect and expose File System Metrics via the OpenTelemetry Collector no no no no yes Collect and expose export (k8s) node metrics via the OpenTelemetry Collector no yes no no no Collect and expose block storage metrics via the OpenTelemetry Collector yes yes no no yes Collect and expose file storage metrics via the OpenTelemetry Collector no no no yes yes Non-disruptive config changes yes yes no yes yes Non-disruptive log level changes yes yes no yes yes Grafana Dashboards for displaying metrics and topology data yes yes no yes yes Supported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.25, 1.26, 1.27 Red Hat OpenShift 4.10, 4.11, 4.12 Rancher Kubernetes Engine yes RHEL 7.x, 8.x CentOS 7.8, 7.9 Supported Storage Platforms PowerFlex PowerStore PowerScale PowerMax Storage Array 3.5.x, 3.6.x, 4.0 1.0.x, 2.0.x, 2.1.x, 3.0, 3.2, 3.5 OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4 PowerMax 2000/8000 PowerMax 2500/8500 PowerMaxOS 10 (6079) , PowerMaxOS 10.0.1 (6079) PowerMax 2000/8000 - 5978.711.xxx, 5978.479.xxx Unisphere 10.0, 10.0.1 Supported CSI Drivers CSM for Observability supports the following CSI drivers and versions. Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerFlex csi-powerflex v2.0 + CSI Driver for Dell PowerStore csi-powerstore v2.0 + CSI Driver for Dell PowerScale csi-powerscale v2.0 + CSI Driver for Dell PowerMax csi-powermax v2.5 + Topology Data CSM for Observability provides Kubernetes administrators with the topology data related to containerized storage. This topology data is visualized using Grafana: Field Description Namespace The namespace associated with the persistent volume claim Persistent Volume Claim The name of the persistent volume claim associated with the persistent volume Persistent Volume The name of the persistent volume Storage Class The storage class associated with the persistent volume Provisioned Size The provisioned size of the persistent volume Status The status of the persistent volume. “Released” indicates the persistent volume does not have a claim. “Bound” indicates the persistent volume has a claim Created The date the persistent volume was created Storage System The storage system ID or IP address the volume is associated with Protocol The storage system protocol type the volume/storage class is associated with Storage Pool The storage pool name the volume/storage class is associated with Storage System Volume Name The name of the volume on the storage system that is associated with the persistent volume TLS Encryption CSM for Observability deployment relies on cert-manager to manage SSL certificates that are used to encrypt communication between various components. When deploying CSM for Observability, cert-manager is installed and configured automatically. The cert-manager components listed below will be installed alongside CSM for Observability.\nComponent cert-manager cert-manager-cainjector cert-manager-webhook If desired you may provide your own certificate key pair to be used inside the cluster by providing the path to the certificate and key in the Helm chart config. If you do not provide a certificate, one will be generated for you on installation.\nNOTE: The certificate provided must be a CA certificate. This is to facilitate automated certificate rotation.\nViewing Logs Logs can be viewed by using the kubectl logs CLI command to output logs for a specific Pod or Deployment.\nFor example, the following script will capture logs of all Pods in the CSM namespace and save the output to one file per Pod.\n#!/bin/bash namespace=[CSM_NAMESPACE] for pod in $(kubectl get pods -n $namespace -o name); do logFileName=$(echo $pod | tr / -).txt kubectl logs -n $namespace $pod --all-containers \u003e $logFileName done ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability\n","excerpt":"Dell Container Storage Modules (CSM) for Observability\n","ref":"/csm-docs/v2/observability/","tags":"","title":"Observability"},{"body":"Container Storage Modules (CSM) for Observability is part of the open-source suite of Kubernetes storage enablers for Dell products.\nIt is an OpenTelemetry agent that collects array-level metrics for Dell storage so they can be scraped into a Prometheus database. With CSM for Observability, you will gain visibility not only on the capacity of the volumes/file shares you manage with Dell CSM CSI (Container Storage Interface) drivers but also their performance in terms of bandwidth, IOPS, and response time.\nThanks to pre-packaged Grafana dashboards, you will be able to go through these metrics history and see the topology between a Kubernetes PV (Persistent Volume) and its translation as a LUN or file share in the backend array. This module also allows Kubernetes admins to collect array level metrics to check the overall capacity and performance directly from the Prometheus/Grafana tools rather than interfacing directly with the storage system itself.\nMetrics data is collected and pushed to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. SSL certificates for TLS between nodes are handled by cert-manager.\nCSM for Observability is composed of several services, each living in its own GitHub repository, that can be installed following one of the four deployments we support here. Contributions can be made to this repository or any of the CSM for Observability repositories listed below.\nName Repository Description Metrics for PowerFlex CSM Metrics for PowerFlex Metrics for PowerFlex captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerFlex. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics, so they can be visualized in Grafana. Please visit the repository for more information. Metrics for PowerStore CSM Metrics for PowerStore Metrics for PowerStore captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerStore. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics, so they can be visualized in Grafana. Please visit the repository for more information. Metrics for PowerScale CSM Metrics for PowerScale Metrics for PowerScale captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerScale. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics, so they can be visualized in Grafana. Please visit the repository for more information. Metrics for PowerMax CSM Metrics for PowerMax Metrics for PowerMax captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerMax. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics, so they can be visualized in Grafana. Please visit the repository for more information. Volume Topology CSM Topology Topology provides Kubernetes administrators with the topology data related to containerized storage that is provisioned by a CSI (Container Storage Interface) Driver for Dell storage products. The Topology service is enabled by default as part of the CSM for Observability Helm Chart values file. Please visit the repository for more information. CSM for Observability Capabilities CSM for Observability provides the following capabilities:\nCapability PowerMax PowerFlex Unity XT PowerScale PowerStore Collect and expose Volume Metrics via the OpenTelemetry Collector yes yes no yes yes Collect and expose File System Metrics via the OpenTelemetry Collector no no no no yes Collect and expose export (k8s) node metrics via the OpenTelemetry Collector no yes no no no Collect and expose block storage metrics via the OpenTelemetry Collector yes yes no no yes Collect and expose file storage metrics via the OpenTelemetry Collector no no no yes yes Non-disruptive config changes yes yes no yes yes Non-disruptive log level changes yes yes no yes yes Grafana Dashboards for displaying metrics and topology data yes yes no yes yes Supported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.22, 1.23, 1.24, 1.25, 1.26 Red Hat OpenShift 4.9, 4.10, 4.11 Rancher Kubernetes Engine yes RHEL 7.x, 8.x CentOS 7.8, 7.9 Supported Storage Platforms PowerFlex PowerStore PowerScale PowerMax Storage Array 3.5.x, 3.6.x, 4.0 1.0.x, 2.0.x, 2.1.x, 3.0, 3.2 OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4 PowerMax 2000/8000 PowerMax 2500/8500 5978.479.479, 5978.711.711, 6079.xxx.xxx\nUnisphere 10.0 Supported CSI Drivers CSM for Observability supports the following CSI drivers and versions. Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerFlex csi-powerflex v2.0 + CSI Driver for Dell PowerStore csi-powerstore v2.0 + CSI Driver for Dell PowerScale csi-powerscale v2.0 + CSI Driver for Dell PowerMax csi-powermax v2.5 + Topology Data CSM for Observability provides Kubernetes administrators with the topology data related to containerized storage. This topology data is visualized using Grafana: Field Description Namespace The namespace associated with the persistent volume claim Persistent Volume Claim The name of the persistent volume claim associated with the persistent volume Persistent Volume The name of the persistent volume Storage Class The storage class associated with the persistent volume Provisioned Size The provisioned size of the persistent volume Status The status of the persistent volume. “Released” indicates the persistent volume does not have a claim. “Bound” indicates the persistent volume has a claim Created The date the persistent volume was created Storage System The storage system ID or IP address the volume is associated with Protocol The storage system protocol type the volume/storage class is associated with Storage Pool The storage pool name the volume/storage class is associated with Storage System Volume Name The name of the volume on the storage system that is associated with the persistent volume TLS Encryption CSM for Observability deployment relies on cert-manager to manage SSL certificates that are used to encrypt communication between various components. When deploying CSM for Observability, cert-manager is installed and configured automatically. The cert-manager components listed below will be installed alongside CSM for Observability.\nComponent cert-manager cert-manager-cainjector cert-manager-webhook If desired you may provide your own certificate key pair to be used inside the cluster by providing the path to the certificate and key in the Helm chart config. If you do not provide a certificate, one will be generated for you on installation.\nNOTE: The certificate provided must be a CA certificate. This is to facilitate automated certificate rotation.\nViewing Logs Logs can be viewed by using the kubectl logs CLI command to output logs for a specific Pod or Deployment.\nFor example, the following script will capture logs of all Pods in the CSM namespace and save the output to one file per Pod.\n#!/bin/bash namespace=[CSM_NAMESPACE] for pod in $(kubectl get pods -n $namespace -o name); do logFileName=$(echo $pod | tr / -).txt kubectl logs -n $namespace $pod --all-containers \u003e $logFileName done ","categories":"","description":"Dell Container Storage Modules (CSM) for Observability\n","excerpt":"Dell Container Storage Modules (CSM) for Observability\n","ref":"/csm-docs/v3/observability/","tags":"","title":"Observability"},{"body":"Release Notes - CSM Installation Wizard 1.2.1 New Features/Changes #947 - [FEATURE]: Support for Kubernetes 1.28 #1066 - [FEATURE]: Support for Openshift 4.14 Fixed Issues #1022 - [BUG]: CSM Installation wizard is issuing the warnings that are false positives Known Issues There are no known issues in this release\n","categories":"","description":"Release notes for CSM Installation Wizard","excerpt":"Release notes for CSM Installation Wizard","ref":"/csm-docs/docs/deployment/csminstallationwizard/release/","tags":"","title":"Release Notes"},{"body":"Release Notes - Container Storage Modules Operator v1.4.3 New Features/Changes #947 - [FEATURE]: Support for Kubernetes 1.28 #1066 - [FEATURE]: Support for Openshift 4.14 #996 - [FEATURE]: Dell CSI to Dell CSM Operator Migration Process #1062 - [FEATURE]: CSM PowerMax: Support PowerMax v10.1 #1158 - [FEATURE]: Automatically create certificates with CSM Operator Observability deployment Fixed Issues #975 - [BUG]: Not able to take volumesnapshots #982 - [BUG]: Update resources limits for controller-manager to fix OOMKilled error #988 - [BUG]: CSM Operator fails to install CSM Replication on the remote cluster #989 - [BUG]: Allow volume prefix to be set via CSM operator #990 - [BUG]: X_CSI_AUTH_TYPE cannot be set in CSM Operator #1110 - [BUG]: Multi Controller defect - sidecars timeout #1117 - [BUG]: Operator crashes when deployed from OpenShift with OLM #1120 - [BUG]: Skip Certificate Validation is not propagated to Authorization module in CSM Operator #1122 - [BUG]: CSM Operator does not calculate status correctly when module is deployed with driver #1103 - [BUG]: CSM Operator doesn’t apply fSGroupPolicy value to CSIDriver Object #1133 - [BUG]: CSM Operator does not calculate status correctly when application-mobility is deployed by itself) #1137 - [BUG]: CSM Operator intermittently does not calculate status correctly when deploying a driver) #1143 - [BUG]: CSM Operator does not calculate status correctly when deploying the authorization proxy server) #1146 - [BUG]: CSM Operator does not calculate status correctly when deploying observability with csi-powerscale) #1147 - [BUG]: CSM Operator labels csm objects with CSMVersion 1.8.0, an old version) #1156 - [BUG]: CSM object in success state when all CSI Powerflex pods are failing due to bad secret credentials) #1157 - [BUG]: If Authorization Proxy Server is installed in an alternate namespace by CSM Operator, the deployment fails) #1159 - [BUG]: CSM status is not always accurate when Observability installed with only one or two components) #1152 - [BUG]: CSI driver changes to facilitate SDC brownfield deployments) #1171 - [BUG]: CSM object occasionally stays in failed state when app-mobility is successfully deployed with csm-operator) Known Issues Issue Workaround The status field of a csm object as deployed by CSM Operator may, in limited cases, display an incorrect status for a deployment. As a workaround, the health of the deployment can be determined by checking the health of the pods. When CSM Operator creates a deployment that includes secrets (e.g., application-mobility, observability, cert-manager, velero), these secrets are not deleted on uninstall and will be left behind. For example, the karavi-topology-tls, otel-collector-tls, and cert-manager-webhook-ca secrets will not be deleted. This should not cause any issues on the system, but all secrets present on the cluster can be found with kubectl get secrets -A, and any unwanted secrets can be deleted with kubectl delete secret -n \u003csecret-namespace\u003e \u003csecret-name\u003e The images of sideCars are currently missing in the sample YAMLs in the offline bundle. As a consequence, the csm-operator is pulling them from registry.k8s.io. We recommend manually updating the images of sideCars in the sample YAML file, for example, storage_csm_powerflex_v291.yaml, before proceeding with the driver installation. Here is an example snippet for the sideCars section in the YAML file: sideCars: # 'k8s' represents a string prepended to each volume created by the CSI driver - name: provisioner image: \u003clocalregistry\u003e/csi-provisioner:v3.6.2 args: [\"--volume-name-prefix=k8s\"] - name: attacher image: \u003clocalregistry\u003e/csi-attacher:v4.4.2 - name: registrar image: \u003clocalregistry\u003e/csi-node-driver-registrar:v2.9.1 - name: resizer image: \u003clocalregistry\u003e/csi-resizer:v1.9.2 - name: snapshotter image: \u003clocalregistry\u003e/csi-snapshotter:v6.3.2 # sdc-monitor is disabled by default, due to high CPU usage - name: sdc-monitor enabled: false image: \u003clocalregistry\u003e/sdc:4.5 envs: - name: HOST_PID value: \"1\" - name: MDM value: \"10.xx.xx.xx,10.xx.xx.xx\" # Do not add mdm value here if it is present in secret # health monitor is disabled by default, refer to driver documentation before enabling it # Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\". - name: csi-external-health-monitor-controller enabled: false image: \u003clocalregistry\u003e/csi-external-health-monitor-controller:v0.10.0 args: [\"--monitor-interval=60s\"] ","categories":"","description":"Release notes for Dell Container Storage Modules Operator\n","excerpt":"Release notes for Dell Container Storage Modules Operator\n","ref":"/csm-docs/docs/deployment/csmoperator/release/","tags":"","title":"Release notes"},{"body":"Release Notes - CSM Observability 1.7.0 New Features/Changes #947 - [FEATURE]: Support for Kubernetes 1.28 #1066 - [FEATURE]: Support for Openshift 4.14 #996 - [FEATURE]: Dell CSI to Dell CSM Operator Migration Process #1031 - [FEATURE]: Update to the latest UBI Micro image for CSM #1062 - [FEATURE]: CSM PowerMax: Support PowerMax v10.1 Fixed Issues #1019 - [BUG]: karavi-metrics-powerscale pod gets an segmentation violation error during start Known Issues ","categories":"","description":"Dell Container Storage Modules (CSM) release notes for observability\n","excerpt":"Dell Container Storage Modules (CSM) release notes for observability\n","ref":"/csm-docs/docs/observability/release/","tags":"","title":"Release notes"},{"body":"Release Notes - CSM Installation Wizard 1.1.0 New Features/Changes Added operator mode of installation for CSI-PowerStore, CSI-PowerMax, CSI-PowerScale and the supported modules\nHelm and Operator based manifest file generation is supported for CSM-1.7 and CSM 1.8 releases\nVolume Limit and Storage Capacity Tracking features have been added.\nRename SDC and approve SDC feature added for CSM-1.7 and CSM-1.8 for CSI-PowerFlex driver.\nNFS volume feature added for CSM-1.8 for CSI-PowerFlex driver.\nFixed Issues #959 - [BUG]: Resiliency fields in the generated values.yaml should be uncommented when resiliency is enabled Known Issues There are no known issues in this release\n","categories":"","description":"Release notes for CSM Installation Wizard","excerpt":"Release notes for CSM Installation Wizard","ref":"/csm-docs/v1/deployment/csminstallationwizard/release/","tags":"","title":"Release Notes"},{"body":"Release Notes - Container Storage Modules Operator v1.3.0 New Features/Changes #724 - [FEATURE]: CSM support for Openshift 4.13 #876 - [FEATURE]: CSI 1.5 spec support -StorageCapacityTracking #939 - [FEATURE]: Add support for Offline Install of CSM Operator in non OLM environment #878 - [FEATURE]: CSI 1.5 spec support: Implement Volume Limits #922 - [FEATURE]: Use ubi9 micro as base image #955 - [FEATURE]: CSI Unity XT Driver: Add upgrade support to the CSM Operator Fixed Issues #898 - [BUG]: Unable to pull podmon image from local repository for offline install Known Issues There are no known issues in this release.\n","categories":"","description":"Release notes for Dell Container Storage Modules Operator\n","excerpt":"Release notes for Dell Container Storage Modules Operator\n","ref":"/csm-docs/v1/deployment/csmoperator/release/","tags":"","title":"Release notes"},{"body":"Release Notes - CSM Observability 1.6.0 New Features/Changes #724 - [FEATURE]: CSM support for Openshift 4.13 #922 - [FEATURE]: Use ubi9 micro as base image Fixed Issues #916 - [BUG]: Remove references to deprecated io/ioutil package Known Issues ","categories":"","description":"Dell Container Storage Modules (CSM) release notes for observability\n","excerpt":"Dell Container Storage Modules (CSM) release notes for observability\n","ref":"/csm-docs/v1/observability/release/","tags":"","title":"Release notes"},{"body":" CSM 1.7.1 is applicable to helm based installations of PowerFlex driver.\nRelease Notes - Container Storage Modules Operator v1.2.0 New Features/Changes Added support for CSI Unity XT Driver Added support for PowerMax Driver Added Replication Support for PowerFlex driver CSM Operator: Support install of Resiliency module Migrated image registry from k8s.gcr.io to registry.k8s.io Added support for OpenShift 4.12 Added support for Kubernetes 1.27 Fixed Issues CSM object goes into failed state when deployments are getting scaled down/up Install issues of the Replication module have been fixed Known Issues There are no known issues in this release.\n","categories":"","description":"Release notes for Dell Container Storage Modules Operator\n","excerpt":"Release notes for Dell Container Storage Modules Operator\n","ref":"/csm-docs/v2/deployment/csmoperator/release/","tags":"","title":"Release notes"},{"body":"Release Notes - CSM Observability 1.5.0 New Features/Changes CSM support for Kubernetes 1.26 Support PowerMax in CSM Observability Fixed Issues Observability - Improve Grafana dashboards for PowerFlex/PowerStore Known Issues ","categories":"","description":"Dell Container Storage Modules (CSM) release notes for observability\n","excerpt":"Dell Container Storage Modules (CSM) release notes for observability\n","ref":"/csm-docs/v2/observability/release/","tags":"","title":"Release notes"},{"body":"Release Notes - Container Storage Modules Operator v1.1.0 New Features/Changes Added support for CSI PowerStore Driver Added support for Kubernetes 1.26 Fixed Issues Fix for CSM Authorization CRD in the CSM Operator not able to custom configurations\nKnown Issues CSM object does not track available deployment count when down scaling to n-1 , where n is number of nodes.\n","categories":"","description":"Release notes for Dell Container Storage Modules Operator\n","excerpt":"Release notes for Dell Container Storage Modules Operator\n","ref":"/csm-docs/v3/deployment/csmoperator/release/","tags":"","title":"Release notes"},{"body":"Release Notes - CSM Observability 1.5.0 New Features/Changes CSM support for Kubernetes 1.26 Support PowerMax in CSM Observability Fixed Issues Observability - Improve Grafana dashboards for PowerFlex/PowerStore Known Issues ","categories":"","description":"Dell Container Storage Modules (CSM) release notes for observability\n","excerpt":"Dell Container Storage Modules (CSM) release notes for observability\n","ref":"/csm-docs/v3/observability/release/","tags":"","title":"Release notes"},{"body":"Release Notes - CSM Application Mobility 0.3.0 New Features/Changes There are no new features in this release\nFixed Issues CSM app-mobility can delete restores but they pop back up after 10 seconds. dellctl crashes on a “backup get” when a trailing “/” is added to the namespace Known Issues There are no known issues in this release.\n","categories":"","description":"Release Notes\n","excerpt":"Release Notes\n","ref":"/csm-docs/docs/applicationmobility/release/","tags":"","title":"Release Notes"},{"body":"New Features/Changes Supports the latest version of CSM. Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\n","categories":"","description":"Release Notes\n","excerpt":"Release Notes\n","ref":"/csm-docs/docs/secure/encryption/release/","tags":"","title":"Release Notes"},{"body":"Release Notes - CSM Application Mobility 0.3.0 New Features/Changes There are no new features in this release\nFixed Issues CSM app-mobility can delete restores but they pop back up after 10 seconds. dellctl crashes on a “backup get” when a trailing “/” is added to the namespace Known Issues There are no known issues in this release.\n","categories":"","description":"Release Notes\n","excerpt":"Release Notes\n","ref":"/csm-docs/v1/applicationmobility/release/","tags":"","title":"Release Notes"},{"body":"New Features/Changes Technical preview release Kubernetes 1.26 support. Security updates. Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\n","categories":"","description":"Release Notes\n","excerpt":"Release Notes\n","ref":"/csm-docs/v1/secure/encryption/release/","tags":"","title":"Release Notes"},{"body":"Release Notes - CSM Application Mobility 0.3.0 New Features/Changes There are no new features in this release\nFixed Issues CSM app-mobility can delete restores but they pop back up after 10 seconds. dellctl crashes on a “backup get” when a trailing “/” is added to the namespace Known Issues There are no known issues in this release.\nRelease Notes - CSM Application Mobility 0.2.0 New Features/Changes Scheduled Backups for Application Mobility Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\nRelease Notes - CSM Application Mobility 0.1.0 New Features/Changes Technical preview release Clone stateful application workloads and application data to other clusters, either on-premise or in the cloud Supports Restic as a data mover for application data Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\n","categories":"","description":"Release Notes\n","excerpt":"Release Notes\n","ref":"/csm-docs/v2/applicationmobility/release/","tags":"","title":"Release Notes"},{"body":"New Features/Changes Technical preview release Kubernetes 1.26 support. Security updates. Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\n","categories":"","description":"Release Notes\n","excerpt":"Release Notes\n","ref":"/csm-docs/v2/secure/encryption/release/","tags":"","title":"Release Notes"},{"body":"Release Notes - CSM Application Mobility 0.3.0 New Features/Changes There are no new features in this release\nFixed Issues CSM app-mobility can delete restores but they pop back up after 10 seconds. dellctl crashes on a “backup get” when a trailing “/” is added to the namespace Known Issues There are no known issues in this release.\nRelease Notes - CSM Application Mobility 0.2.0 New Features/Changes Scheduled Backups for Application Mobility Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\nRelease Notes - CSM Application Mobility 0.1.0 New Features/Changes Technical preview release Clone stateful application workloads and application data to other clusters, either on-premise or in the cloud Supports Restic as a data mover for application data Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\n","categories":"","description":"Release Notes\n","excerpt":"Release Notes\n","ref":"/csm-docs/v3/applicationmobility/release/","tags":"","title":"Release Notes"},{"body":"New Features/Changes Technical preview release Kubernetes 1.26 support. Security updates. Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\n","categories":"","description":"Release Notes\n","excerpt":"Release Notes\n","ref":"/csm-docs/v3/secure/encryption/release/","tags":"","title":"Release Notes"},{"body":"Replication Enabled Storage Classes In order to create replicated volumes \u0026 volume groups, you need to add some extra parameters to your storage class definition. These extra parameters generally carry the prefix replication.storage.dell.com to differentiate them from other provisioning parameters.\nReplication enabled storage classes are always created in pairs within/across clusters and are generally mirrors of each other. Before provisioning replicated volumes, make sure that these pairs of storage classes are created properly.\nCommon Parameters There are 3 mandatory key/value pairs which should always be present in the storage class parameters:\nreplication.storage.dell.com/isReplicationEnabled: 'true' replication.storage.dell.com/remoteClusterID: \u003cRemoteClusterId\u003e replication.storage.dell.com/remoteStorageClassName: \u003cRemoteScName\u003e remoteClusterID This should contain the Cluster ID of the remote cluster where the replicated volume is going to be created. In the case of a single stretched cluster, it should be always set to self.\nremoteStorageClassName This should contain the name of the storage class on the remote cluster which is used to create the remote PersistentVolume.\nNOTE: You still need to create a pair of storage classes even while using a single stretched cluster.\nDriver specific parameters Please refer to the driver specific sections for PowerMax, PowerStore, PowerScale or PowerFlex for a detailed list of parameters.\nPV sync Deletion The dell-csm-replicator supports ‘sync deletion’ of replicated PV resources i.e when a replication enabled PV is deleted its corresponding source or target PV can also be deleted.\nThe decision to whether or not sync delete the corresponding PV depends on a Storage Class parameter which can be configured by the user:\nreplication.storage.dell.com/remotePVRetentionPolicy: 'delete' | 'retain' If the remotePVRetentionPolicy is set to ‘delete’, the corresponding PV would be deleted.\nIf the remotePVRetentionPolicy is set to ‘retain’, the corresponding PV would be retained. This is not applicable for file system replication.\nBy default, if the remotePVRetentionPolicy is not specified in the Storage Class, replicated PV resources are retained.\nRG sync Deletion The dell-csm-replicator supports ‘sync deletion’ of RG (DellCSIReplicationGroup) resources i.e when an RG is deleted its corresponding source or target RG can also be deleted.\nThe decision to whether or not sync delete the corresponding RG depends on a Storage Class parameter which can be configured by the user.\nreplication.storage.dell.com/remoteRGRetentionPolicy: 'delete' | 'retain' If the remoteRGRetentionPolicy is set to ‘delete’, the corresponding RG would be deleted.\nIf the remoteRGRetentionPolicy is set to ‘retain’, the corresponding RG would be retained.\nBy default, if the remoteRGRetentionPolicy is not specified in the Storage Class, replicated RG resources are retained.\nExample If you are setting up replication between two clusters with ClusterID set to Cluster A \u0026 Cluster B, then the storage class definitions in both the clusters would look like:\nCluster A apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rep-src parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteClusterID: ClusterB replication.storage.dell.com/remoteStorageClassName: rep-tgt # Some driver specific replication \u0026 non-replication related params provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate Cluster B apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rep-tgt parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteClusterID: ClusterA replication.storage.dell.com/remoteStorageClassName: rep-src # Some driver specific replication \u0026 non-replication related params provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate ","categories":"","description":"Replication enabled Storage Classes\n","excerpt":"Replication enabled Storage Classes\n","ref":"/csm-docs/docs/replication/deployment/storageclasses/","tags":"","title":"Storage Class"},{"body":"Replication Enabled Storage Classes In order to create replicated volumes \u0026 volume groups, you need to add some extra parameters to your storage class definition. These extra parameters generally carry the prefix replication.storage.dell.com to differentiate them from other provisioning parameters.\nReplication enabled storage classes are always created in pairs within/across clusters and are generally mirrors of each other. Before provisioning replicated volumes, make sure that these pairs of storage classes are created properly.\nCommon Parameters There are 3 mandatory key/value pairs which should always be present in the storage class parameters:\nreplication.storage.dell.com/isReplicationEnabled: 'true' replication.storage.dell.com/remoteClusterID: \u003cRemoteClusterId\u003e replication.storage.dell.com/remoteStorageClassName: \u003cRemoteScName\u003e remoteClusterID This should contain the Cluster ID of the remote cluster where the replicated volume is going to be created. In the case of a single stretched cluster, it should be always set to self.\nremoteStorageClassName This should contain the name of the storage class on the remote cluster which is used to create the remote PersistentVolume.\nNOTE: You still need to create a pair of storage classes even while using a single stretched cluster.\nDriver specific parameters Please refer to the driver specific sections for PowerMax, PowerStore, PowerScale or PowerFlex for a detailed list of parameters.\nPV sync Deletion The dell-csm-replicator supports ‘sync deletion’ of replicated PV resources i.e when a replication enabled PV is deleted its corresponding source or target PV can also be deleted.\nThe decision to whether or not sync delete the corresponding PV depends on a Storage Class parameter which can be configured by the user:\nreplication.storage.dell.com/remotePVRetentionPolicy: 'delete' | 'retain' If the remotePVRetentionPolicy is set to ‘delete’, the corresponding PV would be deleted.\nIf the remotePVRetentionPolicy is set to ‘retain’, the corresponding PV would be retained. This is not applicable for file system replication.\nBy default, if the remotePVRetentionPolicy is not specified in the Storage Class, replicated PV resources are retained.\nRG sync Deletion The dell-csm-replicator supports ‘sync deletion’ of RG (DellCSIReplicationGroup) resources i.e when an RG is deleted its corresponding source or target RG can also be deleted.\nThe decision to whether or not sync delete the corresponding RG depends on a Storage Class parameter which can be configured by the user.\nreplication.storage.dell.com/remoteRGRetentionPolicy: 'delete' | 'retain' If the remoteRGRetentionPolicy is set to ‘delete’, the corresponding RG would be deleted.\nIf the remoteRGRetentionPolicy is set to ‘retain’, the corresponding RG would be retained.\nBy default, if the remoteRGRetentionPolicy is not specified in the Storage Class, replicated RG resources are retained.\nExample If you are setting up replication between two clusters with ClusterID set to Cluster A \u0026 Cluster B, then the storage class definitions in both the clusters would look like:\nCluster A apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rep-src parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteClusterID: ClusterB replication.storage.dell.com/remoteStorageClassName: rep-tgt # Some driver specific replication \u0026 non-replication related params provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate Cluster B apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rep-tgt parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteClusterID: ClusterA replication.storage.dell.com/remoteStorageClassName: rep-src # Some driver specific replication \u0026 non-replication related params provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate ","categories":"","description":"Replication enabled Storage Classes\n","excerpt":"Replication enabled Storage Classes\n","ref":"/csm-docs/v1/replication/deployment/storageclasses/","tags":"","title":"Storage Class"},{"body":"Replication Enabled Storage Classes In order to create replicated volumes \u0026 volume groups, you need to add some extra parameters to your storage class definition. These extra parameters generally carry the prefix replication.storage.dell.com to differentiate them from other provisioning parameters.\nReplication enabled storage classes are always created in pairs within/across clusters and are generally mirrors of each other. Before provisioning replicated volumes, make sure that these pairs of storage classes are created properly.\nCommon Parameters There are 3 mandatory key/value pairs which should always be present in the storage class parameters:\nreplication.storage.dell.com/isReplicationEnabled: 'true' replication.storage.dell.com/remoteClusterID: \u003cRemoteClusterId\u003e replication.storage.dell.com/remoteStorageClassName: \u003cRemoteScName\u003e remoteClusterID This should contain the Cluster ID of the remote cluster where the replicated volume is going to be created. In the case of a single stretched cluster, it should be always set to self.\nremoteStorageClassName This should contain the name of the storage class on the remote cluster which is used to create the remote PersistentVolume.\nNOTE: You still need to create a pair of storage classes even while using a single stretched cluster.\nDriver specific parameters Please refer to the driver specific sections for PowerMax, PowerStore, PowerScale or PowerFlex for a detailed list of parameters.\nPV sync Deletion The dell-csm-replicator supports ‘sync deletion’ of replicated PV resources i.e when a replication enabled PV is deleted its corresponding source or target PV can also be deleted.\nThe decision to whether or not sync delete the corresponding PV depends on a Storage Class parameter which can be configured by the user:\nreplication.storage.dell.com/remotePVRetentionPolicy: 'delete' | 'retain' If the remotePVRetentionPolicy is set to ‘delete’, the corresponding PV would be deleted.\nIf the remotePVRetentionPolicy is set to ‘retain’, the corresponding PV would be retained. This is not applicable for file system replication.\nBy default, if the remotePVRetentionPolicy is not specified in the Storage Class, replicated PV resources are retained.\nRG sync Deletion The dell-csm-replicator supports ‘sync deletion’ of RG (DellCSIReplicationGroup) resources i.e when an RG is deleted its corresponding source or target RG can also be deleted.\nThe decision to whether or not sync delete the corresponding RG depends on a Storage Class parameter which can be configured by the user.\nreplication.storage.dell.com/remoteRGRetentionPolicy: 'delete' | 'retain' If the remoteRGRetentionPolicy is set to ‘delete’, the corresponding RG would be deleted.\nIf the remoteRGRetentionPolicy is set to ‘retain’, the corresponding RG would be retained.\nBy default, if the remoteRGRetentionPolicy is not specified in the Storage Class, replicated RG resources are retained.\nExample If you are setting up replication between two clusters with ClusterID set to Cluster A \u0026 Cluster B, then the storage class definitions in both the clusters would look like:\nCluster A apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rep-src parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteClusterID: ClusterB replication.storage.dell.com/remoteStorageClassName: rep-tgt # Some driver specific replication \u0026 non-replication related params provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate Cluster B apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rep-tgt parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteClusterID: ClusterA replication.storage.dell.com/remoteStorageClassName: rep-src # Some driver specific replication \u0026 non-replication related params provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate ","categories":"","description":"Replication enabled Storage Classes\n","excerpt":"Replication enabled Storage Classes\n","ref":"/csm-docs/v2/replication/deployment/storageclasses/","tags":"","title":"Storage Class"},{"body":"Replication Enabled Storage Classes In order to create replicated volumes \u0026 volume groups, you need to add some extra parameters to your storage class definition. These extra parameters generally carry the prefix replication.storage.dell.com to differentiate them from other provisioning parameters.\nReplication enabled storage classes are always created in pairs within/across clusters and are generally mirrors of each other. Before provisioning replicated volumes, make sure that these pairs of storage classes are created properly.\nCommon Parameters There are 3 mandatory key/value pairs which should always be present in the storage class parameters:\nreplication.storage.dell.com/isReplicationEnabled: 'true' replication.storage.dell.com/remoteClusterID: \u003cRemoteClusterId\u003e replication.storage.dell.com/remoteStorageClassName: \u003cRemoteScName\u003e remoteClusterID This should contain the Cluster ID of the remote cluster where the replicated volume is going to be created. In the case of a single stretched cluster, it should be always set to self.\nremoteStorageClassName This should contain the name of the storage class on the remote cluster which is used to create the remote PersistentVolume.\nNOTE: You still need to create a pair of storage classes even while using a single stretched cluster.\nDriver specific parameters Please refer to the driver specific sections for PowerMax, PowerStore, PowerScale or PowerFlex for a detailed list of parameters.\nPV sync Deletion The dell-csm-replicator supports ‘sync deletion’ of replicated PV resources i.e when a replication enabled PV is deleted its corresponding source or target PV can also be deleted.\nThe decision to whether or not sync delete the corresponding PV depends on a Storage Class parameter which can be configured by the user:\nreplication.storage.dell.com/remotePVRetentionPolicy: 'delete' | 'retain' If the remotePVRetentionPolicy is set to ‘delete’, the corresponding PV would be deleted.\nIf the remotePVRetentionPolicy is set to ‘retain’, the corresponding PV would be retained. This is not applicable for file system replication.\nBy default, if the remotePVRetentionPolicy is not specified in the Storage Class, replicated PV resources are retained.\nRG sync Deletion The dell-csm-replicator supports ‘sync deletion’ of RG (DellCSIReplicationGroup) resources i.e when an RG is deleted its corresponding source or target RG can also be deleted.\nThe decision to whether or not sync delete the corresponding RG depends on a Storage Class parameter which can be configured by the user.\nreplication.storage.dell.com/remoteRGRetentionPolicy: 'delete' | 'retain' If the remoteRGRetentionPolicy is set to ‘delete’, the corresponding RG would be deleted.\nIf the remoteRGRetentionPolicy is set to ‘retain’, the corresponding RG would be retained.\nBy default, if the remoteRGRetentionPolicy is not specified in the Storage Class, replicated RG resources are retained.\nExample If you are setting up replication between two clusters with ClusterID set to Cluster A \u0026 Cluster B, then the storage class definitions in both the clusters would look like:\nCluster A apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rep-src parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteClusterID: ClusterB replication.storage.dell.com/remoteStorageClassName: rep-tgt # Some driver specific replication \u0026 non-replication related params provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate Cluster B apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rep-tgt parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteClusterID: ClusterA replication.storage.dell.com/remoteStorageClassName: rep-src # Some driver specific replication \u0026 non-replication related params provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate ","categories":"","description":"Replication enabled Storage Classes\n","excerpt":"Replication enabled Storage Classes\n","ref":"/csm-docs/v3/replication/deployment/storageclasses/","tags":"","title":"Storage Class"},{"body":" The CSM Authorization RPM will be deprecated in a future release. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nRPM Deployment The Failure of Building an Authorization RPM Running karavictl tenant commands result in an HTTP 504 error Installation fails to install policies After installation, the create-pvc Pod is in an Error state Intermittent 401 issues with generated token The Failure of Building an Authorization RPM This response occurs when running ‘make rpm’ without the proper permissions or correct pathing of the Authorization repository.\nError response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error mounting \"/root/karavi-authorization/bin/deploy\" to rootfs at \"/home/builder/rpm/deploy\": mount /root/karavi-authorization/bin/deploy:/home/builder/rpm/deploy (via /proc/self/fd/6), flags: 0x5000: not a directory: unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type.ERRO[0001] error waiting for container: context canceled Resolution\nEnsure the cloned repository is in a folder independent of the root or home directory. /root/myrepos/karavi-authorization Enable appropriate permissions to the RPM folder (this is where the Authorization RPM is located after being built). chmod o+rwx deploy/rpm Retrieve CSM Authorization Server Logs To retrieve logs from services on the CSM Authorization Server, run the following command (e.g proxy-server logs):\nk3s kubectl logs deploy/proxy-server -n karavi -c proxy-server For OPA related logs, run:\nk3s kubectl logs deploy/proxy-server -n karavi -c opa Running “karavictl tenant” commands result in an HTTP 504 error This situation may occur if there are Iptables or other firewall rules preventing communication with the provided DNS-hostname:\nkaravictl tenant list --addr \u003cDNS-hostname\u003e { \"ErrorMsg\": \"rpc error: code = Unavailable desc = Gateway Timeout: HTTP status code 504; transport: received the unexpected content-type \\\"text/plain; charset=utf-8\\\"\" } Resolution\nConsult with your system administrator or Iptables/firewall documentation. If there are rules in place to prevent communication with the DNS-hostname, either new rules must be created or existing rules must be updated.\nInstallation fails to install policies If SELinux is enabled, the policies may fail to install:\nerror: failed to install policies (see /tmp/policy-install-for-karavi3163047435): exit status 1 Resolution\nThis issue should only occur with older versions of CSM Authorization. If your system is encountering this issue, upgrade to version 1.5.0 or above.\nAfter installation, the create-pvc Pod is in an Error state If SELinux is enabled, the create-pvc Pod may be in an Error state:\nkube-system create-pvc-44a763c7-e70f-4e32-a114-e94615041042 0/1 Error 0 102s Resolution\nRun the following commands to allow the PVC to be created:\nsemanage fcontext -a -t container_file_t \"/var/lib/rancher/k3s/storage(/.*)?\" restorecon -R /var/lib/rancher/k3s/storage/ Intermittent 401 issues with generated token This issue occurs when a new access token is generated in an existing driver installation.\nResolution\nIf you are applying a new token in an existing driver installation, restart the driver pods for the new token to take effect. The token is read once when the driver pods are started and is not dynamically updated.\nkubectl -n \u003cdriver-namespace\u003e rollout restart deploy/\u003cdriver\u003e-controller kubectl -n \u003cdriver-namespace\u003e rollout restart ds/\u003cdriver\u003e-node Helm Deployment The CSI Driver for Dell PowerFlex v2.3.0 is in an Error or CrashLoopBackoff state due to “request denied for path” errors Intermittent 401 issues with generated token The CSI Driver for Dell PowerFlex v2.3.0 is in an Error or CrashLoopBackoff state due to “request denied for path” errors The vxflexos-controller pods will have logs similar to:\ntime=\"2022-06-30T17:35:03Z\" level=error msg=\"failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path \" error=\"rpc error: code = Internal desc = Unable to list volumes: request denied for path\" time=\"2022-06-30T17:35:03Z\" level=error msg=\"array 2d6fb7c6370a990f probe failed: failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path \" ... time=\"2022-06-30T17:35:03Z\" level=fatal msg=\"grpc failed\" error=\"rpc error: code = FailedPrecondition desc = All arrays are not working. Could not proceed further: map[2d6fb7c6370a990f:failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path ]\" The vxflexos-node pods will have logs similar to:\ntime=\"2022-06-30T17:38:32Z\" level=error msg=\"failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path \" error=\"rpc error: code = Internal desc = Unable to list volumes: request denied for path\" time=\"2022-06-30T17:38:32Z\" level=error msg=\"array 2d6fb7c6370a990f probe failed: failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path \" ... time=\"2022-06-30T17:38:32Z\" level=fatal msg=\"grpc failed\" error=\"rpc error: code = FailedPrecondition desc = All arrays are not working. Could not proceed further: map[2d6fb7c6370a990f:failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path ]\" This occurs when the CSM Authorization proxy-server does not allow all driver HTTPS request paths.\nResolution\nEdit the powerflex-urls configMap in the namespace where CSM Authorization is deployed to allow all request paths by default. kubectl -n \u003cnamespace\u003e edit configMap powerflex-urls In the data field, navigate towards the bottom of this field where you see default allow = false. This is highlighted in bold in the example below. Replace false with true and save the edit.\ndata: url.rego: \"# Copyright © 2022 Dell Inc., or its subsidiaries. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n# http:#www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\npackage karavi.authz.url\\n\\nallowlist = [\\n \\\"GET /api/login/\\\",\\n\\t\\t\\\"POST /proxy/refresh-token/\\\",\\n\\t\\t\\\"GET /api/version/\\\",\\n\\t\\t\\\"GET /api/types/System/instances/\\\",\\n\\t\\t\\\"GET /api/types/StoragePool/instances/\\\",\\n\\t\\t\\\"POST /api/types/Volume/instances/\\\",\\n\\t\\t\\\"GET /api/instances/Volume::[a-f0-9]+/$\\\",\\n\\t\\t\\\"POST /api/types/Volume/instances/action/queryIdByKey/\\\",\\n\\t\\t\\\"GET /api/instances/System::[a-f0-9]+/relationships/Sdc/\\\",\\n\\t\\t\\\"GET /api/instances/Sdc::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"GET /api/instances/Sdc::[a-f0-9]+/relationships/Volume/\\\",\\n\\t\\t\\\"GET /api/instances/Volume::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"GET /api/instances/StoragePool::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/addMappedSdc/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/removeMappedSdc/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/removeVolume/\\\"\\n]\\n\\n\u003cb\u003edefault allow = false\u003c/b\u003e\\nallow {\\n\\tregex.match(allowlist[_], sprintf(\\\"%s %s\\\", [input.method, input.url]))\\n}\\n\" Edited data:\ndata: url.rego: \"# Copyright © 2022 Dell Inc., or its subsidiaries. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n# http:#www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\npackage karavi.authz.url\\n\\nallowlist = [\\n \\\"GET /api/login/\\\",\\n\\t\\t\\\"POST /proxy/refresh-token/\\\",\\n\\t\\t\\\"GET /api/version/\\\",\\n\\t\\t\\\"GET /api/types/System/instances/\\\",\\n\\t\\t\\\"GET /api/types/StoragePool/instances/\\\",\\n\\t\\t\\\"POST /api/types/Volume/instances/\\\",\\n\\t\\t\\\"GET /api/instances/Volume::[a-f0-9]+/$\\\",\\n\\t\\t\\\"POST /api/types/Volume/instances/action/queryIdByKey/\\\",\\n\\t\\t\\\"GET /api/instances/System::[a-f0-9]+/relationships/Sdc/\\\",\\n\\t\\t\\\"GET /api/instances/Sdc::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"GET /api/instances/Sdc::[a-f0-9]+/relationships/Volume/\\\",\\n\\t\\t\\\"GET /api/instances/Volume::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"GET /api/instances/StoragePool::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/addMappedSdc/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/removeMappedSdc/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/removeVolume/\\\"\\n]\\n\\n\u003cb\u003edefault allow = true\u003c/b\u003e\\nallow {\\n\\tregex.match(allowlist[_], sprintf(\\\"%s %s\\\", [input.method, input.url]))\\n}\\n\" Rollout restart the CSM Authorization proxy-server so the policy change gets applied. kubectl -n \u003cnamespace\u003e rollout restart deploy/proxy-server Optionally, rollout restart the CSI Driver for Dell PowerFlex to restart the driver pods. Alternatively, wait for the Kubernetes CrashLoopBackoff behavior to restart the driver. kubectl -n \u003cdriver-namespace\u003e rollout restart deploy/vxflexos-controller kubectl -n \u003cdriver-namespace\u003e rollout restart daemonSet/vxflexos-node Intermittent 401 issues with generated token This issue occurs when a new access token is generated in an existing driver installation.\nResolution\nIf you are applying a new token in an existing driver installation, restart the driver pods for the new token to take effect. The token is read once when the driver pods are started and is not dynamically updated.\nkubectl -n \u003cdriver-namespace\u003e rollout restart deploy/\u003cdriver\u003e-controller kubectl -n \u003cdriver-namespace\u003e rollout restart ds/\u003cdriver\u003e-node ","categories":"","description":"Troubleshooting guide\n","excerpt":"Troubleshooting guide\n","ref":"/csm-docs/docs/authorization/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Troubleshooting COSI Driver with logs For logs use:\nkubectl logs \u003cdriver pod\u003e -n dell-cosi Additionaly check kubernetes resources:\nkubectl get bucketclaim -n dell-cosi kubectl get buckets kubectl get bucketaccessclass kubectl get bucketclass kubectl get bucketaccess ","categories":"","description":"Troubleshooting COSI Driver","excerpt":"Troubleshooting COSI Driver","ref":"/csm-docs/docs/cosidriver/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"","categories":"","description":"Troubleshooting for CSI Drivers","excerpt":"Troubleshooting for CSI Drivers","ref":"/csm-docs/docs/csidriver/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Logs and Events The first and in most cases sufficient step in troubleshooting issues with a CSI driver that has Encryption enabled is exploring logs of the Encryption driver and related Kubernetes components. These are some useful log sources:\nCSI Driver Containers Logs The driver creates several controller and node pods. They can be listed with kubectl -n \u003cdriver namespace\u003e get pods. The output will look similar to:\nNAME READY STATUS RESTARTS AGE\risi-controller-84f697c874-2j6d4 10/10 Running 0 16h\risi-node-4gtwf 4/4 Running 0 16h\risi-node-lnzws 4/4 Running 0 16h List containers in pod isi-node-4gtwf with kubectl -n \u003cdriver namespace\u003e logs isi-node-4gtwf. Each pod has containers called driver which is the storage driver container and driver-sec which is the Encryption driver container. These container’s logs tend to provide the most important information, but other containers may give a hint too. View the logs of driver-sec in isi-node-4gtwf with kubectl -n \u003cdriver namespace\u003e logs isi-node-4gtwf driver-sec. The log level of this container can be changed by setting value encryption.logLevel and restarting the driver.\nOften it is necessary to see the logs produced on a specific Kubernetes worker host. To find which node pod is running on which worker host, use kubectl -n \u003cdriver namespace\u003e get pods -o wide.\nPersistentVolume, PersistentVolumeClaim and Application Pod Events Some errors may be logged to the related resource events that can be viewed with kubectl describe command for that resource.\nVault Server Logs Some errors related to communication with the Vault server and key requests may be logged on the Vault server side. If you run a test instance of the server in a Docker container you can view the logs with docker logs vault-server.\nTypical Failure Reasons Incorrect Vault related configuration check logs check vault-auth secret check vault-cert secret check vault-client-conf config map Incorrect Vault server-side configuration check logs check Vault server configuration Expired AppRole secret ID reset the role secret ID Incorrect CSI driver configuration check the related CSI driver troubleshooting steps SSH server is stopped/restarted on the worker host This may manifest in:\nfailure to start the CSI driver failure to create a new encrypted volume failure to access an encrypted volume (IO errors) Resolution:\ncheck SSH server is running on all worker host stop all workloads that use encrypted volumes on the node, then restart them No license provided, or license expired This may manifest in:\nfailure to start the CSI driver failure to create a new encrypted volume Resolution:\nobtain a new valid license check the license is for the cluster on which the encrypted volumes are created check encryption-license secret Typical Rekey Failure reasons If all rekeys in the cluster are failing\ncheck the Rekey controller helm chart values.yaml provisioner name against the Dell CSI driver chart encryption.pluginName, and ensure they match. check the Rekey controller helm chart values.yaml port number against the Dell CSI driver chart encryption.apiPort, and ensure they match. If Rekeys fail for a particular PV\ncheck that the volume is provisioned by the Encryption provisioner check that volume attachments exist for the said PV check that at least one node on which the PV is mounted is available and reachable check the Encryption provisioner logs for details that may indicate the failure reason check the Rekey controller log for the reason for failure If a Rekey results in a Status.Phase of unknown\nthis implies the connection failed during the rekey process which may mean the volume was rekeyed an additional rekey attempt should work assuming a reliable connection to the Encryption provisioner. This may result in the volume being rekeyed twice. ","categories":"","description":"Troubleshooting\n","excerpt":"Troubleshooting\n","ref":"/csm-docs/docs/secure/encryption/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":" The CSM Authorization RPM will be deprecated in a future release. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nRPM Deployment The Failure of Building an Authorization RPM Running karavictl tenant commands result in an HTTP 504 error Installation fails to install policies After installation, the create-pvc Pod is in an Error state Helm Deployment The CSI Driver for Dell PowerFlex v2.3.0 is in an Error or CrashLoopBackoff state due to “request denied for path” errors The Failure of Building an Authorization RPM This response occurs when running ‘make rpm’ without the proper permissions or correct pathing of the Authorization repository.\nError response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error mounting \"/root/karavi-authorization/bin/deploy\" to rootfs at \"/home/builder/rpm/deploy\": mount /root/karavi-authorization/bin/deploy:/home/builder/rpm/deploy (via /proc/self/fd/6), flags: 0x5000: not a directory: unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type.ERRO[0001] error waiting for container: context canceled Resolution\nEnsure the cloned repository is in a folder independent of the root or home directory. /root/myrepos/karavi-authorization Enable appropriate permissions to the RPM folder (this is where the Authorization RPM is located after being built). chmod o+rwx deploy/rpm Retrieve CSM Authorization Server Logs To retrieve logs from services on the CSM Authorization Server, run the following command (e.g proxy-server logs):\nk3s kubectl logs deploy/proxy-server -n karavi -c proxy-server For OPA related logs, run:\nk3s kubectl logs deploy/proxy-server -n karavi -c opa Running “karavictl tenant” commands result in an HTTP 504 error This situation may occur if there are Iptables or other firewall rules preventing communication with the provided DNS-hostname:\nkaravictl tenant list --addr \u003cDNS-hostname\u003e { \"ErrorMsg\": \"rpc error: code = Unavailable desc = Gateway Timeout: HTTP status code 504; transport: received the unexpected content-type \\\"text/plain; charset=utf-8\\\"\" } Resolution\nConsult with your system administrator or Iptables/firewall documentation. If there are rules in place to prevent communication with the DNS-hostname, either new rules must be created or existing rules must be updated.\nInstallation fails to install policies If SELinux is enabled, the policies may fail to install:\nerror: failed to install policies (see /tmp/policy-install-for-karavi3163047435): exit status 1 Resolution\nThis issue should only occur with older versions of CSM Authorization. If your system is encountering this issue, upgrade to version 1.5.0 or above.\nAfter installation, the create-pvc Pod is in an Error state If SELinux is enabled, the create-pvc Pod may be in an Error state:\nkube-system create-pvc-44a763c7-e70f-4e32-a114-e94615041042 0/1 Error 0 102s Resolution\nRun the following commands to allow the PVC to be created:\nsemanage fcontext -a -t container_file_t \"/var/lib/rancher/k3s/storage(/.*)?\" restorecon -R /var/lib/rancher/k3s/storage/ The CSI Driver for Dell PowerFlex v2.3.0 is in an Error or CrashLoopBackoff state due to “request denied for path” errors The vxflexos-controller pods will have logs similar to:\ntime=\"2022-06-30T17:35:03Z\" level=error msg=\"failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path \" error=\"rpc error: code = Internal desc = Unable to list volumes: request denied for path\" time=\"2022-06-30T17:35:03Z\" level=error msg=\"array 2d6fb7c6370a990f probe failed: failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path \" ... time=\"2022-06-30T17:35:03Z\" level=fatal msg=\"grpc failed\" error=\"rpc error: code = FailedPrecondition desc = All arrays are not working. Could not proceed further: map[2d6fb7c6370a990f:failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path ]\" The vxflexos-node pods will have logs similar to:\ntime=\"2022-06-30T17:38:32Z\" level=error msg=\"failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path \" error=\"rpc error: code = Internal desc = Unable to list volumes: request denied for path\" time=\"2022-06-30T17:38:32Z\" level=error msg=\"array 2d6fb7c6370a990f probe failed: failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path \" ... time=\"2022-06-30T17:38:32Z\" level=fatal msg=\"grpc failed\" error=\"rpc error: code = FailedPrecondition desc = All arrays are not working. Could not proceed further: map[2d6fb7c6370a990f:failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path ]\" This occurs when the CSM Authorization proxy-server does not allow all driver HTTPS request paths.\nResolution\nEdit the powerflex-urls configMap in the namespace where CSM Authorization is deployed to allow all request paths by default. kubectl -n \u003cnamespace\u003e edit configMap powerflex-urls In the data field, navigate towards the bottom of this field where you see default allow = false. This is highlighted in bold in the example below. Replace false with true and save the edit.\ndata: url.rego: \"# Copyright © 2022 Dell Inc., or its subsidiaries. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n# http:#www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\npackage karavi.authz.url\\n\\nallowlist = [\\n \\\"GET /api/login/\\\",\\n\\t\\t\\\"POST /proxy/refresh-token/\\\",\\n\\t\\t\\\"GET /api/version/\\\",\\n\\t\\t\\\"GET /api/types/System/instances/\\\",\\n\\t\\t\\\"GET /api/types/StoragePool/instances/\\\",\\n\\t\\t\\\"POST /api/types/Volume/instances/\\\",\\n\\t\\t\\\"GET /api/instances/Volume::[a-f0-9]+/$\\\",\\n\\t\\t\\\"POST /api/types/Volume/instances/action/queryIdByKey/\\\",\\n\\t\\t\\\"GET /api/instances/System::[a-f0-9]+/relationships/Sdc/\\\",\\n\\t\\t\\\"GET /api/instances/Sdc::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"GET /api/instances/Sdc::[a-f0-9]+/relationships/Volume/\\\",\\n\\t\\t\\\"GET /api/instances/Volume::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"GET /api/instances/StoragePool::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/addMappedSdc/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/removeMappedSdc/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/removeVolume/\\\"\\n]\\n\\n\u003cb\u003edefault allow = false\u003c/b\u003e\\nallow {\\n\\tregex.match(allowlist[_], sprintf(\\\"%s %s\\\", [input.method, input.url]))\\n}\\n\" Edited data:\ndata: url.rego: \"# Copyright © 2022 Dell Inc., or its subsidiaries. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n# http:#www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\npackage karavi.authz.url\\n\\nallowlist = [\\n \\\"GET /api/login/\\\",\\n\\t\\t\\\"POST /proxy/refresh-token/\\\",\\n\\t\\t\\\"GET /api/version/\\\",\\n\\t\\t\\\"GET /api/types/System/instances/\\\",\\n\\t\\t\\\"GET /api/types/StoragePool/instances/\\\",\\n\\t\\t\\\"POST /api/types/Volume/instances/\\\",\\n\\t\\t\\\"GET /api/instances/Volume::[a-f0-9]+/$\\\",\\n\\t\\t\\\"POST /api/types/Volume/instances/action/queryIdByKey/\\\",\\n\\t\\t\\\"GET /api/instances/System::[a-f0-9]+/relationships/Sdc/\\\",\\n\\t\\t\\\"GET /api/instances/Sdc::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"GET /api/instances/Sdc::[a-f0-9]+/relationships/Volume/\\\",\\n\\t\\t\\\"GET /api/instances/Volume::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"GET /api/instances/StoragePool::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/addMappedSdc/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/removeMappedSdc/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/removeVolume/\\\"\\n]\\n\\n\u003cb\u003edefault allow = true\u003c/b\u003e\\nallow {\\n\\tregex.match(allowlist[_], sprintf(\\\"%s %s\\\", [input.method, input.url]))\\n}\\n\" Rollout restart the CSM Authorization proxy-server so the policy change gets applied. kubectl -n \u003cnamespace\u003e rollout restart deploy/proxy-server Optionally, rollout restart the CSI Driver for Dell PowerFlex to restart the driver pods. Alternatively, wait for the Kubernetes CrashLoopBackoff behavior to restart the driver. kubectl -n \u003cdriver-namespace\u003e rollout restart deploy/vxflexos-controller kubectl -n \u003cdriver-namespace\u003e rollout restart daemonSet/vxflexos-node ","categories":"","description":"Troubleshooting guide\n","excerpt":"Troubleshooting guide\n","ref":"/csm-docs/v1/authorization/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"","categories":"","description":"Troubleshooting for CSI Drivers","excerpt":"Troubleshooting for CSI Drivers","ref":"/csm-docs/v1/csidriver/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Logs and Events The first and in most cases sufficient step in troubleshooting issues with a CSI driver that has Encryption enabled is exploring logs of the Encryption driver and related Kubernetes components. These are some useful log sources:\nCSI Driver Containers Logs The driver creates several controller and node pods. They can be listed with kubectl -n \u003cdriver namespace\u003e get pods. The output will look similar to:\nNAME READY STATUS RESTARTS AGE isi-controller-84f697c874-2j6d4 10/10 Running 0 16h isi-node-4gtwf 4/4 Running 0 16h isi-node-lnzws 4/4 Running 0 16h List containers in pod isi-node-4gtwf with kubectl -n \u003cdriver namespace\u003e logs isi-node-4gtwf. Each pod has containers called driver which is the storage driver container and driver-sec which is the Encryption driver container. These container’s logs tend to provide the most important information, but other containers may give a hint too. View the logs of driver-sec in isi-node-4gtwf with kubectl -n \u003cdriver namespace\u003e logs isi-node-4gtwf driver-sec. The log level of this container can be changed by setting value encryption.logLevel and restarting the driver.\nOften it is necessary to see the logs produced on a specific Kubernetes worker host. To find which node pod is running on which worker host, use kubectl -n \u003cdriver namespace\u003e get pods -o wide.\nPersistentVolume, PersistentVolumeClaim and Application Pod Events Some errors may be logged to the related resource events that can be viewed with kubectl describe command for that resource.\nVault Server Logs Some errors related to communication with the Vault server and key requests may be logged on the Vault server side. If you run a test instance of the server in a Docker container you can view the logs with docker logs vault-server.\nTypical Failure Reasons Incorrect Vault related configuration check logs check vault-auth secret check vault-cert secret check vault-client-conf config map Incorrect Vault server-side configuration check logs check Vault server configuration Expired AppRole secret ID reset the role secret ID Incorrect CSI driver configuration check the related CSI driver troubleshooting steps SSH server is stopped/restarted on the worker host This may manifest in:\nfailure to start the CSI driver failure to create a new encrypted volume failure to access an encrypted volume (IO errors) Resolution:\ncheck SSH server is running on all worker host stop all workloads that use encrypted volumes on the node, then restart them No license provided, or license expired This may manifest in:\nfailure to start the CSI driver failure to create a new encrypted volume Resolution:\nobtain a new valid license check the license is for the cluster on which the encrypted volumes are created check encryption-license secret Typical Rekey Failure reasons If all rekeys in the cluster are failing\ncheck the Rekey controller helm chart values.yaml provisioner name against the Dell CSI driver chart encryption.pluginName, and ensure they match. check the Rekey controller helm chart values.yaml port number against the Dell CSI driver chart encryption.apiPort, and ensure they match. If Rekeys fail for a particular PV\ncheck that the volume is provisioned by the Encryption provisioner check that volume attachments exist for the said PV check that at least one node on which the PV is mounted is available and reachable check the Encryption provisioner logs for details that may indicate the failure reason check the Rekey controller log for the reason for failure If a Rekey results in a Status.Phase of unknown\nthis implies the connection failed during the rekey process which may mean the volume was rekeyed an additional rekey attempt should work assuming a reliable connection to the Encryption provisioner. This may result in the volume being rekeyed twice. ","categories":"","description":"Troubleshooting\n","excerpt":"Troubleshooting\n","ref":"/csm-docs/v1/secure/encryption/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"RPM Deployment The Failure of Building an Authorization RPM Running karavictl tenant commands result in an HTTP 504 error Installation fails to install policies After installation, the create-pvc Pod is in an Error state Helm Deployment The CSI Driver for Dell PowerFlex v2.3.0 is in an Error or CrashLoopBackoff state due to “request denied for path” errors The Failure of Building an Authorization RPM This response occurs when running ‘make rpm’ without the proper permissions or correct pathing of the Authorization repository.\nError response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error mounting \"/root/karavi-authorization/bin/deploy\" to rootfs at \"/home/builder/rpm/deploy\": mount /root/karavi-authorization/bin/deploy:/home/builder/rpm/deploy (via /proc/self/fd/6), flags: 0x5000: not a directory: unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type.ERRO[0001] error waiting for container: context canceled Resolution\nEnsure the cloned repository is in a folder independent of the root or home directory. /root/myrepos/karavi-authorization Enable appropriate permissions to the RPM folder (this is where the Authorization RPM is located after being built). chmod o+rwx deploy/rpm Retrieve CSM Authorization Server Logs To retrieve logs from services on the CSM Authorization Server, run the following command (e.g proxy-server logs):\nk3s kubectl logs deploy/proxy-server -n karavi -c proxy-server For OPA related logs, run:\nk3s kubectl logs deploy/proxy-server -n karavi -c opa Running “karavictl tenant” commands result in an HTTP 504 error This situation may occur if there are Iptables or other firewall rules preventing communication with the provided DNS-hostname:\nkaravictl tenant list --addr \u003cDNS-hostname\u003e { \"ErrorMsg\": \"rpc error: code = Unavailable desc = Gateway Timeout: HTTP status code 504; transport: received the unexpected content-type \\\"text/plain; charset=utf-8\\\"\" } Resolution\nConsult with your system administrator or Iptables/firewall documentation. If there are rules in place to prevent communication with the DNS-hostname, either new rules must be created or existing rules must be updated.\nInstallation fails to install policies If SELinux is enabled, the policies may fail to install:\nerror: failed to install policies (see /tmp/policy-install-for-karavi3163047435): exit status 1 Resolution\nThis issue should only occur with older versions of CSM Authorization. If your system is encountering this issue, upgrade to version 1.5.0 or above.\nAfter installation, the create-pvc Pod is in an Error state If SELinux is enabled, the create-pvc Pod may be in an Error state:\nkube-system create-pvc-44a763c7-e70f-4e32-a114-e94615041042 0/1 Error 0 102s Resolution\nRun the following commands to allow the PVC to be created:\nsemanage fcontext -a -t container_file_t \"/var/lib/rancher/k3s/storage(/.*)?\" restorecon -R /var/lib/rancher/k3s/storage/ The CSI Driver for Dell PowerFlex v2.3.0 is in an Error or CrashLoopBackoff state due to “request denied for path” errors The vxflexos-controller pods will have logs similar to:\ntime=\"2022-06-30T17:35:03Z\" level=error msg=\"failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path \" error=\"rpc error: code = Internal desc = Unable to list volumes: request denied for path\" time=\"2022-06-30T17:35:03Z\" level=error msg=\"array 2d6fb7c6370a990f probe failed: failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path \" ... time=\"2022-06-30T17:35:03Z\" level=fatal msg=\"grpc failed\" error=\"rpc error: code = FailedPrecondition desc = All arrays are not working. Could not proceed further: map[2d6fb7c6370a990f:failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path ]\" The vxflexos-node pods will have logs similar to:\ntime=\"2022-06-30T17:38:32Z\" level=error msg=\"failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path \" error=\"rpc error: code = Internal desc = Unable to list volumes: request denied for path\" time=\"2022-06-30T17:38:32Z\" level=error msg=\"array 2d6fb7c6370a990f probe failed: failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path \" ... time=\"2022-06-30T17:38:32Z\" level=fatal msg=\"grpc failed\" error=\"rpc error: code = FailedPrecondition desc = All arrays are not working. Could not proceed further: map[2d6fb7c6370a990f:failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path ]\" This occurs when the CSM Authorization proxy-server does not allow all driver HTTPS request paths.\nResolution\nEdit the powerflex-urls configMap in the namespace where CSM Authorization is deployed to allow all request paths by default. kubectl -n \u003cnamespace\u003e edit configMap powerflex-urls In the data field, navigate towards the bottom of this field where you see default allow = false. This is highlighted in bold in the example below. Replace false with true and save the edit.\ndata: url.rego: \"# Copyright © 2022 Dell Inc., or its subsidiaries. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n# http:#www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\npackage karavi.authz.url\\n\\nallowlist = [\\n \\\"GET /api/login/\\\",\\n\\t\\t\\\"POST /proxy/refresh-token/\\\",\\n\\t\\t\\\"GET /api/version/\\\",\\n\\t\\t\\\"GET /api/types/System/instances/\\\",\\n\\t\\t\\\"GET /api/types/StoragePool/instances/\\\",\\n\\t\\t\\\"POST /api/types/Volume/instances/\\\",\\n\\t\\t\\\"GET /api/instances/Volume::[a-f0-9]+/$\\\",\\n\\t\\t\\\"POST /api/types/Volume/instances/action/queryIdByKey/\\\",\\n\\t\\t\\\"GET /api/instances/System::[a-f0-9]+/relationships/Sdc/\\\",\\n\\t\\t\\\"GET /api/instances/Sdc::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"GET /api/instances/Sdc::[a-f0-9]+/relationships/Volume/\\\",\\n\\t\\t\\\"GET /api/instances/Volume::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"GET /api/instances/StoragePool::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/addMappedSdc/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/removeMappedSdc/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/removeVolume/\\\"\\n]\\n\\n\u003cb\u003edefault allow = false\u003c/b\u003e\\nallow {\\n\\tregex.match(allowlist[_], sprintf(\\\"%s %s\\\", [input.method, input.url]))\\n}\\n\" Edited data:\ndata: url.rego: \"# Copyright © 2022 Dell Inc., or its subsidiaries. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n# http:#www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\npackage karavi.authz.url\\n\\nallowlist = [\\n \\\"GET /api/login/\\\",\\n\\t\\t\\\"POST /proxy/refresh-token/\\\",\\n\\t\\t\\\"GET /api/version/\\\",\\n\\t\\t\\\"GET /api/types/System/instances/\\\",\\n\\t\\t\\\"GET /api/types/StoragePool/instances/\\\",\\n\\t\\t\\\"POST /api/types/Volume/instances/\\\",\\n\\t\\t\\\"GET /api/instances/Volume::[a-f0-9]+/$\\\",\\n\\t\\t\\\"POST /api/types/Volume/instances/action/queryIdByKey/\\\",\\n\\t\\t\\\"GET /api/instances/System::[a-f0-9]+/relationships/Sdc/\\\",\\n\\t\\t\\\"GET /api/instances/Sdc::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"GET /api/instances/Sdc::[a-f0-9]+/relationships/Volume/\\\",\\n\\t\\t\\\"GET /api/instances/Volume::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"GET /api/instances/StoragePool::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/addMappedSdc/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/removeMappedSdc/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/removeVolume/\\\"\\n]\\n\\n\u003cb\u003edefault allow = true\u003c/b\u003e\\nallow {\\n\\tregex.match(allowlist[_], sprintf(\\\"%s %s\\\", [input.method, input.url]))\\n}\\n\" Rollout restart the CSM Authorization proxy-server so the policy change gets applied. kubectl -n \u003cnamespace\u003e rollout restart deploy/proxy-server Optionally, rollout restart the CSI Driver for Dell PowerFlex to restart the driver pods. Alternatively, wait for the Kubernetes CrashLoopBackoff behavior to restart the driver. kubectl -n \u003cdriver-namespace\u003e rollout restart deploy/vxflexos-controller kubectl -n \u003cdriver-namespace\u003e rollout restart daemonSet/vxflexos-node ","categories":"","description":"Troubleshooting guide\n","excerpt":"Troubleshooting guide\n","ref":"/csm-docs/v2/authorization/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"","categories":"","description":"Troubleshooting for CSI Drivers","excerpt":"Troubleshooting for CSI Drivers","ref":"/csm-docs/v2/csidriver/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Logs and Events The first and in most cases sufficient step in troubleshooting issues with a CSI driver that has Encryption enabled is exploring logs of the Encryption driver and related Kubernetes components. These are some useful log sources:\nCSI Driver Containers Logs The driver creates several controller and node pods. They can be listed with kubectl -n \u003cdriver namespace\u003e get pods. The output will look similar to:\nNAME READY STATUS RESTARTS AGE isi-controller-84f697c874-2j6d4 10/10 Running 0 16h isi-node-4gtwf 4/4 Running 0 16h isi-node-lnzws 4/4 Running 0 16h List containers in pod isi-node-4gtwf with kubectl -n \u003cdriver namespace\u003e logs isi-node-4gtwf. Each pod has containers called driver which is the storage driver container and driver-sec which is the Encryption driver container. These container’s logs tend to provide the most important information, but other containers may give a hint too. View the logs of driver-sec in isi-node-4gtwf with kubectl -n \u003cdriver namespace\u003e logs isi-node-4gtwf driver-sec. The log level of this container can be changed by setting value encryption.logLevel and restarting the driver.\nOften it is necessary to see the logs produced on a specific Kubernetes worker host. To find which node pod is running on which worker host, use kubectl -n \u003cdriver namespace\u003e get pods -o wide.\nPersistentVolume, PersistentVolumeClaim and Application Pod Events Some errors may be logged to the related resource events that can be viewed with kubectl describe command for that resource.\nVault Server Logs Some errors related to communication with the Vault server and key requests may be logged on the Vault server side. If you run a test instance of the server in a Docker container you can view the logs with docker logs vault-server.\nTypical Failure Reasons Incorrect Vault related configuration check logs check vault-auth secret check vault-cert secret check vault-client-conf config map Incorrect Vault server-side configuration check logs check Vault server configuration Expired AppRole secret ID reset the role secret ID Incorrect CSI driver configuration check the related CSI driver troubleshooting steps SSH server is stopped/restarted on the worker host This may manifest in:\nfailure to start the CSI driver failure to create a new encrypted volume failure to access an encrypted volume (IO errors) Resolution:\ncheck SSH server is running on all worker host stop all workloads that use encrypted volumes on the node, then restart them No license provided, or license expired This may manifest in:\nfailure to start the CSI driver failure to create a new encrypted volume Resolution:\nobtain a new valid license check the license is for the cluster on which the encrypted volumes are created check encryption-license secret Typical Rekey Failure reasons If all rekeys in the cluster are failing\ncheck the Rekey controller helm chart values.yaml provisioner name against the Dell CSI driver chart encryption.pluginName, and ensure they match. check the Rekey controller helm chart values.yaml port number against the Dell CSI driver chart encryption.apiPort, and ensure they match. If Rekeys fail for a particular PV\ncheck that the volume is provisioned by the Encryption provisioner check that volume attachments exist for the said PV check that at least one node on which the PV is mounted is available and reachable check the Encryption provisioner logs for details that may indicate the failure reason check the Rekey controller log for the reason for failure If a Rekey results in a Status.Phase of unknown\nthis implies the connection failed during the rekey process which may mean the volume was rekeyed an additional rekey attempt should work assuming a reliable connection to the Encryption provisioner. This may result in the volume being rekeyed twice. ","categories":"","description":"Troubleshooting\n","excerpt":"Troubleshooting\n","ref":"/csm-docs/v2/secure/encryption/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":" The CSM Authorization karavictl CLI is no longer actively maintained or supported. It will be deprecated in CSM 2.0.\nRPM Deployment The Failure of Building an Authorization RPM Running karavictl tenant commands result in an HTTP 504 error Installation fails to install policies After installation, the create-pvc Pod is in an Error state Helm Deployment The CSI Driver for Dell PowerFlex v2.3.0 is in an Error or CrashLoopBackoff state due to “request denied for path” errors The Failure of Building an Authorization RPM This response occurs when running ‘make rpm’ without the proper permissions or correct pathing of the Authorization repository.\nError response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error mounting \"/root/karavi-authorization/bin/deploy\" to rootfs at \"/home/builder/rpm/deploy\": mount /root/karavi-authorization/bin/deploy:/home/builder/rpm/deploy (via /proc/self/fd/6), flags: 0x5000: not a directory: unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type.ERRO[0001] error waiting for container: context canceled Resolution\nEnsure the cloned repository is in a folder independent of the root or home directory. /root/myrepos/karavi-authorization Enable appropriate permissions to the RPM folder (this is where the Authorization RPM is located after being built). chmod o+rwx deploy/rpm Retrieve CSM Authorization Server Logs To retrieve logs from services on the CSM Authorization Server, run the following command (e.g proxy-server logs):\n$ k3s kubectl logs deploy/proxy-server -n karavi -c proxy-server For OPA related logs, run:\n$ k3s kubectl logs deploy/proxy-server -n karavi -c opa Running “karavictl tenant” commands result in an HTTP 504 error This situation may occur if there are Iptables or other firewall rules preventing communication with the provided \u003cgrpc-address\u003e:\n$ karavictl tenant list --addr \u003cgrpc-address\u003e { \"ErrorMsg\": \"rpc error: code = Unavailable desc = Gateway Timeout: HTTP status code 504; transport: received the unexpected content-type \\\"text/plain; charset=utf-8\\\"\" } Resolution\nConsult with your system administrator or Iptables/firewall documentation. If there are rules in place to prevent communication with the \u003cgrpc-address\u003e, either new rules must be created or existing rules must be updated.\nInstallation fails to install policies If SELinux is enabled, the policies may fail to install:\nerror: failed to install policies (see /tmp/policy-install-for-karavi3163047435): exit status 1 Resolution\nThis issue should only occur with older versions of CSM Authorization. If your system is encountering this issue, upgrade to version 1.5.0 or above.\nAfter installation, the create-pvc Pod is in an Error state If SELinux is enabled, the create-pvc Pod may be in an Error state:\nkube-system create-pvc-44a763c7-e70f-4e32-a114-e94615041042 0/1 Error 0 102s Resolution\nRun the following commands to allow the PVC to be created:\n$ semanage fcontext -a -t container_file_t \"/var/lib/rancher/k3s/storage(/.*)?\" $ restorecon -R /var/lib/rancher/k3s/storage/ The CSI Driver for Dell PowerFlex v2.3.0 is in an Error or CrashLoopBackoff state due to “request denied for path” errors The vxflexos-controller pods will have logs similar to:\ntime=\"2022-06-30T17:35:03Z\" level=error msg=\"failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path \" error=\"rpc error: code = Internal desc = Unable to list volumes: request denied for path\" time=\"2022-06-30T17:35:03Z\" level=error msg=\"array 2d6fb7c6370a990f probe failed: failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path \" ... time=\"2022-06-30T17:35:03Z\" level=fatal msg=\"grpc failed\" error=\"rpc error: code = FailedPrecondition desc = All arrays are not working. Could not proceed further: map[2d6fb7c6370a990f:failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path ]\" The vxflexos-node pods will have logs similar to:\ntime=\"2022-06-30T17:38:32Z\" level=error msg=\"failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path \" error=\"rpc error: code = Internal desc = Unable to list volumes: request denied for path\" time=\"2022-06-30T17:38:32Z\" level=error msg=\"array 2d6fb7c6370a990f probe failed: failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path \" ... time=\"2022-06-30T17:38:32Z\" level=fatal msg=\"grpc failed\" error=\"rpc error: code = FailedPrecondition desc = All arrays are not working. Could not proceed further: map[2d6fb7c6370a990f:failed to list vols for array 2d6fb7c6370a990f : rpc error: code = Internal desc = Unable to list volumes: request denied for path ]\" This occurs when the CSM Authorization proxy-server does not allow all driver HTTPS request paths.\nResolution\nEdit the powerflex-urls configMap in the namespace where CSM Authorization is deployed to allow all request paths by default. kubectl -n \u003cnamespace\u003e edit configMap powerflex-urls In the data field, navigate towards the bottom of this field where you see default allow = false. This is highlighted in bold in the example below. Replace false with true and save the edit.\ndata: url.rego: \"# Copyright © 2022 Dell Inc., or its subsidiaries. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n# http:#www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\npackage karavi.authz.url\\n\\nallowlist = [\\n \\\"GET /api/login/\\\",\\n\\t\\t\\\"POST /proxy/refresh-token/\\\",\\n\\t\\t\\\"GET /api/version/\\\",\\n\\t\\t\\\"GET /api/types/System/instances/\\\",\\n\\t\\t\\\"GET /api/types/StoragePool/instances/\\\",\\n\\t\\t\\\"POST /api/types/Volume/instances/\\\",\\n\\t\\t\\\"GET /api/instances/Volume::[a-f0-9]+/$\\\",\\n\\t\\t\\\"POST /api/types/Volume/instances/action/queryIdByKey/\\\",\\n\\t\\t\\\"GET /api/instances/System::[a-f0-9]+/relationships/Sdc/\\\",\\n\\t\\t\\\"GET /api/instances/Sdc::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"GET /api/instances/Sdc::[a-f0-9]+/relationships/Volume/\\\",\\n\\t\\t\\\"GET /api/instances/Volume::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"GET /api/instances/StoragePool::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/addMappedSdc/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/removeMappedSdc/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/removeVolume/\\\"\\n]\\n\\ndefault allow = false\\nallow {\\n\\tregex.match(allowlist[_], sprintf(\\\"%s %s\\\", [input.method, input.url]))\\n}\\n\" Edited data:\ndata: url.rego: \"# Copyright © 2022 Dell Inc., or its subsidiaries. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n# http:#www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\npackage karavi.authz.url\\n\\nallowlist = [\\n \\\"GET /api/login/\\\",\\n\\t\\t\\\"POST /proxy/refresh-token/\\\",\\n\\t\\t\\\"GET /api/version/\\\",\\n\\t\\t\\\"GET /api/types/System/instances/\\\",\\n\\t\\t\\\"GET /api/types/StoragePool/instances/\\\",\\n\\t\\t\\\"POST /api/types/Volume/instances/\\\",\\n\\t\\t\\\"GET /api/instances/Volume::[a-f0-9]+/$\\\",\\n\\t\\t\\\"POST /api/types/Volume/instances/action/queryIdByKey/\\\",\\n\\t\\t\\\"GET /api/instances/System::[a-f0-9]+/relationships/Sdc/\\\",\\n\\t\\t\\\"GET /api/instances/Sdc::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"GET /api/instances/Sdc::[a-f0-9]+/relationships/Volume/\\\",\\n\\t\\t\\\"GET /api/instances/Volume::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"GET /api/instances/StoragePool::[a-f0-9]+/relationships/Statistics/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/addMappedSdc/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/removeMappedSdc/\\\",\\n\\t\\t\\\"POST /api/instances/Volume::[a-f0-9]+/action/removeVolume/\\\"\\n]\\n\\ndefault allow = true\\nallow {\\n\\tregex.match(allowlist[_], sprintf(\\\"%s %s\\\", [input.method, input.url]))\\n}\\n\" Rollout restart the CSM Authorization proxy-server so the policy change gets applied. kubectl -n \u003cnamespace\u003e rollout restart deploy/proxy-server Optionally, rollout restart the CSI Driver for Dell PowerFlex to restart the driver pods. Alternatively, wait for the Kubernetes CrashLoopBackoff behavior to restart the driver. kubectl -n \u003cdriver-namespace\u003e rollout restart deploy/vxflexos-controller kubectl -n \u003cdriver-namespace\u003e rollout restart daemonSet/vxflexos-node ","categories":"","description":"Troubleshooting guide\n","excerpt":"Troubleshooting guide\n","ref":"/csm-docs/v3/authorization/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"","categories":"","description":"Troubleshooting for CSI Drivers","excerpt":"Troubleshooting for CSI Drivers","ref":"/csm-docs/v3/csidriver/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Logs and Events The first and in most cases sufficient step in troubleshooting issues with a CSI driver that has Encryption enabled is exploring logs of the Encryption driver and related Kubernetes components. These are some useful log sources:\nCSI Driver Containers Logs The driver creates several controller and node pods. They can be listed with kubectl -n \u003cdriver namespace\u003e get pods. The output will look similar to:\nNAME READY STATUS RESTARTS AGE isi-controller-84f697c874-2j6d4 10/10 Running 0 16h isi-node-4gtwf 4/4 Running 0 16h isi-node-lnzws 4/4 Running 0 16h List containers in pod isi-node-4gtwf with kubectl -n \u003cdriver namespace\u003e logs isi-node-4gtwf. Each pod has containers called driver which is the storage driver container and driver-sec which is the Encryption driver container. These container’s logs tend to provide the most important information, but other containers may give a hint too. View the logs of driver-sec in isi-node-4gtwf with kubectl -n \u003cdriver namespace\u003e logs isi-node-4gtwf driver-sec. The log level of this container can be changed by setting value encryption.logLevel and restarting the driver.\nOften it is necessary to see the logs produced on a specific Kubernetes worker host. To find which node pod is running on which worker host, use kubectl -n \u003cdriver namespace\u003e get pods -o wide.\nPersistentVolume, PersistentVolumeClaim and Application Pod Events Some errors may be logged to the related resource events that can be viewed with kubectl describe command for that resource.\nVault Server Logs Some errors related to communication with the Vault server and key requests may be logged on the Vault server side. If you run a test instance of the server in a Docker container you can view the logs with docker logs vault-server.\nTypical Failure Reasons Incorrect Vault related configuration check logs check vault-auth secret check vault-cert secret check vault-client-conf config map Incorrect Vault server-side configuration check logs check Vault server configuration Expired AppRole secret ID reset the role secret ID Incorrect CSI driver configuration check the related CSI driver troubleshooting steps SSH server is stopped/restarted on the worker host This may manifest in:\nfailure to start the CSI driver failure to create a new encrypted volume failure to access an encrypted volume (IO errors) Resolution:\ncheck SSH server is running on all worker host stop all workloads that use encrypted volumes on the node, then restart them No license provided, or license expired This may manifest in:\nfailure to start the CSI driver failure to create a new encrypted volume Resolution:\nobtain a new valid license check the license is for the cluster on which the encrypted volumes are created check encryption-license secret Typical Rekey Failure reasons If all rekeys in the cluster are failing\ncheck the Rekey controller helm chart values.yaml provisioner name against the Dell CSI driver chart encryption.pluginName, and ensure they match. check the Rekey controller helm chart values.yaml port number against the Dell CSI driver chart encryption.apiPort, and ensure they match. If Rekeys fail for a particular PV\ncheck that the volume is provisioned by the Encryption provisioner check that volume attachments exist for the said PV check that at least one node on which the PV is mounted is available and reachable check the Encryption provisioner logs for details that may indicate the failure reason check the Rekey controller log for the reason for failure If a Rekey results in a Status.Phase of unknown\nthis implies the connection failed during the rekey process which may mean the volume was rekeyed an additional rekey attempt should work assuming a reliable connection to the Encryption provisioner. This may result in the volume being rekeyed twice. ","categories":"","description":"Troubleshooting\n","excerpt":"Troubleshooting\n","ref":"/csm-docs/v3/secure/encryption/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Update Driver from v0.1.0 to v0.1.1 using Helm Steps\nRun git clone https://github.com/dell/helm-charts.git to clone the git repository and get the newest helm chart. Run the helm upgrade: helm upgrade \u003crelease_name\u003e ./helm-charts/charts/cosi/ -n \u003cnamespace\u003e ","categories":"","description":"Upgrading COSI Driver","excerpt":"Upgrading COSI Driver","ref":"/csm-docs/docs/cosidriver/upgrade/","tags":"","title":"Upgrade"},{"body":"","categories":"","description":"Support for Array Migration of Volumes\n","excerpt":"Support for Array Migration of Volumes\n","ref":"/csm-docs/docs/replication/migration/","tags":"","title":"Migration"},{"body":"","categories":"","description":"Support for Array Migration of Volumes\n","excerpt":"Support for Array Migration of Volumes\n","ref":"/csm-docs/v1/replication/migration/","tags":"","title":"Migration"},{"body":"","categories":"","description":"Support for Array Migration of Volumes\n","excerpt":"Support for Array Migration of Volumes\n","ref":"/csm-docs/v2/replication/migration/","tags":"","title":"Migration"},{"body":"","categories":"","description":"Support for Array Migration of Volumes\n","excerpt":"Support for Array Migration of Volumes\n","ref":"/csm-docs/v3/replication/migration/","tags":"","title":"Migration"},{"body":"The dell-csm-replicator supports monitoring of DellCSIReplicationGroup Custom Resources (CRs).\nEach RG is polled at a pre-defined interval and for each RG, a gRPC call is made to the driver which returns the status of the protection group on the array.\nIf an RG doesn’t have any PVs associated with it, the driver will not receive any monitoring request for that RG.\nThis status can be obtained from the RG using a standard kubectl get call on the resource name:\nNAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 51d Ready SUSPENDED 2021-09-10T10:48:09Z ","categories":"","description":"DellCSIReplicationGroup Monitoring\n","excerpt":"DellCSIReplicationGroup Monitoring\n","ref":"/csm-docs/docs/replication/monitoring/","tags":"","title":"Monitoring"},{"body":"The dell-csm-replicator supports monitoring of DellCSIReplicationGroup Custom Resources (CRs).\nEach RG is polled at a pre-defined interval and for each RG, a gRPC call is made to the driver which returns the status of the protection group on the array.\nIf an RG doesn’t have any PVs associated with it, the driver will not receive any monitoring request for that RG.\nThis status can be obtained from the RG using a standard kubectl get call on the resource name:\nNAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 51d Ready SUSPENDED 2021-09-10T10:48:09Z ","categories":"","description":"DellCSIReplicationGroup Monitoring\n","excerpt":"DellCSIReplicationGroup Monitoring\n","ref":"/csm-docs/v1/replication/monitoring/","tags":"","title":"Monitoring"},{"body":"The dell-csm-replicator supports monitoring of DellCSIReplicationGroup Custom Resources (CRs).\nEach RG is polled at a pre-defined interval and for each RG, a gRPC call is made to the driver which returns the status of the protection group on the array.\nIf an RG doesn’t have any PVs associated with it, the driver will not receive any monitoring request for that RG.\nThis status can be obtained from the RG using a standard kubectl get call on the resource name:\nNAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 51d Ready SUSPENDED 2021-09-10T10:48:09Z ","categories":"","description":"DellCSIReplicationGroup Monitoring\n","excerpt":"DellCSIReplicationGroup Monitoring\n","ref":"/csm-docs/v2/replication/monitoring/","tags":"","title":"Monitoring"},{"body":"The dell-csm-replicator supports monitoring of DellCSIReplicationGroup Custom Resources (CRs).\nEach RG is polled at a pre-defined interval and for each RG, a gRPC call is made to the driver which returns the status of the protection group on the array.\nIf an RG doesn’t have any PVs associated with it, the driver will not receive any monitoring request for that RG.\nThis status can be obtained from the RG using a standard kubectl get call on the resource name:\nNAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 51d Ready SUSPENDED 2021-09-10T10:48:09Z ","categories":"","description":"DellCSIReplicationGroup Monitoring\n","excerpt":"DellCSIReplicationGroup Monitoring\n","ref":"/csm-docs/v3/replication/monitoring/","tags":"","title":"Monitoring"},{"body":"Enabling Replication In CSI PowerFlex Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerFlex supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Be sure to configure replication between multiple PowerFlex instances using instructions provided by PowerFlex storage.\nEnsure that the remote systems are configured by navigating to the Protection tab and choosing Peer Systems in the UI of the PowerFlex instance.\nThere should be a list of remote systems with the State fields set to Connected.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nRun the following commands to verify that everything is installed correctly:\nCheck controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING Check that the controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target cluster IDs. If you don’t have something installed or something is out-of-place, please refer to installation instructions here.\nInstalling Driver With Replication Module To install the driver with replication enabled, you need to ensure you have set helm parameter replication.enabled in your copy of example values.yaml file (usually called my-powerflex-settings.yaml, myvalues.yaml etc.).\nHere is an example of how that would look:\n... # Set this to true to enable replication replication: enabled: true replicationContextPrefix: \"powerflex\" replicationPrefix: \"replication.storage.dell.com\" ... You can leave other parameters like replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerFlex following the usual installation procedure, just ensure you’ve added the array information for all of the arrays being used in the secret.\nNOTE: You need to install your driver on all clusters where you want to use replication. Both arrays must be accessible from each cluster.\nCreating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nPair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find a sample of a replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-replication provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Retain allowVolumeExpansion: true volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"vxflexos-replication\" replication.storage.dell.com/remoteClusterID: \u003cremoteClusterID\u003e replication.storage.dell.com/remoteSystem: \u003cremoteSystemID\u003e replication.storage.dell.com/remoteStoragePool: \u003cremoteStoragePool\u003e replication.storage.dell.com/rpo: 60 replication.storage.dell.com/volumeGroupPrefix: \"csi\" replication.storage.dell.com/consistencyGroupName: \u003cdesiredConsistencyGroupName\u003e replication.storage.dell.com/protectionDomain: \u003cremoteProtectionDomain\u003e systemID: \u003csourceSystemID\u003e storagepool: \u003csourceStoragePool\u003e protectiondomain: \u003csourceProtectionDomain\u003e Let’s go through each parameter and what it means:\nreplication.storage.dell.com/isReplicationEnabled if set to true will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents the ID of a remote Kubernetes cluster. It is the same id you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system as seen from the current PowerFlex instance. This parameter is the systemID of the array. replication.storage.dell.com/remoteStoragePool is the name of the storage pool on the remote system to be used for creating the remote volumes. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate it from other volume groups. replication.storage.dell.com/consistencyGroupName represents the desired name to give the consistency group on the PowerFlex array. If omitted, the driver will generate a name for the consistency group. replication.storage.dell.com/protectionDomain represents the remote array’s protection domain to use. systemID represents the systemID of the PowerFlex array. storagepool represents the name of the storage pool to be used on the PowerFlex array. protectiondomain represents the array’s protection domain to be used. Let’s follow up that with an example. Let’s assume we have two Kubernetes clusters and two PowerFlex storage arrays:\nClusters have IDs of cluster-1 and cluster-2 Cluster cluster-1 connected to array 000000000001 Cluster cluster-2 connected to array 000000000002 For cluster-1 we plan to use storage pool pool1 and protection domain domain1 For cluster-2 we plan to use storage pool pool1 and protection domain domain1 And this is how our pair of storage classes would look:\nStorageClass to be created in cluster-1:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"vxflexos-replication\" provisioner: \"csi-vxflexos.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate allowVolumeExpansion: true parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"vxflexos-replication\" replication.storage.dell.com/remoteClusterID: \"cluster-2\" replication.storage.dell.com/remoteSystem: \"000000000002\" replication.storage.dell.com/remoteStoragePool: pool1 replication.storage.dell.com/protectionDomain: domain1 replication.storage.dell.com/rpo: 60 replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"000000000001\" storagepool: \"pool1\" protectiondomain: \"domain1\" StorageClass to be created in cluster-2:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"vxflexos-replication\" provisioner: \"csi-vxflexos.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate allowVolumeExpansion: true parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"vxflexos-replication\" replication.storage.dell.com/remoteClusterID: \"cluster-1\" replication.storage.dell.com/remoteSystem: \"000000000001\" replication.storage.dell.com/remoteStoragePool: pool1 replication.storage.dell.com/protectionDomain: domain1 replication.storage.dell.com/rpo: 60 replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"000000000002\" storagepool: \"pool1\" protectiondomain: \"domain1\" After figuring out how storage classes would look, you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with the necessary information. You can find an example in here, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from the manual installation and see how the config would look\nsourceClusterID: \"cluster-1\" targetClusterID: \"cluster-2\" name: \"vxflexos-replication\" driver: \"vxflexos\" reclaimPolicy: \"Retain\" replicationPrefix: \"replication.storage.dell.com\" parameters: storagePool: # populate with storage pool to use of arrays source: \"pool1\" target: \"pool1\" protectionDomain: # populate with protection domain to use of arrays source: \"domain1\" target: \"domain1\" arrayID: # populate with unique ids of storage arrays source: \"0000000000000001\" target: \"0000000000000002\" rpo: \"60\" volumeGroupPrefix: \"csi\" consistencyGroupName: \"\" # optional name to be given to the rcg After preparing the config you can apply it to both clusters with repctl. Just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes, run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes will be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using the ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerFlex driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerFlex driver supports the following list of replication actions:\nFAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC ","categories":"","description":"Enabling Replication feature for CSI PowerFlex","excerpt":"Enabling Replication feature for CSI PowerFlex","ref":"/csm-docs/docs/replication/deployment/powerflex/","tags":"","title":"PowerFlex"},{"body":"Enabling Replication In CSI PowerFlex Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerFlex supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Be sure to configure replication between multiple PowerFlex instances using instructions provided by PowerFlex storage.\nEnsure that the remote systems are configured by navigating to the Protection tab and choosing Peer Systems in the UI of the PowerFlex instance.\nThere should be a list of remote systems with the State fields set to Connected.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nRun the following commands to verify that everything is installed correctly:\nCheck controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING Check that the controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target cluster IDs. If you don’t have something installed or something is out-of-place, please refer to installation instructions here.\nInstalling Driver With Replication Module To install the driver with replication enabled, you need to ensure you have set helm parameter replication.enabled in your copy of example values.yaml file (usually called my-powerflex-settings.yaml, myvalues.yaml etc.).\nHere is an example of how that would look:\n... # Set this to true to enable replication replication: enabled: true image: dellemc/dell-csi-replicator:v1.6.0 replicationContextPrefix: \"powerflex\" replicationPrefix: \"replication.storage.dell.com\" ... You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerFlex following the usual installation procedure, just ensure you’ve added the array information for all of the arrays being used in the secret.\nNOTE: You need to install your driver on all clusters where you want to use replication. Both arrays must be accessible from each cluster.\nCreating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nPair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find a sample of a replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-replication provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Retain allowVolumeExpansion: true volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"vxflexos-replication\" replication.storage.dell.com/remoteClusterID: \u003cremoteClusterID\u003e replication.storage.dell.com/remoteSystem: \u003cremoteSystemID\u003e replication.storage.dell.com/remoteStoragePool: \u003cremoteStoragePool\u003e replication.storage.dell.com/rpo: 60 replication.storage.dell.com/volumeGroupPrefix: \"csi\" replication.storage.dell.com/consistencyGroupName: \u003cdesiredConsistencyGroupName\u003e replication.storage.dell.com/protectionDomain: \u003cremoteProtectionDomain\u003e systemID: \u003csourceSystemID\u003e storagepool: \u003csourceStoragePool\u003e protectiondomain: \u003csourceProtectionDomain\u003e Let’s go through each parameter and what it means:\nreplication.storage.dell.com/isReplicationEnabled if set to true will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents the ID of a remote Kubernetes cluster. It is the same id you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system as seen from the current PowerFlex instance. This parameter is the systemID of the array. replication.storage.dell.com/remoteStoragePool is the name of the storage pool on the remote system to be used for creating the remote volumes. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate it from other volume groups. replication.storage.dell.com/consistencyGroupName represents the desired name to give the consistency group on the PowerFlex array. If omitted, the driver will generate a name for the consistency group. replication.storage.dell.com/protectionDomain represents the remote array’s protection domain to use. systemID represents the systemID of the PowerFlex array. storagepool represents the name of the storage pool to be used on the PowerFlex array. protectiondomain represents the array’s protection domain to be used. Let’s follow up that with an example. Let’s assume we have two Kubernetes clusters and two PowerFlex storage arrays:\nClusters have IDs of cluster-1 and cluster-2 Cluster cluster-1 connected to array 000000000001 Cluster cluster-2 connected to array 000000000002 For cluster-1 we plan to use storage pool pool1 and protection domain domain1 For cluster-2 we plan to use storage pool pool1 and protection domain domain1 And this is how our pair of storage classes would look:\nStorageClass to be created in cluster-1:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"vxflexos-replication\" provisioner: \"csi-vxflexos.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate allowVolumeExpansion: true parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"vxflexos-replication\" replication.storage.dell.com/remoteClusterID: \"cluster-2\" replication.storage.dell.com/remoteSystem: \"000000000002\" replication.storage.dell.com/remoteStoragePool: pool1 replication.storage.dell.com/protectionDomain: domain1 replication.storage.dell.com/rpo: 60 replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"000000000001\" storagepool: \"pool1\" protectiondomain: \"domain1\" StorageClass to be created in cluster-2:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"vxflexos-replication\" provisioner: \"csi-vxflexos.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate allowVolumeExpansion: true parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"vxflexos-replication\" replication.storage.dell.com/remoteClusterID: \"cluster-1\" replication.storage.dell.com/remoteSystem: \"000000000001\" replication.storage.dell.com/remoteStoragePool: pool1 replication.storage.dell.com/protectionDomain: domain1 replication.storage.dell.com/rpo: 60 replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"000000000002\" storagepool: \"pool1\" protectiondomain: \"domain1\" After figuring out how storage classes would look, you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with the necessary information. You can find an example in here, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from the manual installation and see how the config would look\nsourceClusterID: \"cluster-1\" targetClusterID: \"cluster-2\" name: \"vxflexos-replication\" driver: \"vxflexos\" reclaimPolicy: \"Retain\" replicationPrefix: \"replication.storage.dell.com\" parameters: storagePool: # populate with storage pool to use of arrays source: \"pool1\" target: \"pool1\" protectionDomain: # populate with protection domain to use of arrays source: \"domain1\" target: \"domain1\" arrayID: # populate with unique ids of storage arrays source: \"0000000000000001\" target: \"0000000000000002\" rpo: \"60\" volumeGroupPrefix: \"csi\" consistencyGroupName: \"\" # optional name to be given to the rcg After preparing the config you can apply it to both clusters with repctl. Just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes, run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes will be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using the ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerFlex driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerFlex driver supports the following list of replication actions:\nFAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC ","categories":"","description":"Enabling Replication feature for CSI PowerFlex","excerpt":"Enabling Replication feature for CSI PowerFlex","ref":"/csm-docs/v1/replication/deployment/powerflex/","tags":"","title":"PowerFlex"},{"body":"Enabling Replication In CSI PowerFlex Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerFlex supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Be sure to configure replication between multiple PowerFlex instances using instructions provided by PowerFlex storage.\nEnsure that the remote systems are configured by navigating to the Protection tab and choosing Peer Systems in the UI of the PowerFlex instance.\nThere should be a list of remote systems with the State fields set to Connected.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nRun the following commands to verify that everything is installed correctly:\nCheck controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING Check that the controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target cluster IDs. If you don’t have something installed or something is out-of-place, please refer to installation instructions here.\nInstalling Driver With Replication Module To install the driver with replication enabled, you need to ensure you have set helm parameter replication.enabled in your copy of example values.yaml file (usually called my-powerflex-settings.yaml, myvalues.yaml etc.).\nHere is an example of how that would look:\n... # Set this to true to enable replication replication: enabled: true image: dellemc/dell-csi-replicator:v1.2.0 replicationContextPrefix: \"powerflex\" replicationPrefix: \"replication.storage.dell.com\" ... You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerFlex following the usual installation procedure, just ensure you’ve added the array information for all of the arrays being used in the secret.\nNOTE: you need to install your driver at least on the source cluster, but it is recommended to install drivers on all clusters you will use for replication.\nCreating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nPair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find a sample of a replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-replication provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Retain allowVolumeExpansion: true volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"vxflexos-replication\" replication.storage.dell.com/remoteClusterID: \u003cremoteClusterID\u003e replication.storage.dell.com/remoteSystem: \u003cremoteSystemID\u003e replication.storage.dell.com/remoteStoragePool: \u003cremoteStoragePool\u003e replication.storage.dell.com/rpo: 60 replication.storage.dell.com/volumeGroupPrefix: \"csi\" replication.storage.dell.com/consistencyGroupName: \u003cdesiredConsistencyGroupName\u003e replication.storage.dell.com/protectionDomain: \u003cremoteProtectionDomain\u003e systemID: \u003csourceSystemID\u003e storagepool: \u003csourceStoragePool\u003e protectiondomain: \u003csourceProtectionDomain\u003e Let’s go through each parameter and what it means:\nreplication.storage.dell.com/isReplicationEnabled if set to true will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents the ID of a remote cluster. It is the same id you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system as seen from the current PowerFlex instance. This parameter is the systemID of the array. replication.storage.dell.com/remoteStoragePool is the name of the storage pool on the remote system to be used for creating the remote volumes. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate it from other volume groups. replication.storage.dell.com/consistencyGroupName represents the desired name to give the consistency group on the PowerFlex array. If omitted, the driver will generate a name for the consistency group. replication.storage.dell.com/protectionDomain represents the remote array’s protection domain to use. systemID represents the systemID of the PowerFlex array. storagepool represents the name of the storage pool to be used on the PowerFlex array. protectiondomain represents the array’s protection domain to be used. Let’s follow up that with an example. Let’s assume we have two Kubernetes clusters and two PowerFlex storage arrays:\nClusters have IDs of cluster-1 and cluster-2 Cluster cluster-1 connected to array 000000000001 Cluster cluster-2 connected to array 000000000002 For cluster-1 we plan to use storage pool pool1 and protection domain domain1 For cluster-2 we plan to use storage pool pool1 and protection domain domain1 And this is how our pair of storage classes would look:\nStorageClass to be created in cluster-1:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"vxflexos-replication\" provisioner: \"csi-vxflexos.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate allowVolumeExpansion: true parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"vxflexos-replication\" replication.storage.dell.com/remoteClusterID: \"cluster-2\" replication.storage.dell.com/remoteSystem: \"000000000002\" replication.storage.dell.com/remoteStoragePool: pool1 replication.storage.dell.com/protectionDomain: domain1 replication.storage.dell.com/rpo: 60 replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"000000000001\" storagepool: \"pool1\" protectiondomain: \"domain1\" StorageClass to be created in cluster-2:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"vxflexos-replication\" provisioner: \"csi-vxflexos.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate allowVolumeExpansion: true parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"vxflexos-replication\" replication.storage.dell.com/remoteClusterID: \"cluster-1\" replication.storage.dell.com/remoteSystem: \"000000000001\" replication.storage.dell.com/remoteStoragePool: pool1 replication.storage.dell.com/protectionDomain: domain1 replication.storage.dell.com/rpo: 60 replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"000000000002\" storagepool: \"pool1\" protectiondomain: \"domain1\" After figuring out how storage classes would look, you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with the necessary information. You can find an example in here, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from the manual installation and see how the config would look\nsourceClusterID: \"cluster-1\" targetClusterID: \"cluster-2\" name: \"vxflexos-replication\" driver: \"vxflexos\" reclaimPolicy: \"Retain\" replicationPrefix: \"replication.storage.dell.com\" parameters: storagePool: # populate with storage pool to use of arrays source: \"pool1\" target: \"pool1\" protectionDomain: # populate with protection domain to use of arrays source: \"domain1\" target: \"domain1\" arrayID: # populate with unique ids of storage arrays source: \"0000000000000001\" target: \"0000000000000002\" rpo: \"60\" volumeGroupPrefix: \"csi\" consistencyGroupName: \"\" # optional name to be given to the rcg After preparing the config you can apply it to both clusters with repctl. Just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes, run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes will be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using the ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerFlex driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerFlex driver supports the following list of replication actions:\nFAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC ","categories":"","description":"Enabling Replication feature for CSI PowerFlex","excerpt":"Enabling Replication feature for CSI PowerFlex","ref":"/csm-docs/v2/replication/deployment/powerflex/","tags":"","title":"PowerFlex"},{"body":"Enabling Replication In CSI PowerFlex Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerFlex supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Be sure to configure replication between multiple PowerFlex instances using instructions provided by PowerFlex storage.\nEnsure that the remote systems are configured by navigating to the Protection tab and choosing Peer Systems in the UI of the PowerFlex instance.\nThere should be a list of remote systems with the State fields set to Connected.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nRun the following commands to verify that everything is installed correctly:\nCheck controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING Check that the controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target cluster IDs. If you don’t have something installed or something is out-of-place, please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled, you need to ensure you have set helm parameter replication.enabled in your copy of example values.yaml file (usually called my-powerflex-settings.yaml, myvalues.yaml etc.).\nHere is an example of how that would look:\n... # Set this to true to enable replication replication: enabled: true image: dellemc/dell-csi-replicator:v1.2.0 replicationContextPrefix: \"powerflex\" replicationPrefix: \"replication.storage.dell.com\" ... You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerFlex following the usual installation procedure, just ensure you’ve added necessary array connection information to secret.\nNOTE: you need to install your driver at least on the source cluster, but it is recommended to install drivers on all clusters you will use for replication.\nCreating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nPair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find a sample of a replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-replication provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Retain allowVolumeExpansion: true volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"vxflexos-replication\" replication.storage.dell.com/remoteClusterID: \u003cremoteClusterID\u003e replication.storage.dell.com/remoteSystem: \u003cremoteSystemID\u003e replication.storage.dell.com/remoteStoragePool: \u003cremoteStoragePool\u003e replication.storage.dell.com/rpo: 60 replication.storage.dell.com/volumeGroupPrefix: \"csi\" replication.storage.dell.com/consistencyGroupName: \u003cdesiredConsistencyGroupName\u003e replication.storage.dell.com/protectionDomain: \u003cremoteProtectionDomain\u003e systemID: \u003csourceSystemID\u003e storagepool: \u003csourceStoragePool\u003e protectiondomain: \u003csourceProtectionDomain\u003e Let’s go through each parameter and what it means:\nreplication.storage.dell.com/isReplicationEnabled if set to true will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents the ID of a remote cluster. It is the same id you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system as seen from the current PowerFlex instance. This parameter is the systemID of the array. replication.storage.dell.com/remoteStoragePool is the name of the storage pool on the remote system to be used for creating the remote volumes. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate it from other volume groups. replication.storage.dell.com/consistencyGroupName represents the desired name to give the consistency group on the PowerFlex array. If omitted, the driver will generate a name for the consistency group. replication.storage.dell.com/protectionDomain represents the remote array’s protection domain to use. systemID represents the systemID of the PowerFlex array. storagepool represents the name of the storage pool to be used on the PowerFlex array. protectiondomain represents the array’s protection domain to be used. Let’s follow up that with an example. Let’s assume we have two Kubernetes clusters and two PowerFlex storage arrays:\nClusters have IDs of cluster-1 and cluster-2 Cluster cluster-1 connected to array 000000000001 Cluster cluster-2 connected to array 000000000002 For cluster-1 we plan to use storage pool pool1 and protection domain domain1 For cluster-2 we plan to use storage pool pool1 and protection domain domain1 And this is how our pair of storage classes would look:\nStorageClass to be created in cluster-1:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"vxflexos-replication\" provisioner: \"csi-vxflexos.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate allowVolumeExpansion: true parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"vxflexos-replication\" replication.storage.dell.com/remoteClusterID: \"cluster-2\" replication.storage.dell.com/remoteSystem: \"000000000002\" replication.storage.dell.com/remoteStoragePool: pool1 replication.storage.dell.com/protectionDomain: domain1 replication.storage.dell.com/rpo: 60 replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"000000000001\" storagepool: \"pool1\" protectiondomain: \"domain1\" StorageClass to be created in cluster-2:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"vxflexos-replication\" provisioner: \"csi-vxflexos.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate allowVolumeExpansion: true parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"vxflexos-replication\" replication.storage.dell.com/remoteClusterID: \"cluster-1\" replication.storage.dell.com/remoteSystem: \"000000000001\" replication.storage.dell.com/remoteStoragePool: pool1 replication.storage.dell.com/protectionDomain: domain1 replication.storage.dell.com/rpo: 60 replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"000000000002\" storagepool: \"pool1\" protectiondomain: \"domain1\" After figuring out how storage classes would look, you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with the necessary information. You can find an example in here, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from the manual installation and see how the config would look\nsourceClusterID: \"cluster-1\" targetClusterID: \"cluster-2\" name: \"vxflexos-replication\" driver: \"vxflexos\" reclaimPolicy: \"Retain\" replicationPrefix: \"replication.storage.dell.com\" parameters: storagePool: # populate with storage pool to use of arrays source: \"pool1\" target: \"pool1\" protectionDomain: # populate with protection domain to use of arrays source: \"domain1\" target: \"domain1\" arrayID: # populate with unique ids of storage arrays source: \"0000000000000001\" target: \"0000000000000002\" rpo: \"60\" volumeGroupPrefix: \"csi\" consistencyGroupName: \"\" # optional name to be given to the rcg After preparing the config you can apply it to both clusters with repctl. Just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes, run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes will be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using the ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerFlex driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerFlex driver supports the following list of replication actions:\nFAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC ","categories":"","description":"Enabling Replication feature for CSI PowerFlex","excerpt":"Enabling Replication feature for CSI PowerFlex","ref":"/csm-docs/v3/replication/deployment/powerflex/","tags":"","title":"PowerFlex"},{"body":"Enabling Replication In CSI PowerMax Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerMax supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nNote: File Replication for PowerMax is currently not supported\nBefore Installation On Storage Array Configure SRDF connection between multiple PowerMax instances. Follow instructions by PowerMax storage for creating the SRDF Groups between a set of arrays.\nYou can ensure that you configured remote arrays by navigating to the Data Protection tab and choosing SRDF Groups on the managing Unisphere of your array. You should see a list of remote systems with the SRDF Group number that is configured and the Online field set to a green tick.\nWhile using any SRDF groups, ensure that they are for exclusive use by the CSI PowerMax driver:\nAny SRDF group which will be used by the driver is not in use by any other application If an SRDF group is already in use by a CSI driver, don’t use it for provisioning replicated volumes outside CSI provisioning workflows. There are some important limitations that apply to how CSI PowerMax driver uses SRDF groups:\nOne replicated storage group using Async/Sync always contains volumes provisioned from a single namespace. While using SRDF mode Async, a single SRDF group can be used to provision volumes within a single namespace. You can still create multiple storage classes using the same SRDF group for different Service Levels. But all these storage classes will be restricted to provisioning volumes within a single namespace. When using SRDF mode Sync/Metro, a single SRDF group can be used to provision volumes from multiple namespaces. Automatic creation of SRDF Groups CSI Driver for PowerMax supports automatic creation of SRDF Groups as of v2.4.0 with help of 10.0 REST endpoints. To use this feature:\nRemove replication.storage.dell.com/RemoteRDFGroup and replication.storage.dell.com/RDFGroup params from the storage classes before creating first replicated volume. Driver will check next available RDF pair and use them to create volumes. This enables customers to use same storage class across namespace to create volume. Limitation of Auto SRDFG:\nFor Async mode, this feature is supported for namespaces with at most 7 characters. RDF label used to map namespace with the RDF group has limit of 10 char. 3 char is used for cluster prefix to make RDFG unique across clusters. For namespace with more than 7 char, use manual entry of RDF groups in storage class. In Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\nCheck controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING. Check that controller config map is properly populated: kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target clusters IDs. If you don’t have something installed or something is out-of-place, please refer to installation instructions here.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set Helm parameter replication.enabled in your copy of example values.yaml file (usually called my-powermax-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like:\n... # Set this to true to enable replication replication: enabled: true replicationContextPrefix: \"powermax\" replicationPrefix: \"replication.storage.dell.com\" ... You can leave other parameters like replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerMax following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\nNOTE: You need to install your driver on all clusters where you want to use replication. Both arrays must be accessible from each cluster.\nCreating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nA pair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-srdf provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate parameters: SRP: \u003cSRP Name\u003e SYMID: \u003cSYMID\u003e ServiceLevel: \u003cService Level\u003e replication.storage.dell.com/RemoteSYMID: \u003cRemoteSYMID\u003e replication.storage.dell.com/RemoteSRP: \u003cRemoteSRP\u003e replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/RemoteServiceLevel: \u003cRemote Service Level\u003e replication.storage.dell.com/RdfMode: \u003cRdfMode\u003e replication.storage.dell.com/Bias: \"false\" replication.storage.dell.com/RdfGroup: \u003cRdfGroup\u003e # optional replication.storage.dell.com/RemoteRDFGroup: \u003cRemoteRDFGroup\u003e # optional replication.storage.dell.com/remoteStorageClassName: \u003cRemoteStorageClassName\u003e replication.storage.dell.com/remoteClusterID: \u003cRemoteClusterID\u003e Let’s go through each parameter and what it means:\nreplication.storage.dell.com/isReplicationEnabled if set to true, will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/RemoteStorageClassName points to the name of the remote storage class, if you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/RemoteClusterID represents the ID of a remote Kubernetes cluster, it is the same ID you put in the replication controller config map. replication.storage.dell.com/RemoteSYMID is the Symmetrix ID of the remote array. replication.storage.dell.com/RemoteSRP is the storage pool of the remote array. replication.storage.dell.com/RemoteServiceLevel is the service level that will be assigned to remote volumes. replication.storage.dell.com/RdfMode points to the RDF mode you want to use. It should be one out of “ASYNC”, “METRO” and “SYNC”. If mode is set to METRO, driver does not need RemoteStorageClassName and RemoteClusterID as it supports METRO with single cluster configuration. replication.storage.dell.com/Bias when the RdfMode is set to METRO, this parameter is required to indicate driver to use Bias or Witness. If set to true, the driver will configure METRO with Bias, if set to false, the driver will configure METRO with Witness. replication.storage.dell.com/RdfGroup is the local SRDF group number, as configured. It is optional for using Auto SRDF group by driver. replication.storage.dell.com/RemoteRDFGroup is the remote SRDF group number, as configured. It is optional for using Auto SRDF group by driver. Let’s follow up that with an example, let’s assume we have two Kubernetes clusters and two PowerMax storage arrays:\nClusters have IDs of cluster-1 and cluster-2 There are two arrays local Symmetrix array: 000000000001 and remote Symmetrix array: 000000000002 Storage arrays are connected to each other via RdfGroup 1 and RemoteRDFGroup 2 Cluster cluster-1 connected to array 000000000001 Cluster cluster-2 connected to array 000000000002 RDF Mode is ASYNC And this how would our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-srdf provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate parameters: SRP: \"SRP\" SYMID: \"000000000001\" ServiceLevel: \"Optimized\" replication.storage.dell.com/RemoteSYMID: \"000000000002\" replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/RemoteSRP: \"SRP\" replication.storage.dell.com/RemoteServiceLevel: \"Optimized\" replication.storage.dell.com/RdfMode: \"ASYNC\" replication.storage.dell.com/Bias: \"false\" replication.storage.dell.com/RdfGroup: \"1\" replication.storage.dell.com/RemoteRDFGroup: \"2\" replication.storage.dell.com/remoteStorageClassName: \"powermax-srdf\" replication.storage.dell.com/remoteClusterID: \"cluster-2\" StorageClass to be created in cluster-2:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-srdf provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate parameters: SRP: \"SRP\" SYMID: \"000000000002\" ServiceLevel: \"Optimized\" replication.storage.dell.com/RemoteSYMID: \"000000000001\" replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/RemoteServiceLevel: \"Optimized\" replication.storage.dell.com/RemoteSRP: \"SRP\" replication.storage.dell.com/Bias: \"false\" replication.storage.dell.com/RdfMode: \"ASYNC\" replication.storage.dell.com/RdfGroup: \"2\" replication.storage.dell.com/RemoteRDFGroup: \"1\" replication.storage.dell.com/remoteStorageClassName: \"powermax-srdf\" replication.storage.dell.com/remoteClusterID: \"cluster-1\" After creating storage class YAML files, they must be applied to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill the config with necessary information. You can find an example here, copy it, and modify it to your needs.\nIf you open this example you can see similar fields and parameters to what was seen in manual storage class creation.\nLet’s use the same example from manual installation and see what its repctl config file would look like:\nsourceClusterID: \"cluster-1\" targetClusterID: \"cluster-2\" name: \"powermax-replication\" driver: \"powermax\" reclaimPolicy: \"Retain\" replicationPrefix: \"replication.storage.dell.com\" parameters: rdfMode: \"ASYNC\" srp: source: \"SRP_1\" target: \"SRP_1\" symId: source: \"000000000001\" target: \"000000000002\" serviceLevel: source: \"Optimized\" target: \"Optimized\" rdfGroup: source: \"1\" target: \"2\" After preparing the config you can apply it to both clusters with repctl, just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes will be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using the newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nProvisioning Metro Volumes Here is an example of a storage class configured for Metro mode:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-metro provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate parameters: SRP: \"SRP\" SYMID: \"000000000001\" ServiceLevel: \"Optimized\" replication.storage.dell.com/RemoteSYMID: \"000000000002\" replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/RemoteSRP: \"SRP\" replication.storage.dell.com/RemoteServiceLevel: \"Optimized\" replication.storage.dell.com/RdfMode: \"Metro\" replication.storage.dell.com/Bias: \"true\" replication.storage.dell.com/RdfGroup: \"3\" replication.storage.dell.com/RemoteRDFGroup: \"3\" After installing the driver and creating a storage class with Metro config (as shown above) we can create volumes. On your cluster, create a PersistentVolumeClaim using this storage class. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nSupported Replication Actions The CSI PowerMax driver supports the following list of replication actions:\nBasic Site Specific Actions FAILOVER_LOCAL FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL UNPLANNED_FAILOVER_REMOTE REPROTECT_LOCAL REPROTECT_REMOTE Advanced Site Specific Actions In this section, we are going to refer to “Site A” as the original source site \u0026 “Site B” as the original target site. Any action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\nFAILOVER_WITHOUT_SWAP_LOCAL You can use this action to do a failover when you are at Site B, and don’t want to swap the replication direction. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_LOCAL. After receiving this request the CSI driver will attempxt to Fail over to Site B which is the local site. FAILOVER_WITHOUT_SWAP_REMOTE You can use this action to do a failover when you are at Site A, and don’t want to swap the replication direction. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_REMOTE. After receiving this request the CSI driver will attempt to Fail over to Site B which is the remote site. FAILBACK_LOCAL You can use this action to do a failback, and when you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_LOCAL. After receiving this request the CSI driver will attempt to Fail back from Site B to Site A which is the local site. FAILBACK_REMOTE You can use this action to do a failback, and when you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_REMOTE. After receiving this request the CSI driver will attempt to Fail back to Site A from Site B which is the local site. SWAP_LOCAL You can use this action to swap the replication direction, and you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_LOCAL. After receiving this request the CSI driver will attempt to do SWAP at Site A which is the local site. SWAP_REMOTE You can use this action to swap the replication direction, and you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_REMOTE. After receiving this request the CSI driver will attempt to do SWAP at Site B which is the remote site. Maintenance Actions SUSPEND RESUME ESTABLISH SYNC Deletion of DellCSIReplicationGroup The deletion of DellCSIReplicationGroup custom resource triggers the DeleteStorageProtectionGroup call on the driver. The storage protection group on the array can be deleted only if it has no volumes associated with it. If the deletion is triggered on the storage protection group with volumes, the deletion will fail and the dell-csi-driver will return a final error to the dell-csm-replication sidecar.\n","categories":"","description":"Enabling Replication feature for CSI PowerMax","excerpt":"Enabling Replication feature for CSI PowerMax","ref":"/csm-docs/docs/replication/deployment/powermax/","tags":"","title":"PowerMax"},{"body":"Enabling Replication In CSI PowerMax Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerMax supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nNote: File Replication for PowerMax is currently not supported\nBefore Installation On Storage Array Configure SRDF connection between multiple PowerMax instances. Follow instructions by PowerMax storage for creating the SRDF Groups between a set of arrays.\nYou can ensure that you configured remote arrays by navigating to the Data Protection tab and choosing SRDF Groups on the managing Unisphere of your array. You should see a list of remote systems with the SRDF Group number that is configured and the Online field set to a green tick.\nWhile using any SRDF groups, ensure that they are for exclusive use by the CSI PowerMax driver:\nAny SRDF group which will be used by the driver is not in use by any other application If an SRDF group is already in use by a CSI driver, don’t use it for provisioning replicated volumes outside CSI provisioning workflows. There are some important limitations that apply to how CSI PowerMax driver uses SRDF groups:\nOne replicated storage group using Async/Sync always contains volumes provisioned from a single namespace. While using SRDF mode Async, a single SRDF group can be used to provision volumes within a single namespace. You can still create multiple storage classes using the same SRDF group for different Service Levels. But all these storage classes will be restricted to provisioning volumes within a single namespace. When using SRDF mode Sync/Metro, a single SRDF group can be used to provision volumes from multiple namespaces. Automatic creation of SRDF Groups CSI Driver for PowerMax supports automatic creation of SRDF Groups as of v2.4.0 with help of 10.0 REST endpoints. To use this feature:\nRemove replication.storage.dell.com/RemoteRDFGroup and replication.storage.dell.com/RDFGroup params from the storage classes before creating first replicated volume. Driver will check next available RDF pair and use them to create volumes. This enables customers to use same storage class across namespace to create volume. Limitation of Auto SRDFG:\nFor Async mode, this feature is supported for namespaces with at most 7 characters. RDF label used to map namespace with the RDF group has limit of 10 char. 3 char is used for cluster prefix to make RDFG unique across clusters. For namespace with more than 7 char, use manual entry of RDF groups in storage class. In Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\nCheck controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING. Check that controller config map is properly populated: kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target clusters IDs. If you don’t have something installed or something is out-of-place, please refer to installation instructions here.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set Helm parameter replication.enabled in your copy of example values.yaml file (usually called my-powermax-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like:\n... # Set this to true to enable replication replication: enabled: true image: dellemc/dell-csi-replicator:v1.6.0 replicationContextPrefix: \"powermax\" replicationPrefix: \"replication.storage.dell.com\" ... You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerMax following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\nNOTE: You need to install your driver on all clusters where you want to use replication. Both arrays must be accessible from each cluster.\nCreating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nA pair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-srdf provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate parameters: SRP: \u003cSRP Name\u003e SYMID: \u003cSYMID\u003e ServiceLevel: \u003cService Level\u003e replication.storage.dell.com/RemoteSYMID: \u003cRemoteSYMID\u003e replication.storage.dell.com/RemoteSRP: \u003cRemoteSRP\u003e replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/RemoteServiceLevel: \u003cRemote Service Level\u003e replication.storage.dell.com/RdfMode: \u003cRdfMode\u003e replication.storage.dell.com/Bias: \"false\" replication.storage.dell.com/RdfGroup: \u003cRdfGroup\u003e # optional replication.storage.dell.com/RemoteRDFGroup: \u003cRemoteRDFGroup\u003e # optional replication.storage.dell.com/remoteStorageClassName: \u003cRemoteStorageClassName\u003e replication.storage.dell.com/remoteClusterID: \u003cRemoteClusterID\u003e Let’s go through each parameter and what it means:\nreplication.storage.dell.com/isReplicationEnabled if set to true, will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/RemoteStorageClassName points to the name of the remote storage class, if you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/RemoteClusterID represents the ID of a remote Kubernetes cluster, it is the same ID you put in the replication controller config map. replication.storage.dell.com/RemoteSYMID is the Symmetrix ID of the remote array. replication.storage.dell.com/RemoteSRP is the storage pool of the remote array. replication.storage.dell.com/RemoteServiceLevel is the service level that will be assigned to remote volumes. replication.storage.dell.com/RdfMode points to the RDF mode you want to use. It should be one out of “ASYNC”, “METRO” and “SYNC”. If mode is set to METRO, driver does not need RemoteStorageClassName and RemoteClusterID as it supports METRO with single cluster configuration. replication.storage.dell.com/Bias when the RdfMode is set to METRO, this parameter is required to indicate driver to use Bias or Witness. If set to true, the driver will configure METRO with Bias, if set to false, the driver will configure METRO with Witness. replication.storage.dell.com/RdfGroup is the local SRDF group number, as configured. It is optional for using Auto SRDF group by driver. replication.storage.dell.com/RemoteRDFGroup is the remote SRDF group number, as configured. It is optional for using Auto SRDF group by driver. Let’s follow up that with an example, let’s assume we have two Kubernetes clusters and two PowerMax storage arrays:\nClusters have IDs of cluster-1 and cluster-2 There are two arrays local Symmetrix array: 000000000001 and remote Symmetrix array: 000000000002 Storage arrays are connected to each other via RdfGroup 1 and RemoteRDFGroup 2 Cluster cluster-1 connected to array 000000000001 Cluster cluster-2 connected to array 000000000002 RDF Mode is ASYNC And this how would our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-srdf provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate parameters: SRP: \"SRP\" SYMID: \"000000000001\" ServiceLevel: \"Optimized\" replication.storage.dell.com/RemoteSYMID: \"000000000002\" replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/RemoteSRP: \"SRP\" replication.storage.dell.com/RemoteServiceLevel: \"Optimized\" replication.storage.dell.com/RdfMode: \"ASYNC\" replication.storage.dell.com/Bias: \"false\" replication.storage.dell.com/RdfGroup: \"1\" replication.storage.dell.com/RemoteRDFGroup: \"2\" replication.storage.dell.com/remoteStorageClassName: \"powermax-srdf\" replication.storage.dell.com/remoteClusterID: \"cluster-2\" StorageClass to be created in cluster-2:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-srdf provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate parameters: SRP: \"SRP\" SYMID: \"000000000002\" ServiceLevel: \"Optimized\" replication.storage.dell.com/RemoteSYMID: \"000000000001\" replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/RemoteServiceLevel: \"Optimized\" replication.storage.dell.com/RemoteSRP: \"SRP\" replication.storage.dell.com/Bias: \"false\" replication.storage.dell.com/RdfMode: \"ASYNC\" replication.storage.dell.com/RdfGroup: \"2\" replication.storage.dell.com/RemoteRDFGroup: \"1\" replication.storage.dell.com/remoteStorageClassName: \"powermax-srdf\" replication.storage.dell.com/remoteClusterID: \"cluster-1\" After creating storage class YAML files, they must be applied to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill the config with necessary information. You can find an example here, copy it, and modify it to your needs.\nIf you open this example you can see similar fields and parameters to what was seen in manual storage class creation.\nLet’s use the same example from manual installation and see what its repctl config file would look like:\nsourceClusterID: \"cluster-1\" targetClusterID: \"cluster-2\" name: \"powermax-replication\" driver: \"powermax\" reclaimPolicy: \"Retain\" replicationPrefix: \"replication.storage.dell.com\" parameters: rdfMode: \"ASYNC\" srp: source: \"SRP_1\" target: \"SRP_1\" symId: source: \"000000000001\" target: \"000000000002\" serviceLevel: source: \"Optimized\" target: \"Optimized\" rdfGroup: source: \"1\" target: \"2\" After preparing the config you can apply it to both clusters with repctl, just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes will be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using the newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nProvisioning Metro Volumes Here is an example of a storage class configured for Metro mode:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-metro provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate parameters: SRP: \"SRP\" SYMID: \"000000000001\" ServiceLevel: \"Optimized\" replication.storage.dell.com/RemoteSYMID: \"000000000002\" replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/RemoteSRP: \"SRP\" replication.storage.dell.com/RemoteServiceLevel: \"Optimized\" replication.storage.dell.com/RdfMode: \"Metro\" replication.storage.dell.com/Bias: \"true\" replication.storage.dell.com/RdfGroup: \"3\" replication.storage.dell.com/RemoteRDFGroup: \"3\" After installing the driver and creating a storage class with Metro config (as shown above) we can create volumes. On your cluster, create a PersistentVolumeClaim using this storage class. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nSupported Replication Actions The CSI PowerMax driver supports the following list of replication actions:\nBasic Site Specific Actions FAILOVER_LOCAL FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL UNPLANNED_FAILOVER_REMOTE REPROTECT_LOCAL REPROTECT_REMOTE Advanced Site Specific Actions In this section, we are going to refer to “Site A” as the original source site \u0026 “Site B” as the original target site. Any action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\nFAILOVER_WITHOUT_SWAP_LOCAL You can use this action to do a failover when you are at Site B, and don’t want to swap the replication direction. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_LOCAL. After receiving this request the CSI driver will attempxt to Fail over to Site B which is the local site. FAILOVER_WITHOUT_SWAP_REMOTE You can use this action to do a failover when you are at Site A, and don’t want to swap the replication direction. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_REMOTE. After receiving this request the CSI driver will attempt to Fail over to Site B which is the remote site. FAILBACK_LOCAL You can use this action to do a failback, and when you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_LOCAL. After receiving this request the CSI driver will attempt to Fail back from Site B to Site A which is the local site. FAILBACK_REMOTE You can use this action to do a failback, and when you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_REMOTE. After receiving this request the CSI driver will attempt to Fail back to Site A from Site B which is the local site. SWAP_LOCAL You can use this action to swap the replication direction, and you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_LOCAL. After receiving this request the CSI driver will attempt to do SWAP at Site A which is the local site. SWAP_REMOTE You can use this action to swap the replication direction, and you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_REMOTE. After receiving this request the CSI driver will attempt to do SWAP at Site B which is the remote site. Maintenance Actions SUSPEND RESUME ESTABLISH SYNC Deletion of DellCSIReplicationGroup The deletion of DellCSIReplicationGroup custom resource triggers the DeleteStorageProtectionGroup call on the driver. The storage protection group on the array can be deleted only if it has no volumes associated with it. If the deletion is triggered on the storage protection group with volumes, the deletion will fail and the dell-csi-driver will return a final error to the dell-csm-replication sidecar.\n","categories":"","description":"Enabling Replication feature for CSI PowerMax","excerpt":"Enabling Replication feature for CSI PowerMax","ref":"/csm-docs/v1/replication/deployment/powermax/","tags":"","title":"PowerMax"},{"body":"Enabling Replication In CSI PowerMax Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerMax supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Configure SRDF connection between multiple PowerMax instances. Follow instructions by PowerMax storage for creating the SRDF Groups between a set of arrays.\nYou can ensure that you configured remote arrays by navigating to the Data Protection tab and choosing SRDF Groups on the managing Unisphere of your array. You should see a list of remote systems with the SRDF Group number that is configured and the Online field set to a green tick.\nWhile using any SRDF groups, ensure that they are for exclusive use by the CSI PowerMax driver:\nAny SRDF group which will be used by the driver is not in use by any other application If an SRDF group is already in use by a CSI driver, don’t use it for provisioning replicated volumes outside CSI provisioning workflows. There are some important limitations that apply to how CSI PowerMax driver uses SRDF groups:\nOne replicated storage group using Async/Sync always contains volumes provisioned from a single namespace. While using SRDF mode Async, a single SRDF group can be used to provision volumes within a single namespace. You can still create multiple storage classes using the same SRDF group for different Service Levels. But all these storage classes will be restricted to provisioning volumes within a single namespace. When using SRDF mode Sync/Metro, a single SRDF group can be used to provision volumes from multiple namespaces. Automatic creation of SRDF Groups CSI Driver for PowerMax supports automatic creation of SRDF Groups as of v2.4.0 with help of 10.0 REST endpoints. To use this feature:\nRemove replication.storage.dell.com/RemoteRDFGroup and replication.storage.dell.com/RDFGroup params from the storage classes before creating first replicated volume. Driver will check next available RDF pair and use them to create volumes. This enables customers to use same storage class across namespace to create volume. Limitation of Auto SRDFG:\nFor Async mode, this feature is supported for namespaces with at most 7 characters. RDF label used to map namespace with the RDF group has limit of 10 char. 3 char is used for cluster prefix to make RDFG unique across clusters. For namespace with more than 7 char, use manual entry of RDF groups in storage class. In Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\nCheck controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING. Check that controller config map is properly populated: kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target clusters IDs. If you don’t have something installed or something is out-of-place, please refer to installation instructions here.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set Helm parameter replication.enabled in your copy of example values.yaml file (usually called my-powermax-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like:\n... # Set this to true to enable replication replication: enabled: true image: dellemc/dell-csi-replicator:v1.0.0 replicationContextPrefix: \"powermax\" replicationPrefix: \"replication.storage.dell.com\" ... You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerMax following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\nNOTE: You need to install your driver at least on the source cluster, but it is recommended to install drivers on all clusters you will use for replication.\nCreating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nA pair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-srdf provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate parameters: SRP: \u003cSRP Name\u003e SYMID: \u003cSYMID\u003e ServiceLevel: \u003cService Level\u003e replication.storage.dell.com/RemoteSYMID: \u003cRemoteSYMID\u003e replication.storage.dell.com/RemoteSRP: \u003cRemoteSRP\u003e replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/RemoteServiceLevel: \u003cRemote Service Level\u003e replication.storage.dell.com/RdfMode: \u003cRdfMode\u003e replication.storage.dell.com/Bias: \"false\" replication.storage.dell.com/RdfGroup: \u003cRdfGroup\u003e # optional replication.storage.dell.com/RemoteRDFGroup: \u003cRemoteRDFGroup\u003e # optional replication.storage.dell.com/remoteStorageClassName: \u003cRemoteStorageClassName\u003e replication.storage.dell.com/remoteClusterID: \u003cRemoteClusterID\u003e Let’s go through each parameter and what it means:\nreplication.storage.dell.com/isReplicationEnabled if set to true, will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/RemoteStorageClassName points to the name of the remote storage class, if you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/RemoteClusterID represents the ID of a remote cluster, it is the same ID you put in the replication controller config map. replication.storage.dell.com/RemoteSYMID is the Symmetrix ID of the remote array. replication.storage.dell.com/RemoteSRP is the storage pool of the remote array. replication.storage.dell.com/RemoteServiceLevel is the service level that will be assigned to remote volumes. replication.storage.dell.com/RdfMode points to the RDF mode you want to use. It should be one out of “ASYNC”, “METRO” and “SYNC”. If mode is set to METRO, driver does not need RemoteStorageClassName and RemoteClusterID as it supports METRO with single cluster configuration. replication.storage.dell.com/Bias when the RdfMode is set to METRO, this parameter is required to indicate driver to use Bias or Witness. If set to true, the driver will configure METRO with Bias, if set to false, the driver will configure METRO with Witness. replication.storage.dell.com/RdfGroup is the local SRDF group number, as configured. It is optional for using Auto SRDF group by driver. replication.storage.dell.com/RemoteRDFGroup is the remote SRDF group number, as configured. It is optional for using Auto SRDF group by driver. Let’s follow up that with an example, let’s assume we have two Kubernetes clusters and two PowerMax storage arrays:\nClusters have IDs of cluster-1 and cluster-2 There are two arrays local Symmetrix array: 000000000001 and remote Symmetrix array: 000000000002 Storage arrays are connected to each other via RdfGroup 1 and RemoteRDFGroup 2 Cluster cluster-1 connected to array 000000000001 Cluster cluster-2 connected to array 000000000002 RDF Mode is ASYNC And this how would our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-srdf provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate parameters: SRP: \"SRP\" SYMID: \"000000000001\" ServiceLevel: \"Optimized\" replication.storage.dell.com/RemoteSYMID: \"000000000002\" replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/RemoteSRP: \"SRP\" replication.storage.dell.com/RemoteServiceLevel: \"Optimized\" replication.storage.dell.com/RdfMode: \"ASYNC\" replication.storage.dell.com/Bias: \"false\" replication.storage.dell.com/RdfGroup: \"1\" replication.storage.dell.com/RemoteRDFGroup: \"2\" replication.storage.dell.com/remoteStorageClassName: \"powermax-srdf\" replication.storage.dell.com/remoteClusterID: \"cluster-2\" StorageClass to be created in cluster-2:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-srdf provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate parameters: SRP: \"SRP\" SYMID: \"000000000002\" ServiceLevel: \"Optimized\" replication.storage.dell.com/RemoteSYMID: \"000000000001\" replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/RemoteServiceLevel: \"Optimized\" replication.storage.dell.com/RemoteSRP: \"SRP\" replication.storage.dell.com/Bias: \"false\" replication.storage.dell.com/RdfMode: \"ASYNC\" replication.storage.dell.com/RdfGroup: \"2\" replication.storage.dell.com/RemoteRDFGroup: \"1\" replication.storage.dell.com/remoteStorageClassName: \"powermax-srdf\" replication.storage.dell.com/remoteClusterID: \"cluster-1\" After creating storage class YAML files, they must be applied to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill the config with necessary information. You can find an example here, copy it, and modify it to your needs.\nIf you open this example you can see similar fields and parameters to what was seen in manual storage class creation.\nLet’s use the same example from manual installation and see what its repctl config file would look like:\nsourceClusterID: \"cluster-1\" targetClusterID: \"cluster-2\" name: \"powermax-replication\" driver: \"powermax\" reclaimPolicy: \"Retain\" replicationPrefix: \"replication.storage.dell.com\" parameters: rdfMode: \"ASYNC\" srp: source: \"SRP_1\" target: \"SRP_1\" symId: source: \"000000000001\" target: \"000000000002\" serviceLevel: source: \"Optimized\" target: \"Optimized\" rdfGroup: source: \"1\" target: \"2\" After preparing the config you can apply it to both clusters with repctl, just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes will be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using the newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nProvisioning Metro Volumes Here is an example of a storage class configured for Metro mode:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-metro provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate parameters: SRP: \"SRP\" SYMID: \"000000000001\" ServiceLevel: \"Optimized\" replication.storage.dell.com/RemoteSYMID: \"000000000002\" replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/RemoteSRP: \"SRP\" replication.storage.dell.com/RemoteServiceLevel: \"Optimized\" replication.storage.dell.com/RdfMode: \"Metro\" replication.storage.dell.com/Bias: \"true\" replication.storage.dell.com/RdfGroup: \"3\" replication.storage.dell.com/RemoteRDFGroup: \"3\" After installing the driver and creating a storage class with Metro config (as shown above) we can create volumes. On your cluster, create a PersistentVolumeClaim using this storage class. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nSupported Replication Actions The CSI PowerMax driver supports the following list of replication actions:\nBasic Site Specific Actions FAILOVER_LOCAL FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL UNPLANNED_FAILOVER_REMOTE REPROTECT_LOCAL REPROTECT_REMOTE Advanced Site Specific Actions In this section, we are going to refer to “Site A” as the original source site \u0026 “Site B” as the original target site. Any action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\nFAILOVER_WITHOUT_SWAP_LOCAL You can use this action to do a failover when you are at Site B, and don’t want to swap the replication direction. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_LOCAL. After receiving this request the CSI driver will attempxt to Fail over to Site B which is the local site. FAILOVER_WITHOUT_SWAP_REMOTE You can use this action to do a failover when you are at Site A, and don’t want to swap the replication direction. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_REMOTE. After receiving this request the CSI driver will attempt to Fail over to Site B which is the remote site. FAILBACK_LOCAL You can use this action to do a failback, and when you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_LOCAL. After receiving this request the CSI driver will attempt to Fail back from Site B to Site A which is the local site. FAILBACK_REMOTE You can use this action to do a failback, and when you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_REMOTE. After receiving this request the CSI driver will attempt to Fail back to Site A from Site B which is the local site. SWAP_LOCAL You can use this action to swap the replication direction, and you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_LOCAL. After receiving this request the CSI driver will attempt to do SWAP at Site A which is the local site. SWAP_REMOTE You can use this action to swap the replication direction, and you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_REMOTE. After receiving this request the CSI driver will attempt to do SWAP at Site B which is the remote site. Maintenance Actions SUSPEND RESUME ESTABLISH SYNC Deletion of DellCSIReplicationGroup The deletion of DellCSIReplicationGroup custom resource triggers the DeleteStorageProtectionGroup call on the driver. The storage protection group on the array can be deleted only if it has no volumes associated with it. If the deletion is triggered on the storage protection group with volumes, the deletion will fail and the dell-csi-driver will return a final error to the dell-csm-replication sidecar.\n","categories":"","description":"Enabling Replication feature for CSI PowerMax","excerpt":"Enabling Replication feature for CSI PowerMax","ref":"/csm-docs/v2/replication/deployment/powermax/","tags":"","title":"PowerMax"},{"body":"Enabling Replication In CSI PowerMax Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerMax supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Configure SRDF connection between multiple PowerMax instances. Follow instructions by PowerMax storage for creating the SRDF Groups between a set of arrays.\nYou can ensure that you configured remote arrays by navigating to the Data Protection tab and choosing SRDF Groups on the managing Unisphere of your array. You should see a list of remote systems with the SRDF Group number that is configured and the Online field set to a green tick.\nWhile using any SRDF groups, ensure that they are for exclusive use by the CSI PowerMax driver:\nAny SRDF group which will be used by the driver is not in use by any other application If an SRDF group is already in use by a CSI driver, don’t use it for provisioning replicated volumes outside CSI provisioning workflows. There are some important limitations that apply to how CSI PowerMax driver uses SRDF groups:\nOne replicated storage group using Async/Sync always contains volumes provisioned from a single namespace. While using SRDF mode Async, a single SRDF group can be used to provision volumes within a single namespace. You can still create multiple storage classes using the same SRDF group for different Service Levels. But all these storage classes will be restricted to provisioning volumes within a single namespace. When using SRDF mode Sync/Metro, a single SRDF group can be used to provision volumes from multiple namespaces. Automatic creation of SRDF Groups CSI Driver for Powermax supports automatic creation of SRDF Groups as of v2.4.0 with help of 10.0 REST endpoints. To use this feature:\nRemove replication.storage.dell.com/RemoteRDFGroup and replication.storage.dell.com/RDFGroup params from the storage classes before creating first replicated volume. Driver will check next available RDF pair and use them to create volumes. This enables customers to use same storage class across namespace to create volume. Limitation of Auto SRDFG:\nFor Async mode, this feature is supported for namespaces with at most 7 characters. RDF label used to map namespace with the RDF group has limit of 10 char. 3 char is used for cluster prefix to make RDFG unique across clusters. For namespace with more than 7 char, use manual entry of RDF groups in storage class. In Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\nCheck controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING. Check that controller config map is properly populated: kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target clusters IDs. If you don’t have something installed or something out-of-place please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set Helm parameter replication.enabled in your copy of example values.yaml file (usually called my-powermax-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like:\n... # Set this to true to enable replication replication: enabled: true image: dellemc/dell-csi-replicator:v1.0.0 replicationContextPrefix: \"powermax\" replicationPrefix: \"replication.storage.dell.com\" ... You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerMax following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\nNOTE: You need to install your driver at least on the source cluster, but it is recommended to install drivers on all clusters you will use for replication.\nCreating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nA pair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-srdf provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate parameters: SRP: \u003cSRP Name\u003e SYMID: \u003cSYMID\u003e ServiceLevel: \u003cService Level\u003e replication.storage.dell.com/RemoteSYMID: \u003cRemoteSYMID\u003e replication.storage.dell.com/RemoteSRP: \u003cRemoteSRP\u003e replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/RemoteServiceLevel: \u003cRemote Service Level\u003e replication.storage.dell.com/RdfMode: \u003cRdfMode\u003e replication.storage.dell.com/Bias: \"false\" replication.storage.dell.com/RdfGroup: \u003cRdfGroup\u003e # optional replication.storage.dell.com/RemoteRDFGroup: \u003cRemoteRDFGroup\u003e # optional replication.storage.dell.com/remoteStorageClassName: \u003cRemoteStorageClassName\u003e replication.storage.dell.com/remoteClusterID: \u003cRemoteClusterID\u003e Let’s go through each parameter and what it means:\nreplication.storage.dell.com/isReplicationEnabled if set to true, will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/RemoteStorageClassName points to the name of the remote storage class, if you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/RemoteClusterID represents the ID of a remote cluster, it is the same ID you put in the replication controller config map. replication.storage.dell.com/RemoteSYMID is the Symmetrix ID of the remote array. replication.storage.dell.com/RemoteSRP is the storage pool of the remote array. replication.storage.dell.com/RemoteServiceLevel is the service level that will be assigned to remote volumes. replication.storage.dell.com/RdfMode points to the RDF mode you want to use. It should be one out of “ASYNC”, “METRO” and “SYNC”. If mode is set to METRO, driver does not need RemoteStorageClassName and RemoteClusterID as it supports METRO with single cluster configuration. replication.storage.dell.com/Bias when the RdfMode is set to METRO, this parameter is required to indicate driver to use Bias or Witness. If set to true, the driver will configure METRO with Bias, if set to false, the driver will configure METRO with Witness. replication.storage.dell.com/RdfGroup is the local SRDF group number, as configured. It is optional for using Auto SRDF group by driver. replication.storage.dell.com/RemoteRDFGroup is the remote SRDF group number, as configured. It is optional for using Auto SRDF group by driver. Let’s follow up that with an example, let’s assume we have two Kubernetes clusters and two PowerMax storage arrays:\nClusters have IDs of cluster-1 and cluster-2 There are two arrays local Symmetrix array: 000000000001 and remote Symmetrix array: 000000000002 Storage arrays are connected to each other via RdfGroup 1 and RemoteRDFGroup 2 Cluster cluster-1 connected to array 000000000001 Cluster cluster-2 connected to array 000000000002 RDF Mode is ASYNC And this how would our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-srdf provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate parameters: SRP: \"SRP\" SYMID: \"000000000001\" ServiceLevel: \"Optimized\" replication.storage.dell.com/RemoteSYMID: \"000000000002\" replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/RemoteSRP: \"SRP\" replication.storage.dell.com/RemoteServiceLevel: \"Optimized\" replication.storage.dell.com/RdfMode: \"ASYNC\" replication.storage.dell.com/Bias: \"false\" replication.storage.dell.com/RdfGroup: \"1\" replication.storage.dell.com/RemoteRDFGroup: \"2\" replication.storage.dell.com/remoteStorageClassName: \"powermax-srdf\" replication.storage.dell.com/remoteClusterID: \"cluster-2\" StorageClass to be created in cluster-2:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-srdf provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate parameters: SRP: \"SRP\" SYMID: \"000000000002\" ServiceLevel: \"Optimized\" replication.storage.dell.com/RemoteSYMID: \"000000000001\" replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/RemoteServiceLevel: \"Optimized\" replication.storage.dell.com/RemoteSRP: \"SRP\" replication.storage.dell.com/Bias: \"false\" replication.storage.dell.com/RdfMode: \"ASYNC\" replication.storage.dell.com/RdfGroup: \"2\" replication.storage.dell.com/RemoteRDFGroup: \"1\" replication.storage.dell.com/remoteStorageClassName: \"powermax-srdf\" replication.storage.dell.com/remoteClusterID: \"cluster-1\" After creating storage class YAML files, they must be applied to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill the config with necessary information. You can find an example here, copy it, and modify it to your needs.\nIf you open this example you can see similar fields and parameters to what was seen in manual storage class creation.\nLet’s use the same example from manual installation and see what its repctl config file would look like:\nsourceClusterID: \"cluster-1\" targetClusterID: \"cluster-2\" name: \"powermax-replication\" driver: \"powermax\" reclaimPolicy: \"Retain\" replicationPrefix: \"replication.storage.dell.com\" parameters: rdfMode: \"ASYNC\" srp: source: \"SRP_1\" target: \"SRP_1\" symId: source: \"000000000001\" target: \"000000000002\" serviceLevel: source: \"Optimized\" target: \"Optimized\" rdfGroup: source: \"1\" target: \"2\" After preparing the config you can apply it to both clusters with repctl, just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes will be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using the newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nProvisioning Metro Volumes Here is an example of a storage class configured for Metro mode:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-metro provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: Immediate parameters: SRP: \"SRP\" SYMID: \"000000000001\" ServiceLevel: \"Optimized\" replication.storage.dell.com/RemoteSYMID: \"000000000002\" replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/RemoteSRP: \"SRP\" replication.storage.dell.com/RemoteServiceLevel: \"Optimized\" replication.storage.dell.com/RdfMode: \"Metro\" replication.storage.dell.com/Bias: \"true\" replication.storage.dell.com/RdfGroup: \"3\" replication.storage.dell.com/RemoteRDFGroup: \"3\" After installing the driver and creating a storage class with Metro config (as shown above) we can create volumes. On your cluster, create a PersistentVolumeClaim using this storage class. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nSupported Replication Actions The CSI PowerMax driver supports the following list of replication actions:\nBasic Site Specific Actions FAILOVER_LOCAL FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL UNPLANNED_FAILOVER_REMOTE REPROTECT_LOCAL REPROTECT_REMOTE Advanced Site Specific Actions In this section, we are going to refer to “Site A” as the original source site \u0026 “Site B” as the original target site. Any action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\nFAILOVER_WITHOUT_SWAP_LOCAL You can use this action to do a failover when you are at Site B, and don’t want to swap the replication direction. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_LOCAL. After receiving this request the CSI driver will attempxt to Fail over to Site B which is the local site. FAILOVER_WITHOUT_SWAP_REMOTE You can use this action to do a failover when you are at Site A, and don’t want to swap the replication direction. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_REMOTE. After receiving this request the CSI driver will attempt to Fail over to Site B which is the remote site. FAILBACK_LOCAL You can use this action to do a failback, and when you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_LOCAL. After receiving this request the CSI driver will attempt to Fail back from Site B to Site A which is the local site. FAILBACK_REMOTE You can use this action to do a failback, and when you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_REMOTE. After receiving this request the CSI driver will attempt to Fail back to Site A from Site B which is the local site. SWAP_LOCAL You can use this action to swap the replication direction, and you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_LOCAL. After receiving this request the CSI driver will attempt to do SWAP at Site A which is the local site. SWAP_REMOTE You can use this action to swap the replication direction, and you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_REMOTE. After receiving this request the CSI driver will attempt to do SWAP at Site B which is the remote site. Maintenance Actions SUSPEND RESUME ESTABLISH SYNC Deletion of DellCSIReplicationGroup The deletion of DellCSIReplicationGroup custom resource triggers the DeleteStorageProtectionGroup call on the driver. The storage protection group on the array can be deleted only if it has no volumes associated with it. If the deletion is triggered on the storage protection group with volumes, the deletion will fail and the dell-csi-driver will return a final error to the dell-csm-replication sidecar.\n","categories":"","description":"Enabling Replication feature for CSI PowerMax","excerpt":"Enabling Replication feature for CSI PowerMax","ref":"/csm-docs/v3/replication/deployment/powermax/","tags":"","title":"PowerMax"},{"body":"Release Notes - CSM Authorization 1.9.1 New Features/Changes #947 - [FEATURE]: Support for Kubernetes 1.28 #1066 - [FEATURE]: Support for Openshift 4.14 #996 - [FEATURE]: Dell CSI to Dell CSM Operator Migration Process #1031 - [FEATURE]: Update to the latest UBI Micro image for CSM #1062 - [FEATURE]: CSM PowerMax: Support PowerMax v10.1 Fixed Issues Known Issues There are no known issues in this release.\n","categories":"","description":"Dell Container Storage Modules (CSM) release notes for authorization\n","excerpt":"Dell Container Storage Modules (CSM) release notes for authorization\n","ref":"/csm-docs/docs/authorization/release/","tags":"","title":"Release notes"},{"body":"Release Notes - CSM Authorization 1.8.0 New Features/Changes #922 - [FEATURE]: Use ubi9 micro as base image Fixed Issues #895 - [BUG]: Update CSM Authorization karavictl CLI flag descriptions #916 - [BUG]: Remove references to deprecated io/ioutil package Known Issues There are no known issues in this release.\n","categories":"","description":"Dell Container Storage Modules (CSM) release notes for authorization\n","excerpt":"Dell Container Storage Modules (CSM) release notes for authorization\n","ref":"/csm-docs/v1/authorization/release/","tags":"","title":"Release notes"},{"body":"Release Notes - CSM Authorization 1.7.0 New Features/Changes CSM Authorization karavictl requires an admin token. (#725) CSM support for Kubernetes 1.27. (#761) CSM 1.7 release specific changes. (#743) CSM Authorization encryption for secrets in K3S. (#774) Bugs Authorization should have sample CRD for every supported version in csm-operator. (#826) Improve CSM Operator Authorization documentation. (#800) CSM Authorization doesn’t write the status code on error for csi-powerscale. (#787) Authorization RPM installation should use nogpgcheck for k3s-selinux package. (#772) CSM Authorization - karavictl generate token should output valid yaml. (#767) ","categories":"","description":"Dell Container Storage Modules (CSM) release notes for authorization\n","excerpt":"Dell Container Storage Modules (CSM) release notes for authorization\n","ref":"/csm-docs/v2/authorization/release/","tags":"","title":"Release notes"},{"body":"Release Notes - CSM Authorization 1.6.0 New Features/Changes Restrict the version of TLS to v1.2 for all requests to CSM authorization proxy server. (#642) PowerFlex preapproved GUIDs. (#402) CSM 1.6 release specific changes. (#583) Bugs CSM Authorization quota of zero should allow infinite use for PowerFlex and PowerMax. (#654) CSM Authorization CRD in the CSM Operator doesn’t read custom configurations. (#633) ","categories":"","description":"Dell Container Storage Modules (CSM) release notes for authorization\n","excerpt":"Dell Container Storage Modules (CSM) release notes for authorization\n","ref":"/csm-docs/v3/authorization/release/","tags":"","title":"Release notes"},{"body":"Release Notes - COSI Driver v0.1.1 New Features/Changes #1031 - [FEATURE]: Update to the latest UBI Micro image for CSM ","categories":"","description":"Release Notes for COSI Driver","excerpt":"Release Notes for COSI Driver","ref":"/csm-docs/docs/cosidriver/release/","tags":"","title":"Release Notes"},{"body":"","categories":"","description":"Release Notes for all the CSI Drivers and deployment","excerpt":"Release Notes for all the CSI Drivers and deployment","ref":"/csm-docs/docs/csidriver/release/","tags":"","title":"Release Notes"},{"body":"","categories":"","description":"Release Notes for all the CSI Drivers and deployment","excerpt":"Release Notes for all the CSI Drivers and deployment","ref":"/csm-docs/v1/csidriver/release/","tags":"","title":"Release Notes"},{"body":"","categories":"","description":"Release Notes for all the CSI Drivers and deployment","excerpt":"Release Notes for all the CSI Drivers and deployment","ref":"/csm-docs/v2/csidriver/release/","tags":"","title":"Release Notes"},{"body":"","categories":"","description":"Release Notes for all the CSI Drivers and deployment","excerpt":"Release Notes for all the CSI Drivers and deployment","ref":"/csm-docs/v3/csidriver/release/","tags":"","title":"Release Notes"},{"body":"Container Storage Modules (CSM) for Replication is part of the open-source suite of Kubernetes storage enablers for Dell products.\nCSM for Replication project aims to bring Replication \u0026 Disaster Recovery capabilities of Dell Storage Arrays to Kubernetes clusters. It helps you replicate groups of volumes using the native replication technology available on the storage array and can provide you a way to restart applications in case of both planned and unplanned migration.\nCSM for Replication Capabilities CSM for Replication provides the following capabilities:\nCapability PowerMax PowerStore PowerScale PowerFlex Unity Replicate data using native storage array based replication yes yes yes yes no Asynchronous file volume replication no no yes no no Asynchronous block volume replication yes yes n/a yes no Synchronous file volume replication no no no no no Synchronous block volume replication yes no n/a no no Active-Active (Metro) block volume replication yes no n/a no no Active-Active (Metro) file volume replication no no no no no Create PersistentVolume objects in the cluster representing the replicated volume yes yes yes yes no Create DellCSIReplicationGroup objects in the cluster yes yes yes yes no Failover \u0026 Reprotect applications using the replicated volumes yes yes yes yes no Online Volume Expansion for replicated volumes yes no no yes no Provides a command line utility - repctl for configuring \u0026 managing replication related resources across multiple clusters yes yes yes yes no Supported Operating Systems/Container Orchestrator Platforms COP/OS PowerMax PowerStore PowerScale PowerFlex Kubernetes 1.26, 1.27, 1.28 1.26, 1.27, 1.28 1.26, 1.27, 1.28 1.26, 1.27, 1.28 Red Hat OpenShift 4.13, 4.14 4.13, 4.14 4.13, 4.14 4.13, 4.14 Supported Storage Platforms PowerMax PowerStore PowerScale PowerFlex Storage Array PowerMax 2500/8500 PowerMaxOS 10 (6079) , PowerMaxOS 10.0.1 (6079) , PowerMaxOS 10.1 (6079)\nPowerMax 2000/8000 - 5978.711.xxx, 5978.479.xxx Unisphere 10.0,10.0.1,10.1 3.0, 3.2, 3.5 OneFS 9.3, 9.4, 9.5.0.x (x \u003e= 5) 3.6.x, 4.0.x, 4.5 Note: File Replication for PowerMax is currently not supported\nDetails As on the storage arrays, all replication related Kubernetes entities are required/created in pairs -\nPair of Kubernetes Clusters Pair of replication enabled Storage classes Pair of PersistentVolumes representing the replicated pair on the storage array Pair of DellCSIReplicationGroup objects representing the replicated protection groups on the storage array You can also use a single stretched Kubernetes cluster for protecting your applications. Even in this topology, rest of the objects still exist in pairs.\nWhat it does not do Replicate application manifests within/across clusters. Stop applications before the planned/unplanned migration. Start applications after the migration. Replicate PersistentVolumeClaim objects within/across clusters. Replication with METRO mode does not need Replicator sidecar and common controller. Different namespaces cannot share the same RDF group for creating volumes with ASYNC mode for PowerMax. Same RDF group cannot be shared across different replication modes for PowerMax. QuickStart Install all required components: Enable replication during CSI driver installation Install CSM Replication Controller \u0026 repctl Create replication enabled storage classes Create PersistentVolumeClaim using the replication enabled storage class How it works At a high level, the following happens when you create a PersistentVolumeClaim object using a replication enabled storage class -\nCSI driver creates protection group on the storage array (if required) CSI driver creates the volume and adds it to the protection group. There will be a corresponding group and pair on the remote storage array A DellCSIReplicationGroup object is created in the cluster representing the protection group on the storage array A replica of the PersistentVolume \u0026 DellCSIReplicationGroup is created You can refer this page for more details about the architecture.\nOnce the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), you can exercise the general Disaster Recovery workflows -\nPlanned Migration to the target cluster/array Unplanned Migration to the target cluster/array Reprotect volumes at the target cluster/array Maintenance activities like - Suspend, Resume, Establish replication ","categories":"","description":"Dell Container Storage Modules (CSM) for Replication\n","excerpt":"Dell Container Storage Modules (CSM) for Replication\n","ref":"/csm-docs/docs/replication/","tags":"","title":"Replication"},{"body":"Container Storage Modules (CSM) for Replication is part of the open-source suite of Kubernetes storage enablers for Dell products.\nCSM for Replication project aims to bring Replication \u0026 Disaster Recovery capabilities of Dell Storage Arrays to Kubernetes clusters. It helps you replicate groups of volumes using the native replication technology available on the storage array and can provide you a way to restart applications in case of both planned and unplanned migration.\nCSM for Replication Capabilities CSM for Replication provides the following capabilities:\nCapability PowerMax PowerStore PowerScale PowerFlex Unity Replicate data using native storage array based replication yes yes yes yes no Asynchronous file volume replication no no yes no no Asynchronous block volume replication yes yes n/a yes no Synchronous file volume replication no no no no no Synchronous block volume replication yes no n/a no no Active-Active (Metro) block volume replication yes no n/a no no Active-Active (Metro) file volume replication no no no no no Create PersistentVolume objects in the cluster representing the replicated volume yes yes yes yes no Create DellCSIReplicationGroup objects in the cluster yes yes yes yes no Failover \u0026 Reprotect applications using the replicated volumes yes yes yes yes no Online Volume Expansion for replicated volumes yes no no yes no Provides a command line utility - repctl for configuring \u0026 managing replication related resources across multiple clusters yes yes yes yes no Supported Operating Systems/Container Orchestrator Platforms COP/OS PowerMax PowerStore PowerScale PowerFlex Kubernetes 1.25, 1.26, 1.27 1.25, 1.26, 1.27 1.25, 1.26, 1.27 1.25, 1.26, 1.27 Red Hat OpenShift 4.12, 4.13 4.12, 4.13 4.12, 4.13 4.12, 4.13 RHEL 7.x, 8.x 7.x, 8.x 7.x, 8.x 7.x, 8.x CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9 Ubuntu 20.04 20.04 20.04 20.04 SLES 15SP4 15SP2 15SP2 15SP3 Supported Storage Platforms PowerMax PowerStore PowerScale PowerFlex Storage Array PowerMax 2500/8500 PowerMaxOS 10 (6079) , PowerMaxOS 10.0.1 (6079) PowerMax 2000/8000 - 5978.711.xxx, 5978.479.xxx Unisphere 10.0,10.0.1 1.0.x, 2.0.x, 2.1.x, 3.0, 3.2, 3.5 OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4, 9.5 3.6.x, 4.0 Note: File Replication for PowerMax is currently not supported\nDetails As on the storage arrays, all replication related Kubernetes entities are required/created in pairs -\nPair of Kubernetes Clusters Pair of replication enabled Storage classes Pair of PersistentVolumes representing the replicated pair on the storage array Pair of DellCSIReplicationGroup objects representing the replicated protection groups on the storage array You can also use a single stretched Kubernetes cluster for protecting your applications. Even in this topology, rest of the objects still exist in pairs.\nWhat it does not do Replicate application manifests within/across clusters. Stop applications before the planned/unplanned migration. Start applications after the migration. Replicate PersistentVolumeClaim objects within/across clusters. Replication with METRO mode does not need Replicator sidecar and common controller. Different namespaces cannot share the same RDF group for creating volumes with ASYNC mode for PowerMax. Same RDF group cannot be shared across different replication modes for PowerMax. QuickStart Install all required components: Enable replication during CSI driver installation Install CSM Replication Controller \u0026 repctl Create replication enabled storage classes Create PersistentVolumeClaim using the replication enabled storage class How it works At a high level, the following happens when you create a PersistentVolumeClaim object using a replication enabled storage class -\nCSI driver creates protection group on the storage array (if required) CSI driver creates the volume and adds it to the protection group. There will be a corresponding group and pair on the remote storage array A DellCSIReplicationGroup object is created in the cluster representing the protection group on the storage array A replica of the PersistentVolume \u0026 DellCSIReplicationGroup is created You can refer this page for more details about the architecture.\nOnce the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), you can exercise the general Disaster Recovery workflows -\nPlanned Migration to the target cluster/array Unplanned Migration to the target cluster/array Reprotect volumes at the target cluster/array Maintenance activities like - Suspend, Resume, Establish replication ","categories":"","description":"Dell Container Storage Modules (CSM) for Replication\n","excerpt":"Dell Container Storage Modules (CSM) for Replication\n","ref":"/csm-docs/v1/replication/","tags":"","title":"Replication"},{"body":"Container Storage Modules (CSM) for Replication is part of the open-source suite of Kubernetes storage enablers for Dell products.\nCSM for Replication project aims to bring Replication \u0026 Disaster Recovery capabilities of Dell Storage Arrays to Kubernetes clusters. It helps you replicate groups of volumes using the native replication technology available on the storage array and can provide you a way to restart applications in case of both planned and unplanned migration.\nCSM for Replication Capabilities CSM for Replication provides the following capabilities:\nCapability PowerMax PowerStore PowerScale PowerFlex Unity Replicate data using native storage array based replication yes yes yes yes no Asynchronous file volume replication no no yes no no Asynchronous block volume replication yes yes n/a yes no Synchronous file volume replication no no no no no Synchronous block volume replication yes no n/a no no Active-Active (Metro) block volume replication yes no n/a no no Active-Active (Metro) file volume replication no no no no no Create PersistentVolume objects in the cluster representing the replicated volume yes yes yes yes no Create DellCSIReplicationGroup objects in the cluster yes yes yes yes no Failover \u0026 Reprotect applications using the replicated volumes yes yes yes yes no Online Volume Expansion for replicated volumes yes no no yes no Provides a command line utility - repctl for configuring \u0026 managing replication related resources across multiple clusters yes yes yes yes no Supported Operating Systems/Container Orchestrator Platforms COP/OS PowerMax PowerStore PowerScale PowerFlex Kubernetes 1.25, 1.26, 1.27 1.25, 1.26, 1.27 1.25, 1.26, 1.27 1.25, 1.26, 1.27 Red Hat OpenShift 4.11, 4.12 4.11, 4.12 4.11, 4.12 4.11, 4.12 RHEL 7.x, 8.x 7.x, 8.x 7.x, 8.x 7.x, 8.x CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9 Ubuntu 20.04 20.04 20.04 20.04 SLES 15SP4 15SP2 15SP2 15SP3 Supported Storage Platforms PowerMax PowerStore PowerScale PowerFlex Storage Array PowerMax 2500/8500 PowerMaxOS 10 (6079) , PowerMaxOS 10.0.1 (6079) PowerMax 2000/8000 - 5978.711.xxx, 5978.479.xxx Unisphere 10.0,10.0.1 1.0.x, 2.0.x, 2.1.x, 3.0, 3.2, 3.5 OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4 3.6.x, 4.0 Details As on the storage arrays, all replication related Kubernetes entities are required/created in pairs -\nPair of Kubernetes Clusters Pair of replication enabled Storage classes Pair of PersistentVolumes representing the replicated pair on the storage array Pair of DellCSIReplicationGroup objects representing the replicated protection groups on the storage array You can also use a single stretched Kubernetes cluster for protecting your applications. Even in this topology, rest of the objects still exist in pairs.\nWhat it does not do Replicate application manifests within/across clusters. Stop applications before the planned/unplanned migration. Start applications after the migration. Replicate PersistentVolumeClaim objects within/across clusters. Replication with METRO mode does not need Replicator sidecar and common controller. Different namespaces cannot share the same RDF group for creating volumes with ASYNC mode for PowerMax. Same RDF group cannot be shared across different replication modes for PowerMax. QuickStart Install all required components: Enable replication during CSI driver installation Install CSM Replication Controller \u0026 repctl Create replication enabled storage classes Create PersistentVolumeClaim using the replication enabled storage class How it works At a high level, the following happens when you create a PersistentVolumeClaim object using a replication enabled storage class -\nCSI driver creates protection group on the storage array (if required) CSI driver creates the volume and adds it to the protection group. There will be a corresponding group and pair on the remote storage array A DellCSIReplicationGroup object is created in the cluster representing the protection group on the storage array A replica of the PersistentVolume \u0026 DellCSIReplicationGroup is created You can refer this page for more details about the architecture.\nOnce the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), you can exercise the general Disaster Recovery workflows -\nPlanned Migration to the target cluster/array Unplanned Migration to the target cluster/array Reprotect volumes at the target cluster/array Maintenance activities like - Suspend, Resume, Establish replication ","categories":"","description":"Dell Container Storage Modules (CSM) for Replication\n","excerpt":"Dell Container Storage Modules (CSM) for Replication\n","ref":"/csm-docs/v2/replication/","tags":"","title":"Replication"},{"body":"Container Storage Modules (CSM) for Replication is part of the open-source suite of Kubernetes storage enablers for Dell products.\nCSM for Replication project aims to bring Replication \u0026 Disaster Recovery capabilities of Dell Storage Arrays to Kubernetes clusters. It helps you replicate groups of volumes using the native replication technology available on the storage array and can provide you a way to restart applications in case of both planned and unplanned migration.\nCSM for Replication Capabilities CSM for Replication provides the following capabilities:\nCapability PowerMax PowerStore PowerScale PowerFlex Unity Replicate data using native storage array based replication yes yes yes yes no Asynchronous file volume replication no no yes no no Asynchronous block volume replication yes yes n/a yes no Synchronous file volume replication no no no no no Synchronous block volume replication yes no n/a no no Active-Active (Metro) block volume replication yes no n/a no no Active-Active (Metro) file volume replication no no no no no Create PersistentVolume objects in the cluster representing the replicated volume yes yes yes yes no Create DellCSIReplicationGroup objects in the cluster yes yes yes yes no Failover \u0026 Reprotect applications using the replicated volumes yes yes yes yes no Online Volume Expansion for replicated volumes yes no no yes no Provides a command line utility - repctl for configuring \u0026 managing replication related resources across multiple clusters yes yes yes yes no Supported Operating Systems/Container Orchestrator Platforms COP/OS PowerMax PowerStore PowerScale PowerFlex Kubernetes 1.24, 1.25, 1.26 1.24, 1.25, 1.26 1.24, 1.25, 1.26 1.24, 1.25, 1.26 Red Hat OpenShift 4.10, 4.11 4.10, 4.11 4.10, 4.11 4.10, 4.11 RHEL 7.x, 8.x 7.x, 8.x 7.x, 8.x 7.x, 8.x CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9 Ubuntu 20.04 20.04 20.04 20.04 SLES 15SP4 15SP2 15SP2 15SP3 Supported Storage Platforms PowerMax PowerStore PowerScale PowerFlex Storage Array PowerMax 2000/8000 PowerMax 2500/8500 5978.479.479, 5978.711.711, 6079.xxx.xxx, Unisphere 10.0 1.0.x, 2.0.x, 2.1.x, 3.0, 3.2, 3.5 OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4 3.6.x, 4.0 Supported CSI Drivers CSM for Replication supports the following CSI drivers and versions. Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerMax csi-powermax v2.0 + CSI Driver for Dell PowerStore csi-powerstore v2.0 + CSI Driver for Dell PowerScale csi-powerscale v2.2 + CSI Driver for Dell PowerFlex csi-powerflex v2.6 + For compatibility with storage arrays please refer to corresponding CSI drivers\nDetails As on the storage arrays, all replication related Kubernetes entities are required/created in pairs -\nPair of Kubernetes Clusters Pair of replication enabled Storage classes Pair of PersistentVolumes representing the replicated pair on the storage array Pair of DellCSIReplicationGroup objects representing the replicated protection groups on the storage array You can also use a single stretched Kubernetes cluster for protecting your applications. Even in this topology, rest of the objects still exist in pairs.\nWhat it does not do Replicate application manifests within/across clusters. Stop applications before the planned/unplanned migration. Start applications after the migration. Replicate PersistentVolumeClaim objects within/across clusters. Replication with METRO mode does not need Replicator sidecar and common controller. Different namespaces cannot share the same RDF group for creating volumes with ASYNC mode for PowerMax. Same RDF group cannot be shared across different replication modes for PowerMax. QuickStart Install all required components: Enable replication during CSI driver installation Install CSM Replication Controller \u0026 repctl Create replication enabled storage classes Create PersistentVolumeClaim using the replication enabled storage class How it works At a high level, the following happens when you create a PersistentVolumeClaim object using a replication enabled storage class -\nCSI driver creates protection group on the storage array (if required) CSI driver creates the volume and adds it to the protection group. There will be a corresponding group and pair on the remote storage array A DellCSIReplicationGroup object is created in the cluster representing the protection group on the storage array A replica of the PersistentVolume \u0026 DellCSIReplicationGroup is created You can refer this page for more details about the architecture.\nOnce the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), you can exercise the general Disaster Recovery workflows -\nPlanned Migration to the target cluster/array Unplanned Migration to the target cluster/array Reprotect volumes at the target cluster/array Maintenance activities like - Suspend, Resume, Establish replication ","categories":"","description":"Dell Container Storage Modules (CSM) for Replication\n","excerpt":"Dell Container Storage Modules (CSM) for Replication\n","ref":"/csm-docs/v3/replication/","tags":"","title":"Replication"},{"body":"You can exercise native replication control operations from Dell storage arrays by performing “Actions” on the replicated group of volumes using the DellCSIReplicationGroup (RG) object.\nYou can patch the DellCSIReplicationGroup Custom Resource (CR) and set the action field in the spec to one of the allowed values (refer to tables in this document).\nWhen you set the action field in the Custom Resource object, the following happens:\nState of the RG CR is set to action_in_progress. For e.g. if you set the action field to SYNC, then the state will change to SYNC_IN_PROGRESS, action field will reset to empty dell-csi-replicator sidecar issues the command to the CSI driver to perform the appropriate action Once the CSI driver has completed the operation, State of the RG CR goes back to Ready While the action is in progress, you shouldn’t update the action field. Any attempt to change the action field will be rejected and it will be reset to empty. There are certain pre-requisites that have to be fulfilled before any action can be done on the RG CR. For example, you can’t perform a Reprotect without doing a Failover first. There are some “Workflows” defined in Disaster Recovery which provide a sequence of operations for some common use-cases. An important exception to these rules is the action UNPLANNED_FAILOVER, which can be run at any time.\nNOTE: Throughout this document, we are going to refer to “Site A” as the original source site \u0026 “Site B” as the original target site.\nSite Specific Actions These actions can be run at any site, but they have some site-specific context included.\nAny action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\nFor example:\nIf the CR at Site A is patched with action FAILOVER_REMOTE, it means that the driver will attempt to Fail Over to Site B which is the remote site. If the CR at Site B is patched with action FAILOVER_LOCAL, it means that the driver will attempt to Fail Over to Site B which is the local site. If the CR at Site B is patched with REPROTECT_LOCAL, it means that the driver will Re-protect the volumes at Site B which is the local site. The following table lists details of what actions should be used in different Disaster Recovery workflows \u0026 the equivalent operation done on the storage array:\nWorkflow Actions PowerMax PowerStore PowerScale PowerFlex Planned Migration FAILOVER_LOCAL\nFAILOVER_REMOTE symrdf failover -swap FAILOVER (no REPROTECT after FAILOVER) allow_writes on target, disable local policy FAILOVER (no REPROTECT after FAILOVER) Reprotect REPROTECT_LOCAL\nREPROTECT_REMOTE symrdf resume/est REPROTECT Delete policy on source, create policy on target REPROTECT Unplanned Migration UNPLANNED_FAILOVER_LOCAL\nUNPLANNED_FAILOVER_REMOTE symrdf failover -force FAILOVER (at target site) allow_writes on target FAILOVER (at target site) Maintenance Actions These actions can be run at any site and are used to change the replication link state for maintenance activities. The following table lists the supported maintenance actions and the equivalent operation done on the storage arrays:\nAction Description PowerMax PowerStore PowerScale PowerFlex SUSPEND Temporarily suspend replication symrdf suspend PAUSE disable local policy PAUSE RESUME Resume replication symrdf resume RESUME enable local policy RESUME SYNC Synchronize all changes from source to target symrdf establish SYNCHRONIZE NOW start syncIQ job SYNC NOW How to perform actions We strongly recommend using repctl to perform any actions on DellCSIReplicationGroup objects. You can find detailed steps here.\nIf you wish to use kubectl to perform actions, then use kubectl edit/patch operations and set the action field in the Custom Resource. While performing site-specific actions, please consult each driver’s documentation to get an exhaustive list of all the supported actions.\nFor a brief guide on using actions for various DR workflows, please refer to this document.\nDeleting Replication Groups To delete a replication group from a cluster, the group must be empty. This means that all associated replication pairs must have been deleted/removed.\nThis command will delete the empty replication group from the cluster and the associated replication group from the backend array.\nkubectl delete rg \u003crg-id\u003e ","categories":"","description":"DellCSIReplicationGroup Actions\n","excerpt":"DellCSIReplicationGroup Actions\n","ref":"/csm-docs/docs/replication/replication-actions/","tags":"","title":"Replication Actions"},{"body":"You can exercise native replication control operations from Dell storage arrays by performing “Actions” on the replicated group of volumes using the DellCSIReplicationGroup (RG) object.\nYou can patch the DellCSIReplicationGroup Custom Resource (CR) and set the action field in the spec to one of the allowed values (refer to tables in this document).\nWhen you set the action field in the Custom Resource object, the following happens:\nState of the RG CR is set to action_in_progress. For e.g. if you set the action field to SYNC, then the state will change to SYNC_IN_PROGRESS, action field will reset to empty dell-csi-replicator sidecar issues the command to the CSI driver to perform the appropriate action Once the CSI driver has completed the operation, State of the RG CR goes back to Ready While the action is in progress, you shouldn’t update the action field. Any attempt to change the action field will be rejected and it will be reset to empty. There are certain pre-requisites that have to be fulfilled before any action can be done on the RG CR. For example, you can’t perform a Reprotect without doing a Failover first. There are some “Workflows” defined in Disaster Recovery which provide a sequence of operations for some common use-cases. An important exception to these rules is the action UNPLANNED_FAILOVER, which can be run at any time.\nNOTE: Throughout this document, we are going to refer to “Site A” as the original source site \u0026 “Site B” as the original target site.\nSite Specific Actions These actions can be run at any site, but they have some site-specific context included.\nAny action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\nFor example:\nIf the CR at Site A is patched with action FAILOVER_REMOTE, it means that the driver will attempt to Fail Over to Site B which is the remote site. If the CR at Site B is patched with action FAILOVER_LOCAL, it means that the driver will attempt to Fail Over to Site B which is the local site. If the CR at Site B is patched with REPROTECT_LOCAL, it means that the driver will Re-protect the volumes at Site B which is the local site. The following table lists details of what actions should be used in different Disaster Recovery workflows \u0026 the equivalent operation done on the storage array:\nWorkflow Actions PowerMax PowerStore PowerScale PowerFlex Planned Migration FAILOVER_LOCAL\nFAILOVER_REMOTE symrdf failover -swap FAILOVER (no REPROTECT after FAILOVER) allow_writes on target, disable local policy FAILOVER (no REPROTECT after FAILOVER) Reprotect REPROTECT_LOCAL\nREPROTECT_REMOTE symrdf resume/est REPROTECT Delete policy on source, create policy on target REPROTECT Unplanned Migration UNPLANNED_FAILOVER_LOCAL\nUNPLANNED_FAILOVER_REMOTE symrdf failover -force FAILOVER (at target site) allow_writes on target FAILOVER (at target site) Maintenance Actions These actions can be run at any site and are used to change the replication link state for maintenance activities. The following table lists the supported maintenance actions and the equivalent operation done on the storage arrays:\nAction Description PowerMax PowerStore PowerScale PowerFlex SUSPEND Temporarily suspend replication symrdf suspend PAUSE disable local policy PAUSE RESUME Resume replication symrdf resume RESUME enable local policy RESUME SYNC Synchronize all changes from source to target symrdf establish SYNCHRONIZE NOW start syncIQ job SYNC NOW How to perform actions We strongly recommend using repctl to perform any actions on DellCSIReplicationGroup objects. You can find detailed steps here.\nIf you wish to use kubectl to perform actions, then use kubectl edit/patch operations and set the action field in the Custom Resource. While performing site-specific actions, please consult each driver’s documentation to get an exhaustive list of all the supported actions.\nFor a brief guide on using actions for various DR workflows, please refer to this document.\nDeleting Replication Groups To delete a replication group from a cluster, the group must be empty. This means that all associated replication pairs must have been deleted/removed.\nThis command will delete the empty replication group from the cluster and the associated replication group from the backend array.\nkubectl delete rg \u003crg-id\u003e ","categories":"","description":"DellCSIReplicationGroup Actions\n","excerpt":"DellCSIReplicationGroup Actions\n","ref":"/csm-docs/v1/replication/replication-actions/","tags":"","title":"Replication Actions"},{"body":"You can exercise native replication control operations from Dell storage arrays by performing “Actions” on the replicated group of volumes using the DellCSIReplicationGroup (RG) object.\nYou can patch the DellCSIReplicationGroup Custom Resource (CR) and set the action field in the spec to one of the allowed values (refer to tables in this document).\nWhen you set the action field in the Custom Resource object, the following happens:\nState of the RG CR is set to action_in_progress. For e.g. if you set the action field to SYNC, then the state will change to SYNC_IN_PROGRESS, action field will reset to empty dell-csi-replicator sidecar issues the command to the CSI driver to perform the appropriate action Once the CSI driver has completed the operation, State of the RG CR goes back to Ready While the action is in progress, you shouldn’t update the action field. Any attempt to change the action field will be rejected and it will be reset to empty. There are certain pre-requisites that have to be fulfilled before any action can be done on the RG CR. For example, you can’t perform a Reprotect without doing a Failover first. There are some “Workflows” defined in Disaster Recovery which provide a sequence of operations for some common use-cases. An important exception to these rules is the action UNPLANNED_FAILOVER, which can be run at any time.\nNOTE: Throughout this document, we are going to refer to “Site A” as the original source site \u0026 “Site B” as the original target site.\nSite Specific Actions These actions can be run at any site, but they have some site-specific context included.\nAny action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\nFor example:\nIf the CR at Site A is patched with action FAILOVER_REMOTE, it means that the driver will attempt to Fail Over to Site B which is the remote site. If the CR at Site B is patched with action FAILOVER_LOCAL, it means that the driver will attempt to Fail Over to Site B which is the local site. If the CR at Site B is patched with REPROTECT_LOCAL, it means that the driver will Re-protect the volumes at Site B which is the local site. The following table lists details of what actions should be used in different Disaster Recovery workflows \u0026 the equivalent operation done on the storage array:\nWorkflow Actions PowerMax PowerStore PowerScale PowerFlex Planned Migration FAILOVER_LOCAL\nFAILOVER_REMOTE symrdf failover -swap FAILOVER (no REPROTECT after FAILOVER) allow_writes on target, disable local policy FAILOVER (no REPROTECT after FAILOVER) Reprotect REPROTECT_LOCAL\nREPROTECT_REMOTE symrdf resume/est REPROTECT Delete policy on source, create policy on target REPROTECT Unplanned Migration UNPLANNED_FAILOVER_LOCAL\nUNPLANNED_FAILOVER_REMOTE symrdf failover -force FAILOVER (at target site) allow_writes on target FAILOVER (at target site) Maintenance Actions These actions can be run at any site and are used to change the replication link state for maintenance activities. The following table lists the supported maintenance actions and the equivalent operation done on the storage arrays:\nAction Description PowerMax PowerStore PowerScale PowerFlex SUSPEND Temporarily suspend replication symrdf suspend PAUSE disable local policy PAUSE RESUME Resume replication symrdf resume RESUME enable local policy RESUME SYNC Synchronize all changes from source to target symrdf establish SYNCHRONIZE NOW start syncIQ job SYNC NOW How to perform actions We strongly recommend using repctl to perform any actions on DellCSIReplicationGroup objects. You can find detailed steps here.\nIf you wish to use kubectl to perform actions, then use kubectl edit/patch operations and set the action field in the Custom Resource. While performing site-specific actions, please consult each driver’s documentation to get an exhaustive list of all the supported actions.\nFor a brief guide on using actions for various DR workflows, please refer to this document.\nDeleting Replication Groups To delete a replication group from a cluster, the group must be empty. This means that all associated replication pairs must have been deleted/removed.\nThis command will delete the empty replication group from the cluster and the associated replication group from the backend array.\nkubectl delete rg \u003crg-id\u003e ","categories":"","description":"DellCSIReplicationGroup Actions\n","excerpt":"DellCSIReplicationGroup Actions\n","ref":"/csm-docs/v2/replication/replication-actions/","tags":"","title":"Replication Actions"},{"body":"You can exercise native replication control operations from Dell storage arrays by performing “Actions” on the replicated group of volumes using the DellCSIReplicationGroup (RG) object.\nYou can patch the DellCSIReplicationGroup Custom Resource (CR) and set the action field in the spec to one of the allowed values (refer to tables in this document).\nWhen you set the action field in the Custom Resource object, the following happens:\nState of the RG CR is set to action_in_progress. For e.g. if you set the action field to SYNC, then the state will change to SYNC_IN_PROGRESS, action field will reset to empty dell-csi-replicator sidecar issues the command to the CSI driver to perform the appropriate action Once the CSI driver has completed the operation, State of the RG CR goes back to Ready While the action is in progress, you shouldn’t update the action field. Any attempt to change the action field will be rejected and it will be reset to empty. There are certain pre-requisites that have to be fulfilled before any action can be done on the RG CR. For example, you can’t perform a Reprotect without doing a Failover first. There are some “Workflows” defined in Disaster Recovery which provide a sequence of operations for some common use-cases. An important exception to these rules is the action UNPLANNED_FAILOVER, which can be run at any time.\nNOTE: Throughout this document, we are going to refer to “Site A” as the original source site \u0026 “Site B” as the original target site.\nSite Specific Actions These actions can be run at any site, but they have some site-specific context included.\nAny action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\nFor example:\nIf the CR at Site A is patched with action FAILOVER_REMOTE, it means that the driver will attempt to Fail Over to Site B which is the remote site. If the CR at Site B is patched with action FAILOVER_LOCAL, it means that the driver will attempt to Fail Over to Site B which is the local site. If the CR at Site B is patched with REPROTECT_LOCAL, it means that the driver will Re-protect the volumes at Site B which is the local site. The following table lists details of what actions should be used in different Disaster Recovery workflows \u0026 the equivalent operation done on the storage array:\nWorkflow Actions PowerMax PowerStore PowerScale PowerFlex Planned Migration FAILOVER_LOCAL\nFAILOVER_REMOTE symrdf failover -swap FAILOVER (no REPROTECT after FAILOVER) allow_writes on target, disable local policy FAILOVER (no REPROTECT after FAILOVER) Reprotect REPROTECT_LOCAL\nREPROTECT_REMOTE symrdf resume/est REPROTECT Delete policy on source, create policy on target REPROTECT Unplanned Migration UNPLANNED_FAILOVER_LOCAL\nUNPLANNED_FAILOVER_REMOTE symrdf failover -force FAILOVER (at target site) allow_writes on target FAILOVER (at target site) Maintenance Actions These actions can be run at any site and are used to change the replication link state for maintenance activities. The following table lists the supported maintenance actions and the equivalent operation done on the storage arrays:\nAction Description PowerMax PowerStore PowerScale PowerFlex SUSPEND Temporarily suspend replication symrdf suspend PAUSE disable local policy PAUSE RESUME Resume replication symrdf resume RESUME enable local policy RESUME SYNC Synchronize all changes from source to target symrdf establish SYNCHRONIZE NOW start syncIQ job SYNC NOW How to perform actions We strongly recommend using repctl to perform any actions on DellCSIReplicationGroup objects. You can find detailed steps here.\nIf you wish to use kubectl to perform actions, then use kubectl edit/patch operations and set the action field in the Custom Resource. While performing site-specific actions, please consult each driver’s documentation to get an exhaustive list of all the supported actions.\nFor a brief guide on using actions for various DR workflows, please refer to this document.\n","categories":"","description":"DellCSIReplicationGroup Actions\n","excerpt":"DellCSIReplicationGroup Actions\n","ref":"/csm-docs/v3/replication/replication-actions/","tags":"","title":"Replication Actions"},{"body":"Container Storage Modules (CSM) for Resiliency is part of the open-source suite of Kubernetes storage enablers for Dell products.\nUser applications can have problems if you want their Pods to be resilient to node failure. This is especially true of those deployed with StatefulSets that use PersistentVolumeClaims. Kubernetes guarantees that there will never be two copies of the same StatefulSet Pod running at the same time and accessing storage. Therefore, it does not clean up StatefulSet Pods if the node executing them fails.\nFor the complete discussion and rationale, you can read the pod-safety design proposal.\nFor more background on the forced deletion of Pods in a StatefulSet, please visit Force Delete StatefulSet Pods.\nCSM for Resiliency and Non graceful node shutdown are mutually exclusive. One shall use either CSM for Resiliency or Non graceful node shutdown feature provided by Kubernetes.\nCSM for Resiliency High-Level Description CSM for Resiliency is designed to make Kubernetes Applications, including those that utilize persistent storage, more resilient to various failures. The first component of the Resiliency module is a pod monitor that is specifically designed to protect stateful applications from various failures. It is not a standalone application, but rather is deployed as a sidecar to CSI (Container Storage Interface) drivers, in both the driver’s controller pods and the driver’s node pods. Deploying CSM for Resiliency as a sidecar allows it to make direct requests to the driver through the Unix domain socket that Kubernetes sidecars use to make CSI requests.\nSome of the methods CSM for Resiliency invokes in the driver are standard CSI methods, such as NodeUnpublishVolume, NodeUnstageVolume, and ControllerUnpublishVolume. CSM for Resiliency also uses proprietary calls that are not part of the standard CSI specification. Currently, there is only one, ValidateVolumeHostConnectivity that returns information on whether a host is connected to the storage system and/or whether any I/O activity has happened in the recent past from a list of specified volumes. This allows CSM for Resiliency to make more accurate determinations about the state of the system and its persistent volumes. CSM for Resiliency is designed to adhere to pod affinity settings of pods.\nAccordingly, CSM for Resiliency is adapted to and qualified with each CSI driver it is to be used with. Different storage systems have different nuances and characteristics that CSM for Resiliency must take into account.\nCSM for Resiliency Capabilities CSM for Resiliency provides the following capabilities:\nCapability PowerScale Unity XT PowerStore PowerFlex PowerMax Detect pod failures when: Node failure, K8S Control Plane Network failure, K8S Control Plane failure, Array I/O Network failure yes yes yes yes no Cleanup pod artifacts from failed nodes yes yes yes yes no Revoke PV access from failed nodes yes yes yes yes no Supported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.26, 1.27, 1.28 Red Hat OpenShift 4.13, 4.14 Supported Storage Platforms PowerFlex Unity XT PowerScale PowerStore Storage Array 3.6.x, 4.0.x, 4.5 5.1.x, 5.2.x, 5.3.0 OneFS 9.3, 9.4, 9.5.0.x (x \u003e= 5) 3.0, 3.2, 3.5 Supported CSI Drivers CSM for Resiliency supports the following CSI drivers and versions. Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerFlex csi-powerflex v2.0.0 + CSI Driver for Dell Unity XT csi-unity v2.0.0 + CSI Driver for Dell PowerScale csi-powerscale v2.3.0 + CSI Driver for Dell PowerStore csi-powerstore v2.6.0 + PowerFlex Support PowerFlex is a highly scalable array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerFlex leverages these PowerFlex features:\nVery quick detection of Array I/O Network Connectivity status changes (generally takes 1-2 seconds for the array to detect changes) A robust mechanism if Nodes are doing I/O to volumes (sampled over a 5-second period). Low latency REST API supports fast CSI provisioning and de-provisioning operations. A proprietary network protocol provided by the SDC component that can run over the same IP interface as the K8S control plane or over a separate IP interface for Array I/O. Unity XT Support Dell Unity XT is targeted for midsized deployments, remote or branch offices, and cost-sensitive mixed workloads. Unity XT systems are designed to deliver the best value in the market. They support all-Flash, and are available in purpose-built (all Flash or hybrid Flash), converged deployment options (through VxBlock), and software-defined virtual edition.\nUnity XT (purpose-built): A modern midrange storage solution, engineered from the groundup to meet market demands for Flash, affordability and incredible simplicity. The Unity XT Family is available in 12 All Flash models and 12 Hybrid models. VxBlock (converged): Unity XT storage options are also available in Dell VxBlock System 1000. UnityVSA (virtual): The Unity XT Virtual Storage Appliance (VSA) allows the advanced unified storage and data management features of the Unity XT family to be easily deployed on VMware ESXi servers. This allows for a ‘software defined’ approach. UnityVSA is available in two editions: Community Edition is a free downloadable 4 TB solution recommended for nonproduction use. Professional Edition is a licensed subscription-based offering available at capacity levels of 10 TB, 25 TB, and 50 TB. The subscription includes access to online support resources, EMC Secure Remote Services (ESRS), and on-call software- and systems-related support. All three deployment options, Unity XT, UnityVSA, and Unity-based VxBlock, enjoy one architecture, one interface with consistent features and rich data services.\nPowerScale Support PowerScale is a highly scalable NFS array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerScale leverages the following PowerScale features:\nDetection of Array I/O Network Connectivity status changes. A robust mechanism to detect if Nodes are actively doing I/O to volumes. Low latency REST API supports fast CSI provisioning and de-provisioning operations. PowerStore Support PowerStore is a highly scalable array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerStore leverages the following PowerStore features:\nDetection of Array I/O Network Connectivity status changes. A robust mechanism to detect if Nodes are actively doing I/O to volumes. Low latency REST API supports fast CSI provisioning and de-provisioning operations. Limitations and Exclusions This file contains information on Limitations and Exclusions that users should be aware of. Additionally, there are driver specific limitations and exclusions that may be called out in the Deploying CSM for Resiliency page.\nSupported and Tested Operating Modes The following provisioning types are supported and have been tested:\nDynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “FileSystem”. Dynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “Block”. Use of the above volumes with Pods created by StatefulSets. Up to 12 or so protected pods on a given node. Failing up to 3 nodes at a time in 9 worker node clusters, or failing 1 node at a time in smaller clusters. Application recovery times are dependent on the number of pods that need to be moved as a result of the failure. See the section on “Testing and Performance” for some of the details. Multi-array are supported. In case of CSI Driver for PowerScale and CSI Driver for Unity, if any one of the array is not connected, the array connectivity will be false. CSI Driver for Powerflex connectivity will be determined by connection to default array. Not Tested But Assumed to Work Deployments with the above volume types, provided two pods from the same deployment do not reside on the same node. At the current time anti-affinity rules should be used to guarantee no two pods accessing the same volumes are scheduled to the same node. Not Yet Tested or Supported Pods that use persistent volumes from multiple CSI drivers. This cannot be supported because multiple controller-podmons (one for each driver type) would be trying to manage the failover with conflicting actions.\nReadWriteMany volumes. This may have issues if a node has multiple pods accessing the same volumes. In any case once pod cleanup fences the volumes on a node, they will no longer be available to any pods using those volumes on that node. We will endeavor to support this in the future.\nMultiple instances of the same driver type (for example two CSI driver for Dell PowerFlex deployments.)\nDeploying and Managing Applications Protected by CSM for Resiliency The first thing to remember about CSM for Resiliency is that it only takes action on pods configured with the designated label. Both the key and the value have to match what is in the podmon helm configuration. CSM for Resiliency emits a log message at startup with the label key and value it is using to monitor pods:\nlabelSelector: {map[podmon.dellemc.com/driver:csi-vxflexos] The above message indicates the key is: podmon.dellemc.com/driver and the label value is csi-vxflexos. To search for the pods that would be monitored, try this:\nkubectl get pods -A -l podmon.dellemc.com/driver=csi-vxflexos NAMESPACE NAME READY STATUS RESTARTS AGE pmtu1 podmontest-0 1/1 Running 0 3m7s pmtu2 podmontest-0 1/1 Running 0 3m8s pmtu3 podmontest-0 1/1 Running 0 3m6s If CSM for Resiliency detects a problem with a pod caused by a node or other failure that it can initiate remediation for, it will add an event to that pod’s events:\nkubectl get events -n pmtu1 ... 61s Warning NodeFailure pod/podmontest-0 podmon cleaning pod [7520ba2a-cec5-4dff-8537-20c9bdafbe26 node.example.com] with force delete ... CSM for Resiliency may also generate events if it is unable to clean up a pod for some reason. For example, it may not clean up a pod because the pod is still doing I/O to the array.\nSimilarly, the label selector for csi-powerscale and csi-unity would be as shown respectively.\nlabelSelector: {map[podmon.dellemc.com/driver:csi-isilon] labelSelector: {map[podmon.dellemc.com/driver:csi-unity] Important Before putting an application into production that relies on CSM for Resiliency monitoring, it is important to do a few test failovers first. To do this take the node that is running the pod offline for at least 2-3 minutes. Verify that there is an event message similar to the one above is logged, and that the pod recovers and restarts normally with no loss of data. (Note that if the node is running many CSM for Resiliency protected pods, the node may need to be down longer for CSM for Resiliency to have time to evacuate all the protected pods.)\nApplication Recommendations It is recommended that pods that will be monitored by CSM for Resiliency be configured to exit if they receive any I/O errors. That should help achieve the recovery as quickly as possible.\nCSM for Resiliency does not directly monitor application health. However, if standard Kubernetes health checks are configured, that may help reduce pod recovery time in the event of node failure, as CSM for Resiliency should receive an event that the application is Not Ready. Note that a Not Ready pod is not sufficient to trigger CSM for Resiliency action unless there is also some condition indicating a Node failure or problem, such as the Node is tainted, or the array has lost connectivity to the node.\nAs noted previously in the Limitations and Exclusions section, CSM for Resiliency has not yet been verified to work with ReadWriteMany or ReadOnlyMany volumes. Also, it has not been verified to work with pod controllers other than StatefulSet.\nRecovering From Failures Normally CSM for Resiliency should be able to move pods that have been impacted by Node Failures to a healthy node. After the failed nodes have come back online, CSM for Resiliency cleans them up (especially any potential zombie pods) and then automatically removes the CSM for Resiliency node taint that prevents pods from being scheduled to the failed node(s). There are a few cases where this cannot be fully automated and operator intervention is required, including:\nCSM for Resiliency expects that when a node failure occurs, all CSM for Resiliency labeled pods are evacuated and rescheduled on other nodes. This process may not complete however if the node comes back online before CSM for Resiliency has had time to evacuate all the labeled pods. The remaining pods may not restart correctly, going to “Error” or “CrashLoopBackoff”. We are considering some possible remediation for this condition but have not implemented them yet.\nIf this happens, try deleting the pod with “kubectl delete pod …”. In our experience this normally will cause the pod to be restarted and transition to the “Running” state.\nPodmon-node is responsible for cleaning up failed nodes after the nodes’ communication has been restored. The algorithm checks to see that all the monitored pods have terminated and their volumes and mounts have been cleaned up.\nIf some of the monitored pods are still executing, node-podmon will emit the following log message at the end of a cleanup cycle (and retry the cleanup after a delay):\npods skipped for cleanup because still present: \u003cpod-list\u003e If this happens, DO NOT manually remove the CSM for Resiliency node taint. Doing so could possibly cause data corruption if volumes were not cleaned up, and a pod using those volumes was subsequently scheduled to that node.\nThe correct course of action in this case is to reboot the failed node(s) that have not removed their taints in a reasonable time (5-10 minutes after the node is online again.) The operator can delay executing this reboot until it is convenient, but new pods will not be scheduled to it in the interim. This reboot will cancel any potential zombie pods. After the reboot, node-podmon should automatically remove the node taint after a short time.\nTesting Methodology and Results A three tier testing methodology is used for CSM for Resiliency:\nUnit testing with high coverage (\u003e90% statement) tests the program logic and is especially used to test the error paths by injecting faults. An integration test describes test scenarios in Gherkin that sets up specific testing scenarios executed against a Kubernetes test cluster. The tests use ranges for many of the parameters to add an element of “chaos testing”. Script based testing supports longevity testing in a Kubernetes cluster. For example, one test repeatedly fails three different lists of nodes in succession and is used to fail 1/3 of the cluster’s worker nodes on a cyclic basis and repeat indefinitely. This test collect statistics on length of time for pod evacuation, pod recovery, and node cleanup. ","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency\n","ref":"/csm-docs/docs/resiliency/","tags":"","title":"Resiliency"},{"body":"Container Storage Modules (CSM) for Resiliency is part of the open-source suite of Kubernetes storage enablers for Dell products.\nUser applications can have problems if you want their Pods to be resilient to node failure. This is especially true of those deployed with StatefulSets that use PersistentVolumeClaims. Kubernetes guarantees that there will never be two copies of the same StatefulSet Pod running at the same time and accessing storage. Therefore, it does not clean up StatefulSet Pods if the node executing them fails.\nFor the complete discussion and rationale, you can read the pod-safety design proposal.\nFor more background on the forced deletion of Pods in a StatefulSet, please visit Force Delete StatefulSet Pods.\nCSM for Resiliency and Non graceful node shutdown are mutually exclusive. One shall use either CSM for Resiliency or Non graceful node shutdown feature provided by Kubernetes.\nCSM for Resiliency High-Level Description CSM for Resiliency is designed to make Kubernetes Applications, including those that utilize persistent storage, more resilient to various failures. The first component of the Resiliency module is a pod monitor that is specifically designed to protect stateful applications from various failures. It is not a standalone application, but rather is deployed as a sidecar to CSI (Container Storage Interface) drivers, in both the driver’s controller pods and the driver’s node pods. Deploying CSM for Resiliency as a sidecar allows it to make direct requests to the driver through the Unix domain socket that Kubernetes sidecars use to make CSI requests.\nSome of the methods CSM for Resiliency invokes in the driver are standard CSI methods, such as NodeUnpublishVolume, NodeUnstageVolume, and ControllerUnpublishVolume. CSM for Resiliency also uses proprietary calls that are not part of the standard CSI specification. Currently, there is only one, ValidateVolumeHostConnectivity that returns information on whether a host is connected to the storage system and/or whether any I/O activity has happened in the recent past from a list of specified volumes. This allows CSM for Resiliency to make more accurate determinations about the state of the system and its persistent volumes. CSM for Resiliency is designed to adhere to pod affinity settings of pods.\nAccordingly, CSM for Resiliency is adapted to and qualified with each CSI driver it is to be used with. Different storage systems have different nuances and characteristics that CSM for Resiliency must take into account.\nCSM for Resiliency Capabilities CSM for Resiliency provides the following capabilities:\nCapability PowerScale Unity XT PowerStore PowerFlex PowerMax Detect pod failures when: Node failure, K8S Control Plane Network failure, K8S Control Plane failure, Array I/O Network failure yes yes yes yes no Cleanup pod artifacts from failed nodes yes yes yes yes no Revoke PV access from failed nodes yes yes yes yes no Supported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.25, 1.26, 1.27 Red Hat OpenShift 4.11, 4.12, 4.13 RHEL 7.x, 8.x CentOS 7.8, 7.9 Supported Storage Platforms PowerFlex Unity XT PowerScale PowerStore Storage Array 3.5.x, 3.6.x 5.0.5, 5.0.6, 5.0.7, 5.1.0, 5.1.2, 5.2, 5.3 OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4 1.0.x, 2.0.x, 2.1.x, 3.0, 3.2 Supported CSI Drivers CSM for Resiliency supports the following CSI drivers and versions. Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerFlex csi-powerflex v2.0.0 + CSI Driver for Dell Unity XT csi-unity v2.0.0 + CSI Driver for Dell PowerScale csi-powerscale v2.3.0 + CSI Driver for Dell PowerStore csi-powerstore v2.6.0 + PowerFlex Support PowerFlex is a highly scalable array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerFlex leverages these PowerFlex features:\nVery quick detection of Array I/O Network Connectivity status changes (generally takes 1-2 seconds for the array to detect changes) A robust mechanism if Nodes are doing I/O to volumes (sampled over a 5-second period). Low latency REST API supports fast CSI provisioning and de-provisioning operations. A proprietary network protocol provided by the SDC component that can run over the same IP interface as the K8S control plane or over a separate IP interface for Array I/O. Unity XT Support Dell Unity XT is targeted for midsized deployments, remote or branch offices, and cost-sensitive mixed workloads. Unity XT systems are designed to deliver the best value in the market. They support all-Flash, and are available in purpose-built (all Flash or hybrid Flash), converged deployment options (through VxBlock), and software-defined virtual edition.\nUnity XT (purpose-built): A modern midrange storage solution, engineered from the groundup to meet market demands for Flash, affordability and incredible simplicity. The Unity XT Family is available in 12 All Flash models and 12 Hybrid models. VxBlock (converged): Unity XT storage options are also available in Dell VxBlock System 1000. UnityVSA (virtual): The Unity XT Virtual Storage Appliance (VSA) allows the advanced unified storage and data management features of the Unity XT family to be easily deployed on VMware ESXi servers. This allows for a ‘software defined’ approach. UnityVSA is available in two editions: Community Edition is a free downloadable 4 TB solution recommended for nonproduction use. Professional Edition is a licensed subscription-based offering available at capacity levels of 10 TB, 25 TB, and 50 TB. The subscription includes access to online support resources, EMC Secure Remote Services (ESRS), and on-call software- and systems-related support. All three deployment options, Unity XT, UnityVSA, and Unity-based VxBlock, enjoy one architecture, one interface with consistent features and rich data services.\nPowerScale Support PowerScale is a highly scalable NFS array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerScale leverages the following PowerScale features:\nDetection of Array I/O Network Connectivity status changes. A robust mechanism to detect if Nodes are actively doing I/O to volumes. Low latency REST API supports fast CSI provisioning and de-provisioning operations. PowerStore Support PowerStore is a highly scalable array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerStore leverages the following PowerStore features:\nDetection of Array I/O Network Connectivity status changes. A robust mechanism to detect if Nodes are actively doing I/O to volumes. Low latency REST API supports fast CSI provisioning and de-provisioning operations. Limitations and Exclusions This file contains information on Limitations and Exclusions that users should be aware of. Additionally, there are driver specific limitations and exclusions that may be called out in the Deploying CSM for Resiliency page.\nSupported and Tested Operating Modes The following provisioning types are supported and have been tested:\nDynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “FileSystem”. Dynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “Block”. Use of the above volumes with Pods created by StatefulSets. Up to 12 or so protected pods on a given node. Failing up to 3 nodes at a time in 9 worker node clusters, or failing 1 node at a time in smaller clusters. Application recovery times are dependent on the number of pods that need to be moved as a result of the failure. See the section on “Testing and Performance” for some of the details. Multi-array are supported. In case of CSI Driver for PowerScale and CSI Driver for Unity, if any one of the array is not connected, the array connectivity will be false. CSI Driver for Powerflex connectivity will be determined by connection to default array. Not Tested But Assumed to Work Deployments with the above volume types, provided two pods from the same deployment do not reside on the same node. At the current time anti-affinity rules should be used to guarantee no two pods accessing the same volumes are scheduled to the same node. Not Yet Tested or Supported Pods that use persistent volumes from multiple CSI drivers. This cannot be supported because multiple controller-podmons (one for each driver type) would be trying to manage the failover with conflicting actions.\nReadWriteMany volumes. This may have issues if a node has multiple pods accessing the same volumes. In any case once pod cleanup fences the volumes on a node, they will no longer be available to any pods using those volumes on that node. We will endeavor to support this in the future.\nMultiple instances of the same driver type (for example two CSI driver for Dell PowerFlex deployments.)\nDeploying and Managing Applications Protected by CSM for Resiliency The first thing to remember about CSM for Resiliency is that it only takes action on pods configured with the designated label. Both the key and the value have to match what is in the podmon helm configuration. CSM for Resiliency emits a log message at startup with the label key and value it is using to monitor pods:\nlabelSelector: {map[podmon.dellemc.com/driver:csi-vxflexos] The above message indicates the key is: podmon.dellemc.com/driver and the label value is csi-vxflexos. To search for the pods that would be monitored, try this:\nkubectl get pods -A -l podmon.dellemc.com/driver=csi-vxflexos NAMESPACE NAME READY STATUS RESTARTS AGE pmtu1 podmontest-0 1/1 Running 0 3m7s pmtu2 podmontest-0 1/1 Running 0 3m8s pmtu3 podmontest-0 1/1 Running 0 3m6s If CSM for Resiliency detects a problem with a pod caused by a node or other failure that it can initiate remediation for, it will add an event to that pod’s events:\nkubectl get events -n pmtu1 ... 61s Warning NodeFailure pod/podmontest-0 podmon cleaning pod [7520ba2a-cec5-4dff-8537-20c9bdafbe26 node.example.com] with force delete ... CSM for Resiliency may also generate events if it is unable to clean up a pod for some reason. For example, it may not clean up a pod because the pod is still doing I/O to the array.\nSimilarly, the label selector for csi-powerscale and csi-unity would be as shown respectively.\nlabelSelector: {map[podmon.dellemc.com/driver:csi-isilon] labelSelector: {map[podmon.dellemc.com/driver:csi-unity] Important Before putting an application into production that relies on CSM for Resiliency monitoring, it is important to do a few test failovers first. To do this take the node that is running the pod offline for at least 2-3 minutes. Verify that there is an event message similar to the one above is logged, and that the pod recovers and restarts normally with no loss of data. (Note that if the node is running many CSM for Resiliency protected pods, the node may need to be down longer for CSM for Resiliency to have time to evacuate all the protected pods.)\nApplication Recommendations It is recommended that pods that will be monitored by CSM for Resiliency be configured to exit if they receive any I/O errors. That should help achieve the recovery as quickly as possible.\nCSM for Resiliency does not directly monitor application health. However, if standard Kubernetes health checks are configured, that may help reduce pod recovery time in the event of node failure, as CSM for Resiliency should receive an event that the application is Not Ready. Note that a Not Ready pod is not sufficient to trigger CSM for Resiliency action unless there is also some condition indicating a Node failure or problem, such as the Node is tainted, or the array has lost connectivity to the node.\nAs noted previously in the Limitations and Exclusions section, CSM for Resiliency has not yet been verified to work with ReadWriteMany or ReadOnlyMany volumes. Also, it has not been verified to work with pod controllers other than StatefulSet.\nRecovering From Failures Normally CSM for Resiliency should be able to move pods that have been impacted by Node Failures to a healthy node. After the failed nodes have come back online, CSM for Resiliency cleans them up (especially any potential zombie pods) and then automatically removes the CSM for Resiliency node taint that prevents pods from being scheduled to the failed node(s). There are a few cases where this cannot be fully automated and operator intervention is required, including:\nCSM for Resiliency expects that when a node failure occurs, all CSM for Resiliency labeled pods are evacuated and rescheduled on other nodes. This process may not complete however if the node comes back online before CSM for Resiliency has had time to evacuate all the labeled pods. The remaining pods may not restart correctly, going to “Error” or “CrashLoopBackoff”. We are considering some possible remediation for this condition but have not implemented them yet.\nIf this happens, try deleting the pod with “kubectl delete pod …”. In our experience this normally will cause the pod to be restarted and transition to the “Running” state.\nPodmon-node is responsible for cleaning up failed nodes after the nodes’ communication has been restored. The algorithm checks to see that all the monitored pods have terminated and their volumes and mounts have been cleaned up.\nIf some of the monitored pods are still executing, node-podmon will emit the following log message at the end of a cleanup cycle (and retry the cleanup after a delay):\npods skipped for cleanup because still present: \u003cpod-list\u003e If this happens, DO NOT manually remove the CSM for Resiliency node taint. Doing so could possibly cause data corruption if volumes were not cleaned up, and a pod using those volumes was subsequently scheduled to that node.\nThe correct course of action in this case is to reboot the failed node(s) that have not removed their taints in a reasonable time (5-10 minutes after the node is online again.) The operator can delay executing this reboot until it is convenient, but new pods will not be scheduled to it in the interim. This reboot will cancel any potential zombie pods. After the reboot, node-podmon should automatically remove the node taint after a short time.\nTesting Methodology and Results A three tier testing methodology is used for CSM for Resiliency:\nUnit testing with high coverage (\u003e90% statement) tests the program logic and is especially used to test the error paths by injecting faults. An integration test describes test scenarios in Gherkin that sets up specific testing scenarios executed against a Kubernetes test cluster. The tests use ranges for many of the parameters to add an element of “chaos testing”. Script based testing supports longevity testing in a Kubernetes cluster. For example, one test repeatedly fails three different lists of nodes in succession and is used to fail 1/3 of the cluster’s worker nodes on a cyclic basis and repeat indefinitely. This test collect statistics on length of time for pod evacuation, pod recovery, and node cleanup. ","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency\n","ref":"/csm-docs/v1/resiliency/","tags":"","title":"Resiliency"},{"body":"Container Storage Modules (CSM) for Resiliency is part of the open-source suite of Kubernetes storage enablers for Dell products.\nUser applications can have problems if you want their Pods to be resilient to node failure. This is especially true of those deployed with StatefulSets that use PersistentVolumeClaims. Kubernetes guarantees that there will never be two copies of the same StatefulSet Pod running at the same time and accessing storage. Therefore, it does not clean up StatefulSet Pods if the node executing them fails.\nFor the complete discussion and rationale, you can read the pod-safety design proposal.\nFor more background on the forced deletion of Pods in a StatefulSet, please visit Force Delete StatefulSet Pods.\nCSM for Resiliency and Non graceful node shutdown are mutually exclusive. One shall use either CSM for Resiliency or Non graceful node shutdown feature provided by Kubernetes.\nCSM for Resiliency High-Level Description CSM for Resiliency is designed to make Kubernetes Applications, including those that utilize persistent storage, more resilient to various failures. The first component of the Resiliency module is a pod monitor that is specifically designed to protect stateful applications from various failures. It is not a standalone application, but rather is deployed as a sidecar to CSI (Container Storage Interface) drivers, in both the driver’s controller pods and the driver’s node pods. Deploying CSM for Resiliency as a sidecar allows it to make direct requests to the driver through the Unix domain socket that Kubernetes sidecars use to make CSI requests.\nSome of the methods CSM for Resiliency invokes in the driver are standard CSI methods, such as NodeUnpublishVolume, NodeUnstageVolume, and ControllerUnpublishVolume. CSM for Resiliency also uses proprietary calls that are not part of the standard CSI specification. Currently, there is only one, ValidateVolumeHostConnectivity that returns information on whether a host is connected to the storage system and/or whether any I/O activity has happened in the recent past from a list of specified volumes. This allows CSM for Resiliency to make more accurate determinations about the state of the system and its persistent volumes. CSM for Resiliency is designed to adhere to pod affinity settings of pods.\nAccordingly, CSM for Resiliency is adapted to and qualified with each CSI driver it is to be used with. Different storage systems have different nuances and characteristics that CSM for Resiliency must take into account.\nCSM for Resiliency Capabilities CSM for Resiliency provides the following capabilities:\nCapability PowerScale Unity XT PowerStore PowerFlex PowerMax Detect pod failures when: Node failure, K8S Control Plane Network failure, K8S Control Plane failure, Array I/O Network failure yes yes yes yes no Cleanup pod artifacts from failed nodes yes yes yes yes no Revoke PV access from failed nodes yes yes yes yes no Supported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.25, 1.26, 1.27 Red Hat OpenShift 4.11, 4.12 RHEL 7.x, 8.x CentOS 7.8, 7.9 Supported Storage Platforms PowerFlex Unity XT PowerScale PowerStore Storage Array 3.5.x, 3.6.x 5.0.5, 5.0.6, 5.0.7, 5.1.0, 5.1.2, 5.2, 5.3 OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4 1.0.x, 2.0.x, 2.1.x, 3.0, 3.2 Supported CSI Drivers CSM for Resiliency supports the following CSI drivers and versions. Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerFlex csi-powerflex v2.0.0 + CSI Driver for Dell Unity XT csi-unity v2.0.0 + CSI Driver for Dell PowerScale csi-powerscale v2.3.0 + CSI Driver for Dell PowerStore csi-powerstore v2.6.0 + PowerFlex Support PowerFlex is a highly scalable array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerFlex leverages these PowerFlex features:\nVery quick detection of Array I/O Network Connectivity status changes (generally takes 1-2 seconds for the array to detect changes) A robust mechanism if Nodes are doing I/O to volumes (sampled over a 5-second period). Low latency REST API supports fast CSI provisioning and de-provisioning operations. A proprietary network protocol provided by the SDC component that can run over the same IP interface as the K8S control plane or over a separate IP interface for Array I/O. Unity XT Support Dell Unity XT is targeted for midsized deployments, remote or branch offices, and cost-sensitive mixed workloads. Unity XT systems are designed to deliver the best value in the market. They support all-Flash, and are available in purpose-built (all Flash or hybrid Flash), converged deployment options (through VxBlock), and software-defined virtual edition.\nUnity XT (purpose-built): A modern midrange storage solution, engineered from the groundup to meet market demands for Flash, affordability and incredible simplicity. The Unity XT Family is available in 12 All Flash models and 12 Hybrid models. VxBlock (converged): Unity XT storage options are also available in Dell VxBlock System 1000. UnityVSA (virtual): The Unity XT Virtual Storage Appliance (VSA) allows the advanced unified storage and data management features of the Unity XT family to be easily deployed on VMware ESXi servers. This allows for a ‘software defined’ approach. UnityVSA is available in two editions: Community Edition is a free downloadable 4 TB solution recommended for nonproduction use. Professional Edition is a licensed subscription-based offering available at capacity levels of 10 TB, 25 TB, and 50 TB. The subscription includes access to online support resources, EMC Secure Remote Services (ESRS), and on-call software- and systems-related support. All three deployment options, Unity XT, UnityVSA, and Unity-based VxBlock, enjoy one architecture, one interface with consistent features and rich data services.\nPowerScale Support PowerScale is a highly scalable NFS array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerScale leverages the following PowerScale features:\nDetection of Array I/O Network Connectivity status changes. A robust mechanism to detect if Nodes are actively doing I/O to volumes. Low latency REST API supports fast CSI provisioning and de-provisioning operations. PowerStore Support PowerStore is a highly scalable array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerStore leverages the following PowerStore features:\nDetection of Array I/O Network Connectivity status changes. A robust mechanism to detect if Nodes are actively doing I/O to volumes. Low latency REST API supports fast CSI provisioning and de-provisioning operations. Limitations and Exclusions This file contains information on Limitations and Exclusions that users should be aware of. Additionally, there are driver specific limitations and exclusions that may be called out in the Deploying CSM for Resiliency page.\nSupported and Tested Operating Modes The following provisioning types are supported and have been tested:\nDynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “FileSystem”. Dynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “Block”. Use of the above volumes with Pods created by StatefulSets. Up to 12 or so protected pods on a given node. Failing up to 3 nodes at a time in 9 worker node clusters, or failing 1 node at a time in smaller clusters. Application recovery times are dependent on the number of pods that need to be moved as a result of the failure. See the section on “Testing and Performance” for some of the details. Multi-array are supported. In case of CSI Driver for PowerScale and CSI Driver for Unity, if any one of the array is not connected, the array connectivity will be false. CSI Driver for Powerflex connectivity will be determined by connection to default array. Not Tested But Assumed to Work Deployments with the above volume types, provided two pods from the same deployment do not reside on the same node. At the current time anti-affinity rules should be used to guarantee no two pods accessing the same volumes are scheduled to the same node. Not Yet Tested or Supported Pods that use persistent volumes from multiple CSI drivers. This cannot be supported because multiple controller-podmons (one for each driver type) would be trying to manage the failover with conflicting actions.\nReadWriteMany volumes. This may have issues if a node has multiple pods accessing the same volumes. In any case once pod cleanup fences the volumes on a node, they will no longer be available to any pods using those volumes on that node. We will endeavor to support this in the future.\nMultiple instances of the same driver type (for example two CSI driver for Dell PowerFlex deployments.)\nDeploying and Managing Applications Protected by CSM for Resiliency The first thing to remember about CSM for Resiliency is that it only takes action on pods configured with the designated label. Both the key and the value have to match what is in the podmon helm configuration. CSM for Resiliency emits a log message at startup with the label key and value it is using to monitor pods:\nlabelSelector: {map[podmon.dellemc.com/driver:csi-vxflexos] The above message indicates the key is: podmon.dellemc.com/driver and the label value is csi-vxflexos. To search for the pods that would be monitored, try this:\nkubectl get pods -A -l podmon.dellemc.com/driver=csi-vxflexos NAMESPACE NAME READY STATUS RESTARTS AGE pmtu1 podmontest-0 1/1 Running 0 3m7s pmtu2 podmontest-0 1/1 Running 0 3m8s pmtu3 podmontest-0 1/1 Running 0 3m6s If CSM for Resiliency detects a problem with a pod caused by a node or other failure that it can initiate remediation for, it will add an event to that pod’s events:\nkubectl get events -n pmtu1 ... 61s Warning NodeFailure pod/podmontest-0 podmon cleaning pod [7520ba2a-cec5-4dff-8537-20c9bdafbe26 node.example.com] with force delete ... CSM for Resiliency may also generate events if it is unable to clean up a pod for some reason. For example, it may not clean up a pod because the pod is still doing I/O to the array.\nSimilarly, the label selector for csi-powerscale and csi-unity would be as shown respectively.\nlabelSelector: {map[podmon.dellemc.com/driver:csi-isilon] labelSelector: {map[podmon.dellemc.com/driver:csi-unity] Important Before putting an application into production that relies on CSM for Resiliency monitoring, it is important to do a few test failovers first. To do this take the node that is running the pod offline for at least 2-3 minutes. Verify that there is an event message similar to the one above is logged, and that the pod recovers and restarts normally with no loss of data. (Note that if the node is running many CSM for Resiliency protected pods, the node may need to be down longer for CSM for Resiliency to have time to evacuate all the protected pods.)\nApplication Recommendations It is recommended that pods that will be monitored by CSM for Resiliency be configured to exit if they receive any I/O errors. That should help achieve the recovery as quickly as possible.\nCSM for Resiliency does not directly monitor application health. However, if standard Kubernetes health checks are configured, that may help reduce pod recovery time in the event of node failure, as CSM for Resiliency should receive an event that the application is Not Ready. Note that a Not Ready pod is not sufficient to trigger CSM for Resiliency action unless there is also some condition indicating a Node failure or problem, such as the Node is tainted, or the array has lost connectivity to the node.\nAs noted previously in the Limitations and Exclusions section, CSM for Resiliency has not yet been verified to work with ReadWriteMany or ReadOnlyMany volumes. Also, it has not been verified to work with pod controllers other than StatefulSet.\nRecovering From Failures Normally CSM for Resiliency should be able to move pods that have been impacted by Node Failures to a healthy node. After the failed nodes have come back online, CSM for Resiliency cleans them up (especially any potential zombie pods) and then automatically removes the CSM for Resiliency node taint that prevents pods from being scheduled to the failed node(s). There are a few cases where this cannot be fully automated and operator intervention is required, including:\nCSM for Resiliency expects that when a node failure occurs, all CSM for Resiliency labeled pods are evacuated and rescheduled on other nodes. This process may not complete however if the node comes back online before CSM for Resiliency has had time to evacuate all the labeled pods. The remaining pods may not restart correctly, going to “Error” or “CrashLoopBackoff”. We are considering some possible remediation for this condition but have not implemented them yet.\nIf this happens, try deleting the pod with “kubectl delete pod …”. In our experience this normally will cause the pod to be restarted and transition to the “Running” state.\nPodmon-node is responsible for cleaning up failed nodes after the nodes’ communication has been restored. The algorithm checks to see that all the monitored pods have terminated and their volumes and mounts have been cleaned up.\nIf some of the monitored pods are still executing, node-podmon will emit the following log message at the end of a cleanup cycle (and retry the cleanup after a delay):\npods skipped for cleanup because still present: \u003cpod-list\u003e If this happens, DO NOT manually remove the CSM for Resiliency node taint. Doing so could possibly cause data corruption if volumes were not cleaned up, and a pod using those volumes was subsequently scheduled to that node.\nThe correct course of action in this case is to reboot the failed node(s) that have not removed their taints in a reasonable time (5-10 minutes after the node is online again.) The operator can delay executing this reboot until it is convenient, but new pods will not be scheduled to it in the interim. This reboot will cancel any potential zombie pods. After the reboot, node-podmon should automatically remove the node taint after a short time.\nTesting Methodology and Results A three tier testing methodology is used for CSM for Resiliency:\nUnit testing with high coverage (\u003e90% statement) tests the program logic and is especially used to test the error paths by injecting faults. An integration test describes test scenarios in Gherkin that sets up specific testing scenarios executed against a Kubernetes test cluster. The tests use ranges for many of the parameters to add an element of “chaos testing”. Script based testing supports longevity testing in a Kubernetes cluster. For example, one test repeatedly fails three different lists of nodes in succession and is used to fail 1/3 of the cluster’s worker nodes on a cyclic basis and repeat indefinitely. This test collect statistics on length of time for pod evacuation, pod recovery, and node cleanup. ","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency\n","ref":"/csm-docs/v2/resiliency/","tags":"","title":"Resiliency"},{"body":"Container Storage Modules (CSM) for Resiliency is part of the open-source suite of Kubernetes storage enablers for Dell products.\nUser applications can have problems if you want their Pods to be resilient to node failure. This is especially true of those deployed with StatefulSets that use PersistentVolumeClaims. Kubernetes guarantees that there will never be two copies of the same StatefulSet Pod running at the same time and accessing storage. Therefore, it does not clean up StatefulSet Pods if the node executing them fails.\nFor the complete discussion and rationale, you can read the pod-safety design proposal.\nFor more background on the forced deletion of Pods in a StatefulSet, please visit Force Delete StatefulSet Pods.\nCSM for Resiliency and Non graceful node shutdown are mutually exclusive. One shall use either CSM for Resiliency or Non graceful node shutdown feature provided by Kubernetes.\nCSM for Resiliency High-Level Description CSM for Resiliency is designed to make Kubernetes Applications, including those that utilize persistent storage, more resilient to various failures. The first component of the Resiliency module is a pod monitor that is specifically designed to protect stateful applications from various failures. It is not a standalone application, but rather is deployed as a sidecar to CSI (Container Storage Interface) drivers, in both the driver’s controller pods and the driver’s node pods. Deploying CSM for Resiliency as a sidecar allows it to make direct requests to the driver through the Unix domain socket that Kubernetes sidecars use to make CSI requests.\nSome of the methods CSM for Resiliency invokes in the driver are standard CSI methods, such as NodeUnpublishVolume, NodeUnstageVolume, and ControllerUnpublishVolume. CSM for Resiliency also uses proprietary calls that are not part of the standard CSI specification. Currently, there is only one, ValidateVolumeHostConnectivity that returns information on whether a host is connected to the storage system and/or whether any I/O activity has happened in the recent past from a list of specified volumes. This allows CSM for Resiliency to make more accurate determinations about the state of the system and its persistent volumes. CSM for Resiliency is designed to adhere to pod affinity settings of pods.\nAccordingly, CSM for Resiliency is adapted to and qualified with each CSI driver it is to be used with. Different storage systems have different nuances and characteristics that CSM for Resiliency must take into account.\nCSM for Resiliency Capabilities CSM for Resiliency provides the following capabilities:\nCapability PowerScale Unity XT PowerStore PowerFlex PowerMax Detect pod failures when: Node failure, K8S Control Plane Network failure, K8S Control Plane failure, Array I/O Network failure yes yes yes yes no Cleanup pod artifacts from failed nodes yes yes yes yes no Revoke PV access from failed nodes yes yes yes yes no Supported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.25, 1.26, 1.27 Red Hat OpenShift 4.10, 4.11, 4.12 RHEL 7.x, 8.x CentOS 7.8, 7.9 Supported Storage Platforms PowerFlex Unity XT PowerScale PowerStore Storage Array 3.5.x, 3.6.x 5.0.5, 5.0.6, 5.0.7, 5.1.0, 5.1.2 OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4 1.0.x, 2.0.x, 2.1.x, 3.0, 3.2 Supported CSI Drivers CSM for Resiliency supports the following CSI drivers and versions. Storage Array CSI Driver Supported Versions CSI Driver for Dell PowerFlex csi-powerflex v2.0.0 + CSI Driver for Dell Unity XT csi-unity v2.0.0 + CSI Driver for Dell PowerScale csi-powerscale v2.3.0 + CSI Driver for Dell PowerStore csi-powerstore v2.6.0 + PowerFlex Support PowerFlex is a highly scalable array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerFlex leverages these PowerFlex features:\nVery quick detection of Array I/O Network Connectivity status changes (generally takes 1-2 seconds for the array to detect changes) A robust mechanism if Nodes are doing I/O to volumes (sampled over a 5-second period). Low latency REST API supports fast CSI provisioning and de-provisioning operations. A proprietary network protocol provided by the SDC component that can run over the same IP interface as the K8S control plane or over a separate IP interface for Array I/O. Unity XT Support Dell Unity XT is targeted for midsized deployments, remote or branch offices, and cost-sensitive mixed workloads. Unity XT systems are designed to deliver the best value in the market. They support all-Flash, and are available in purpose-built (all Flash or hybrid Flash), converged deployment options (through VxBlock), and software-defined virtual edition.\nUnity XT (purpose-built): A modern midrange storage solution, engineered from the groundup to meet market demands for Flash, affordability and incredible simplicity. The Unity XT Family is available in 12 All Flash models and 12 Hybrid models. VxBlock (converged): Unity XT storage options are also available in Dell VxBlock System 1000. UnityVSA (virtual): The Unity XT Virtual Storage Appliance (VSA) allows the advanced unified storage and data management features of the Unity XT family to be easily deployed on VMware ESXi servers. This allows for a ‘software defined’ approach. UnityVSA is available in two editions: Community Edition is a free downloadable 4 TB solution recommended for nonproduction use. Professional Edition is a licensed subscription-based offering available at capacity levels of 10 TB, 25 TB, and 50 TB. The subscription includes access to online support resources, EMC Secure Remote Services (ESRS), and on-call software- and systems-related support. All three deployment options, Unity XT, UnityVSA, and Unity-based VxBlock, enjoy one architecture, one interface with consistent features and rich data services.\nPowerScale Support PowerScale is a highly scalable NFS array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerScale leverages the following PowerScale features:\nDetection of Array I/O Network Connectivity status changes. A robust mechanism to detect if Nodes are actively doing I/O to volumes. Low latency REST API supports fast CSI provisioning and de-provisioning operations. PowerStore Support PowerStore is a highly scalable array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerStore leverages the following PowerStore features:\nDetection of Array I/O Network Connectivity status changes. A robust mechanism to detect if Nodes are actively doing I/O to volumes. Low latency REST API supports fast CSI provisioning and de-provisioning operations. Limitations and Exclusions This file contains information on Limitations and Exclusions that users should be aware of. Additionally, there are driver specific limitations and exclusions that may be called out in the Deploying CSM for Resiliency page.\nSupported and Tested Operating Modes The following provisioning types are supported and have been tested:\nDynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “FileSystem”. Dynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “Block”. Use of the above volumes with Pods created by StatefulSets. Up to 12 or so protected pods on a given node. Failing up to 3 nodes at a time in 9 worker node clusters, or failing 1 node at a time in smaller clusters. Application recovery times are dependent on the number of pods that need to be moved as a result of the failure. See the section on “Testing and Performance” for some of the details. Multi-array are supported. In case of CSI Driver for PowerScale and CSI Driver for Unity, if any one of the array is not connected, the array connectivity will be false. CSI Driver for Powerflex connectivity will be determined by connection to default array. Not Tested But Assumed to Work Deployments with the above volume types, provided two pods from the same deployment do not reside on the same node. At the current time anti-affinity rules should be used to guarantee no two pods accessing the same volumes are scheduled to the same node. Not Yet Tested or Supported Pods that use persistent volumes from multiple CSI drivers. This cannot be supported because multiple controller-podmons (one for each driver type) would be trying to manage the failover with conflicting actions.\nReadWriteMany volumes. This may have issues if a node has multiple pods accessing the same volumes. In any case once pod cleanup fences the volumes on a node, they will no longer be available to any pods using those volumes on that node. We will endeavor to support this in the future.\nMultiple instances of the same driver type (for example two CSI driver for Dell PowerFlex deployments.)\nDeploying and Managing Applications Protected by CSM for Resiliency The first thing to remember about CSM for Resiliency is that it only takes action on pods configured with the designated label. Both the key and the value have to match what is in the podmon helm configuration. CSM for Resiliency emits a log message at startup with the label key and value it is using to monitor pods:\nlabelSelector: {map[podmon.dellemc.com/driver:csi-vxflexos] The above message indicates the key is: podmon.dellemc.com/driver and the label value is csi-vxflexos. To search for the pods that would be monitored, try this:\n[root@lglbx209 podmontest]# kubectl get pods -A -l podmon.dellemc.com/driver=csi-vxflexos NAMESPACE NAME READY STATUS RESTARTS AGE pmtu1 podmontest-0 1/1 Running 0 3m7s pmtu2 podmontest-0 1/1 Running 0 3m8s pmtu3 podmontest-0 1/1 Running 0 3m6s If CSM for Resiliency detects a problem with a pod caused by a node or other failure that it can initiate remediation for, it will add an event to that pod’s events:\nkubectl get events -n pmtu1 ... 61s Warning NodeFailure pod/podmontest-0 podmon cleaning pod [7520ba2a-cec5-4dff-8537-20c9bdafbe26 node.example.com] with force delete ... CSM for Resiliency may also generate events if it is unable to clean up a pod for some reason. For example, it may not clean up a pod because the pod is still doing I/O to the array.\nSimilarly, the label selector for csi-powerscale and csi-unity would be as shown respectively.\nlabelSelector: {map[podmon.dellemc.com/driver:csi-isilon] labelSelector: {map[podmon.dellemc.com/driver:csi-unity] Important Before putting an application into production that relies on CSM for Resiliency monitoring, it is important to do a few test failovers first. To do this take the node that is running the pod offline for at least 2-3 minutes. Verify that there is an event message similar to the one above is logged, and that the pod recovers and restarts normally with no loss of data. (Note that if the node is running many CSM for Resiliency protected pods, the node may need to be down longer for CSM for Resiliency to have time to evacuate all the protected pods.)\nApplication Recommendations It is recommended that pods that will be monitored by CSM for Resiliency be configured to exit if they receive any I/O errors. That should help achieve the recovery as quickly as possible.\nCSM for Resiliency does not directly monitor application health. However, if standard Kubernetes health checks are configured, that may help reduce pod recovery time in the event of node failure, as CSM for Resiliency should receive an event that the application is Not Ready. Note that a Not Ready pod is not sufficient to trigger CSM for Resiliency action unless there is also some condition indicating a Node failure or problem, such as the Node is tainted, or the array has lost connectivity to the node.\nAs noted previously in the Limitations and Exclusions section, CSM for Resiliency has not yet been verified to work with ReadWriteMany or ReadOnlyMany volumes. Also, it has not been verified to work with pod controllers other than StatefulSet.\nRecovering From Failures Normally CSM for Resiliency should be able to move pods that have been impacted by Node Failures to a healthy node. After the failed nodes have come back online, CSM for Resiliency cleans them up (especially any potential zombie pods) and then automatically removes the CSM for Resiliency node taint that prevents pods from being scheduled to the failed node(s). There are a few cases where this cannot be fully automated and operator intervention is required, including:\nCSM for Resiliency expects that when a node failure occurs, all CSM for Resiliency labeled pods are evacuated and rescheduled on other nodes. This process may not complete however if the node comes back online before CSM for Resiliency has had time to evacuate all the labeled pods. The remaining pods may not restart correctly, going to “Error” or “CrashLoopBackoff”. We are considering some possible remediation for this condition but have not implemented them yet.\nIf this happens, try deleting the pod with “kubectl delete pod …”. In our experience this normally will cause the pod to be restarted and transition to the “Running” state.\nPodmon-node is responsible for cleaning up failed nodes after the nodes’ communication has been restored. The algorithm checks to see that all the monitored pods have terminated and their volumes and mounts have been cleaned up.\nIf some of the monitored pods are still executing, node-podmon will emit the following log message at the end of a cleanup cycle (and retry the cleanup after a delay):\npods skipped for cleanup because still present: \u003cpod-list\u003e If this happens, DO NOT manually remove the CSM for Resiliency node taint. Doing so could possibly cause data corruption if volumes were not cleaned up, and a pod using those volumes was subsequently scheduled to that node.\nThe correct course of action in this case is to reboot the failed node(s) that have not removed their taints in a reasonable time (5-10 minutes after the node is online again.) The operator can delay executing this reboot until it is convenient, but new pods will not be scheduled to it in the interim. This reboot will cancel any potential zombie pods. After the reboot, node-podmon should automatically remove the node taint after a short time.\nTesting Methodology and Results A three tier testing methodology is used for CSM for Resiliency:\nUnit testing with high coverage (\u003e90% statement) tests the program logic and is especially used to test the error paths by injecting faults. An integration test describes test scenarios in Gherkin that sets up specific testing scenarios executed against a Kubernetes test cluster. The tests use ranges for many of the parameters to add an element of “chaos testing”. Script based testing supports longevity testing in a Kubernetes cluster. For example, one test repeatedly fails three different lists of nodes in succession and is used to fail 1/3 of the cluster’s worker nodes on a cyclic basis and repeat indefinitely. This test collect statistics on length of time for pod evacuation, pod recovery, and node cleanup. ","categories":"","description":"Dell Container Storage Modules (CSM) for Resiliency\n","excerpt":"Dell Container Storage Modules (CSM) for Resiliency\n","ref":"/csm-docs/v3/resiliency/","tags":"","title":"Resiliency"},{"body":"Starting in v2.4.0, the CSI PowerMax driver supports the expansion of Replicated Persistent Volumes (PVs). This expansion is done online, which is when the PVC is attached to any node.\nPrerequisites To use this feature, enable resizer in values.yaml: resizer: enabled: true To use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. Basic Usage To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pmax-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 10Gi #Updated size from 5Gi to 10Gi storageClassName: powermax-expand-sc Update remote PVC with expanded size:\nUpdate the remote PVC size with the same size as on local PVC.\nAfter sync with remote CSI driver, volume size will be updated to show new size.\nNOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of the volume, it cannot be used to shrink a volume.\n","categories":"","description":"Online expansion of replicated volumes\n","excerpt":"Online expansion of replicated volumes\n","ref":"/csm-docs/docs/replication/volume_expansion/","tags":"","title":"Volume Expansion"},{"body":"Starting in v2.4.0, the CSI PowerMax driver supports the expansion of Replicated Persistent Volumes (PVs). This expansion is done online, which is when the PVC is attached to any node.\nPrerequisites To use this feature, enable resizer in values.yaml: resizer: enabled: true To use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. Basic Usage To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pmax-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 10Gi #Updated size from 5Gi to 10Gi storageClassName: powermax-expand-sc Update remote PVC with expanded size:\nUpdate the remote PVC size with the same size as on local PVC.\nAfter sync with remote CSI driver, volume size will be updated to show new size.\nNOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of the volume, it cannot be used to shrink a volume.\n","categories":"","description":"Online expansion of replicated volumes\n","excerpt":"Online expansion of replicated volumes\n","ref":"/csm-docs/v1/replication/volume_expansion/","tags":"","title":"Volume Expansion"},{"body":"Starting in v2.4.0, the CSI PowerMax driver supports the expansion of Replicated Persistent Volumes (PVs). This expansion is done online, which is when the PVC is attached to any node.\nPrerequisites To use this feature, enable resizer in values.yaml: resizer: enabled: true To use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. Basic Usage To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pmax-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 10Gi #Updated size from 5Gi to 10Gi storageClassName: powermax-expand-sc Update remote PVC with expanded size:\nUpdate the remote PVC size with the same size as on local PVC.\nAfter sync with remote CSI driver, volume size will be updated to show new size.\nNOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of the volume, it cannot be used to shrink a volume.\n","categories":"","description":"Online expansion of replicated volumes\n","excerpt":"Online expansion of replicated volumes\n","ref":"/csm-docs/v2/replication/volume_expansion/","tags":"","title":"Volume Expansion"},{"body":"Starting in v2.4.0, the CSI PowerMax driver supports the expansion of Replicated Persistent Volumes (PVs). This expansion is done online, which is when the PVC is attached to any node.\nPrerequisites To use this feature, enable resizer in values.yaml: resizer: enabled: true To use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. Basic Usage To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pmax-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 10Gi #Updated size from 5Gi to 10Gi storageClassName: powermax-expand-sc Update remote PVC with expanded size:\nUpdate the remote PVC size with the same size as on local PVC.\nAfter sync with remote CSI driver, volume size will be updated to show new size.\nNOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of the volume, it cannot be used to shrink a volume.\n","categories":"","description":"Online expansion of replicated volumes\n","excerpt":"Online expansion of replicated volumes\n","ref":"/csm-docs/v3/replication/volume_expansion/","tags":"","title":"Volume Expansion"},{"body":"Enabling Replication in CSI PowerScale Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerScale supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Ensure that SyncIQ service is enabled on both arrays, you can do that by navigating to SyncIQ section under Data protection tab.\nThe current implementation supports one-to-one replication so you need to ensure that one array can reach another and vice versa.\nSyncIQ encryption If you wish to use SyncIQ encryption you should ensure that you’ve added a server certificate first by navigating to Data protection-\u003eSyncIQ-\u003eSettings.\nAfter adding the certificate, you can choose to use it by checking Encrypt SyncIQ connection from the dropdown.\nAfter that, you can add similar certificates of other arrays in SyncIQ-\u003e Certificates, and ensure you’ve added the certificate of the array you want to replicate to.\nSimilar steps should be done in the reverse direction, so array-1 has the array-2 certificate visible in SyncIQ-\u003e Certificates tab and array-2 has the array-1 certificate visible in its own SyncIQ-\u003eCertificates tab.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\nCheck controller pods: kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING. Check that controller config map is properly populated: kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target clusters IDs. If you don’t have something installed or something is out-of-place, please refer to installation instructions here.\nInstalling Driver With Replication Module To install the driver with replication enabled, you need to ensure you have set helm parameter controller.replication.enabled in your copy of example values.yaml file (usually called my-isilon-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like:\n... # controller: configure controller specific parameters controller: ... # replication: allows to configure replication replication: enabled: true replicationContextPrefix: \"powerscale\" replicationPrefix: \"replication.storage.dell.com\" ... You can leave other parameters like replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module, you can continue to install the CSI driver for PowerScale following the usual installation procedure. Just ensure you’ve added the necessary array connection information to the Kubernetes secret for the PowerScale driver.\nNOTE: You need to install your driver on all clusters where you want to use replication. Both arrays must be accessible from each cluster.\nSyncIQ encryption If you plan to use encryption, you need to set replicationCertificateID in the array connection secret. To check the ID of the certificate for the cluster, you can navigate to Data protection-\u003eSyncIQ-\u003eSettings, find your certificate in the Server Certificates section and then push the View/Edit button. It will open a dialog that should contain the Id field. Use the value of that field to set replicationCertificateID.\nCreating Storage Classes To provision replicated volumes, you need to create adequately configured storage classes on both the source and target clusters.\nA pair of storage classes on the source, and target clusters would be essentially mirrored copies of one another. You can create them manually or with the help of repctl.\nManual Storage Class Creation You can find a sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon-replication provisioner: csi-isilon.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"isilon-replication\" replication.storage.dell.com/remoteClusterID: \"target\" replication.storage.dell.com/remoteSystem: \"cluster-2\" replication.storage.dell.com/remoteAccessZone: System replication.storage.dell.com/remoteAzServiceIP: 192.168.1.2 replication.storage.dell.com/remoteRootClientEnabled: \"false\" replication.storage.dell.com/rpo: Five_Minutes replication.storage.dell.com/ignoreNamespaces: \"false\" replication.storage.dell.com/volumeGroupPrefix: \"csi\" AccessZone: System AzServiceIP: 192.168.1.1 IsiPath: /ifs/data/csi RootClientEnabled: \"false\" ClusterName: cluster-1 Let’s go through each parameter and what it means:\nreplication.storage.dell.com/isReplicationEnabled if set to true, will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents the ID of a remote Kubernetes cluster. It is the same ID you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote PowerScale system that should match whatever clusterName you called it in isilon-creds secret. replication.storage.dell.com/remoteAccessZone is the name of the access zone a remote volume can be created in. replication.storage.dell.com/remoteAzServiceIP AccessZone groupnet service IP. It is optional and can be provided if different than the remote system endpoint. replication.storage.dell.com/remoteRootClientEnabled determines whether the driver should enable root squashing or not for the remote volume. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. NOTE: Available RPO values “Five_Minutes”, “Fifteen_Minutes”, “Thirty_Minutes”, “One_Hour”, “Six_Hours”, “Twelve_Hours”, “One_Day”\nreplication.storage.dell.com/ignoreNamespaces, if set to true PowerScale driver, it will ignore in what namespace volumes are created and put every volume created using this storage class into a single volume group. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate them. It is important to not use the same prefix for different kubernetes clusters, otherwise any action on a replication group in one kubernetes cluster will impact the other. NOTE: To configure the VolumeGroupPrefix, the name format of '\u003cvolumeGroupPrefix\u003e-\u003cnamespace\u003e-\u003cSystem IP Address OR FQDN\u003e-\u003crpo\u003e' cannot be more than 63 characters.\nAccesszone is the name of the access zone a volume can be created in. AzServiceIP AccessZone groupnet service IP. It is optional and can be provided if different than the PowerScale cluster endpoint. IsiPath is the base path for the volumes to be created on the PowerScale cluster. If not specified in the storage class, the IsiPath defined in the storage array’s secret will be used. If that is not specified, the IsiPath defined in the values.yaml file used for driver installation is used as the lowest-priority. IsiPath between source and target Replication Groups must be consistent. RootClientEnabled determines whether the driver should enable root squashing or not. ClusterName name of PowerScale cluster, where PV will be provisioned, specified as it was listed in isilon-creds secret. After creating storage class YAML files, they must be applied to your Kubernetes clusters with kubectl.\nStorage Class creation with repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example here, copy it, and modify it to your needs.\nIf you open this example you can see similar fields and parameters to what was seen in manual storage class creation.\nLet’s use the same example from manual installation and see what its repctl config file would look like:\nsourceClusterID: \"source\" targetClusterID: \"target\" name: \"isilon-replication\" driver: \"isilon\" reclaimPolicy: \"Delete\" replicationPrefix: \"replication.storage.dell.com\" remoteRetentionPolicy: RG: \"Delete\" PV: \"Delete\" parameters: rpo: \"Five_Minutes\" ignoreNamespaces: \"false\" volumeGroupPrefix: \"csi\" isiPath: \"/ifs/data/csi\" clusterName: source: \"cluster-1\" target: \"cluster-2\" rootClientEnabled: source: \"false\" target: \"false\" accessZone: source: \"System\" target: \"System\" azServiceIP: source: \"192.168.1.1\" target: \"192.168.1.2\" After preparing the config, you can apply it to both clusters with repctl. Before you do this, ensure you’ve added your clusters to repctl via the add command.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes, you are good to create volumes using the newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication-enabled Storage Classes. The CSI PowerScale driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerScale driver supports the following list of replication actions:\nFAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL FAILBACK_LOCAL ACTION_FAILBACK_DISCARD_CHANGES_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC ","categories":"","description":"Enabling Replication feature for CSI PowerScale\n","excerpt":"Enabling Replication feature for CSI PowerScale\n","ref":"/csm-docs/docs/replication/deployment/powerscale/","tags":"","title":"PowerScale"},{"body":"Enabling Replication in CSI PowerScale Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerScale supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Ensure that SyncIQ service is enabled on both arrays, you can do that by navigating to SyncIQ section under Data protection tab.\nThe current implementation supports one-to-one replication so you need to ensure that one array can reach another and vice versa.\nSyncIQ encryption If you wish to use SyncIQ encryption you should ensure that you’ve added a server certificate first by navigating to Data protection-\u003eSyncIQ-\u003eSettings.\nAfter adding the certificate, you can choose to use it by checking Encrypt SyncIQ connection from the dropdown.\nAfter that, you can add similar certificates of other arrays in SyncIQ-\u003e Certificates, and ensure you’ve added the certificate of the array you want to replicate to.\nSimilar steps should be done in the reverse direction, so array-1 has the array-2 certificate visible in SyncIQ-\u003e Certificates tab and array-2 has the array-1 certificate visible in its own SyncIQ-\u003eCertificates tab.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\nCheck controller pods: kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING. Check that controller config map is properly populated: kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target clusters IDs. If you don’t have something installed or something is out-of-place, please refer to installation instructions here.\nInstalling Driver With Replication Module To install the driver with replication enabled, you need to ensure you have set helm parameter controller.replication.enabled in your copy of example values.yaml file (usually called my-isilon-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like:\n... # controller: configure controller specific parameters controller: ... # replication: allows to configure replication replication: enabled: true image: dellemc/dell-csi-replicator:v1.6.0 replicationContextPrefix: \"powerscale\" replicationPrefix: \"replication.storage.dell.com\" ... You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module, you can continue to install the CSI driver for PowerScale following the usual installation procedure. Just ensure you’ve added the necessary array connection information to the Kubernetes secret for the PowerScale driver.\nNOTE: You need to install your driver on all clusters where you want to use replication. Both arrays must be accessible from each cluster.\nSyncIQ encryption If you plan to use encryption, you need to set replicationCertificateID in the array connection secret. To check the ID of the certificate for the cluster, you can navigate to Data protection-\u003eSyncIQ-\u003eSettings, find your certificate in the Server Certificates section and then push the View/Edit button. It will open a dialog that should contain the Id field. Use the value of that field to set replicationCertificateID.\nCreating Storage Classes To provision replicated volumes, you need to create adequately configured storage classes on both the source and target clusters.\nA pair of storage classes on the source, and target clusters would be essentially mirrored copies of one another. You can create them manually or with the help of repctl.\nManual Storage Class Creation You can find a sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon-replication provisioner: csi-isilon.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"isilon-replication\" replication.storage.dell.com/remoteClusterID: \"target\" replication.storage.dell.com/remoteSystem: \"cluster-2\" replication.storage.dell.com/remoteAccessZone: System replication.storage.dell.com/remoteAzServiceIP: 192.168.1.2 replication.storage.dell.com/remoteRootClientEnabled: \"false\" replication.storage.dell.com/rpo: Five_Minutes replication.storage.dell.com/ignoreNamespaces: \"false\" replication.storage.dell.com/volumeGroupPrefix: \"csi\" AccessZone: System AzServiceIP: 192.168.1.1 IsiPath: /ifs/data/csi RootClientEnabled: \"false\" ClusterName: cluster-1 Let’s go through each parameter and what it means:\nreplication.storage.dell.com/isReplicationEnabled if set to true, will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents the ID of a remote Kubernetes cluster. It is the same ID you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote PowerScale system that should match whatever clusterName you called it in isilon-creds secret. replication.storage.dell.com/remoteAccessZone is the name of the access zone a remote volume can be created in. replication.storage.dell.com/remoteAzServiceIP AccessZone groupnet service IP. It is optional and can be provided if different than the remote system endpoint. replication.storage.dell.com/remoteRootClientEnabled determines whether the driver should enable root squashing or not for the remote volume. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. NOTE: Available RPO values “Five_Minutes”, “Fifteen_Minutes”, “Thirty_Minutes”, “One_Hour”, “Six_Hours”, “Twelve_Hours”, “One_Day”\nreplication.storage.dell.com/ignoreNamespaces, if set to true PowerScale driver, it will ignore in what namespace volumes are created and put every volume created using this storage class into a single volume group. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate them. It is important to not use the same prefix for different kubernetes clusters, otherwise any action on a replication group in one kubernetes cluster will impact the other. NOTE: To configure the VolumeGroupPrefix, the name format of '\u003cvolumeGroupPrefix\u003e-\u003cnamespace\u003e-\u003cSystem IP Address OR FQDN\u003e-\u003crpo\u003e' cannot be more than 63 characters.\nAccesszone is the name of the access zone a volume can be created in. AzServiceIP AccessZone groupnet service IP. It is optional and can be provided if different than the PowerScale cluster endpoint. IsiPath is the base path for the volumes to be created on the PowerScale cluster. If not specified in the storage class, the IsiPath defined in the storage array’s secret will be used. If that is not specified, the IsiPath defined in the values.yaml file used for driver installation is used as the lowest-priority. IsiPath between source and target Replication Groups must be consistent. RootClientEnabled determines whether the driver should enable root squashing or not. ClusterName name of PowerScale cluster, where PV will be provisioned, specified as it was listed in isilon-creds secret. After creating storage class YAML files, they must be applied to your Kubernetes clusters with kubectl.\nStorage Class creation with repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example here, copy it, and modify it to your needs.\nIf you open this example you can see similar fields and parameters to what was seen in manual storage class creation.\nLet’s use the same example from manual installation and see what its repctl config file would look like:\nsourceClusterID: \"source\" targetClusterID: \"target\" name: \"isilon-replication\" driver: \"isilon\" reclaimPolicy: \"Delete\" replicationPrefix: \"replication.storage.dell.com\" remoteRetentionPolicy: RG: \"Delete\" PV: \"Delete\" parameters: rpo: \"Five_Minutes\" ignoreNamespaces: \"false\" volumeGroupPrefix: \"csi\" isiPath: \"/ifs/data/csi\" clusterName: source: \"cluster-1\" target: \"cluster-2\" rootClientEnabled: source: \"false\" target: \"false\" accessZone: source: \"System\" target: \"System\" azServiceIP: source: \"192.168.1.1\" target: \"192.168.1.2\" After preparing the config, you can apply it to both clusters with repctl. Before you do this, ensure you’ve added your clusters to repctl via the add command.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes, you are good to create volumes using the newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication-enabled Storage Classes. The CSI PowerScale driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerScale driver supports the following list of replication actions:\nFAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL FAILBACK_LOCAL ACTION_FAILBACK_DISCARD_CHANGES_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC ","categories":"","description":"Enabling Replication feature for CSI PowerScale\n","excerpt":"Enabling Replication feature for CSI PowerScale\n","ref":"/csm-docs/v1/replication/deployment/powerscale/","tags":"","title":"PowerScale"},{"body":"Enabling Replication in CSI PowerScale Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerScale supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Ensure that SyncIQ service is enabled on both arrays, you can do that by navigating to SyncIQ section under Data protection tab.\nThe current implementation supports one-to-one replication so you need to ensure that one array can reach another and vice versa.\nSyncIQ encryption If you wish to use SyncIQ encryption you should ensure that you’ve added a server certificate first by navigating to Data protection-\u003eSyncIQ-\u003eSettings.\nAfter adding the certificate, you can choose to use it by checking Encrypt SyncIQ connection from the dropdown.\nAfter that, you can add similar certificates of other arrays in SyncIQ-\u003e Certificates, and ensure you’ve added the certificate of the array you want to replicate to.\nSimilar steps should be done in the reverse direction, so array-1 has the array-2 certificate visible in SyncIQ-\u003e Certificates tab and array-2 has the array-1 certificate visible in its own SyncIQ-\u003eCertificates tab.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\nCheck controller pods: kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING. Check that controller config map is properly populated: kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target clusters IDs. If you don’t have something installed or something is out-of-place, please refer to installation instructions here.\nInstalling Driver With Replication Module To install the driver with replication enabled, you need to ensure you have set helm parameter controller.replication.enabled in your copy of example values.yaml file (usually called my-isilon-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like:\n... # controller: configure controller specific parameters controller: ... # replication: allows to configure replication replication: enabled: true image: dellemc/dell-csi-replicator:v1.2.0 replicationContextPrefix: \"powerscale\" replicationPrefix: \"replication.storage.dell.com\" ... You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module, you can continue to install the CSI driver for PowerScale following the usual installation procedure. Just ensure you’ve added the necessary array connection information to the Kubernetes secret for the PowerScale driver.\nSyncIQ encryption If you plan to use encryption, you need to set replicationCertificateID in the array connection secret. To check the ID of the certificate for the cluster, you can navigate to Data protection-\u003eSyncIQ-\u003eSettings, find your certificate in the Server Certificates section and then push the View/Edit button. It will open a dialog that should contain the Id field. Use the value of that field to set replicationCertificateID.\nNOTE: You need to install your driver on ALL clusters where you want to use replication. Both arrays must be accessible from each cluster.\nCreating Storage Classes To provision replicated volumes, you need to create adequately configured storage classes on both the source and target clusters.\nA pair of storage classes on the source, and target clusters would be essentially mirrored copies of one another. You can create them manually or with the help of repctl.\nManual Storage Class Creation You can find a sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon-replication provisioner: csi-isilon.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"isilon-replication\" replication.storage.dell.com/remoteClusterID: \"target\" replication.storage.dell.com/remoteSystem: \"cluster-2\" replication.storage.dell.com/remoteAccessZone: System replication.storage.dell.com/remoteAzServiceIP: 192.168.1.2 replication.storage.dell.com/remoteRootClientEnabled: \"false\" replication.storage.dell.com/rpo: Five_Minutes replication.storage.dell.com/ignoreNamespaces: \"false\" replication.storage.dell.com/volumeGroupPrefix: \"csi\" AccessZone: System AzServiceIP: 192.168.1.1 IsiPath: /ifs/data/csi RootClientEnabled: \"false\" ClusterName: cluster-1 Let’s go through each parameter and what it means:\nreplication.storage.dell.com/isReplicationEnabled if set to true, will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents the ID of a remote cluster. It is the same ID you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system that should match whatever clusterName you called it in isilon-creds secret. replication.storage.dell.com/remoteAccessZone is the name of the access zone a remote volume can be created in. replication.storage.dell.com/remoteAzServiceIP AccessZone groupnet service IP. It is optional and can be provided if different than the remote system endpoint. replication.storage.dell.com/remoteRootClientEnabled determines whether the driver should enable root squashing or not for the remote volume. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. NOTE: Available RPO values “Five_Minutes”, “Fifteen_Minutes”, “Thirty_Minutes”, “One_Hour”, “Six_Hours”, “Twelve_Hours”, “One_Day”\nreplication.storage.dell.com/ignoreNamespaces, if set to true PowerScale driver, it will ignore in what namespace volumes are created and put every volume created using this storage class into a single volume group. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate them. It is important to not use the same prefix for different kubernetes clusters, otherwise any action on a replication group in one kubernetes cluster will impact the other. NOTE: To configure the VolumeGroupPrefix, the name format of '\u003cvolumeGroupPrefix\u003e-\u003cnamespace\u003e-\u003cSystem IP Address OR FQDN\u003e-\u003crpo\u003e' cannot be more than 63 characters.\nAccesszone is the name of the access zone a volume can be created in. AzServiceIP AccessZone groupnet service IP. It is optional and can be provided if different than the PowerScale cluster endpoint. IsiPath is the base path for the volumes to be created on the PowerScale cluster. If not specified in the storage class, the IsiPath defined in the storage array’s secret will be used. If that is not specified, the IsiPath defined in the values.yaml file used for driver installation is used as the lowest-priority. IsiPath between source and target Replication Groups must be consistent. RootClientEnabled determines whether the driver should enable root squashing or not. ClusterName name of PowerScale cluster, where PV will be provisioned, specified as it was listed in isilon-creds secret. After creating storage class YAML files, they must be applied to your Kubernetes clusters with kubectl.\nStorage Class creation with repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example here, copy it, and modify it to your needs.\nIf you open this example you can see similar fields and parameters to what was seen in manual storage class creation.\nLet’s use the same example from manual installation and see what its repctl config file would look like:\nsourceClusterID: \"source\" targetClusterID: \"target\" name: \"isilon-replication\" driver: \"isilon\" reclaimPolicy: \"Delete\" replicationPrefix: \"replication.storage.dell.com\" remoteRetentionPolicy: RG: \"Delete\" PV: \"Delete\" parameters: rpo: \"Five_Minutes\" ignoreNamespaces: \"false\" volumeGroupPrefix: \"csi\" isiPath: \"/ifs/data/csi\" clusterName: source: \"cluster-1\" target: \"cluster-2\" rootClientEnabled: source: \"false\" target: \"false\" accessZone: source: \"System\" target: \"System\" azServiceIP: source: \"192.168.1.1\" target: \"192.168.1.2\" After preparing the config, you can apply it to both clusters with repctl. Before you do this, ensure you’ve added your clusters to repctl via the add command.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes, you are good to create volumes using the newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication-enabled Storage Classes. The CSI PowerScale driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerScale driver supports the following list of replication actions:\nFAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL FAILBACK_LOCAL ACTION_FAILBACK_DISCARD_CHANGES_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC ","categories":"","description":"Enabling Replication feature for CSI PowerScale\n","excerpt":"Enabling Replication feature for CSI PowerScale\n","ref":"/csm-docs/v2/replication/deployment/powerscale/","tags":"","title":"PowerScale"},{"body":"Enabling Replication in CSI PowerScale Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerScale supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Ensure that SyncIQ service is enabled on both arrays, you can do that by navigating to SyncIQ section under Data protection tab.\nThe current implementation supports one-to-one replication so you need to ensure that one array can reach another and vice versa.\nSyncIQ encryption If you wish to use SyncIQ encryption you should ensure that you’ve added a server certificate first by navigating to Data protection-\u003eSyncIQ-\u003eSettings.\nAfter adding the certificate, you can choose to use it by checking Encrypt SyncIQ connection from the dropdown.\nAfter that, you can add similar certificates of other arrays in SyncIQ-\u003e Certificates, and ensure you’ve added the certificate of the array you want to replicate to.\nSimilar steps should be done in the reverse direction, so array-1 has the array-2 certificate visible in SyncIQ-\u003e Certificates tab and array-2 has the array-1 certificate visible in its own SyncIQ-\u003eCertificates tab.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\nCheck controller pods: kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING. Check that controller config map is properly populated: kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target clusters IDs. If you don’t have something installed or something is out-of-place, please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled, you need to ensure you have set helm parameter controller.replication.enabled in your copy of example values.yaml file (usually called my-isilon-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like:\n... # controller: configure controller specific parameters controller: ... # replication: allows to configure replication replication: enabled: true image: dellemc/dell-csi-replicator:v1.2.0 replicationContextPrefix: \"powerscale\" replicationPrefix: \"replication.storage.dell.com\" ... You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module, you can continue to install the CSI driver for PowerScale following the usual installation procedure. Just ensure you’ve added the necessary array connection information to the Kubernetes secret for the PowerScale driver.\nSyncIQ encryption If you plan to use encryption, you need to set replicationCertificateID in the array connection secret. To check the ID of the certificate for the cluster, you can navigate to Data protection-\u003eSyncIQ-\u003eSettings, find your certificate in the Server Certificates section and then push the View/Edit button. It will open a dialog that should contain the Id field. Use the value of that field to set replicationCertificateID.\nNOTE: You need to install your driver on ALL clusters where you want to use replication. Both arrays must be accessible from each cluster.\nCreating Storage Classes To provision replicated volumes, you need to create adequately configured storage classes on both the source and target clusters.\nA pair of storage classes on the source, and target clusters would be essentially mirrored copies of one another. You can create them manually or with the help of repctl.\nManual Storage Class Creation You can find a sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon-replication provisioner: csi-isilon.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"isilon-replication\" replication.storage.dell.com/remoteClusterID: \"target\" replication.storage.dell.com/remoteSystem: \"cluster-2\" replication.storage.dell.com/remoteAccessZone: System replication.storage.dell.com/remoteAzServiceIP: 192.168.1.2 replication.storage.dell.com/remoteRootClientEnabled: \"false\" replication.storage.dell.com/rpo: Five_Minutes replication.storage.dell.com/ignoreNamespaces: \"false\" replication.storage.dell.com/volumeGroupPrefix: \"csi\" AccessZone: System AzServiceIP: 192.168.1.1 IsiPath: /ifs/data/csi RootClientEnabled: \"false\" ClusterName: cluster-1 Let’s go through each parameter and what it means:\nreplication.storage.dell.com/isReplicationEnabled if set to true, will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents the ID of a remote cluster. It is the same ID you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system that should match whatever clusterName you called it in isilon-creds secret. replication.storage.dell.com/remoteAccessZone is the name of the access zone a remote volume can be created in. replication.storage.dell.com/remoteAzServiceIP AccessZone groupnet service IP. It is optional and can be provided if different than the remote system endpoint. replication.storage.dell.com/remoteRootClientEnabled determines whether the driver should enable root squashing or not for the remote volume. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. NOTE: Available RPO values “Five_Minutes”, “Fifteen_Minutes”, “Thirty_Minutes”, “One_Hour”, “Six_Hours”, “Twelve_Hours”, “One_Day”\nreplication.storage.dell.com/ignoreNamespaces, if set to true PowerScale driver, it will ignore in what namespace volumes are created and put every volume created using this storage class into a single volume group. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate them. It is important to not use the same prefix for different kubernetes clusters, otherwise any action on a replication group in one kubernetes cluster will impact the other. NOTE: To configure the VolumeGroupPrefix, the name format of '\u003cvolumeGroupPrefix\u003e-\u003cnamespace\u003e-\u003cSystem IP Address OR FQDN\u003e-\u003crpo\u003e' cannot be more than 63 characters.\nAccesszone is the name of the access zone a volume can be created in. AzServiceIP AccessZone groupnet service IP. It is optional and can be provided if different than the PowerScale cluster endpoint. IsiPath is the base path for the volumes to be created on the PowerScale cluster. If not specified in the storage class, the IsiPath defined in the storage array’s secret will be used. If that is not specified, the IsiPath defined in the values.yaml file used for driver installation is used as the lowest-priority. IsiPath between source and target Replication Groups must be consistent. RootClientEnabled determines whether the driver should enable root squashing or not. ClusterName name of PowerScale cluster, where PV will be provisioned, specified as it was listed in isilon-creds secret. After creating storage class YAML files, they must be applied to your Kubernetes clusters with kubectl.\nStorage Class creation with repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example here, copy it, and modify it to your needs.\nIf you open this example you can see similar fields and parameters to what was seen in manual storage class creation.\nLet’s use the same example from manual installation and see what its repctl config file would look like:\nsourceClusterID: \"source\" targetClusterID: \"target\" name: \"isilon-replication\" driver: \"isilon\" reclaimPolicy: \"Delete\" replicationPrefix: \"replication.storage.dell.com\" remoteRetentionPolicy: RG: \"Delete\" PV: \"Delete\" parameters: rpo: \"Five_Minutes\" ignoreNamespaces: \"false\" volumeGroupPrefix: \"csi\" isiPath: \"/ifs/data/csi\" clusterName: source: \"cluster-1\" target: \"cluster-2\" rootClientEnabled: source: \"false\" target: \"false\" accessZone: source: \"System\" target: \"System\" azServiceIP: source: \"192.168.1.1\" target: \"192.168.1.2\" After preparing the config, you can apply it to both clusters with repctl. Before you do this, ensure you’ve added your clusters to repctl via the add command.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes, you are good to create volumes using the newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication-enabled Storage Classes. The CSI PowerScale driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerScale driver supports the following list of replication actions:\nFAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL FAILBACK_LOCAL ACTION_FAILBACK_DISCARD_CHANGES_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC ","categories":"","description":"Enabling Replication feature for CSI PowerScale\n","excerpt":"Enabling Replication feature for CSI PowerScale\n","ref":"/csm-docs/v3/replication/deployment/powerscale/","tags":"","title":"PowerScale"},{"body":"Enabling Replication In CSI PowerStore Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerStore supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Be sure to configure replication between multiple PowerStore instances using instructions provided by PowerStore storage.\nYou can ensure that you configured remote systems by navigating to the Protection tab and choosing Remote System in UI of your PowerStore instance.\nYou should see a list of remote systems with both Management State and Data Connection fields set to OK.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\nCheck controller pods: kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING. Check that controller config map is properly populated: kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by list of target clusters IDs. If you don’t have something installed or something is out-of-place, please refer to installation instructions here.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set Helm parameter controller.replication.enabled in your copy of example values.yaml file (usually called my-powerstore-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like:\n... # controller: configure controller specific parameters controller: ... # replication: allows to configure replication replication: enabled: true replicationContextPrefix: \"powerstore\" replicationPrefix: \"replication.storage.dell.com\" ... You can leave other parameters like replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerStore following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\nNOTE: You need to install your driver on all clusters where you want to use replication. Both arrays must be accessible from each cluster.\nCreating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nA pair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"powerstore-replication\" provisioner: \"csi-powerstore.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"powerstore-replication\" replication.storage.dell.com/remoteClusterID: \"tgt-cluster-id\" replication.storage.dell.com/remoteSystem: \"RT-0000\" replication.storage.dell.com/rpo: Five_Minutes replication.storage.dell.com/ignoreNamespaces: \"false\" replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"Unique\" Let’s go through each parameter and what it means:\nreplication.storage.dell.com/isReplicationEnabled if set to true will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents ID of a remote Kubernetes cluster. It is the same ID you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system as seen from the current PowerStore instance. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. replication.storage.dell.com/ignoreNamespaces, if set to true PowerStore driver, it will ignore in what namespace volumes are created and put every volume created using this storage class into a single volume group. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate them. It is important to not use the same prefix for different kubernetes clusters, otherwise any action on a replication group in one kubernetes cluster will impact the other. NOTE: To configure the VolumeGroupPrefix, the name format of '\u003cvolumeGroupPrefix\u003e-\u003cnamespace\u003e-\u003cCluster Name\u003e-\u003crpo\u003e’ cannot be more than 63 characters.\narrayID is a unique identifier of the storage array you specified in array connection secret. Let’s follow up that with an example. Let’s assume you have two Kubernetes clusters and two PowerStore storage arrays:\nClusters have IDs of cluster-1 and cluster-2 Storage arrays connected between each other and show up as remote systems with names RT-0001 and RT-0002 Cluster cluster-1 connected to array RT-0001 Cluster cluster-2 connected to array RT-0002 Storage array RT-0001 has a unique ID of PS000000001 Storage array RT-0002 has a unique ID of PS000000002 And this is what our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"powerstore-replication\" provisioner: \"csi-powerstore.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"powerstore-replication\" replication.storage.dell.com/remoteClusterID: \"cluster-2\" replication.storage.dell.com/remoteSystem: \"RT-0002\" replication.storage.dell.com/rpo: Five_Minutes replication.storage.dell.com/ignoreNamespaces: \"false\" replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"PS000000001\" StorageClass to be created in cluster-2:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"powerstore-replication\" provisioner: \"csi-powerstore.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"powerstore-replication\" replication.storage.dell.com/remoteClusterID: \"cluster-1\" replication.storage.dell.com/remoteSystem: \"RT-0001\" replication.storage.dell.com/rpo: Five_Minutes replication.storage.dell.com/ignoreNamespaces: \"false\" replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"PS000000002\" After creating storage class YAML files, they must be applied to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill the config with necessary information. You can find an example in here, copy it, and modify it to your needs.\nIf you open this example you can see similar fields and parameters to what was seen in manual storage class creation.\nLet’s use the same example from manual installation and see what its repctl config file would look like:\nsourceClusterID: \"cluster-1\" targetClusterID: \"cluster-2\" name: \"powerstore-replication\" driver: \"powerstore\" reclaimPolicy: \"Retain\" replicationPrefix: \"replication.storage.dell.com\" parameters: arrayID: source: \"PS000000001\" target: \"PS000000002\" remoteSystem: source: \"RT-0002\" target: \"RT-0001\" rpo: \"Five_Minutes\" ignoreNamespaces: \"false\" volumeGroupPrefix: \"csi\" After preparing the config you can apply it to both clusters with repctl. Just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes will be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using the newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerStore driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerStore driver supports the following list of replication actions:\nFAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC ","categories":"","description":"Enabling Replication feature for CSI PowerStore\n","excerpt":"Enabling Replication feature for CSI PowerStore\n","ref":"/csm-docs/docs/replication/deployment/powerstore/","tags":"","title":"PowerStore"},{"body":"Enabling Replication In CSI PowerStore Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerStore supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Be sure to configure replication between multiple PowerStore instances using instructions provided by PowerStore storage.\nYou can ensure that you configured remote systems by navigating to the Protection tab and choosing Remote System in UI of your PowerStore instance.\nYou should see a list of remote systems with both Management State and Data Connection fields set to OK.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\nCheck controller pods: kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING. Check that controller config map is properly populated: kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by list of target clusters IDs. If you don’t have something installed or something is out-of-place, please refer to installation instructions here.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set Helm parameter controller.replication.enabled in your copy of example values.yaml file (usually called my-powerstore-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like:\n... # controller: configure controller specific parameters controller: ... # replication: allows to configure replication replication: enabled: true image: dellemc/dell-csi-replicator:v1.6.0 replicationContextPrefix: \"powerstore\" replicationPrefix: \"replication.storage.dell.com\" ... You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerStore following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\nNOTE: You need to install your driver on all clusters where you want to use replication. Both arrays must be accessible from each cluster.\nCreating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nA pair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"powerstore-replication\" provisioner: \"csi-powerstore.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"powerstore-replication\" replication.storage.dell.com/remoteClusterID: \"tgt-cluster-id\" replication.storage.dell.com/remoteSystem: \"RT-0000\" replication.storage.dell.com/rpo: Five_Minutes replication.storage.dell.com/ignoreNamespaces: \"false\" replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"Unique\" Let’s go through each parameter and what it means:\nreplication.storage.dell.com/isReplicationEnabled if set to true will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents ID of a remote Kubernetes cluster. It is the same ID you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system as seen from the current PowerStore instance. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. replication.storage.dell.com/ignoreNamespaces, if set to true PowerStore driver, it will ignore in what namespace volumes are created and put every volume created using this storage class into a single volume group. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate them. It is important to not use the same prefix for different kubernetes clusters, otherwise any action on a replication group in one kubernetes cluster will impact the other. NOTE: To configure the VolumeGroupPrefix, the name format of '\u003cvolumeGroupPrefix\u003e-\u003cnamespace\u003e-\u003cCluster Name\u003e-\u003crpo\u003e’ cannot be more than 63 characters.\narrayID is a unique identifier of the storage array you specified in array connection secret. Let’s follow up that with an example. Let’s assume you have two Kubernetes clusters and two PowerStore storage arrays:\nClusters have IDs of cluster-1 and cluster-2 Storage arrays connected between each other and show up as remote systems with names RT-0001 and RT-0002 Cluster cluster-1 connected to array RT-0001 Cluster cluster-2 connected to array RT-0002 Storage array RT-0001 has a unique ID of PS000000001 Storage array RT-0002 has a unique ID of PS000000002 And this is what our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"powerstore-replication\" provisioner: \"csi-powerstore.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"powerstore-replication\" replication.storage.dell.com/remoteClusterID: \"cluster-2\" replication.storage.dell.com/remoteSystem: \"RT-0002\" replication.storage.dell.com/rpo: Five_Minutes replication.storage.dell.com/ignoreNamespaces: \"false\" replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"PS000000001\" StorageClass to be created in cluster-2:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"powerstore-replication\" provisioner: \"csi-powerstore.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"powerstore-replication\" replication.storage.dell.com/remoteClusterID: \"cluster-1\" replication.storage.dell.com/remoteSystem: \"RT-0001\" replication.storage.dell.com/rpo: Five_Minutes replication.storage.dell.com/ignoreNamespaces: \"false\" replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"PS000000002\" After creating storage class YAML files, they must be applied to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill the config with necessary information. You can find an example in here, copy it, and modify it to your needs.\nIf you open this example you can see similar fields and parameters to what was seen in manual storage class creation.\nLet’s use the same example from manual installation and see what its repctl config file would look like:\nsourceClusterID: \"cluster-1\" targetClusterID: \"cluster-2\" name: \"powerstore-replication\" driver: \"powerstore\" reclaimPolicy: \"Retain\" replicationPrefix: \"replication.storage.dell.com\" parameters: arrayID: source: \"PS000000001\" target: \"PS000000002\" remoteSystem: source: \"RT-0002\" target: \"RT-0001\" rpo: \"Five_Minutes\" ignoreNamespaces: \"false\" volumeGroupPrefix: \"csi\" After preparing the config you can apply it to both clusters with repctl. Just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes will be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using the newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerStore driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerStore driver supports the following list of replication actions:\nFAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC ","categories":"","description":"Enabling Replication feature for CSI PowerStore\n","excerpt":"Enabling Replication feature for CSI PowerStore\n","ref":"/csm-docs/v1/replication/deployment/powerstore/","tags":"","title":"PowerStore"},{"body":"Enabling Replication In CSI PowerStore Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerStore supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Be sure to configure replication between multiple PowerStore instances using instructions provided by PowerStore storage.\nYou can ensure that you configured remote systems by navigating to the Protection tab and choosing Remote System in UI of your PowerStore instance.\nYou should see a list of remote systems with both Management State and Data Connection fields set to OK.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\nCheck controller pods: kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING. Check that controller config map is properly populated: kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by list of target clusters IDs. If you don’t have something installed or something is out-of-place, please refer to installation instructions here.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set Helm parameter controller.replication.enabled in your copy of example values.yaml file (usually called my-powerstore-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like:\n... # controller: configure controller specific parameters controller: ... # replication: allows to configure replication replication: enabled: true image: dellemc/dell-csi-replicator:v1.0.0 replicationContextPrefix: \"powerstore\" replicationPrefix: \"replication.storage.dell.com\" ... You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerStore following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\nNOTE: you need to install your driver at least on the source cluster, but it is recommended to install drivers on all clusters you will use for replication.\nCreating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nA pair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"powerstore-replication\" provisioner: \"csi-powerstore.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"powerstore-replication\" replication.storage.dell.com/remoteClusterID: \"tgt-cluster-id\" replication.storage.dell.com/remoteSystem: \"RT-0000\" replication.storage.dell.com/rpo: Five_Minutes replication.storage.dell.com/ignoreNamespaces: \"false\" replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"Unique\" Let’s go through each parameter and what it means:\nreplication.storage.dell.com/isReplicationEnabled if set to true will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents ID of a remote cluster. It is the same ID you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system as seen from the current PowerStore instance. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. replication.storage.dell.com/ignoreNamespaces, if set to true PowerStore driver, it will ignore in what namespace volumes are created and put every volume created using this storage class into a single volume group. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate them. It is important to not use the same prefix for different kubernetes clusters, otherwise any action on a replication group in one kubernetes cluster will impact the other. NOTE: To configure the VolumeGroupPrefix, the name format of '\u003cvolumeGroupPrefix\u003e-\u003cnamespace\u003e-\u003cCluster Name\u003e-\u003crpo\u003e’ cannot be more than 63 characters.\narrayID is a unique identifier of the storage array you specified in array connection secret. Let’s follow up that with an example. Let’s assume you have two Kubernetes clusters and two PowerStore storage arrays:\nClusters have IDs of cluster-1 and cluster-2 Storage arrays connected between each other and show up as remote systems with names RT-0001 and RT-0002 Cluster cluster-1 connected to array RT-0001 Cluster cluster-2 connected to array RT-0002 Storage array RT-0001 has a unique ID of PS000000001 Storage array RT-0002 has a unique ID of PS000000002 And this is what our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"powerstore-replication\" provisioner: \"csi-powerstore.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"powerstore-replication\" replication.storage.dell.com/remoteClusterID: \"cluster-2\" replication.storage.dell.com/remoteSystem: \"RT-0002\" replication.storage.dell.com/rpo: Five_Minutes replication.storage.dell.com/ignoreNamespaces: \"false\" replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"PS000000001\" StorageClass to be created in cluster-2:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"powerstore-replication\" provisioner: \"csi-powerstore.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"powerstore-replication\" replication.storage.dell.com/remoteClusterID: \"cluster-1\" replication.storage.dell.com/remoteSystem: \"RT-0001\" replication.storage.dell.com/rpo: Five_Minutes replication.storage.dell.com/ignoreNamespaces: \"false\" replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"PS000000002\" After creating storage class YAML files, they must be applied to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill the config with necessary information. You can find an example in here, copy it, and modify it to your needs.\nIf you open this example you can see similar fields and parameters to what was seen in manual storage class creation.\nLet’s use the same example from manual installation and see what its repctl config file would look like:\nsourceClusterID: \"cluster-1\" targetClusterID: \"cluster-2\" name: \"powerstore-replication\" driver: \"powerstore\" reclaimPolicy: \"Retain\" replicationPrefix: \"replication.storage.dell.com\" parameters: arrayID: source: \"PS000000001\" target: \"PS000000002\" remoteSystem: source: \"RT-0002\" target: \"RT-0001\" rpo: \"Five_Minutes\" ignoreNamespaces: \"false\" volumeGroupPrefix: \"csi\" After preparing the config you can apply it to both clusters with repctl. Just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes will be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using the newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerStore driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerStore driver supports the following list of replication actions:\nFAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC ","categories":"","description":"Enabling Replication feature for CSI PowerStore\n","excerpt":"Enabling Replication feature for CSI PowerStore\n","ref":"/csm-docs/v2/replication/deployment/powerstore/","tags":"","title":"PowerStore"},{"body":"Enabling Replication In CSI PowerStore Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerStore supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Be sure to configure replication between multiple PowerStore instances using instructions provided by PowerStore storage.\nYou can ensure that you configured remote systems by navigating to the Protection tab and choosing Remote System in UI of your PowerStore instance.\nYou should see a list of remote systems with both Management State and Data Connection fields set to OK.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\nCheck controller pods: kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING. Check that controller config map is properly populated: kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by list of target clusters IDs. If you don’t have something installed or something is out-of-place, please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set Helm parameter controller.replication.enabled in your copy of example values.yaml file (usually called my-powerstore-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like:\n... # controller: configure controller specific parameters controller: ... # replication: allows to configure replication replication: enabled: true image: dellemc/dell-csi-replicator:v1.0.0 replicationContextPrefix: \"powerstore\" replicationPrefix: \"replication.storage.dell.com\" ... You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerStore following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\nNOTE: you need to install your driver at least on the source cluster, but it is recommended to install drivers on all clusters you will use for replication.\nCreating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nA pair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"powerstore-replication\" provisioner: \"csi-powerstore.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"powerstore-replication\" replication.storage.dell.com/remoteClusterID: \"tgt-cluster-id\" replication.storage.dell.com/remoteSystem: \"RT-0000\" replication.storage.dell.com/rpo: Five_Minutes replication.storage.dell.com/ignoreNamespaces: \"false\" replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"Unique\" Let’s go through each parameter and what it means:\nreplication.storage.dell.com/isReplicationEnabled if set to true will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents ID of a remote cluster. It is the same ID you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system as seen from the current PowerStore instance. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. replication.storage.dell.com/ignoreNamespaces, if set to true PowerStore driver, it will ignore in what namespace volumes are created and put every volume created using this storage class into a single volume group. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate them. It is important to not use the same prefix for different kubernetes clusters, otherwise any action on a replication group in one kubernetes cluster will impact the other. NOTE: To configure the VolumeGroupPrefix, the name format of '\u003cvolumeGroupPrefix\u003e-\u003cnamespace\u003e-\u003cCluster Name\u003e-\u003crpo\u003e’ cannot be more than 63 characters.\narrayID is a unique identifier of the storage array you specified in array connection secret. Let’s follow up that with an example. Let’s assume you have two Kubernetes clusters and two PowerStore storage arrays:\nClusters have IDs of cluster-1 and cluster-2 Storage arrays connected between each other and show up as remote systems with names RT-0001 and RT-0002 Cluster cluster-1 connected to array RT-0001 Cluster cluster-2 connected to array RT-0002 Storage array RT-0001 has a unique ID of PS000000001 Storage array RT-0002 has a unique ID of PS000000002 And this is what our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"powerstore-replication\" provisioner: \"csi-powerstore.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"powerstore-replication\" replication.storage.dell.com/remoteClusterID: \"cluster-2\" replication.storage.dell.com/remoteSystem: \"RT-0002\" replication.storage.dell.com/rpo: Five_Minutes replication.storage.dell.com/ignoreNamespaces: \"false\" replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"PS000000001\" StorageClass to be created in cluster-2:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \"powerstore-replication\" provisioner: \"csi-powerstore.dellemc.com\" reclaimPolicy: Retain volumeBindingMode: Immediate parameters: replication.storage.dell.com/isReplicationEnabled: \"true\" replication.storage.dell.com/remoteStorageClassName: \"powerstore-replication\" replication.storage.dell.com/remoteClusterID: \"cluster-1\" replication.storage.dell.com/remoteSystem: \"RT-0001\" replication.storage.dell.com/rpo: Five_Minutes replication.storage.dell.com/ignoreNamespaces: \"false\" replication.storage.dell.com/volumeGroupPrefix: \"csi\" arrayID: \"PS000000002\" After creating storage class YAML files, they must be applied to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill the config with necessary information. You can find an example in here, copy it, and modify it to your needs.\nIf you open this example you can see similar fields and parameters to what was seen in manual storage class creation.\nLet’s use the same example from manual installation and see what its repctl config file would look like:\nsourceClusterID: \"cluster-1\" targetClusterID: \"cluster-2\" name: \"powerstore-replication\" driver: \"powerstore\" reclaimPolicy: \"Retain\" replicationPrefix: \"replication.storage.dell.com\" parameters: arrayID: source: \"PS000000001\" target: \"PS000000002\" remoteSystem: source: \"RT-0002\" target: \"RT-0001\" rpo: \"Five_Minutes\" ignoreNamespaces: \"false\" volumeGroupPrefix: \"csi\" After preparing the config you can apply it to both clusters with repctl. Just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes will be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using the newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerStore driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerStore driver supports the following list of replication actions:\nFAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC ","categories":"","description":"Enabling Replication feature for CSI PowerStore\n","excerpt":"Enabling Replication feature for CSI PowerStore\n","ref":"/csm-docs/v3/replication/deployment/powerstore/","tags":"","title":"PowerStore"},{"body":"repctl repctl is a command-line client for configuring replication and managing replicated resources between multiple Kubernetes clusters.\nUsage Managing Clusters To begin managing replication with repctl you need to add your Kubernetes clusters, you can do that using cluster add command:\n./repctl cluster add -f \u003cconfig-file\u003e -n \u003cname\u003e You can view clusters that are currently being managed by repctl by running cluster get command:\n./repctl cluster get Or, alternatively, using get cluster command:\n./repctl get cluster Also, you can inject information about all of your current clusters as config maps into the same clusters, so it can be used by dell-csi-replicator:\n./repctl cluster inject You can also generate kubeconfigs from existing replication service accounts and inject them in config maps by providing --use-sa flag:\n./repctl cluster inject --use-sa Querying Resources After adding clusters you want to manage with repctl you can query resources from multiple clusters at once using get command.\nFor example, this command will list all storage classes in all clusters that currently are being managed by repctl:\n./repctl get storageclasses --all If you want to query some particular clusters you can do that by specifying with the clusters flag:\n./repctl get pv --clusters cluster-1,cluster-3 All other different flags for querying resources you can check using included into the tool help flag -h.\nCreating Resources Generic Generic create command allows you to apply provided config file into multiple clusters at once:\n/repctl create -f \u003cpath-to-file\u003e PersistentVolumeClaims You can use repctl to create PVCs from Replication Group’s PVs on the target cluster:\n./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false By default, ‘create pvc’ will do a ‘dry-run’ while creating PVCs. If you don’t encounter any issues in the dry-run, then you can re-run the command by turning off the dry-run flag to false.\nStorage Classes repctl can create special replication enabled storage classes from provided config, you can find example configs in examples folder. The command would look similar to below:\n./repctl create sc --from-config \u003cconfig-file\u003e` Single Cluster Replication repctl supports working with replication within a single Kubernetes cluster.\nJust add cluster you want to use with cluster add command, and you can list, filter, and create resources.\nVolumes and ReplicationGroups created as “target” resources would be prefixed with replicated- so you can easily differentiate them.\nYou can also differentiate between single cluster replication configured StorageClasses and ReplicationGroups and multi-cluster ones by checking remoteClusterID field, for a single cluster the field would be set to self.\nTo create replication enabled storage classes for single cluster replication using create sc command, be sure to set both sourceClusterID and targetClusterID to the same clusterID and continue as usual with executing the command. The name of the StorageClass resource that is created as the “target” will be appended with -tgt.\nExecuting Actions repctl can be used to execute various replication actions on ReplicationGroups.\nFailover This command will perform a planned failover to a cluster or an RG.\nWhen working with multiple clusters, you can perform failover by specifying the target cluster ID. To do that, use --target \u003ctargetClusterID\u003e parameter:\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform failover by specifying the target replication group ID. To do that, use --target \u003crg-id\u003e parameter:\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-rg-id\u003e In both scenarios, repctl will patch the CR at the source site with action FAILOVER_REMOTE.\nYou can also provide --unplanned parameter, then repctl will perform an unplanned failover to a given cluster or an RG. Instead of FAILOVER_REMOTE on the source cluster’s CR, repctl will patch CR at target cluster with action UNPLANNED_FAILOVER_LOCAL.\nReprotect This command will perform a reprotect at the specified cluster or the RG.\nWhen working with multiple clusters, you can perform reprotect by specifying the cluster ID. To do that, use --at \u003cclusterID\u003e parameter:\n./repctl --rg \u003crg-id\u003e reprotect --at \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform reprotect by specifying the replication group ID. To do that, use --rg \u003crg-id\u003e parameter:\n./repctl --rg \u003crg-id\u003e reprotect In both scenarios repctl will patch the CR at the source site with action REPROTECT_LOCAL.\nFailback This command will perform a planned failback to a cluster or an RG.\nWhen working with multiple clusters, you can perform failback by specifying the cluster ID. To do that, use --target \u003cclusterID\u003e parameter:\n./repctl --rg \u003crg-id\u003e failback --target \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform failback by specifying the replication group ID. To do that, use --target \u003crg-id\u003e parameter:\n./repctl --rg \u003crg-id\u003e failback --target \u003ctgt-rg-id\u003e In both scenarios, repctl will patch the CR at the source site with action FAILBACK_LOCAL.\nYou can also provide --discard parameter, then repctl will perform a failback but discard any writes at target, instead of FAILBACK_LOCAL repctl will patch CR at target cluster with action ACTION_FAILBACK_DISCARD_CHANGES_LOCAL.\nSwap This command will perform a swap at a specified cluster or an RG.\nWhen working with multiple clusters, you can perform swap by specifying the cluster ID. To do that, use --at \u003cclusterID\u003e parameter:\n./repctl --rg \u003crg-id\u003e swap --at \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform swap by specifying the replication group ID. To do that, use --rg \u003crg-id\u003e parameter:\n./repctl --rg \u003crg-id\u003e swap repctl will patch CR at the source cluster with action SWAP_LOCAL.\nWait For Completion When executing actions you can provide --wait argument to make repctl wait for completion of specified action.\nFor example when executing failover:\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-cluster-id\u003e --wait Maintenance Actions You can also use exec command to execute maintenance actions such as suspend, resume, and sync.\nFor single or multi-cluster config:\n./repctl --rg \u003crg-id\u003e exec -a \u003cACTION\u003e Where \u003cACTION\u003e can be one of the following:\nsuspend will suspend replication, changes will no longer be synced between replication sites. resume will resume replication, canceling the effect of suspend action. sync will force synchronization of change between replication sites. ","categories":"","description":"repctl tool for Replication feature in detail\n","excerpt":"repctl tool for Replication feature in detail\n","ref":"/csm-docs/docs/replication/tools/","tags":"","title":"Tools"},{"body":"repctl repctl is a command-line client for configuring replication and managing replicated resources between multiple Kubernetes clusters.\nUsage Managing Clusters To begin managing replication with repctl you need to add your Kubernetes clusters, you can do that using cluster add command:\n./repctl cluster add -f \u003cconfig-file\u003e -n \u003cname\u003e You can view clusters that are currently being managed by repctl by running cluster get command:\n./repctl cluster get Or, alternatively, using get cluster command:\n./repctl get cluster Also, you can inject information about all of your current clusters as config maps into the same clusters, so it can be used by dell-csi-replicator:\n./repctl cluster inject You can also generate kubeconfigs from existing replication service accounts and inject them in config maps by providing --use-sa flag:\n./repctl cluster inject --use-sa Querying Resources After adding clusters you want to manage with repctl you can query resources from multiple clusters at once using get command.\nFor example, this command will list all storage classes in all clusters that currently are being managed by repctl:\n./repctl get storageclasses --all If you want to query some particular clusters you can do that by specifying with the clusters flag:\n./repctl get pv --clusters cluster-1,cluster-3 All other different flags for querying resources you can check using included into the tool help flag -h.\nCreating Resources Generic Generic create command allows you to apply provided config file into multiple clusters at once:\n/repctl create -f \u003cpath-to-file\u003e PersistentVolumeClaims You can use repctl to create PVCs from Replication Group’s PVs on the target cluster:\n./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false By default, ‘create pvc’ will do a ‘dry-run’ while creating PVCs. If you don’t encounter any issues in the dry-run, then you can re-run the command by turning off the dry-run flag to false.\nStorage Classes repctl can create special replication enabled storage classes from provided config, you can find example configs in examples folder. The command would look similar to below:\n./repctl create sc --from-config \u003cconfig-file\u003e` Single Cluster Replication repctl supports working with replication within a single Kubernetes cluster.\nJust add cluster you want to use with cluster add command, and you can list, filter, and create resources.\nVolumes and ReplicationGroups created as “target” resources would be prefixed with replicated- so you can easily differentiate them.\nYou can also differentiate between single cluster replication configured StorageClasses and ReplicationGroups and multi-cluster ones by checking remoteClusterID field, for a single cluster the field would be set to self.\nTo create replication enabled storage classes for single cluster replication using create sc command, be sure to set both sourceClusterID and targetClusterID to the same clusterID and continue as usual with executing the command. The name of the StorageClass resource that is created as the “target” will be appended with -tgt.\nExecuting Actions repctl can be used to execute various replication actions on ReplicationGroups.\nFailover This command will perform a planned failover to a cluster or an RG.\nWhen working with multiple clusters, you can perform failover by specifying the target cluster ID. To do that, use --target \u003ctargetClusterID\u003e parameter:\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform failover by specifying the target replication group ID. To do that, use --target \u003crg-id\u003e parameter:\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-rg-id\u003e In both scenarios, repctl will patch the CR at the source site with action FAILOVER_REMOTE.\nYou can also provide --unplanned parameter, then repctl will perform an unplanned failover to a given cluster or an RG. Instead of FAILOVER_REMOTE on the source cluster’s CR, repctl will patch CR at target cluster with action UNPLANNED_FAILOVER_LOCAL.\nReprotect This command will perform a reprotect at the specified cluster or the RG.\nWhen working with multiple clusters, you can perform reprotect by specifying the cluster ID. To do that, use --at \u003cclusterID\u003e parameter:\n./repctl --rg \u003crg-id\u003e reprotect --at \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform reprotect by specifying the replication group ID. To do that, use --rg \u003crg-id\u003e parameter:\n./repctl --rg \u003crg-id\u003e reprotect In both scenarios repctl will patch the CR at the source site with action REPROTECT_LOCAL.\nFailback This command will perform a planned failback to a cluster or an RG.\nWhen working with multiple clusters, you can perform failback by specifying the cluster ID. To do that, use --target \u003cclusterID\u003e parameter:\n./repctl --rg \u003crg-id\u003e failback --target \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform failback by specifying the replication group ID. To do that, use --target \u003crg-id\u003e parameter:\n./repctl --rg \u003crg-id\u003e failback --target \u003ctgt-rg-id\u003e In both scenarios, repctl will patch the CR at the source site with action FAILBACK_LOCAL.\nYou can also provide --discard parameter, then repctl will perform a failback but discard any writes at target, instead of FAILBACK_LOCAL repctl will patch CR at target cluster with action ACTION_FAILBACK_DISCARD_CHANGES_LOCAL.\nSwap This command will perform a swap at a specified cluster or an RG.\nWhen working with multiple clusters, you can perform swap by specifying the cluster ID. To do that, use --at \u003cclusterID\u003e parameter:\n./repctl --rg \u003crg-id\u003e swap --at \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform swap by specifying the replication group ID. To do that, use --rg \u003crg-id\u003e parameter:\n./repctl --rg \u003crg-id\u003e swap repctl will patch CR at the source cluster with action SWAP_LOCAL.\nWait For Completion When executing actions you can provide --wait argument to make repctl wait for completion of specified action.\nFor example when executing failover:\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-cluster-id\u003e --wait Maintenance Actions You can also use exec command to execute maintenance actions such as suspend, resume, and sync.\nFor single or multi-cluster config:\n./repctl --rg \u003crg-id\u003e exec -a \u003cACTION\u003e Where \u003cACTION\u003e can be one of the following:\nsuspend will suspend replication, changes will no longer be synced between replication sites. resume will resume replication, canceling the effect of suspend action. sync will force synchronization of change between replication sites. ","categories":"","description":"repctl tool for Replication feature in detail\n","excerpt":"repctl tool for Replication feature in detail\n","ref":"/csm-docs/v1/replication/tools/","tags":"","title":"Tools"},{"body":"repctl repctl is a command-line client for configuring replication and managing replicated resources between multiple Kubernetes clusters.\nUsage Managing Clusters To begin managing replication with repctl you need to add your Kubernetes clusters, you can do that using cluster add command:\n./repctl cluster add -f \u003cconfig-file\u003e -n \u003cname\u003e You can view clusters that are currently being managed by repctl by running cluster get command:\n./repctl cluster get Or, alternatively, using get cluster command:\n./repctl get cluster Also, you can inject information about all of your current clusters as config maps into the same clusters, so it can be used by dell-csi-replicator:\n./repctl cluster inject You can also generate kubeconfigs from existing replication service accounts and inject them in config maps by providing --use-sa flag:\n./repctl cluster inject --use-sa Querying Resources After adding clusters you want to manage with repctl you can query resources from multiple clusters at once using get command.\nFor example, this command will list all storage classes in all clusters that currently are being managed by repctl:\n./repctl get storageclasses --all If you want to query some particular clusters you can do that by specifying with the clusters flag:\n./repctl get pv --clusters cluster-1,cluster-3 All other different flags for querying resources you can check using included into the tool help flag -h.\nCreating Resources Generic Generic create command allows you to apply provided config file into multiple clusters at once:\n/repctl create -f \u003cpath-to-file\u003e PersistentVolumeClaims You can use repctl to create PVCs from Replication Group’s PVs on the target cluster:\n./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false By default, ‘create pvc’ will do a ‘dry-run’ while creating PVCs. If you don’t encounter any issues in the dry-run, then you can re-run the command by turning off the dry-run flag to false.\nStorage Classes repctl can create special replication enabled storage classes from provided config, you can find example configs in examples folder. The command would look similar to below:\n./repctl create sc --from-config \u003cconfig-file\u003e` Single Cluster Replication repctl supports working with replication within a single Kubernetes cluster.\nJust add cluster you want to use with cluster add command, and you can list, filter, and create resources.\nVolumes and ReplicationGroups created as “target” resources would be prefixed with replicated- so you can easily differentiate them.\nYou can also differentiate between single cluster replication configured StorageClasses and ReplicationGroups and multi-cluster ones by checking remoteClusterID field, for a single cluster the field would be set to self.\nTo create replication enabled storage classes for single cluster replication using create sc command, be sure to set both sourceClusterID and targetClusterID to the same clusterID and continue as usual with executing the command. The name of the StorageClass resource that is created as the “target” will be appended with -tgt.\nExecuting Actions repctl can be used to execute various replication actions on ReplicationGroups.\nFailover This command will perform a planned failover to a cluster or an RG.\nWhen working with multiple clusters, you can perform failover by specifying the target cluster ID. To do that, use --target \u003ctargetClusterID\u003e parameter:\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform failover by specifying the target replication group ID. To do that, use --target \u003crg-id\u003e parameter:\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-rg-id\u003e In both scenarios, repctl will patch the CR at the source site with action FAILOVER_REMOTE.\nYou can also provide --unplanned parameter, then repctl will perform an unplanned failover to a given cluster or an RG. Instead of FAILOVER_REMOTE on the source cluster’s CR, repctl will patch CR at target cluster with action UNPLANNED_FAILOVER_LOCAL.\nReprotect This command will perform a reprotect at the specified cluster or the RG.\nWhen working with multiple clusters, you can perform reprotect by specifying the cluster ID. To do that, use --at \u003cclusterID\u003e parameter:\n./repctl --rg \u003crg-id\u003e reprotect --at \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform reprotect by specifying the replication group ID. To do that, use --rg \u003crg-id\u003e parameter:\n./repctl --rg \u003crg-id\u003e reprotect In both scenarios repctl will patch the CR at the source site with action REPROTECT_LOCAL.\nFailback This command will perform a planned failback to a cluster or an RG.\nWhen working with multiple clusters, you can perform failback by specifying the cluster ID. To do that, use --target \u003cclusterID\u003e parameter:\n./repctl --rg \u003crg-id\u003e failback --target \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform failback by specifying the replication group ID. To do that, use --target \u003crg-id\u003e parameter:\n./repctl --rg \u003crg-id\u003e failback --target \u003ctgt-rg-id\u003e In both scenarios, repctl will patch the CR at the source site with action FAILBACK_LOCAL.\nYou can also provide --discard parameter, then repctl will perform a failback but discard any writes at target, instead of FAILBACK_LOCAL repctl will patch CR at target cluster with action ACTION_FAILBACK_DISCARD_CHANGES_LOCAL.\nSwap This command will perform a swap at a specified cluster or an RG.\nWhen working with multiple clusters, you can perform swap by specifying the cluster ID. To do that, use --at \u003cclusterID\u003e parameter:\n./repctl --rg \u003crg-id\u003e swap --at \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform swap by specifying the replication group ID. To do that, use --rg \u003crg-id\u003e parameter:\n./repctl --rg \u003crg-id\u003e swap repctl will patch CR at the source cluster with action SWAP_LOCAL.\nWait For Completion When executing actions you can provide --wait argument to make repctl wait for completion of specified action.\nFor example when executing failover:\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-cluster-id\u003e --wait Maintenance Actions You can also use exec command to execute maintenance actions such as suspend, resume, and sync.\nFor single or multi-cluster config:\n./repctl --rg \u003crg-id\u003e exec -a \u003cACTION\u003e Where \u003cACTION\u003e can be one of the following:\nsuspend will suspend replication, changes will no longer be synced between replication sites. resume will resume replication, canceling the effect of suspend action. sync will force synchronization of change between replication sites. ","categories":"","description":"repctl tool for Replication feature in detail\n","excerpt":"repctl tool for Replication feature in detail\n","ref":"/csm-docs/v2/replication/tools/","tags":"","title":"Tools"},{"body":"repctl repctl is a command-line client for configuring replication and managing replicated resources between multiple Kubernetes clusters.\nUsage Managing Clusters To begin managing replication with repctl you need to add your Kubernetes clusters, you can do that using cluster add command:\n./repctl cluster add -f \u003cconfig-file\u003e -n \u003cname\u003e You can view clusters that are currently being managed by repctl by running cluster get command:\n./repctl cluster get Or, alternatively, using get cluster command:\n./repctl get cluster Also, you can inject information about all of your current clusters as config maps into the same clusters, so it can be used by dell-csi-replicator:\n./repctl cluster inject You can also generate kubeconfigs from existing replication service accounts and inject them in config maps by providing --use-sa flag:\n./repctl cluster inject --use-sa Querying Resources After adding clusters you want to manage with repctl you can query resources from multiple clusters at once using get command.\nFor example, this command will list all storage classes in all clusters that currently are being managed by repctl:\n./repctl get storageclasses --all If you want to query some particular clusters you can do that by specifying with the clusters flag:\n./repctl get pv --clusters cluster-1,cluster-3 All other different flags for querying resources you can check using included into the tool help flag -h.\nCreating Resources Generic Generic create command allows you to apply provided config file into multiple clusters at once:\n/repctl create -f \u003cpath-to-file\u003e PersistentVolumeClaims You can use repctl to create PVCs from Replication Group’s PVs on the target cluster:\n./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false By default, ‘create pvc’ will do a ‘dry-run’ while creating PVCs. If you don’t encounter any issues in the dry-run, then you can re-run the command by turning off the dry-run flag to false.\nStorage Classes repctl can create special replication enabled storage classes from provided config, you can find example configs in examples folder. The command would look similar to below:\n./repctl create sc --from-config \u003cconfig-file\u003e` Single Cluster Replication repctl supports working with replication within a single Kubernetes cluster.\nJust add cluster you want to use with cluster add command, and you can list, filter, and create resources.\nVolumes and ReplicationGroups created as “target” resources would be prefixed with replicated- so you can easily differentiate them.\nYou can also differentiate between single cluster replication configured StorageClasses and ReplicationGroups and multi-cluster ones by checking remoteClusterID field, for a single cluster the field would be set to self.\nTo create replication enabled storage classes for single cluster replication using create sc command, be sure to set both sourceClusterID and targetClusterID to the same clusterID and continue as usual with executing the command. The name of the StorageClass resource that is created as the “target” will be appended with -tgt.\nExecuting Actions repctl can be used to execute various replication actions on ReplicationGroups.\nFailover This command will perform a planned failover to a cluster or an RG.\nWhen working with multiple clusters, you can perform failover by specifying the target cluster ID. To do that, use --target \u003ctargetClusterID\u003e parameter:\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform failover by specifying the target replication group ID. To do that, use --target \u003crg-id\u003e parameter:\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-rg-id\u003e In both scenarios, repctl will patch the CR at the source site with action FAILOVER_REMOTE.\nYou can also provide --unplanned parameter, then repctl will perform an unplanned failover to a given cluster or an RG. Instead of FAILOVER_REMOTE on the source cluster’s CR, repctl will patch CR at target cluster with action UNPLANNED_FAILOVER_LOCAL.\nReprotect This command will perform a reprotect at the specified cluster or the RG.\nWhen working with multiple clusters, you can perform reprotect by specifying the cluster ID. To do that, use --at \u003cclusterID\u003e parameter:\n./repctl --rg \u003crg-id\u003e reprotect --at \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform reprotect by specifying the replication group ID. To do that, use --rg \u003crg-id\u003e parameter:\n./repctl --rg \u003crg-id\u003e reprotect In both scenarios repctl will patch the CR at the source site with action REPROTECT_LOCAL.\nFailback This command will perform a planned failback to a cluster or an RG.\nWhen working with multiple clusters, you can perform failback by specifying the cluster ID. To do that, use --target \u003cclusterID\u003e parameter:\n./repctl --rg \u003crg-id\u003e failback --target \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform failback by specifying the replication group ID. To do that, use --target \u003crg-id\u003e parameter:\n./repctl --rg \u003crg-id\u003e failback --target \u003ctgt-rg-id\u003e In both scenarios, repctl will patch the CR at the source site with action FAILBACK_LOCAL.\nYou can also provide --discard parameter, then repctl will perform a failback but discard any writes at target, instead of FAILBACK_LOCAL repctl will patch CR at target cluster with action ACTION_FAILBACK_DISCARD_CHANGES_LOCAL.\nSwap This command will perform a swap at a specified cluster or an RG.\nWhen working with multiple clusters, you can perform swap by specifying the cluster ID. To do that, use --at \u003cclusterID\u003e parameter:\n./repctl --rg \u003crg-id\u003e swap --at \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform swap by specifying the replication group ID. To do that, use --rg \u003crg-id\u003e parameter:\n./repctl --rg \u003crg-id\u003e swap repctl will patch CR at the source cluster with action SWAP_LOCAL.\nWait For Completion When executing actions you can provide --wait argument to make repctl wait for completion of specified action.\nFor example when executing failover:\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-cluster-id\u003e --wait Maintenance Actions You can also use exec command to execute maintenance actions such as suspend, resume, and sync.\nFor single or multi-cluster config:\n./repctl --rg \u003crg-id\u003e exec -a \u003cACTION\u003e Where \u003cACTION\u003e can be one of the following:\nsuspend will suspend replication, changes will no longer be synced between replication sites. resume will resume replication, canceling the effect of suspend action. sync will force synchronization of change between replication sites. ","categories":"","description":"repctl tool for Replication feature in detail\n","excerpt":"repctl tool for Replication feature in detail\n","ref":"/csm-docs/v3/replication/tools/","tags":"","title":"Tools"},{"body":"Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class Note: From v1.7, the CSI driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the /samples/volumesnapshotclass folder under respective drivers.\nCreating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 spec: volumeSnapshotClassName: csm-snapclass source: persistentVolumeClaimName: pvol0 After the VolumeSnapshot has been successfully created by the CSI driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nNote: VolumeSnapshots can be listed using the command kubectl get volumesnapshot -n \u003cnamespace\u003e\n(Optional) Volume Snapshot Requirements Applicable only if you decide to enable the snapshot feature in values.yaml.\nsnapshot: enabled: true Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. For installation, use v6.2.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers to support Volume snapshots.\nA common snapshot controller A CSI external-snapshotter sidecar The common snapshot controller must be installed only once in the cluster, irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v6.2.x\nNOTE:\nThe manifests available on GitHub install the snapshotter image: quay.io/k8scsi/csi-snapshotter:v4.0.x The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration. Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-6.2 kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - NOTE:\nIt is recommended to use the 6.2.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration. ","categories":"","description":"Snapshot module of Dell CSI drivers\n","excerpt":"Snapshot module of Dell CSI drivers\n","ref":"/csm-docs/docs/snapshots/","tags":"","title":"Snapshots"},{"body":"Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class Note: From v1.7, the CSI driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the /samples/volumesnapshotclass folder under respective drivers.\nCreating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 spec: volumeSnapshotClassName: csm-snapclass source: persistentVolumeClaimName: pvol0 After the VolumeSnapshot has been successfully created by the CSI driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nNote: VolumeSnapshots can be listed using the command kubectl get volumesnapshot -n \u003cnamespace\u003e\n(Optional) Volume Snapshot Requirements Applicable only if you decide to enable the snapshot feature in values.yaml.\nsnapshot: enabled: true Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. For installation, use v6.2.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers to support Volume snapshots.\nA common snapshot controller A CSI external-snapshotter sidecar The common snapshot controller must be installed only once in the cluster, irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v6.2.x\nNOTE:\nThe manifests available on GitHub install the snapshotter image: quay.io/k8scsi/csi-snapshotter:v4.0.x The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration. Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-6.2 kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - NOTE:\nIt is recommended to use the 6.2.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration. ","categories":"","description":"Snapshot module of Dell CSI drivers\n","excerpt":"Snapshot module of Dell CSI drivers\n","ref":"/csm-docs/v1/snapshots/","tags":"","title":"Snapshots"},{"body":"Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class Note: From v1.7, the CSI driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the /samples/volumesnapshotclass folder under respective drivers.\nCreating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 spec: volumeSnapshotClassName: csm-snapclass source: persistentVolumeClaimName: pvol0 After the VolumeSnapshot has been successfully created by the CSI driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nNote: VolumeSnapshots can be listed using the command kubectl get volumesnapshot -n \u003cnamespace\u003e\n(Optional) Volume Snapshot Requirements Applicable only if you decide to enable the snapshot feature in values.yaml.\nsnapshot: enabled: true Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. For installation, use v6.2.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers to support Volume snapshots.\nA common snapshot controller A CSI external-snapshotter sidecar The common snapshot controller must be installed only once in the cluster, irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v6.2.x\nNOTE:\nThe manifests available on GitHub install the snapshotter image: quay.io/k8scsi/csi-snapshotter:v4.0.x The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration. Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-6.2.0 kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - NOTE:\nIt is recommended to use the 6.2.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration. ","categories":"","description":"Snapshot module of Dell CSI drivers\n","excerpt":"Snapshot module of Dell CSI drivers\n","ref":"/csm-docs/v2/snapshots/","tags":"","title":"Snapshots"},{"body":"Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\nKubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class Note: From v1.7, the CSI driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the /samples/volumesnapshotclass folder under respective drivers.\nCreating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 spec: volumeSnapshotClassName: csm-snapclass source: persistentVolumeClaimName: pvol0 After the VolumeSnapshot has been successfully created by the CSI driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nNote: VolumeSnapshots can be listed using the command kubectl get volumesnapshot -n \u003cnamespace\u003e\n(Optional) Volume Snapshot Requirements Applicable only if you decide to enable the snapshot feature in values.yaml.\nsnapshot: enabled: true Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. For installation, use v6.2.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers to support Volume snapshots.\nA common snapshot controller A CSI external-snapshotter sidecar The common snapshot controller must be installed only once in the cluster, irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v6.2.x\nNOTE:\nThe manifests available on GitHub install the snapshotter image: quay.io/k8scsi/csi-snapshotter:v4.0.x The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration. Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-6.2.0 kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - NOTE:\nIt is recommended to use the 6.2.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration. ","categories":"","description":"Snapshot module of Dell CSI drivers\n","excerpt":"Snapshot module of Dell CSI drivers\n","ref":"/csm-docs/v3/snapshots/","tags":"","title":"Snapshots"},{"body":" Symptoms Prevention, Resolution or Workaround Persistent volumes don’t get created on the target cluster. Run kubectl describe on one of the pods of replication controller and see if event says Config update won't be applied because of invalid configmap/secrets. Please fix the invalid configuration. If it does, then ensure you correctly populated replication ConfigMap. You can check the current status by running kubectl describe cm -n dell-replication-controller dell-replication-controller-config. If ConfigMap is empty, please edit it yourself or use repctl cluster inject command. Persistent volumes don’t get created on the target cluster. You don’t see any events on the replication-controller pod. Check logs of replication controller by running kubectl logs -n dell-replication-controller dell-replication-controller-manager-\u003cgenerated-symbols\u003e. If you see clusterId - \u003cclusterID\u003e not found errors then be sure to check if you specified the same clusterIDs in both your ConfigMap and replication enabled StorageClass. You apply replication action by manually editing ReplicationGroup resource field spec.action and don’t see any change of ReplicationGroup state after a while. Check events of the replication-controller pod, if it says Cannot proceed with action \u003cyour-action\u003e. [unsupported action] then check spelling of your action and consult the Replication Actions page. Alternatively, you can use repctl instead of manually editing ReplicationGroup resources. You execute failover action using repctl failover command and see failover: error executing failover to source site. This means you tried to failover to a cluster that is already marked source. If you still want to execute failover for RG, just choose another cluster. You’ve created PersistentVolumeClaim using replication enabled StorageClass but don’t see any RGs created in the source cluster. Check annotations of created PersistentVolumeClaim. If it doesn’t have annotations that start with replication.storage.dell.com then please wait for a couple of minutes for them to be added and RG to be created. When installing common replication controller using helm you see an error that states invalid ownership metadata and missing key \"app.kubernetes.io/managed-by\": must be set to \"Helm\" This means that you haven’t fully deleted the previous release, you can fix it by either deleting entire manifest by using kubectl delete -f deploy/controller.yaml or manually deleting conflicting resources (ClusterRoles, ClusterRoleBinding, etc.) PV and/or PVCs are not being created at the source/target cluster. If you check the controller’s logs you can see no such host errors Make sure cluster-1’s API is pingable from cluster-2 and vice versa. If one of your clusters is OpenShift located in a private network and needs records in /etc/hosts, exec into controller pod and modify /etc/hosts manually. After upgrading to Replication v1.4.0, if kubectl get rg returns an error Unable to list \"replication.storage.dell.com/v1alpha1, Resource=dellcsireplicationgroups\" This means kubectl still doesn’t recognize the new version of CRD dellcsireplicationgroups.replication.storage.dell.com after upgrade. Running the command kubectl get DellCSIReplicationGroup.v1.replication.storage.dell.com/\u003crg-id\u003e -o yaml will resolve the issue. ","categories":"","description":"Troubleshooting guide\n","excerpt":"Troubleshooting guide\n","ref":"/csm-docs/docs/replication/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":" Symptoms Prevention, Resolution or Workaround Persistent volumes don’t get created on the target cluster. Run kubectl describe on one of the pods of replication controller and see if event says Config update won't be applied because of invalid configmap/secrets. Please fix the invalid configuration. If it does, then ensure you correctly populated replication ConfigMap. You can check the current status by running kubectl describe cm -n dell-replication-controller dell-replication-controller-config. If ConfigMap is empty, please edit it yourself or use repctl cluster inject command. Persistent volumes don’t get created on the target cluster. You don’t see any events on the replication-controller pod. Check logs of replication controller by running kubectl logs -n dell-replication-controller dell-replication-controller-manager-\u003cgenerated-symbols\u003e. If you see clusterId - \u003cclusterID\u003e not found errors then be sure to check if you specified the same clusterIDs in both your ConfigMap and replication enabled StorageClass. You apply replication action by manually editing ReplicationGroup resource field spec.action and don’t see any change of ReplicationGroup state after a while. Check events of the replication-controller pod, if it says Cannot proceed with action \u003cyour-action\u003e. [unsupported action] then check spelling of your action and consult the Replication Actions page. Alternatively, you can use repctl instead of manually editing ReplicationGroup resources. You execute failover action using repctl failover command and see failover: error executing failover to source site. This means you tried to failover to a cluster that is already marked source. If you still want to execute failover for RG, just choose another cluster. You’ve created PersistentVolumeClaim using replication enabled StorageClass but don’t see any RGs created in the source cluster. Check annotations of created PersistentVolumeClaim. If it doesn’t have annotations that start with replication.storage.dell.com then please wait for a couple of minutes for them to be added and RG to be created. When installing common replication controller using helm you see an error that states invalid ownership metadata and missing key \"app.kubernetes.io/managed-by\": must be set to \"Helm\" This means that you haven’t fully deleted the previous release, you can fix it by either deleting entire manifest by using kubectl delete -f deploy/controller.yaml or manually deleting conflicting resources (ClusterRoles, ClusterRoleBinding, etc.) PV and/or PVCs are not being created at the source/target cluster. If you check the controller’s logs you can see no such host errors Make sure cluster-1’s API is pingable from cluster-2 and vice versa. If one of your clusters is OpenShift located in a private network and needs records in /etc/hosts, exec into controller pod and modify /etc/hosts manually. After upgrading to Replication v1.4.0, if kubectl get rg returns an error Unable to list \"replication.storage.dell.com/v1alpha1, Resource=dellcsireplicationgroups\" This means kubectl still doesn’t recognize the new version of CRD dellcsireplicationgroups.replication.storage.dell.com after upgrade. Running the command kubectl get DellCSIReplicationGroup.v1.replication.storage.dell.com/\u003crg-id\u003e -o yaml will resolve the issue. ","categories":"","description":"Troubleshooting guide\n","excerpt":"Troubleshooting guide\n","ref":"/csm-docs/v1/replication/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":" Symptoms Prevention, Resolution or Workaround Persistent volumes don’t get created on the target cluster. Run kubectl describe on one of the pods of replication controller and see if event says Config update won't be applied because of invalid configmap/secrets. Please fix the invalid configuration. If it does, then ensure you correctly populated replication ConfigMap. You can check the current status by running kubectl describe cm -n dell-replication-controller dell-replication-controller-config. If ConfigMap is empty, please edit it yourself or use repctl cluster inject command. Persistent volumes don’t get created on the target cluster. You don’t see any events on the replication-controller pod. Check logs of replication controller by running kubectl logs -n dell-replication-controller dell-replication-controller-manager-\u003cgenerated-symbols\u003e. If you see clusterId - \u003cclusterID\u003e not found errors then be sure to check if you specified the same clusterIDs in both your ConfigMap and replication enabled StorageClass. You apply replication action by manually editing ReplicationGroup resource field spec.action and don’t see any change of ReplicationGroup state after a while. Check events of the replication-controller pod, if it says Cannot proceed with action \u003cyour-action\u003e. [unsupported action] then check spelling of your action and consult the Replication Actions page. Alternatively, you can use repctl instead of manually editing ReplicationGroup resources. You execute failover action using repctl failover command and see failover: error executing failover to source site. This means you tried to failover to a cluster that is already marked source. If you still want to execute failover for RG, just choose another cluster. You’ve created PersistentVolumeClaim using replication enabled StorageClass but don’t see any RGs created in the source cluster. Check annotations of created PersistentVolumeClaim. If it doesn’t have annotations that start with replication.storage.dell.com then please wait for a couple of minutes for them to be added and RG to be created. When installing common replication controller using helm you see an error that states invalid ownership metadata and missing key \"app.kubernetes.io/managed-by\": must be set to \"Helm\" This means that you haven’t fully deleted the previous release, you can fix it by either deleting entire manifest by using kubectl delete -f deploy/controller.yaml or manually deleting conflicting resources (ClusterRoles, ClusterRoleBinding, etc.) PV and/or PVCs are not being created at the source/target cluster. If you check the controller’s logs you can see no such host errors Make sure cluster-1’s API is pingable from cluster-2 and vice versa. If one of your clusters is OpenShift located in a private network and needs records in /etc/hosts, exec into controller pod and modify /etc/hosts manually. After upgrading to Replication v1.4.0, if kubectl get rg returns an error Unable to list \"replication.storage.dell.com/v1alpha1, Resource=dellcsireplicationgroups\" This means kubectl still doesn’t recognize the new version of CRD dellcsireplicationgroups.replication.storage.dell.com after upgrade. Running the command kubectl get DellCSIReplicationGroup.v1.replication.storage.dell.com/\u003crg-id\u003e -o yaml will resolve the issue. ","categories":"","description":"Troubleshooting guide\n","excerpt":"Troubleshooting guide\n","ref":"/csm-docs/v2/replication/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":" Symptoms Prevention, Resolution or Workaround Persistent volumes don’t get created on the target cluster. Run kubectl describe on one of the pods of replication controller and see if event says Config update won't be applied because of invalid configmap/secrets. Please fix the invalid configuration. If it does, then ensure you correctly populated replication ConfigMap. You can check the current status by running kubectl describe cm -n dell-replication-controller dell-replication-controller-config. If ConfigMap is empty, please edit it yourself or use repctl cluster inject command. Persistent volumes don’t get created on the target cluster. You don’t see any events on the replication-controller pod. Check logs of replication controller by running kubectl logs -n dell-replication-controller dell-replication-controller-manager-\u003cgenerated-symbols\u003e. If you see clusterId - \u003cclusterID\u003e not found errors then be sure to check if you specified the same clusterIDs in both your ConfigMap and replication enabled StorageClass. You apply replication action by manually editing ReplicationGroup resource field spec.action and don’t see any change of ReplicationGroup state after a while. Check events of the replication-controller pod, if it says Cannot proceed with action \u003cyour-action\u003e. [unsupported action] then check spelling of your action and consult the Replication Actions page. Alternatively, you can use repctl instead of manually editing ReplicationGroup resources. You execute failover action using repctl failover command and see failover: error executing failover to source site. This means you tried to failover to a cluster that is already marked source. If you still want to execute failover for RG, just choose another cluster. You’ve created PersistentVolumeClaim using replication enabled StorageClass but don’t see any RGs created in the source cluster. Check annotations of created PersistentVolumeClaim. If it doesn’t have annotations that start with replication.storage.dell.com then please wait for a couple of minutes for them to be added and RG to be created. When installing common replication controller using helm you see an error that states invalid ownership metadata and missing key \"app.kubernetes.io/managed-by\": must be set to \"Helm\" This means that you haven’t fully deleted the previous release, you can fix it by either deleting entire manifest by using kubectl delete -f deploy/controller.yaml or manually deleting conflicting resources (ClusterRoles, ClusterRoleBinding, etc.) PV and/or PVCs are not being created at the source/target cluster. If you check the controller’s logs you can see no such host errors Make sure cluster-1’s API is pingable from cluster-2 and vice versa. If one of your clusters is OpenShift located in a private network and needs records in /etc/hosts, exec into controller pod and modify /etc/hosts manually. After upgrading to Replication v1.4.0, if kubectl get rg returns an error Unable to list \"replication.storage.dell.com/v1alpha1, Resource=dellcsireplicationgroups\" This means kubectl still doesn’t recognize the new version of CRD dellcsireplicationgroups.replication.storage.dell.com after upgrade. Running the command kubectl get DellCSIReplicationGroup.v1.replication.storage.dell.com/\u003crg-id\u003e -o yaml will resolve the issue. ","categories":"","description":"Troubleshooting guide\n","excerpt":"Troubleshooting guide\n","ref":"/csm-docs/v3/replication/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Volume Group Snapshot Feature The Dell CSM Volume Group Snapshotter is an operator which extends Kubernetes API to support crash-consistent snapshots of groups of volumes. Volume Group Snapshot supports PowerFlex and PowerStore driver.\nInstallation To install and use the Volume Group Snapshotter, you need to install pre-requisites in your cluster, then install the CRD in your cluster and deploy it with the driver.\n1. Install Pre-Requisites The only pre-requisite required is the external-snapshotter, which is available here. Version 4.1+ is required. This is also required for the driver, so if the driver has already been installed, this pre-requisite should already be fulfilled as well.\nThe external-snapshotter is split into two controllers, the common snapshot controller and a CSI external-snapshotter sidecar. The common snapshot controller must be installed only once per cluster.\nHere are sample instructions on installing the external-snapshotter CRDs:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller 2. Install VGS CRD IMPORTANT: delete previous v1aplha2 version of CRD and vgs resources created using alpha version. Snapshots on array will remain if memberReclaimPolicy=retain was used. If you want to install the VGS CRD from a pre-generated yaml, you can do so with this command (run in top-level folder):\ngit clone https://github.com/dell/csi-volumegroup-snapshotter.git cd csi-volumegroup-snapshotter kubectl apply -f config/crd/vgs-install.yaml If you want to create your own CRD for installation with Kustomize, then the command make install can be used to create and install the Custom Resource Definitions in your Kubernetes cluster.\n3. Deploy VGS in CSI Driver with Helm Chart Parameters The drivers that support Helm chart deployment allow the CSM Volume Group Snapshotter to be optionally deployed by variables in the chart. There is a vgsnapshotter block specified in the values.yaml file of the chart that will look similar this default text:\n# volume group snapshotter(vgsnapshotter) details # These options control the running of the vgsnapshotter container vgsnapshotter: enabled: false image: Note: It is recommended you set controllerCount to 1 in your values file, to avoid duplicate vgs controllers running\nTo deploy CSM Volume Group Snapshotter with the driver, these changes are required:\nEnable CSM Volume Group Snapshotter by changing the vgsnapshotter.enabled boolean to true. In the vgsnapshotter.image field, put the location of the image you created, or link to the one already built (such as the one on DockerHub, dellemc/csi-volumegroup-snapshotter:v1.3.0). Install/upgrade the driver normally. You should now have VGS successfully deployed with the driver! Creating Volume Group Snapshots This is a sample manifest for creating a Volume Group Snapshot:\napiVersion: volumegroup.storage.dell.com/v1 kind: DellCsiVolumeGroupSnapshot metadata: name: \"vgs-test\" namespace: \"test\" spec: # Add fields here driverName: \"csi-\u003cdriver-name\u003e.dellemc.com\" # Example: \"csi-powerstore.dellemc.com\" # defines how to process VolumeSnapshot members when volume group snapshot is deleted # \"Retain\" - keep VolumeSnapshot instances # \"Delete\" - delete VolumeSnapshot instances memberReclaimPolicy: \"Retain\" volumesnapshotclass: \"\u003csnapshot-class\u003e\" timeout: 90sec pvcLabel: \"vgs-snap-label\" # pvcList: # - \"pvcName1\" # - \"pvcName2\" Run the command kubectl create -f vg.yaml to take the specified snapshot.\nThe PVC labels field specifies a label that must be present in PVCs that are to be snapshotted. Here is a sample of that portion of a .yaml for a PVC:\nmetadata: name: volume1 namespace: test labels: volume-group: vgs-snap-label How to create policy based Volume Group Snapshots Currently, array based policies are not supported. This will be addressed in an upcoming release. For a temporary solution, cronjob can be used to mimic policy based Volume Group Snapshots. The only supported policy is how often the group should be created. To create a cronjob that creates a volume group snapshot periodically, use the template found in samples/ directory. Once the template is filled out, use the command kubectl create -f samples/cron-template.yaml to create the configmap and cronjob.\nNote: Cronjob is only supported on Kubernetes versions 1.21 or higher\nVolumeSnapshotContent watcher A VolumeSnapshotContent watcher is implemented to watch for VG’s managing VolumeSnapshotContent. When any of the VolumeSnapshotContents get deleted, its managing VG, if there is one, will update Status.Snapshots to remove that snapshot. If all the snapshots are deleted, the VG will be also deleted automatically.\nDeleting policy based Volume Group Snapshots Currently, automatic deletion of Volume Group Snapshots is not supported. All deletion must be done manually.\nMore details about the installation and use of the VolumeGroup Snapshotter can be found here: dell-csi-volumegroup-snapshotter.\nNote: Volume group cannot be seen from the Kubernetes level as of now only volume group snapshots can be viewed as a CRD\nVolume Group Snapshots feature is supported with Helm.\n","categories":"","description":"Volume Group Snapshot module of Dell CSI drivers\n","excerpt":"Volume Group Snapshot module of Dell CSI drivers\n","ref":"/csm-docs/docs/snapshots/volume-group-snapshots/","tags":"","title":"Volume Group Snapshots"},{"body":"Volume Group Snapshot Feature The Dell CSM Volume Group Snapshotter is an operator which extends Kubernetes API to support crash-consistent snapshots of groups of volumes. Volume Group Snapshot supports PowerFlex and PowerStore driver.\nInstallation To install and use the Volume Group Snapshotter, you need to install pre-requisites in your cluster, then install the CRD in your cluster and deploy it with the driver.\n1. Install Pre-Requisites The only pre-requisite required is the external-snapshotter, which is available here. Version 4.1+ is required. This is also required for the driver, so if the driver has already been installed, this pre-requisite should already be fulfilled as well.\nThe external-snapshotter is split into two controllers, the common snapshot controller and a CSI external-snapshotter sidecar. The common snapshot controller must be installed only once per cluster.\nHere are sample instructions on installing the external-snapshotter CRDs:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller 2. Install VGS CRD IMPORTANT: delete previous v1aplha2 version of CRD and vgs resources created using alpha version. Snapshots on array will remain if memberReclaimPolicy=retain was used. If you want to install the VGS CRD from a pre-generated yaml, you can do so with this command (run in top-level folder):\ngit clone https://github.com/dell/csi-volumegroup-snapshotter.git cd csi-volumegroup-snapshotter kubectl apply -f config/crd/vgs-install.yaml If you want to create your own CRD for installation with Kustomize, then the command make install can be used to create and install the Custom Resource Definitions in your Kubernetes cluster.\n3. Deploy VGS in CSI Driver with Helm Chart Parameters The drivers that support Helm chart deployment allow the CSM Volume Group Snapshotter to be optionally deployed by variables in the chart. There is a vgsnapshotter block specified in the values.yaml file of the chart that will look similar this default text:\n# volume group snapshotter(vgsnapshotter) details # These options control the running of the vgsnapshotter container vgsnapshotter: enabled: false image: Note: It is recommended you set controllerCount to 1 in your values file, to avoid duplicate vgs controllers running\nTo deploy CSM Volume Group Snapshotter with the driver, these changes are required:\nEnable CSM Volume Group Snapshotter by changing the vgsnapshotter.enabled boolean to true. In the vgsnapshotter.image field, put the location of the image you created, or link to the one already built (such as the one on DockerHub, dellemc/csi-volumegroup-snapshotter:v1.3.0). Install/upgrade the driver normally. You should now have VGS successfully deployed with the driver! Creating Volume Group Snapshots This is a sample manifest for creating a Volume Group Snapshot:\napiVersion: volumegroup.storage.dell.com/v1 kind: DellCsiVolumeGroupSnapshot metadata: name: \"vgs-test\" namespace: \"test\" spec: # Add fields here driverName: \"csi-\u003cdriver-name\u003e.dellemc.com\" # Example: \"csi-powerstore.dellemc.com\" # defines how to process VolumeSnapshot members when volume group snapshot is deleted # \"Retain\" - keep VolumeSnapshot instances # \"Delete\" - delete VolumeSnapshot instances memberReclaimPolicy: \"Retain\" volumesnapshotclass: \"\u003csnapshot-class\u003e\" timeout: 90sec pvcLabel: \"vgs-snap-label\" # pvcList: # - \"pvcName1\" # - \"pvcName2\" Run the command kubectl create -f vg.yaml to take the specified snapshot.\nThe PVC labels field specifies a label that must be present in PVCs that are to be snapshotted. Here is a sample of that portion of a .yaml for a PVC:\nmetadata: name: volume1 namespace: test labels: volume-group: vgs-snap-label How to create policy based Volume Group Snapshots Currently, array based policies are not supported. This will be addressed in an upcoming release. For a temporary solution, cronjob can be used to mimic policy based Volume Group Snapshots. The only supported policy is how often the group should be created. To create a cronjob that creates a volume group snapshot periodically, use the template found in samples/ directory. Once the template is filled out, use the command kubectl create -f samples/cron-template.yaml to create the configmap and cronjob.\nNote: Cronjob is only supported on Kubernetes versions 1.21 or higher\nVolumeSnapshotContent watcher A VolumeSnapshotContent watcher is implemented to watch for VG’s managing VolumeSnapshotContent. When any of the VolumeSnapshotContents get deleted, its managing VG, if there is one, will update Status.Snapshots to remove that snapshot. If all the snapshots are deleted, the VG will be also deleted automatically.\nDeleting policy based Volume Group Snapshots Currently, automatic deletion of Volume Group Snapshots is not supported. All deletion must be done manually.\nMore details about the installation and use of the VolumeGroup Snapshotter can be found here: dell-csi-volumegroup-snapshotter.\nNote: Volume group cannot be seen from the Kubernetes level as of now only volume group snapshots can be viewed as a CRD\nVolume Group Snapshots feature is supported with Helm.\n","categories":"","description":"Volume Group Snapshot module of Dell CSI drivers\n","excerpt":"Volume Group Snapshot module of Dell CSI drivers\n","ref":"/csm-docs/v1/snapshots/volume-group-snapshots/","tags":"","title":"Volume Group Snapshots"},{"body":"Volume Group Snapshot Feature The Dell CSM Volume Group Snapshotter is an operator which extends Kubernetes API to support crash-consistent snapshots of groups of volumes. Volume Group Snapshot supports PowerFlex and PowerStore driver.\nInstallation To install and use the Volume Group Snapshotter, you need to install pre-requisites in your cluster, then install the CRD in your cluster and deploy it with the driver.\n1. Install Pre-Requisites The only pre-requisite required is the external-snapshotter, which is available here. Version 4.1+ is required. This is also required for the driver, so if the driver has already been installed, this pre-requisite should already be fulfilled as well.\nThe external-snapshotter is split into two controllers, the common snapshot controller and a CSI external-snapshotter sidecar. The common snapshot controller must be installed only once per cluster.\nHere are sample instructions on installing the external-snapshotter CRDs:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller 2. Install VGS CRD IMPORTANT: delete previous v1aplha2 version of CRD and vgs resources created using alpha version. Snapshots on array will remain if memberReclaimPolicy=retain was used. If you want to install the VGS CRD from a pre-generated yaml, you can do so with this command (run in top-level folder):\ngit clone https://github.com/dell/csi-volumegroup-snapshotter.git cd csi-volumegroup-snapshotter kubectl apply -f config/crd/vgs-install.yaml If you want to create your own CRD for installation with Kustomize, then the command make install can be used to create and install the Custom Resource Definitions in your Kubernetes cluster.\n3. Deploy VGS in CSI Driver with Helm Chart Parameters The drivers that support Helm chart deployment allow the CSM Volume Group Snapshotter to be optionally deployed by variables in the chart. There is a vgsnapshotter block specified in the values.yaml file of the chart that will look similar this default text:\n# volume group snapshotter(vgsnapshotter) details # These options control the running of the vgsnapshotter container vgsnapshotter: enabled: false image: Note: It is recommended you set controllerCount to 1 in your values file, to avoid duplicate vgs controllers running\nTo deploy CSM Volume Group Snapshotter with the driver, these changes are required:\nEnable CSM Volume Group Snapshotter by changing the vgsnapshotter.enabled boolean to true. In the vgsnapshotter.image field, put the location of the image you created, or link to the one already built (such as the one on DockerHub, dellemc/csi-volumegroup-snapshotter:v1.2.0). Install/upgrade the driver normally. You should now have VGS successfully deployed with the driver! Creating Volume Group Snapshots This is a sample manifest for creating a Volume Group Snapshot:\napiVersion: volumegroup.storage.dell.com/v1 kind: DellCsiVolumeGroupSnapshot metadata: name: \"vgs-test\" namespace: \"test\" spec: # Add fields here driverName: \"csi-\u003cdriver-name\u003e.dellemc.com\" # Example: \"csi-powerstore.dellemc.com\" # defines how to process VolumeSnapshot members when volume group snapshot is deleted # \"Retain\" - keep VolumeSnapshot instances # \"Delete\" - delete VolumeSnapshot instances memberReclaimPolicy: \"Retain\" volumesnapshotclass: \"\u003csnapshot-class\u003e\" timeout: 90sec pvcLabel: \"vgs-snap-label\" # pvcList: # - \"pvcName1\" # - \"pvcName2\" Run the command kubectl create -f vg.yaml to take the specified snapshot.\nThe PVC labels field specifies a label that must be present in PVCs that are to be snapshotted. Here is a sample of that portion of a .yaml for a PVC:\nmetadata: name: volume1 namespace: test labels: volume-group: vgs-snap-label How to create policy based Volume Group Snapshots Currently, array based policies are not supported. This will be addressed in an upcoming release. For a temporary solution, cronjob can be used to mimic policy based Volume Group Snapshots. The only supported policy is how often the group should be created. To create a cronjob that creates a volume group snapshot periodically, use the template found in samples/ directory. Once the template is filled out, use the command kubectl create -f samples/cron-template.yaml to create the configmap and cronjob.\nNote: Cronjob is only supported on Kubernetes versions 1.21 or higher\nVolumeSnapshotContent watcher A VolumeSnapshotContent watcher is implemented to watch for VG’s managing VolumeSnapshotContent. When any of the VolumeSnapshotContents get deleted, its managing VG, if there is one, will update Status.Snapshots to remove that snapshot. If all the snapshots are deleted, the VG will be also deleted automatically.\nDeleting policy based Volume Group Snapshots Currently, automatic deletion of Volume Group Snapshots is not supported. All deletion must be done manually.\nMore details about the installation and use of the VolumeGroup Snapshotter can be found here: dell-csi-volumegroup-snapshotter.\nNote: Volume group cannot be seen from the Kubernetes level as of now only volume group snapshots can be viewed as a CRD\nVolume Group Snapshots feature is supported with Helm.\n","categories":"","description":"Volume Group Snapshot module of Dell CSI drivers\n","excerpt":"Volume Group Snapshot module of Dell CSI drivers\n","ref":"/csm-docs/v2/snapshots/volume-group-snapshots/","tags":"","title":"Volume Group Snapshots"},{"body":"Volume Group Snapshot Feature The Dell CSM Volume Group Snapshotter is an operator which extends Kubernetes API to support crash-consistent snapshots of groups of volumes. Volume Group Snapshot supports PowerFlex and PowerStore driver.\nInstallation To install and use the Volume Group Snapshotter, you need to install pre-requisites in your cluster, then install the CRD in your cluster and deploy it with the driver.\n1. Install Pre-Requisites The only pre-requisite required is the external-snapshotter, which is available here. Version 4.1+ is required. This is also required for the driver, so if the driver has already been installed, this pre-requisite should already be fulfilled as well.\nThe external-snapshotter is split into two controllers, the common snapshot controller and a CSI external-snapshotter sidecar. The common snapshot controller must be installed only once per cluster.\nHere are sample instructions on installing the external-snapshotter CRDs:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller 2. Install VGS CRD IMPORTANT: delete previous v1aplha2 version of CRD and vgs resources created using alpha version. Snapshots on array will remain if memberReclaimPolicy=retain was used. If you want to install the VGS CRD from a pre-generated yaml, you can do so with this command (run in top-level folder):\ngit clone https://github.com/dell/csi-volumegroup-snapshotter.git cd csi-volumegroup-snapshotter kubectl apply -f config/crd/vgs-install.yaml If you want to create your own CRD for installation with Kustomize, then the command make install can be used to create and install the Custom Resource Definitions in your Kubernetes cluster.\n3. Deploy VGS in CSI Driver with Helm Chart Parameters The drivers that support Helm chart deployment allow the CSM Volume Group Snapshotter to be optionally deployed by variables in the chart. There is a vgsnapshotter block specified in the values.yaml file of the chart that will look similar this default text:\n# volume group snapshotter(vgsnapshotter) details # These options control the running of the vgsnapshotter container vgsnapshotter: enabled: false image: To deploy CSM Volume Group Snapshotter with the driver, these changes are required:\nEnable CSM Volume Group Snapshotter by changing the vgsnapshotter.enabled boolean to true. In the vgsnapshotter.image field, put the location of the image you created, or link to the one already built (such as the one on DockerHub, dellemc/csi-volumegroup-snapshotter:v1.2.0). Install/upgrade the driver normally. You should now have VGS successfully deployed with the driver! Creating Volume Group Snapshots This is a sample manifest for creating a Volume Group Snapshot:\napiVersion: volumegroup.storage.dell.com/v1 kind: DellCsiVolumeGroupSnapshot metadata: name: \"vgs-test\" namespace: \"test\" spec: # Add fields here driverName: \"csi-\u003cdriver-name\u003e.dellemc.com\" # Example: \"csi-powerstore.dellemc.com\" # defines how to process VolumeSnapshot members when volume group snapshot is deleted # \"Retain\" - keep VolumeSnapshot instances # \"Delete\" - delete VolumeSnapshot instances memberReclaimPolicy: \"Retain\" volumesnapshotclass: \"\u003csnapshot-class\u003e\" timeout: 90sec pvcLabel: \"vgs-snap-label\" # pvcList: # - \"pvcName1\" # - \"pvcName2\" Run the command kubectl create -f vg.yaml to take the specified snapshot.\nThe PVC labels field specifies a label that must be present in PVCs that are to be snapshotted. Here is a sample of that portion of a .yaml for a PVC:\nmetadata: name: volume1 namespace: test labels: volume-group: vgs-snap-label How to create policy based Volume Group Snapshots Currently, array based policies are not supported. This will be addressed in an upcoming release. For a temporary solution, cronjob can be used to mimic policy based Volume Group Snapshots. The only supported policy is how often the group should be created. To create a cronjob that creates a volume group snapshot periodically, use the template found in samples/ directory. Once the template is filled out, use the command kubectl create -f samples/cron-template.yaml to create the configmap and cronjob.\nNote: Cronjob is only supported on Kubernetes versions 1.21 or higher\nVolumeSnapshotContent watcher A VolumeSnapshotContent watcher is implemented to watch for VG’s managing VolumeSnapshotContent. When any of the VolumeSnapshotContents get deleted, its managing VG, if there is one, will update Status.Snapshots to remove that snapshot. If all the snapshots are deleted, the VG will be also deleted automatically.\nDeleting policy based Volume Group Snapshots Currently, automatic deletion of Volume Group Snapshots is not supported. All deletion must be done manually.\nMore details about the installation and use of the VolumeGroup Snapshotter can be found here: dell-csi-volumegroup-snapshotter.\nNote: Volume group cannot be seen from the Kubernetes level as of now only volume group snapshots can be viewed as a CRD\nVolume Group Snapshots feature is supported with Helm.\n","categories":"","description":"Volume Group Snapshot module of Dell CSI drivers\n","excerpt":"Volume Group Snapshot module of Dell CSI drivers\n","ref":"/csm-docs/v3/snapshots/volume-group-snapshots/","tags":"","title":"Volume Group Snapshots"},{"body":" NOTE: This tech-preview release is not intended for use in production environment.\nNOTE: Application Mobility requires a time-based license. See Deployment for instructions.\nContainer Storage Modules for Application Mobility provide Kubernetes administrators the ability to clone their stateful application workloads and application data to other clusters, either on-premise or in the cloud.\nApplication Mobility uses Velero and its integration of Restic to copy both application metadata and data to object storage. When a backup is requested, Application Mobility uses these options to determine how the application data is backed up:\nIf Volume Group Snapshots are enabled on the CSI driver backing the application’s Persistent Volumes, crash consistent snapshots of all volumes are used for the backup. If Volume Snapshots are enabled on the Kubernetes cluster and supported by the CSI driver, individual snapshots are used for each Persistent Volume used by the application. If no snapshot options are enabled, default to using full copies of each Persistent Volume used by the application. After a backup has been created, it can be restored on the same Kubernetes cluster or any other cluster(s) if this criteria is met:\nApplication Mobility is installed on the target cluster(s). The target cluster(s) has access to the object store bucket. For example, if backing up and restoring an application from an on-premise Kubernetes cluster to AWS EKS, an S3 bucket can be used if both the on-premise and EKS cluster have access to it. Storage Class is defined on the target cluster(s) to support creating the required Persistent Volumes used by the application. Supported Data Movers Data Mover Description Restic Persistent Volume data will be stored in the provided object store bucket Supported Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.26, 1.27. 1.28 Red Hat OpenShift 4.13, 4.14 ","categories":"","description":"Application Mobility\n","excerpt":"Application Mobility\n","ref":"/csm-docs/docs/applicationmobility/","tags":"","title":"Application Mobility"},{"body":" NOTE: This tech-preview release is not intended for use in production environment.\nNOTE: Application Mobility requires a time-based license. See Deployment for instructions.\nContainer Storage Modules for Application Mobility provide Kubernetes administrators the ability to clone their stateful application workloads and application data to other clusters, either on-premise or in the cloud.\nApplication Mobility uses Velero and its integration of Restic to copy both application metadata and data to object storage. When a backup is requested, Application Mobility uses these options to determine how the application data is backed up:\nIf Volume Group Snapshots are enabled on the CSI driver backing the application’s Persistent Volumes, crash consistent snapshots of all volumes are used for the backup. If Volume Snapshots are enabled on the Kubernetes cluster and supported by the CSI driver, individual snapshots are used for each Persistent Volume used by the application. If no snapshot options are enabled, default to using full copies of each Persistent Volume used by the application. After a backup has been created, it can be restored on the same Kubernetes cluster or any other cluster(s) if this criteria is met:\nApplication Mobility is installed on the target cluster(s). The target cluster(s) has access to the object store bucket. For example, if backing up and restoring an application from an on-premise Kubernetes cluster to AWS EKS, an S3 bucket can be used if both the on-premise and EKS cluster have access to it. Storage Class is defined on the target cluster(s) to support creating the required Persistent Volumes used by the application. Supported Data Movers Data Mover Description Restic Persistent Volume data will be stored in the provided object store bucket Supported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.23, 1.24, 1.25, 1.26 Red Hat OpenShift 4.10, 4.11 RHEL 7.x, 8.x CentOS 7.8, 7.9 ","categories":"","description":"Application Mobility\n","excerpt":"Application Mobility\n","ref":"/csm-docs/v1/applicationmobility/","tags":"","title":"Application Mobility"},{"body":" NOTE: This tech-preview release is not intended for use in production environment.\nNOTE: Application Mobility requires a time-based license. See Deployment for instructions.\nContainer Storage Modules for Application Mobility provide Kubernetes administrators the ability to clone their stateful application workloads and application data to other clusters, either on-premise or in the cloud.\nApplication Mobility uses Velero and its integration of Restic to copy both application metadata and data to object storage. When a backup is requested, Application Mobility uses these options to determine how the application data is backed up:\nIf Volume Group Snapshots are enabled on the CSI driver backing the application’s Persistent Volumes, crash consistent snapshots of all volumes are used for the backup. If Volume Snapshots are enabled on the Kubernetes cluster and supported by the CSI driver, individual snapshots are used for each Persistent Volume used by the application. If no snapshot options are enabled, default to using full copies of each Persistent Volume used by the application. After a backup has been created, it can be restored on the same Kubernetes cluster or any other cluster(s) if this criteria is met:\nApplication Mobility is installed on the target cluster(s). The target cluster(s) has access to the object store bucket. For example, if backing up and restoring an application from an on-premise Kubernetes cluster to AWS EKS, an S3 bucket can be used if both the on-premise and EKS cluster have access to it. Storage Class is defined on the target cluster(s) to support creating the required Persistent Volumes used by the application. Supported Data Movers Data Mover Description Restic Persistent Volume data will be stored in the provided object store bucket Supported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.23, 1.24, 1.25, 1.26 Red Hat OpenShift 4.10, 4.11 RHEL 7.x, 8.x CentOS 7.8, 7.9 ","categories":"","description":"Application Mobility\n","excerpt":"Application Mobility\n","ref":"/csm-docs/v2/applicationmobility/","tags":"","title":"Application Mobility"},{"body":" NOTE: This tech-preview release is not intended for use in production environment.\nNOTE: Application Mobility requires a time-based license. See Deployment for instructions.\nContainer Storage Modules for Application Mobility provide Kubernetes administrators the ability to clone their stateful application workloads and application data to other clusters, either on-premise or in the cloud.\nApplication Mobility uses Velero and its integration of Restic to copy both application metadata and data to object storage. When a backup is requested, Application Mobility uses these options to determine how the application data is backed up:\nIf Volume Group Snapshots are enabled on the CSI driver backing the application’s Persistent Volumes, crash consistent snapshots of all volumes are used for the backup. If Volume Snapshots are enabled on the Kubernetes cluster and supported by the CSI driver, individual snapshots are used for each Persistent Volume used by the application. If no snapshot options are enabled, default to using full copies of each Persistent Volume used by the application. After a backup has been created, it can be restored on the same Kubernetes cluster or any other cluster(s) if this criteria is met:\nApplication Mobility is installed on the target cluster(s). The target cluster(s) has access to the object store bucket. For example, if backing up and restoring an application from an on-premise Kubernetes cluster to AWS EKS, an S3 bucket can be used if both the on-premise and EKS cluster have access to it. Storage Class is defined on the target cluster(s) to support creating the required Persistent Volumes used by the application. Supported Data Movers Data Mover Description Restic Persistent Volume data will be stored in the provided object store bucket Supported Operating Systems/Container Orchestrator Platforms COP/OS Supported Versions Kubernetes 1.23, 1.24, 1.25, 1.26 Red Hat OpenShift 4.10, 4.11 RHEL 7.x, 8.x CentOS 7.8, 7.9 ","categories":"","description":"Application Mobility\n","excerpt":"Application Mobility\n","ref":"/csm-docs/v3/applicationmobility/","tags":"","title":"Application Mobility"},{"body":"","categories":"","description":"Our Ecosystem Partners","excerpt":"Our Ecosystem Partners","ref":"/csm-docs/v1/csidriver/partners/","tags":"","title":"Our Ecosystem Partners"},{"body":"","categories":"","description":"Our Ecosystem Partners","excerpt":"Our Ecosystem Partners","ref":"/csm-docs/v2/csidriver/partners/","tags":"","title":"Our Ecosystem Partners"},{"body":"","categories":"","description":"Our Ecosystem Partners","excerpt":"Our Ecosystem Partners","ref":"/csm-docs/v3/csidriver/partners/","tags":"","title":"Our Ecosystem Partners"},{"body":"Release Notes - CSM Replication 1.7.1 New Features/Changes #947 - [FEATURE]: Support for Kubernetes 1.28 #1066 - [FEATURE]: Support for Openshift 4.14 #996 - [FEATURE]: Dell CSI to Dell CSM Operator Migration Process #1031 - [FEATURE]: Update to the latest UBI Micro image for CSM #1062 - [FEATURE]: CSM PowerMax: Support PowerMax v10.1 Fixed Issues #988 - [BUG]: CSM Operator fails to install CSM Replication on the remote cluster #1002 - [BUG]: CSM Replication - secret file requirement for both sites not documented Known Issues ","categories":"","description":"Dell Container Storage Modules (CSM) release notes for replication\n","excerpt":"Dell Container Storage Modules (CSM) release notes for replication\n","ref":"/csm-docs/docs/replication/release/","tags":"","title":"Release notes"},{"body":"Release Notes - CSM Replication 1.6.0 New Features/Changes #724 - [FEATURE]: CSM support for Openshift 4.13 #877 - [FEATURE]: Make standalone helm chart available from helm repository : https://dell.github.io/dell/helm-charts Fixed Issues #916 - [BUG]: Remove references to deprecated io/ioutil package #928 - [BUG]: PowerStore Replication - Delete RG request hangs #968 - [BUG]: Creating StorageClass for replication failed with unmarshal error Known Issues Github ID Description 753 PowerScale: When Persistent Volumes (PVs) are created with quota enabled on CSM versions 1.6.0 and before, an incorrect quota gets set for the target side read-only PVs/directories based on the consumed non-zero source size instead of the assigned quota of the source. This can create issues when the user performs failover and wants to write data to the failed over site. If lower quota limit is set, no new writes can be performed on the target side post failover. Workaround using PowerScale cluster CLI or UI: For each Persistent Volume on the source kubernetes cluster, 1. Get the quota assigned for the directory on the source PowerScale cluster. The path to the directory information can be obtained from the specification field of the Persistent Volume object. 2. Verify the quota of the target directory on the target PowerScale cluster. If incorrect quota is set, update the quota on the target directory with the same information as on the source. If no quota is set, create a quota for the target directory. ","categories":"","description":"Dell Container Storage Modules (CSM) release notes for replication\n","excerpt":"Dell Container Storage Modules (CSM) release notes for replication\n","ref":"/csm-docs/v1/replication/release/","tags":"","title":"Release notes"},{"body":"Release Notes - CSM Replication 1.5.0 New Features/Changes Target Backend Volume Deletion Base Image Updated to ubi-micro Fixed Issues Github ID Description 782 PowerScale: Target NFS exports are not deleted even though target directories are deleted Known Issues Github ID Description 753 PowerScale: When Persistent Volumes (PVs) are created with quota enabled on CSM versions 1.6.0 and before, an incorrect quota gets set for the target side read-only PVs/directories based on the consumed non-zero source size instead of the assigned quota of the source. This can create issues when the user performs failover and wants to write data to the failed over site. If lower quota limit is set, no new writes can be performed on the target side post failover. Workaround using PowerScale cluster CLI or UI: For each Persistent Volume on the source kubernetes cluster, 1. Get the quota assigned for the directory on the source PowerScale cluster. The path to the directory information can be obtained from the specification field of the Persistent Volume object. 2. Verify the quota of the target directory on the target PowerScale cluster. If incorrect quota is set, update the quota on the target directory with the same information as on the source. If no quota is set, create a quota for the target directory. ","categories":"","description":"Dell Container Storage Modules (CSM) release notes for replication\n","excerpt":"Dell Container Storage Modules (CSM) release notes for replication\n","ref":"/csm-docs/v2/replication/release/","tags":"","title":"Release notes"},{"body":"Release Notes - CSM Replication 1.4.0 New Features/Changes PowerScale - Implement Failback functionality PowerScale - Implement Reprotect functionality PowerScale - SyncIQ policy improvements PowerFlex - Initial Replication Support Replication APIs to be moved from alpha phase Fixed Issues Github ID Description 523 PowerScale: Artifacts are not properly cleaned after deletion. Known Issues Github ID Description 753 PowerScale: When Persistent Volumes (PVs) are created with quota enabled on CSM versions 1.6.0 and before, an incorrect quota gets set for the target side read-only PVs/directories based on the consumed non-zero source size instead of the assigned quota of the source. This can create issues when the user performs failover and wants to write data to the failed over site. If lower quota limit is set, no new writes can be performed on the target side post failover. Workaround using PowerScale cluster CLI or UI: For each Persistent Volume on the source kubernetes cluster, 1. Get the quota assigned for the directory on the source PowerScale cluster. The path to the directory information can be obtained from the specification field of the Persistent Volume object. 2. Verify the quota of the target directory on the target PowerScale cluster. If incorrect quota is set, update the quota on the target directory with the same information as on the source. If no quota is set, create a quota for the target directory. ","categories":"","description":"Dell Container Storage Modules (CSM) release notes for replication\n","excerpt":"Dell Container Storage Modules (CSM) release notes for replication\n","ref":"/csm-docs/v3/replication/release/","tags":"","title":"Release notes"},{"body":"Secure is a suite of Dell Container Storage Modules (CSM) that brings security related features to Kubernetes users of Dell storage products.\n","categories":"","description":"Security features for Dell CSI drivers\n","excerpt":"Security features for Dell CSI drivers\n","ref":"/csm-docs/docs/secure/","tags":"","title":"Secure"},{"body":"Secure is a suite of Dell Container Storage Modules (CSM) that brings security related features to Kubernetes users of Dell storage products.\n","categories":"","description":"Security features for Dell CSI drivers\n","excerpt":"Security features for Dell CSI drivers\n","ref":"/csm-docs/v1/secure/","tags":"","title":"Secure"},{"body":"Secure is a suite of Dell Container Storage Modules (CSM) that brings security related features to Kubernetes users of Dell storage products.\n","categories":"","description":"Security features for Dell CSI drivers\n","excerpt":"Security features for Dell CSI drivers\n","ref":"/csm-docs/v2/secure/","tags":"","title":"Secure"},{"body":"Secure is a suite of Dell Container Storage Modules (CSM) that brings security related features to Kubernetes users of Dell storage products.\n","categories":"","description":"Security features for Dell CSI drivers\n","excerpt":"Security features for Dell CSI drivers\n","ref":"/csm-docs/v3/secure/","tags":"","title":"Secure"},{"body":"CSM Replication module consists of two components:\nCSM Replication sidecar (installed along with the driver) CSM Replication controller Those two components should be upgraded separately. When upgrading them ensure that you use the same versions for both sidecar and controller, because different versions could be incompatible with each other.\nNote: While upgrading the module via helm, the replicas variable in myvalues.yaml can be at most one less than the number of worker nodes.\nUpdating CSM Replication sidecar To upgrade the CSM Replication sidecar that is installed along with the driver, the following steps are required.\nNote: These steps refer to the values file and csi-install.sh script that was used during the initial installation of the Dell CSI driver.\nSteps\nUpdate the controller.replication.image value in the values files to reference the new CSM Replication sidecar image. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace \u003cyour-namespace\u003e --values ./myvalues.yaml --upgrade Run the same command on the second Kubernetes cluster if you use multi-cluster replication topology For more information on upgrading the CSI driver, please visit the CSI driver upgrade page.\nPowerScale On PowerScale systems, an additional step is needed when upgrading to CSM Replication v1.4.0 or later. Because the SyncIQ policy created on the target-side storage array is no longer used, it must be deleted for any existing DellCSIReplicationGroup objects after performing the upgrade to the CSM Replication sidecar and PowerScale CSI driver. These steps should be performed before the DellCSIReplicationGroup objects are used with the new version of the CSI driver. Until this step is performed, existing DellCSIReplicationGroup objects will display an UNKNOWN link state.\nLog in to the target PowerScale array. Navigate to the Data Protection \u003e SyncIQ page and select the Policies tab. Delete disabled, target-side SyncIQ policies that are used for CSM Replication. Such policies will be distinguished by their names, of the format \u003cprefix\u003e-\u003ckubernetes namespace\u003e-\u003cIP of replication destination\u003e-\u003cRPO duration\u003e. Updating CSM Replication controller Make sure the appropriate release branch is available on the machine performing the upgrade by running:\ngit clone -b v1.7.1 https://github.com/dell/csm-replication.git Upgrading with Helm This option will only work if you have previously installed replication via Helm chart, available since version 1.1. If you used simple manifest or repctl please use upgrading with repctl\nSteps\nUpdate the image value in the values files to reference the new CSM Replication controller image or use a new version of the csm-replication Helm chart.\nRun the install script with the option --upgrade by running:\ncd ./scripts \u0026\u0026 ./install.sh --values ./myvalues.yaml --upgrade Run the same command on the second Kubernetes cluster if you use multi-cluster replication topology.\nNote: Upgrade won’t override currently existing ConfigMap, even if you change templated values in myvalues.yaml file. If you want to change the logLevel - edit ConfigMap from within the cluster using bash kubectl edit cm -n dell-replication-controller dell-replication-controller-config Upgrading with repctl Note: These steps assume that you already have repctl configured to use correct clusters, if you don’t know how to do that please refer to installing with repctl\nSteps\nFind a new version of deployment manifest that can be found in deploy/controller.yaml, with newer image pointing to the version of CSM Replication controller you want to upgrade to.\nApply said manifest using the usual repctl create command like so:\n./repctl create -f ../deploy/controller.yaml The output should have this line Successfully updated existing deployment: dell-replication-controller-manager\nCheck if everything is OK by querying your Kubernetes clusters using kubectl using a kubectl get:\nkubectl get pods -n dell-replication-controller` Your pods should be READY and RUNNING.\nReplication CRD version update CRD dellcsireplicationgroups.replication.storage.dell.com has been updated to version v1 in CSM Replication v1.4.0. To facilitate the continued use of existing DellCSIReplicationGroup CR objects after upgrading to CSM Replication v1.4.0 or later, an init container will be deployed during upgrade. The init container updates the existing CRs with necessary steps for their continued use.\nNote: Do not update the CRD as part of upgrade. An init container included in the replication controller pod takes care of updating existing CRD and CR versions.\nStarting from CSM Replication v1.6.0, the init container has been removed. Therefore, when upgrading from versions older than v1.4.0 to v1.6.0 or any later versions, it is mandatory to perform an intermediate upgrade to either v1.4.0 or v1.5.0 before proceeding with any further upgrades.\n","categories":"","description":"Upgrade guide\n","excerpt":"Upgrade guide\n","ref":"/csm-docs/docs/replication/upgrade/","tags":"","title":"Upgrade"},{"body":"CSM Replication module consists of two components:\nCSM Replication sidecar (installed along with the driver) CSM Replication controller Those two components should be upgraded separately. When upgrading them ensure that you use the same versions for both sidecar and controller, because different versions could be incompatible with each other.\nNote: While upgrading the module via helm, the replicas variable in myvalues.yaml can be at most one less than the number of worker nodes.\nUpdating CSM Replication sidecar To upgrade the CSM Replication sidecar that is installed along with the driver, the following steps are required.\nNote: These steps refer to the values file and csi-install.sh script that was used during the initial installation of the Dell CSI driver.\nSteps\nUpdate the controller.replication.image value in the values files to reference the new CSM Replication sidecar image. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace \u003cyour-namespace\u003e --values ./myvalues.yaml --upgrade Run the same command on the second Kubernetes cluster if you use multi-cluster replication topology For more information on upgrading the CSI driver, please visit the CSI driver upgrade page.\nPowerScale On PowerScale systems, an additional step is needed when upgrading to CSM Replication v1.4.0 or later. Because the SyncIQ policy created on the target-side storage array is no longer used, it must be deleted for any existing DellCSIReplicationGroup objects after performing the upgrade to the CSM Replication sidecar and PowerScale CSI driver. These steps should be performed before the DellCSIReplicationGroup objects are used with the new version of the CSI driver. Until this step is performed, existing DellCSIReplicationGroup objects will display an UNKNOWN link state.\nLog in to the target PowerScale array. Navigate to the Data Protection \u003e SyncIQ page and select the Policies tab. Delete disabled, target-side SyncIQ policies that are used for CSM Replication. Such policies will be distinguished by their names, of the format \u003cprefix\u003e-\u003ckubernetes namespace\u003e-\u003cIP of replication destination\u003e-\u003cRPO duration\u003e. Updating CSM Replication controller Make sure the appropriate release branch is available on the machine performing the upgrade by running:\ngit clone -b v1.6.0 https://github.com/dell/csm-replication.git Upgrading with Helm This option will only work if you have previously installed replication via Helm chart, available since version 1.1. If you used simple manifest or repctl please use upgrading with repctl\nSteps\nUpdate the image value in the values files to reference the new CSM Replication controller image or use a new version of the csm-replication Helm chart.\nRun the install script with the option --upgrade by running:\ncd ./scripts \u0026\u0026 ./install.sh --values ./myvalues.yaml --upgrade Run the same command on the second Kubernetes cluster if you use multi-cluster replication topology.\nNote: Upgrade won’t override currently existing ConfigMap, even if you change templated values in myvalues.yaml file. If you want to change the logLevel - edit ConfigMap from within the cluster using bash kubectl edit cm -n dell-replication-controller dell-replication-controller-config Upgrading with repctl Note: These steps assume that you already have repctl configured to use correct clusters, if you don’t know how to do that please refer to installing with repctl\nSteps\nFind a new version of deployment manifest that can be found in deploy/controller.yaml, with newer image pointing to the version of CSM Replication controller you want to upgrade to.\nApply said manifest using the usual repctl create command like so:\n./repctl create -f ../deploy/controller.yaml The output should have this line Successfully updated existing deployment: dell-replication-controller-manager\nCheck if everything is OK by querying your Kubernetes clusters using kubectl using a kubectl get:\nkubectl get pods -n dell-replication-controller` Your pods should be READY and RUNNING.\nReplication CRD version update CRD dellcsireplicationgroups.replication.storage.dell.com has been updated to version v1 in CSM Replication v1.4.0. To facilitate the continued use of existing DellCSIReplicationGroup CR objects after upgrading to CSM Replication v1.4.0 or later, an init container will be deployed during upgrade. The init container updates the existing CRs with necessary steps for their continued use.\nNote: Do not update the CRD as part of upgrade. An init container included in the replication controller pod takes care of updating existing CRD and CR versions.\nStarting from CSM Replication v1.6.0, the init container has been removed. Therefore, when upgrading from versions older than v1.4.0 to v1.6.0 or any later versions, it is mandatory to perform an intermediate upgrade to either v1.4.0 or v1.5.0 before proceeding with any further upgrades.\n","categories":"","description":"Upgrade guide\n","excerpt":"Upgrade guide\n","ref":"/csm-docs/v1/replication/upgrade/","tags":"","title":"Upgrade"},{"body":"CSM Replication module consists of two components:\nCSM Replication sidecar (installed along with the driver) CSM Replication controller Those two components should be upgraded separately. When upgrading them ensure that you use the same versions for both sidecar and controller, because different versions could be incompatible with each other.\nNote: While upgrading the module via helm, the replicas variable in myvalues.yaml can be at most one less than the number of worker nodes.\nUpdating CSM Replication sidecar To upgrade the CSM Replication sidecar that is installed along with the driver, the following steps are required.\nNote: These steps refer to the values file and csi-install.sh script that was used during the initial installation of the Dell CSI driver.\nSteps\nUpdate the controller.replication.image value in the values files to reference the new CSM Replication sidecar image. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace \u003cyour-namespace\u003e --values ./myvalues.yaml --upgrade Run the same command on the second Kubernetes cluster if you use multi-cluster replication topology For more information on upgrading the CSI driver, please visit the CSI driver upgrade page.\nPowerScale On PowerScale systems, an additional step is needed when upgrading to CSM Replication v1.4.0 or later. Because the SyncIQ policy created on the target-side storage array is no longer used, it must be deleted for any existing DellCSIReplicationGroup objects after performing the upgrade to the CSM Replication sidecar and PowerScale CSI driver. These steps should be performed before the DellCSIReplicationGroup objects are used with the new version of the CSI driver. Until this step is performed, existing DellCSIReplicationGroup objects will display an UNKNOWN link state.\nLog in to the target PowerScale array. Navigate to the Data Protection \u003e SyncIQ page and select the Policies tab. Delete disabled, target-side SyncIQ policies that are used for CSM Replication. Such policies will be distinguished by their names, of the format \u003cprefix\u003e-\u003ckubernetes namespace\u003e-\u003cIP of replication destination\u003e-\u003cRPO duration\u003e. Updating CSM Replication controller Make sure the appropriate release branch is available on the machine performing the upgrade by running:\ngit clone -b v1.5.0 https://github.com/dell/csm-replication.git Upgrading with Helm This option will only work if you have previously installed replication via Helm chart, available since version 1.1. If you used simple manifest or repctl please use upgrading with repctl\nSteps\nUpdate the image value in the values files to reference the new CSM Replication controller image or use a new version of the csm-replication Helm chart.\nRun the install script with the option --upgrade by running:\ncd ./scripts \u0026\u0026 ./install.sh --values ./myvalues.yaml --upgrade Run the same command on the second Kubernetes cluster if you use multi-cluster replication topology.\nNote: Upgrade won’t override currently existing ConfigMap, even if you change templated values in myvalues.yaml file. If you want to change the logLevel - edit ConfigMap from within the cluster using bash kubectl edit cm -n dell-replication-controller dell-replication-controller-config Upgrading with repctl Note: These steps assume that you already have repctl configured to use correct clusters, if you don’t know how to do that please refer to installing with repctl\nSteps\nFind a new version of deployment manifest that can be found in deploy/controller.yaml, with newer image pointing to the version of CSM Replication controller you want to upgrade to.\nApply said manifest using the usual repctl create command like so:\n./repctl create -f ../deploy/controller.yaml The output should have this line Successfully updated existing deployment: dell-replication-controller-manager\nCheck if everything is OK by querying your Kubernetes clusters using kubectl using a kubectl get:\nkubectl get pods -n dell-replication-controller` Your pods should be READY and RUNNING.\nReplication CRD version update CRD dellcsireplicationgroups.replication.storage.dell.com has been updated to version v1 in CSM Replication v1.4.0. To facilitate the continued use of existing DellCSIReplicationGroup CR objects after upgrading to CSM Replication v1.4.0 or later, an init container will be deployed during upgrade. The init container updates the existing CRs with necessary steps for their continued use.\nNote: Do not update the CRD as part of upgrade. An init container included in the replication controller pod takes care of updating existing CRD and CR versions.\n","categories":"","description":"Upgrade guide\n","excerpt":"Upgrade guide\n","ref":"/csm-docs/v2/replication/upgrade/","tags":"","title":"Upgrade"},{"body":"CSM Replication module consists of two components:\nCSM Replication sidecar (installed along with the driver) CSM Replication controller Those two components should be upgraded separately. When upgrading them ensure that you use the same versions for both sidecar and controller, because different versions could be incompatible with each other.\nNote: While upgrading the module via helm, the replicas variable in myvalues.yaml can be at most one less than the number of worker nodes.\nUpdating CSM Replication sidecar To upgrade the CSM Replication sidecar that is installed along with the driver, the following steps are required.\nNote: These steps refer to the values file and csi-install.sh script that was used during the initial installation of the Dell CSI driver.\nSteps\nUpdate the controller.replication.image value in the values files to reference the new CSM Replication sidecar image. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace \u003cyour-namespace\u003e --values ./myvalues.yaml --upgrade Run the same command on the second Kubernetes cluster if you use multi-cluster replication topology For more information on upgrading the CSI driver, please visit the CSI driver upgrade page.\nPowerScale On PowerScale systems, an additional step is needed when upgrading to CSM Replication v1.4.0 or later. Because the SyncIQ policy created on the target-side storage array is no longer used, it must be deleted for any existing DellCSIReplicationGroup objects after performing the upgrade to the CSM Replication sidecar and PowerScale CSI driver. These steps should be performed before the DellCSIReplicationGroup objects are used with the new version of the CSI driver. Until this step is performed, existing DellCSIReplicationGroup objects will display an UNKNOWN link state.\nLog in to the target PowerScale array. Navigate to the Data Protection \u003e SyncIQ page and select the Policies tab. Delete disabled, target-side SyncIQ policies that are used for CSM Replication. Such policies will be distinguished by their names, of the format \u003cprefix\u003e-\u003ckubernetes namespace\u003e-\u003cIP of replication destination\u003e-\u003cRPO duration\u003e. Updating CSM Replication controller Make sure the appropriate release branch is available on the machine performing the upgrade by running:\ngit clone -b \u003crelease-branch\u003e https://github.com/dell/csm-replication.git\nUpgrading with Helm This option will only work if you have previously installed replication via Helm chart, available since version 1.1. If you used simple manifest or repctl please use upgrading with repctl\nSteps\nUpdate the image value in the values files to reference the new CSM Replication controller image or use a new version of the csm-replication Helm chart.\nRun the install script with the option --upgrade by running:\ncd ./scripts \u0026\u0026 ./install.sh --values ./myvalues.yaml --upgrade\nRun the same command on the second Kubernetes cluster if you use multi-cluster replication topology.\nNote: Upgrade won’t override currently existing ConfigMap, even if you change templated values in myvalues.yaml file. If you want to change the logLevel - edit ConfigMap from within the cluster using kubectl edit cm -n dell-replication-controller dell-replication-controller-config\nUpgrading with repctl Note: These steps assume that you already have repctl configured to use correct clusters, if you don’t know how to do that please refer to installing with repctl\nSteps\nFind a new version of deployment manifest that can be found in deploy/controller.yaml, with newer image pointing to the version of CSM Replication controller you want to upgrade to.\nApply said manifest using the usual repctl create command like so:\n./repctl create -f ../deploy/controller.yaml.\nThe output should have this line Successfully updated existing deployment: dell-replication-controller-manager\nCheck if everything is OK by querying your Kubernetes clusters using kubectl using a kubectl get:\nkubectl get pods -n dell-replication-controller\nYour pods should be READY and RUNNING.\nReplication CRD version update CRD dellcsireplicationgroups.replication.storage.dell.com has been updated to version v1 in CSM Replication v1.4.0. To facilitate the continued use of existing DellCSIReplicationGroup CR objects after upgrading to CSM Replication v1.4.0 or later, an init container will be deployed during upgrade. The init container updates the existing CRs with necessary steps for their continued use.\nNote: Do not update the CRD as part of upgrade. An init container included in the replication controller pod takes care of updating existing CRD and CR versions.\n","categories":"","description":"Upgrade guide\n","excerpt":"Upgrade guide\n","ref":"/csm-docs/v3/replication/upgrade/","tags":"","title":"Upgrade"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Replication.\nUninstalling replication controller To uninstall the replication controller, you can use the script uninstall.sh located in the scripts folder:\n./uninstall.sh This script will automatically detect how the current version was installed (repctl or Helm) and use the correct method to delete it.\nYou can also manually uninstall the replication controller using a method that depends on how you installed it.\nIf replication controller was installed using helm, use this command:\nhelm delete -n dell-replication-controller replication If you used controller.yaml manifest with either kubectl or repctl, use this:\nkubectl delete -f deploy/controller.yaml To delete the replication group CRD, you can run the command:\nkubectl delete crd dellcsireplicationgroups.replication.storage.dell.com All replication groups should be deleted before deleting the replication group CRD.\nNOTE: Be sure to run the chosen command on all clusters where you want to uninstall the replication controller/CRD.\nUninstalling the replication sidecar To uninstall the replication sidecar, you need to uninstall the CSI Driver. Please view the uninstall page for the driver itself.\n","categories":"","description":"Dell Container Storage Modules (CSM) for Replication Uninstallation\n","excerpt":"Dell Container Storage Modules (CSM) for Replication Uninstallation\n","ref":"/csm-docs/docs/replication/uninstall/","tags":"","title":"Uninstall"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Replication.\nUninstalling replication controller To uninstall the replication controller, you can use the script uninstall.sh located in the scripts folder:\n./uninstall.sh This script will automatically detect how the current version was installed (repctl or Helm) and use the correct method to delete it.\nYou can also manually uninstall the replication controller using a method that depends on how you installed it.\nIf replication controller was installed using helm, use this command:\nhelm delete -n dell-replication-controller replication If you used controller.yaml manifest with either kubectl or repctl, use this:\nkubectl delete -f deploy/controller.yaml To delete the replication group CRD, you can run the command:\nkubectl delete crd dellcsireplicationgroups.replication.storage.dell.com All replication groups should be deleted before deleting the replication group CRD.\nNOTE: Be sure to run the chosen command on all clusters where you want to uninstall the replication controller/CRD.\nUninstalling the replication sidecar To uninstall the replication sidecar, you need to uninstall the CSI Driver. Please view the uninstall page for the driver itself.\n","categories":"","description":"Dell Container Storage Modules (CSM) for Replication Uninstallation\n","excerpt":"Dell Container Storage Modules (CSM) for Replication Uninstallation\n","ref":"/csm-docs/v1/replication/uninstall/","tags":"","title":"Uninstall"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Replication.\nUninstalling replication controller To uninstall the replication controller, you can use the script uninstall.sh located in the scripts folder:\n./uninstall.sh This script will automatically detect how the current version was installed (repctl or Helm) and use the correct method to delete it.\nYou can also manually uninstall the replication controller using a method that depends on how you installed it.\nIf replication controller was installed using helm, use this command:\nhelm delete -n dell-replication-controller replication If you used controller.yaml manifest with either kubectl or repctl, use this:\nkubectl delete -f deploy/controller.yaml To delete the replication group CRD, you can run the command:\nkubectl delete crd dellcsireplicationgroups.replication.storage.dell.com All replication groups should be deleted before deleting the replication group CRD.\nNOTE: Be sure to run the chosen command on all clusters where you want to uninstall the replication controller/CRD.\nUninstalling the replication sidecar To uninstall the replication sidecar, you need to uninstall the CSI Driver. Please view the uninstall page for the driver itself.\n","categories":"","description":"Dell Container Storage Modules (CSM) for Replication Uninstallation\n","excerpt":"Dell Container Storage Modules (CSM) for Replication Uninstallation\n","ref":"/csm-docs/v2/replication/uninstall/","tags":"","title":"Uninstall"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Replication.\nUninstalling replication controller To uninstall the replication controller, you can use the script uninstall.sh located in the scripts folder:\n./uninstall.sh This script will automatically detect how the current version was installed (repctl or Helm) and use the correct method to delete it.\nYou can also manually uninstall the replication controller using a method that depends on how you installed it.\nIf replication controller was installed using helm, use this command:\nhelm delete -n dell-replication-controller replication If you used controller.yaml manifest with either kubectl or repctl, use this:\nkubectl delete -f deploy/controller.yaml To delete the replication group CRD, you can run the command:\nkubectl delete crd dellcsireplicationgroups.replication.storage.dell.com All replication groups should be deleted before deleting the replication group CRD.\nNOTE: Be sure to run the chosen command on all clusters where you want to uninstall the replication controller/CRD.\nUninstalling the replication sidecar To uninstall the replication sidecar, you need to uninstall the CSI Driver. Please view the uninstall page for the driver itself.\n","categories":"","description":"Dell Container Storage Modules (CSM) for Replication Uninstallation\n","excerpt":"Dell Container Storage Modules (CSM) for Replication Uninstallation\n","ref":"/csm-docs/v3/replication/uninstall/","tags":"","title":"Uninstall"},{"body":"Troubleshooting links for Container Storage Modules:\nCSI Drivers\nCSM for Authorization\nCSM for Observability\nCSM for Replication\nCSM for Resiliency\nCSM for Encryption\nCSM for Application Mobility\nCSM Operator\n","categories":"","description":"Dell Container Storage Modules (CSM) troubleshooting information\n","excerpt":"Dell Container Storage Modules (CSM) troubleshooting information\n","ref":"/csm-docs/docs/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Troubleshooting links for Container Storage Modules:\nCSI Drivers\nCSM for Authorization\nCSM for Observability\nCSM for Replication\nCSM for Resiliency\nCSM for Encryption\nCSM for Application Mobility\nCSM Operator\n","categories":"","description":"Dell Container Storage Modules (CSM) troubleshooting information\n","excerpt":"Dell Container Storage Modules (CSM) troubleshooting information\n","ref":"/csm-docs/v1/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Troubleshooting links for Container Storage Modules:\nCSI Drivers\nCSM for Authorization\nCSM for Observability\nCSM for Replication\nCSM for Resiliency\nCSM for Encryption\nCSM for Application Mobility\nCSM Operator\n","categories":"","description":"Dell Container Storage Modules (CSM) troubleshooting information\n","excerpt":"Dell Container Storage Modules (CSM) troubleshooting information\n","ref":"/csm-docs/v2/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Troubleshooting links for Container Storage Modules:\nCSI Drivers\nCSM for Authorization\nCSM for Observability\nCSM for Replication\nCSM for Resiliency\nCSM for Encryption\nCSM for Application Mobility\nCSM Operator\n","categories":"","description":"Dell Container Storage Modules (CSM) troubleshooting information\n","excerpt":"Dell Container Storage Modules (CSM) troubleshooting information\n","ref":"/csm-docs/v3/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"The tech-preview releases of Container Storage Modules for Application Mobility and Encryption require a license. This section details how to request a license.\nRequesting a License Request a license using the Container Storage Modules License Request by providing these details: Full Name: Full name of the person requesting the license Email Address: The license will be emailed to this email address Company / Organization: Company or organization where the license will be used License Type: Select either Application Mobility or Encryption, depending on the CSM module that will be used with the license List of kube-system namespace UIDs: The license will only function on the provided list of Kubernetes clusters. Find the UID of the kube-system namespace using kubectl get ns kube-system -o yaml or similar oc command. Provide as a comma separated list of UIDs. (Optional) Send me a copy of my responses: A copy of the license request will be sent to the provided email address After submitting the form, a response will be provided within several business days with an attachment containing the license. Refer to the specific CSM module documentation for adding the license to the Kubernetes cluster. ","categories":"","description":"Dell Container Storage Modules (CSM) License\n","excerpt":"Dell Container Storage Modules (CSM) License\n","ref":"/csm-docs/docs/license/","tags":"","title":"License"},{"body":"The tech-preview releases of Container Storage Modules for Application Mobility and Encryption require a license. This section details how to request a license.\nRequesting a License Request a license using the Container Storage Modules License Request by providing these details: Full Name: Full name of the person requesting the license Email Address: The license will be emailed to this email address Company / Organization: Company or organization where the license will be used License Type: Select either Application Mobility or Encryption, depending on the CSM module that will be used with the license List of kube-system namespace UIDs: The license will only function on the provided list of Kubernetes clusters. Find the UID of the kube-system namespace using kubectl get ns kube-system -o yaml or similar oc command. Provide as a comma separated list of UIDs. (Optional) Send me a copy of my responses: A copy of the license request will be sent to the provided email address After submitting the form, a response will be provided within several business days with an attachment containing the license. Refer to the specific CSM module documentation for adding the license to the Kubernetes cluster. ","categories":"","description":"Dell Container Storage Modules (CSM) License\n","excerpt":"Dell Container Storage Modules (CSM) License\n","ref":"/csm-docs/v1/license/","tags":"","title":"License"},{"body":"The tech-preview releases of Container Storage Modules for Application Mobility and Encryption require a license. This section details how to request a license.\nRequesting a License Request a license using the Container Storage Modules License Request by providing these details: Full Name: Full name of the person requesting the license Email Address: The license will be emailed to this email address Company / Organization: Company or organization where the license will be used License Type: Select either Application Mobility or Encryption, depending on the CSM module that will be used with the license List of kube-system namespace UIDs: The license will only function on the provided list of Kubernetes clusters. Find the UID of the kube-system namespace using kubectl get ns kube-system -o yaml or similar oc command. Provide as a comma separated list of UIDs. (Optional) Send me a copy of my responses: A copy of the license request will be sent to the provided email address After submitting the form, a response will be provided within several business days with an attachment containing the license. Refer to the specific CSM module documentation for adding the license to the Kubernetes cluster. ","categories":"","description":"Dell Container Storage Modules (CSM) License\n","excerpt":"Dell Container Storage Modules (CSM) License\n","ref":"/csm-docs/v2/license/","tags":"","title":"License"},{"body":"The tech-preview releases of Container Storage Modules for Application Mobility and Encryption require a license. This section details how to request a license.\nRequesting a License Request a license using the Container Storage Modules License Request by providing these details: Full Name: Full name of the person requesting the license Email Address: The license will be emailed to this email address Company / Organization: Company or organization where the license will be used License Type: Select either Application Mobility or Encryption, depending on the CSM module that will be used with the license List of kube-system namespace UIDs: The license will only function on the provided list of Kubernetes clusters. Find the UID of the kube-system namespace using kubectl get ns kube-system -o yaml or similar oc command. Provide as a comma separated list of UIDs. (Optional) Send me a copy of my responses: A copy of the license request will be sent to the provided email address After submitting the form, a response will be provided within several business days with an attachment containing the license. Refer to the specific CSM module documentation for adding the license to the Kubernetes cluster. ","categories":"","description":"Dell Container Storage Modules (CSM) License\n","excerpt":"Dell Container Storage Modules (CSM) License\n","ref":"/csm-docs/v3/license/","tags":"","title":"License"},{"body":"Release notes for Container Storage Modules:\nCSI Drivers\nCSM for Authorization\nCSM for Observability\nCSM for Replication\nCSM for Resiliency\nCSM for Encryption\nCSM for Application Mobility\nCSM Operator\nCSM Installation Wizard\n","categories":"","description":"Dell Container Storage Modules (CSM) release notes\n","excerpt":"Dell Container Storage Modules (CSM) release notes\n","ref":"/csm-docs/docs/release/","tags":"","title":"Release notes"},{"body":"Release notes for Container Storage Modules:\nCSI Drivers\nCSM for Authorization\nCSM for Observability\nCSM for Replication\nCSM for Resiliency\nCSM for Encryption\nCSM for Application Mobility\nCSM Operator\nCSM Installation Wizard\n","categories":"","description":"Dell Container Storage Modules (CSM) release notes\n","excerpt":"Dell Container Storage Modules (CSM) release notes\n","ref":"/csm-docs/v1/release/","tags":"","title":"Release notes"},{"body":" CSM 1.7.1 is applicable to helm based installations of PowerFlex driver.\nRelease notes for Container Storage Modules:\nCSI Drivers\nCSM for Authorization\nCSM for Observability\nCSM for Replication\nCSM for Resiliency\nCSM for Encryption\nCSM for Application Mobility\nCSM Operator\n","categories":"","description":"Dell Container Storage Modules (CSM) release notes\n","excerpt":"Dell Container Storage Modules (CSM) release notes\n","ref":"/csm-docs/v2/release/","tags":"","title":"Release notes"},{"body":"Release notes for Container Storage Modules:\nCSI Drivers\nCSM for Authorization\nCSM for Observability\nCSM for Replication\nCSM for Resiliency\nCSM for Encryption\nCSM for Application Mobility\nCSM Operator\n","categories":"","description":"Dell Container Storage Modules (CSM) release notes\n","excerpt":"Dell Container Storage Modules (CSM) release notes\n","ref":"/csm-docs/v3/release/","tags":"","title":"Release notes"},{"body":"Reporting Security Issues/Vulnerabilities The Dell Container Storage Modules team and community take security bugs seriously. We sincerely appreciate all your efforts and responsibility to disclose your findings.\nTo report a security issue, please submit the security advisory form “Report a Vulnerability”.\nCSM recommends staying on the latest release of Dell Container Storage Modules to take advantage of new features, enhancements, bug fixes, and security fixes.\n","categories":"","description":"Dell Container Storage Modules (CSM) Security Policy\n","excerpt":"Dell Container Storage Modules (CSM) Security Policy\n","ref":"/csm-docs/docs/securitypolicy/","tags":"","title":"Security Policy"},{"body":"The CSM services/repositories are inspected for security vulnerabilities via gosec, Instructions for reporting a vulnerability can be found on the Dell Vulnerability Policy\nCSM recommends to stay on the lastest release of Dell Container Storage Modules to take advantage of new features, enhancements, bug fixes, and security fixes.\n","categories":"","description":"Dell Container Storage Modules (CSM) Security Policy\n","excerpt":"Dell Container Storage Modules (CSM) Security Policy\n","ref":"/csm-docs/v1/securitypolicy/","tags":"","title":"Security Policy"},{"body":"The CSM services/repositories are inspected for security vulnerabilities via gosec, Instructions for reporting a vulnerability can be found on the Dell Vulnerability Policy\nCSM recommends to stay on the lastest release of Dell Container Storage Modules to take advantage of new features, enhancements, bug fixes, and security fixes.\n","categories":"","description":"Dell Container Storage Modules (CSM) Security Policy\n","excerpt":"Dell Container Storage Modules (CSM) Security Policy\n","ref":"/csm-docs/v2/securitypolicy/","tags":"","title":"Security Policy"},{"body":"The CSM services/repositories are inspected for security vulnerabilities via gosec, Instructions for reporting a vulnerability can be found on the Dell Vulnerability Policy\nCSM recommends to stay on the lastest release of Dell Container Storage Modules to take advantage of new features, enhancements, bug fixes, and security fixes.\n","categories":"","description":"Dell Container Storage Modules (CSM) Security Policy\n","excerpt":"Dell Container Storage Modules (CSM) Security Policy\n","ref":"/csm-docs/v3/securitypolicy/","tags":"","title":"Security Policy"},{"body":"For all your support needs or to follow the latest ongoing discussions and updates, join our Slack group. Click Here to request your invite.\nYou can also interact with us on GitHub by creating a GitHub Issue.\n","categories":"","description":"Dell Container Storage Modules (CSM) support\n","excerpt":"Dell Container Storage Modules (CSM) support\n","ref":"/csm-docs/docs/support/","tags":"","title":"Support"},{"body":"For all your support needs or to follow the latest ongoing discussions and updates, join our Slack group. Click Here to request your invite.\nYou can also interact with us on GitHub by creating a GitHub Issue.\n","categories":"","description":"Dell Container Storage Modules (CSM) support\n","excerpt":"Dell Container Storage Modules (CSM) support\n","ref":"/csm-docs/v1/support/","tags":"","title":"Support"},{"body":"For all your support needs or to follow the latest ongoing discussions and updates, join our Slack group. Click Here to request your invite.\nYou can also interact with us on GitHub by creating a GitHub Issue.\n","categories":"","description":"Dell Container Storage Modules (CSM) support\n","excerpt":"Dell Container Storage Modules (CSM) support\n","ref":"/csm-docs/v2/support/","tags":"","title":"Support"},{"body":"For all your support needs or to follow the latest ongoing discussions and updates, join our Slack group. Click Here to request your invite.\nYou can also interact with us on GitHub by creating a GitHub Issue.\n","categories":"","description":"Dell Container Storage Modules (CSM) support\n","excerpt":"Dell Container Storage Modules (CSM) support\n","ref":"/csm-docs/v3/support/","tags":"","title":"Support"},{"body":"","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) References\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) References\n","ref":"/csm-docs/docs/references/","tags":"","title":"References"},{"body":"","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) References\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) References\n","ref":"/csm-docs/v1/references/","tags":"","title":"References"},{"body":"","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) References\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) References\n","ref":"/csm-docs/v2/references/","tags":"","title":"References"},{"body":"","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) References\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) References\n","ref":"/csm-docs/v3/references/","tags":"","title":"References"},{"body":" The CSM Authorization RPM will be deprecated in a future release. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nThe Dell Technologies (Dell) Container Storage Modules (CSM) enables simple and consistent integration and automation experiences, extending enterprise storage capabilities to Kubernetes for cloud-native stateful applications. It reduces management complexity so developers can independently consume enterprise storage with ease and automate daily operations such as provisioning, snapshotting, replication, observability, authorization, application mobility, encryption, and resiliency.\nCSM is made up of multiple components including modules (enterprise capabilities), CSI drivers (storage enablement), and other related applications (deployment, feature controllers, etc).\nAuthorization CSM for Authorization provides storage and Kubernetes administrators the ability to apply RBAC for Dell CSI Drivers. It does this by deploying a proxy between the CSI driver and the storage system to enforce role-based access and usage rules.\n…Learn more Supports PowerFlex PowerScale PowerMax Replication CSM for Replication project aims to bring Replication \u0026 Disaster Recovery capabilities of Dell Storage Arrays to Kubernetes clusters. It helps you replicate groups of volumes and can provide you a way to restart applications in case of both planned and unplanned migration. …Learn more Supports PowerFlex PowerStore PowerScale PowerMax Resiliency CSM for Resiliency is designed to make Kubernetes Applications, including those that utilize persistent storage, more resilient to various failures. …Learn more Supports PowerFlex PowerScale Unity PowerStore Observability CSM for Observability provides visibility on the capacity of the volumes/file shares that is being managed with Dell CSM CSI (Container Storage Interface) drivers along with their performance in terms of bandwidth, IOPS, and response time. …Learn more Supports PowerFlex PowerStore PowerScale PowerMax Application Mobility Container Storage Modules for Application Mobility provide Kubernetes administrators the ability to clone their stateful application workloads and application data to other clusters, either on-premise or in the cloud. …Learn more Supports all platforms Encryption Encryption provides the capability to encrypt user data residing on volumes created by Dell CSI Drivers. …Learn more Supports PowerScale License The tech-preview releases of Application Mobility and Encryption require a license. Request a license using the Container Storage Modules License Request by providing the requested details. …Learn more Required for Application Mobility \u0026 Encryption ","categories":"","description":"","excerpt":" The CSM Authorization RPM will be deprecated in a future release. It …","ref":"/csm-docs/docs/","tags":"","title":"Container Storage Modules"},{"body":"\u003c!DOCTYPE html\u003e CSM Installation Wizard | Dell Technologies Container Storage Modules (CSM) Installation Wizard Configure installation settings and repository Installation Type Select the Installation type Helm Operator Array Select the Array PowerStore PowerMax PowerScale PowerFlex Unity XT Image Repository Reset to default CSM Version CSM 1.7 CSM 1.8 CSM 1.9.3 Select modules for installation Observability Observability Metrics Observability Observability Metrics Observability Topology Observability Otel Collector Replication Target Cluster ID Target Array ID Target Unisphere Resiliency Resiliency Label Value Reset to default Array Connectivity Poll Rate Reset to default Skip Array Connection Validation Leader Election Driver Pod Label Value Reset to default Ignore Volumeless Pods Array Connectivity ConnectionLoss Threshold Reset to default Select the features for installation Authorization Authorization Skip Certificate Validation Proxy Host Cert Manager Health Monitor Migration Resizer Snapshot Snap Name Prefix Reset to default Rename SDC SDC Prefix Approve SDC Storage Capacity Tracking Volume Group Snapshot Max Volumes Per Node Reset to default NFS volume operations Quota Other configurations for installation Storage Array Id Storage Array Endpoint URL Storage Array Backup Endpoint URL Managed Array Id Managed Array Endpoint URL CSI Reverse Proxy Cluster Prefix Port Groups Transport Protocol iSCSI CHAP Node Topology vSphere fc Port Group fc Host Name vCenter Host vCenter Cred Secret Allow Monitoring Pod Certificate Secret Count FS Group Policy ReadWriteOnceWithFSType File None Controller Pods Count Reset to default Volume Name Prefix Reset to default Install Controller Pods On Control Plane Install Node Pods On Control Plane Node Selector Label Reset to default Taint Reset to default Namespace Reset to default Generate YAML Copy Click on the documentation link for the steps to generate the certificates © 2024 The Dell Technologies All Rights Reserved Privacy Policy ","categories":"","description":"","excerpt":"\u003c!DOCTYPE html\u003e CSM Installation Wizard | Dell Technologies Container …","ref":"/csm-docs/docs/deployment/csminstallationwizard/src/","tags":"","title":""},{"body":"\u003c!DOCTYPE html\u003e CSM Installation Wizard | Dell Technologies Container Storage Modules (CSM) Installation Wizard Configure installation settings and repository Installation Type Select the Installation type Helm Operator Array Select the Array PowerStore PowerMax PowerScale PowerFlex Unity XT Image Repository Reset to default CSM Version CSM 1.7 CSM 1.8 Select modules for installation Observability Observability Metrics Observability Observability Metrics Observability Topology Observability Otel Collector Replication Target Cluster ID Target Array ID Target Unisphere Resiliency Resiliency Label Value Reset to default Array Connectivity Poll Rate Reset to default Skip Array Connection Validation Leader Election Driver Pod Label Value Reset to default Ignore Volumeless Pods Array Connectivity ConnectionLoss Threshold Reset to default Select the features for installation Authorization Authorization Skip Certificate Validation Proxy Host Cert Manager Health Monitor Migration Resizer Snapshot Snap Name Prefix Reset to default Rename SDC SDC Prefix Approve SDC Storage Capacity Tracking Volume Group Snapshot Max Volumes Per Node Reset to default NFS volume operations Quota Other configurations for installation Storage Array Id Storage Array Endpoint URL Storage Array Backup Endpoint URL Managed Array Id Managed Array Endpoint URL CSI Reverse Proxy Cluster Prefix Port Groups Transport Protocol iSCSI CHAP Node Topology vSphere fc Port Group fc Host Name vCenter Host vCenter Cred Secret Allow Monitoring Pod Certificate Secret Count FS Group Policy ReadWriteOnceWithFSType File None Controller Pods Count Reset to default Volume Name Prefix Reset to default Install Controller Pods On Control Plane Install Node Pods On Control Plane Node Selector Label Reset to default Taint Reset to default Namespace Reset to default Generate YAML Copy Click on the documentation link for the steps to generate the certificates © 2023 The Dell Technologies All Rights Reserved Privacy Policy ","categories":"","description":"","excerpt":"\u003c!DOCTYPE html\u003e CSM Installation Wizard | Dell Technologies Container …","ref":"/csm-docs/v1/deployment/csminstallationwizard/src/","tags":"","title":""},{"body":"\u003c!DOCTYPE html\u003e CSM Installation Wizard | Dell Technologies Container Storage Modules (CSM) Installation Wizard Configure installation settings and repository Installation Type Select the Installation type Helm Operator Array Select the Array PowerStore PowerMax PowerScale PowerFlex Unity Image Repository Reset to default CSM Version CSM 1.7.0 CSM 1.7.1 Select modules for installation Observability Observability Metrics Replication Resiliency Select the features for installation Authorization Authorization Skip Certificate Validation Proxy Host Cert Manager Health Monitor Migration Resizer Snapshot Snap Name Prefix Reset to default Storage Capacity Tracking Volume Group Snapshot Other configurations for installation Storage Array Id Storage Array Endpoint URL Storage Array Backup Endpoint URL CSI Reverse Proxy Cluster Prefix Port Groups vSphere fc Port Group fc Host Name vCenter Host vCenter Cred Secret Allow Monitoring Pod Certificate Secret Count FS Group Policy ReadWriteOnceWithFSType File None Controller Pods Count Reset to default Volume Name Prefix Reset to default Install Controller Pods On Control Plane Install Node Pods On Control Plane Node Selector Label Reset to default Taint Reset to default Namespace Reset to default Generate YAML Copy Click on the documentation link for the steps to generate the certificates © 2023 The Dell Technologies All Rights Reserved Privacy Policy ","categories":"","description":"","excerpt":"\u003c!DOCTYPE html\u003e CSM Installation Wizard | Dell Technologies Container …","ref":"/csm-docs/v2/deployment/csminstallationwizard/src/","tags":"","title":""},{"body":"\u003c!DOCTYPE html\u003e CSM Installation Wizard | Dell Technologies Container Storage Modules (CSM) Installation Wizard Configure installation settings and repository Installation Type Select the Installation type Helm Operator Array Select the Array PowerStore PowerMax PowerScale PowerFlex Unity Image Repository Reset to default CSM Version CSM 1.6 Select modules for installation Application Mobility Velero Observability Observability Metrics Replication Resiliency Select the features for installation. Authorization Authorization Skip Certificate Validation Proxy Host Cert Manager Health Monitor Migration Resizer Snapshot Storage Capacity Tracking Volume Group Snapshot Other configurations for installation Storage Array Id Storage Array Endpoint URL Storage Array Backup Endpoint URL CSI Reverse Proxy Cluster Prefix Port Groups vSphere fc Port Group fc Host Name vCenter Host vCenter Cred Secret Controller Pods Count Reset to default Install Controller Pods On Control Plane Install Node Pods On Control Plane Node Selector Label Reset to default Single Namespace Driver Namespace Reset to default Module Namespace Reset to default Generate YAML Copy Click on the documentation link for the steps to generate the certificates © 2023 The Dell Technologies All Rights Reserved Privacy Policy ","categories":"","description":"","excerpt":"\u003c!DOCTYPE html\u003e CSM Installation Wizard | Dell Technologies Container …","ref":"/csm-docs/v3/deployment/csminstallationwizard/src/","tags":"","title":""},{"body":" The CSM Authorization karavictl CLI is no longer actively maintained or supported. It will be deprecated in CSM 2.0.\nInstall CSM Authorization via Dell CSM Operator The CSM Authorization module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nPrerequisite Execute kubectl create namespace authorization to create the authorization namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘authorization’.\nInstall cert-manager CRDs\nkubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.crds.yaml Prepare samples/authorization/config.yaml provided here which contains the JWT signing secret. The following table lists the configuration parameters.\nParameter Description Required Default web.jwtsigningsecret String used to sign JSON Web Tokens true secret Example:\nweb: jwtsigningsecret: randomString123 After editing the file, run this command to create a secret called karavi-config-secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/authorization/config.yaml Use this command to replace or update the secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/authorization/config.yaml -o yaml --dry-run=client | kubectl replace -f - Create the karavi-storage-secret using the file provided here to store storage system credentials.\nUse this command to create the secret:\nkubectl create -f samples/authorization/karavi-storage-secret.yaml Prepare a storage class for Redis to use for persistence. If not supplied, the default storage class in your environment is used.\nExample, if using CSM Authorization for PowerScale:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon provisioner: csi-isilon.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: # The name of the access zone a volume can be created in # Optional: true # Default value: default value specified in values.yaml # Examples: System, zone1 AccessZone: System # The base path for the volumes to be created on PowerScale cluster. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Optional: true # Default value: value specified in values.yaml for isiPath # Examples: /ifs/data/csi, /ifs/engineering IsiPath: /ifs/data/csi # The permissions for isi volume directory path # This value overrides the isiVolumePathPermissions attribute of corresponding cluster config in secret, if present # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" #IsiVolumePathPermissions: \"0777\" # AccessZone groupnet service IP. Update AzServiceIP if different than endpoint. # Optional: true # Default value: endpoint of the cluster ClusterName #AzServiceIP : 192.168.2.1 # When a PVC is being created, this parameter determines, when a node mounts the PVC, # whether to add the k8s node to the \"Root clients\" field or \"Clients\" field of the NFS export # Allowed values: # \"true\": adds k8s node to the \"Root clients\" field of the NFS export # \"false\": adds k8s node to the \"Clients\" field of the NFS export # Optional: true # Default value: \"false\" RootClientEnabled: \"false\" # Name of PowerScale cluster, where pv will be provisioned. # This name should match with name of one of the cluster configs in isilon-creds secret. # If this parameter is not specified, then default cluster config in isilon-creds secret # will be considered if available. # Optional: true #ClusterName: \u003ccluster_name\u003e # Sets the filesystem type which will be used to format the new volume # Optional: true # Default value: None #csi.storage.k8s.io/fstype: \"nfs\" # volumeBindingMode controls when volume binding and dynamic provisioning should occur. # Allowed values: # Immediate: indicates that volume binding and dynamic provisioning occurs once the # PersistentVolumeClaim is created # WaitForFirstConsumer: will delay the binding and provisioning of a PersistentVolume # until a Pod using the PersistentVolumeClaim is created # Default value: Immediate volumeBindingMode: Immediate # allowedTopologies helps scheduling pods on worker nodes which match all of below expressions. # If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologies # Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server #allowedTopologies: # - matchLabelExpressions: # - key: csi-isilon.dellemc.com/\u003cISILON_IP\u003e # values: # - csi-isilon.dellemc.com # specify additional mount options for when a Persistent Volume is being mounted on a node. # To mount volume with NFSv4, specify mount option vers=4. Make sure NFSv4 is enabled on the Isilon Cluster #mountOptions: [\"\u003cmountOption1\u003e\", \"\u003cmountOption2\u003e\", ..., \"\u003cmountOptionN\u003e\"] Save the file and create it by using\nkubectl create -f \u003cinput_file.yaml\u003e Install CSM Authorization Proxy Server Follow all the prerequisites.\nCreate a CR (Custom Resource) for Authorization using the sample file provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in the CR. This table lists the primary configurable parameters of the Authorization Proxy Server and their default values:\nParameter Description Required Default authorization This section configures the CSM-Authorization components. - - PROXY_HOST The hostname to configure the self-signed certificate (if applicable), and the proxy service Ingress. Yes csm-authorization.com PROXY_INGRESS_CLASSNAME The ingressClassName of the proxy-service Ingress. Yes nginx PROXY_INGRESS_HOSTS Additional host rules to be applied to the proxy-service Ingress. No authorization-ingress-nginx-controller.authorization.svc.cluster.local REDIS_STORAGE_CLASS The storage class for Redis to use for persistence. If not supplied, the default storage class is used. Yes - ingress-nginx This section configures the enablement of the NGINX Ingress Controller. - - enabled Enable/Disable deployment of the NGINX Ingress Controller. Set to false if you already have an Ingress Controller installed. No true cert-manager This section configures the enablement of cert-manager. - - enabled Enable/Disable deployment of cert-manager. Set to false if you already have cert-manager installed. No true Optional: To enable reporting of trace data with Zipkin, use the csm-config-params configMap in the sample CR or dynamically by editing the configMap.\nAdd the Zipkin values to the configMap where ZIPKIN_ADDRESS is the IP address or hostname of the Zipkin server.\nZIPKIN_URI: \"http://ZIPKIN_ADDRESS:9411/api/v2/spans\" ZIPKIN_PROBABILITY: \"1.0\" Execute this command to create the Authorization CR:\nkubectl create -f samples/authorization/csm_authorization_proxy_server_v190.yaml Note:\nThis command will deploy the Authorization Proxy Server in the namespace specified in the input YAML file. Create the karavi-auth-tls secret using your own certificate or by using a self-signed certificate generated via cert-manager.\nIf using your own certificate that is valid for each Ingress hostname, use this command to create the karavi-auth-tls secret:\nkubectl create secret tls karavi-auth-tls -n authorization --key \u003clocation-of-private-key-file\u003e --cert \u003clocation-of-certificate-file\u003e If using a self-signed certificate, prepare samples/authorization/certificate_v190.yaml provided here. An entry for each hostname specified in the CR must be added under dnsNames for the certificate to be valid for each Ingress.\nUse this command to create the karavi-auth-tls secret:\nkubectl create -f samples/authorization/certificate_v190.yaml Verify Installation of the CSM Authorization Proxy Server Once the Authorization CR is created, you can verify the installation as mentioned below:\nkubectl describe csm/\u003cname-of-custom-resource\u003e -n \u003cnamespace\u003e Install Karavictl Follow the instructions available in CSM Authorization for Installing karavictl.\nConfiguring the CSM Authorization Proxy Server Follow the instructions available in CSM Authorization for Configuring the CSM Authorization Proxy Server.\nConfiguring a Dell CSI Driver with CSM Authorization Follow the instructions available in CSM Authorization for Configuring a Dell CSI Driver with CSM for Authorization.\n","categories":"","description":"Installing Authorization via Dell CSM Operator\n","excerpt":"Installing Authorization via Dell CSM Operator\n","ref":"/csm-docs/docs/deployment/csmoperator/modules/authorization/","tags":"","title":"Authorization"},{"body":" The CSM Authorization karavictl CLI is no longer actively maintained or supported. It will be deprecated in CSM 2.0.\nInstall CSM Authorization via Dell CSM Operator The CSM Authorization module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nPrerequisite Execute kubectl create namespace authorization to create the authorization namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘authorization’.\nInstall cert-manager CRDs\nkubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.crds.yaml Prepare samples/authorization/config.yaml provided here which contains the JWT signing secret. The following table lists the configuration parameters.\nParameter Description Required Default web.jwtsigningsecret String used to sign JSON Web Tokens true secret Example:\nweb: jwtsigningsecret: randomString123 After editing the file, run this command to create a secret called karavi-config-secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/authorization/config.yaml Use this command to replace or update the secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/authorization/config.yaml -o yaml --dry-run=client | kubectl replace -f - Create the karavi-storage-secret using the file provided here to store storage system credentials.\nUse this command to create the secret:\nkubectl create -f samples/authorization/karavi-storage-secret.yaml Prepare a storage class for Redis to use for persistence. If not supplied, the default storage class in your environment is used.\nExample, if using CSM Authorization for PowerScale:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon provisioner: csi-isilon.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: # The name of the access zone a volume can be created in # Optional: true # Default value: default value specified in values.yaml # Examples: System, zone1 AccessZone: System # The base path for the volumes to be created on PowerScale cluster. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Optional: true # Default value: value specified in values.yaml for isiPath # Examples: /ifs/data/csi, /ifs/engineering IsiPath: /ifs/data/csi # The permissions for isi volume directory path # This value overrides the isiVolumePathPermissions attribute of corresponding cluster config in secret, if present # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" #IsiVolumePathPermissions: \"0777\" # AccessZone groupnet service IP. Update AzServiceIP if different than endpoint. # Optional: true # Default value: endpoint of the cluster ClusterName #AzServiceIP : 192.168.2.1 # When a PVC is being created, this parameter determines, when a node mounts the PVC, # whether to add the k8s node to the \"Root clients\" field or \"Clients\" field of the NFS export # Allowed values: # \"true\": adds k8s node to the \"Root clients\" field of the NFS export # \"false\": adds k8s node to the \"Clients\" field of the NFS export # Optional: true # Default value: \"false\" RootClientEnabled: \"false\" # Name of PowerScale cluster, where pv will be provisioned. # This name should match with name of one of the cluster configs in isilon-creds secret. # If this parameter is not specified, then default cluster config in isilon-creds secret # will be considered if available. # Optional: true #ClusterName: \u003ccluster_name\u003e # Sets the filesystem type which will be used to format the new volume # Optional: true # Default value: None #csi.storage.k8s.io/fstype: \"nfs\" # volumeBindingMode controls when volume binding and dynamic provisioning should occur. # Allowed values: # Immediate: indicates that volume binding and dynamic provisioning occurs once the # PersistentVolumeClaim is created # WaitForFirstConsumer: will delay the binding and provisioning of a PersistentVolume # until a Pod using the PersistentVolumeClaim is created # Default value: Immediate volumeBindingMode: Immediate # allowedTopologies helps scheduling pods on worker nodes which match all of below expressions. # If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologies # Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server #allowedTopologies: # - matchLabelExpressions: # - key: csi-isilon.dellemc.com/\u003cISILON_IP\u003e # values: # - csi-isilon.dellemc.com # specify additional mount options for when a Persistent Volume is being mounted on a node. # To mount volume with NFSv4, specify mount option vers=4. Make sure NFSv4 is enabled on the Isilon Cluster #mountOptions: [\"\u003cmountOption1\u003e\", \"\u003cmountOption2\u003e\", ..., \"\u003cmountOptionN\u003e\"] Save the file and create it by using\nkubectl create -f \u003cinput_file.yaml\u003e Install CSM Authorization Proxy Server Follow all the prerequisites.\nCreate a CR (Custom Resource) for Authorization using the sample file provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in the CR. This table lists the primary configurable parameters of the Authorization Proxy Server and their default values:\nParameter Description Required Default authorization This section configures the CSM-Authorization components. - - PROXY_HOST The hostname to configure the self-signed certificate (if applicable), and the proxy service Ingress. Yes csm-authorization.com PROXY_INGRESS_CLASSNAME The ingressClassName of the proxy-service Ingress. Yes nginx PROXY_INGRESS_HOSTS Additional host rules to be applied to the proxy-service Ingress. No authorization-ingress-nginx-controller.authorization.svc.cluster.local REDIS_STORAGE_CLASS The storage class for Redis to use for persistence. If not supplied, the default storage class is used. Yes - ingress-nginx This section configures the enablement of the NGINX Ingress Controller. - - enabled Enable/Disable deployment of the NGINX Ingress Controller. Set to false if you already have an Ingress Controller installed. No true cert-manager This section configures the enablement of cert-manager. - - enabled Enable/Disable deployment of cert-manager. Set to false if you already have cert-manager installed. No true Optional: To enable reporting of trace data with Zipkin, use the csm-config-params configMap in the sample CR or dynamically by editing the configMap.\nAdd the Zipkin values to the configMap where ZIPKIN_ADDRESS is the IP address or hostname of the Zipkin server.\nZIPKIN_URI: \"http://ZIPKIN_ADDRESS:9411/api/v2/spans\" ZIPKIN_PROBABILITY: \"1.0\" Execute this command to create the Authorization CR:\nkubectl create -f samples/authorization/csm_authorization_proxy_server_v170.yaml Note:\nThis command will deploy the Authorization Proxy Server in the namespace specified in the input YAML file. Create the karavi-auth-tls secret using your own certificate or by using a self-signed certificate generated via cert-manager.\nIf using your own certificate that is valid for each Ingress hostname, use this command to create the karavi-auth-tls secret:\nkubectl create secret tls karavi-auth-tls -n authorization --key \u003clocation-of-private-key-file\u003e --cert \u003clocation-of-certificate-file\u003e If using a self-signed certificate, prepare samples/authorization/certificate_v170.yaml provided here. An entry for each hostname specified in the CR must be added under dnsNames for the certificate to be valid for each Ingress.\nUse this command to create the karavi-auth-tls secret:\nkubectl create -f samples/authorization/certificate_v170.yaml Verify Installation of the CSM Authorization Proxy Server Once the Authorization CR is created, you can verify the installation as mentioned below:\nkubectl describe csm/\u003cname-of-custom-resource\u003e -n \u003cnamespace\u003e Install Karavictl Follow the instructions available in CSM Authorization for Installing karavictl.\nConfiguring the CSM Authorization Proxy Server Follow the instructions available in CSM Authorization for Configuring the CSM Authorization Proxy Server.\nConfiguring a Dell CSI Driver with CSM Authorization Follow the instructions available in CSM Authorization for Configuring a Dell CSI Driver with CSM for Authorization.\n","categories":"","description":"Installing Authorization via Dell CSM Operator\n","excerpt":"Installing Authorization via Dell CSM Operator\n","ref":"/csm-docs/v1/deployment/csmoperator/modules/authorization/","tags":"","title":"Authorization"},{"body":"Install CSM Authorization via Dell CSM Operator The CSM Authorization module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nPrerequisite Execute kubectl create namespace authorization to create the authorization namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘authorization’.\nInstall cert-manager CRDs\nkubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.crds.yaml Prepare samples/authorization/config.yaml provided here which contains the JWT signing secret. The following table lists the configuration parameters.\nParameter Description Required Default web.jwtsigningsecret String used to sign JSON Web Tokens true secret Example:\nweb: jwtsigningsecret: randomString123 After editing the file, run this command to create a secret called karavi-config-secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/authorization/config.yaml Use this command to replace or update the secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/authorization/config.yaml -o yaml --dry-run=client | kubectl replace -f - Create the karavi-storage-secret using the file provided here to store storage system credentials.\nUse this command to create the secret:\nkubectl create -f samples/authorization/karavi-storage-secret.yaml Prepare a storage class for Redis to use for persistence. If not supplied, the default storage class in your environment is used.\nExample, if using CSM Authorization for PowerScale:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon provisioner: csi-isilon.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: # The name of the access zone a volume can be created in # Optional: true # Default value: default value specified in values.yaml # Examples: System, zone1 AccessZone: System # The base path for the volumes to be created on PowerScale cluster. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Optional: true # Default value: value specified in values.yaml for isiPath # Examples: /ifs/data/csi, /ifs/engineering IsiPath: /ifs/data/csi # The permissions for isi volume directory path # This value overrides the isiVolumePathPermissions attribute of corresponding cluster config in secret, if present # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" #IsiVolumePathPermissions: \"0777\" # AccessZone groupnet service IP. Update AzServiceIP if different than endpoint. # Optional: true # Default value: endpoint of the cluster ClusterName #AzServiceIP : 192.168.2.1 # When a PVC is being created, this parameter determines, when a node mounts the PVC, # whether to add the k8s node to the \"Root clients\" field or \"Clients\" field of the NFS export # Allowed values: # \"true\": adds k8s node to the \"Root clients\" field of the NFS export # \"false\": adds k8s node to the \"Clients\" field of the NFS export # Optional: true # Default value: \"false\" RootClientEnabled: \"false\" # Name of PowerScale cluster, where pv will be provisioned. # This name should match with name of one of the cluster configs in isilon-creds secret. # If this parameter is not specified, then default cluster config in isilon-creds secret # will be considered if available. # Optional: true #ClusterName: \u003ccluster_name\u003e # Sets the filesystem type which will be used to format the new volume # Optional: true # Default value: None #csi.storage.k8s.io/fstype: \"nfs\" # volumeBindingMode controls when volume binding and dynamic provisioning should occur. # Allowed values: # Immediate: indicates that volume binding and dynamic provisioning occurs once the # PersistentVolumeClaim is created # WaitForFirstConsumer: will delay the binding and provisioning of a PersistentVolume # until a Pod using the PersistentVolumeClaim is created # Default value: Immediate volumeBindingMode: Immediate # allowedTopologies helps scheduling pods on worker nodes which match all of below expressions. # If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologies # Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server #allowedTopologies: # - matchLabelExpressions: # - key: csi-isilon.dellemc.com/\u003cISILON_IP\u003e # values: # - csi-isilon.dellemc.com # specify additional mount options for when a Persistent Volume is being mounted on a node. # To mount volume with NFSv4, specify mount option vers=4. Make sure NFSv4 is enabled on the Isilon Cluster #mountOptions: [\"\u003cmountOption1\u003e\", \"\u003cmountOption2\u003e\", ..., \"\u003cmountOptionN\u003e\"] Save the file and create it by using\nkubectl create -f \u003cinput_file.yaml\u003e Install CSM Authorization Proxy Server Follow all the prerequisites.\nCreate a CR (Custom Resource) for Authorization using the sample file provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in the CR. This table lists the primary configurable parameters of the Authorization Proxy Server and their default values:\nParameter Description Required Default authorization This section configures the CSM-Authorization components. - - PROXY_HOST The hostname to configure the self-signed certificate (if applicable), and the proxy service Ingress. Yes csm-authorization.com PROXY_INGRESS_CLASSNAME The ingressClassName of the proxy-service Ingress. Yes nginx PROXY_INGRESS_HOSTS Additional host rules to be applied to the proxy-service Ingress. No authorization-ingress-nginx-controller.authorization.svc.cluster.local REDIS_STORAGE_CLASS The storage class for Redis to use for persistence. If not supplied, the default storage class is used. Yes - ingress-nginx This section configures the enablement of the NGINX Ingress Controller. - - enabled Enable/Disable deployment of the NGINX Ingress Controller. Set to false if you already have an Ingress Controller installed. No true cert-manager This section configures the enablement of cert-manager. - - enabled Enable/Disable deployment of cert-manager. Set to false if you already have cert-manager installed. No true Optional: To enable reporting of trace data with Zipkin, use the csm-config-params configMap in the sample CR or dynamically by editing the configMap.\nAdd the Zipkin values to the configMap where ZIPKIN_ADDRESS is the IP address or hostname of the Zipkin server.\nZIPKIN_URI: \"http://ZIPKIN_ADDRESS:9411/api/v2/spans\" ZIPKIN_PROBABILITY: \"1.0\" Execute this command to create the Authorization CR:\nkubectl create -f samples/authorization/csm_authorization_proxy_server_v170.yaml Note:\nThis command will deploy the Authorization Proxy Server in the namespace specified in the input YAML file. Create the karavi-auth-tls secret using your own certificate or by using a self-signed certificate generated via cert-manager.\nIf using your own certificate that is valid for each Ingress hostname, use this command to create the karavi-auth-tls secret:\nkubectl create secret tls karavi-auth-tls -n authorization --key \u003clocation-of-private-key-file\u003e --cert \u003clocation-of-certificate-file\u003e If using a self-signed certificate, prepare samples/authorization/certificate_v170.yaml provided here. An entry for each hostname specified in the CR must be added under dnsNames for the certificate to be valid for each Ingress.\nUse this command to create the karavi-auth-tls secret:\nkubectl create -f samples/authorization/certificate_v170.yaml Verify Installation of the CSM Authorization Proxy Server Once the Authorization CR is created, you can verify the installation as mentioned below:\nkubectl describe csm/\u003cname-of-custom-resource\u003e -n \u003cnamespace\u003e Install Karavictl Follow the instructions available in CSM Authorization for Installing karavictl.\nConfiguring the CSM Authorization Proxy Server Follow the instructions available in CSM Authorization for Configuring the CSM Authorization Proxy Server.\nConfiguring a Dell CSI Driver with CSM Authorization Follow the instructions available in CSM Authorization for Configuring a Dell CSI Driver with CSM for Authorization.\n","categories":"","description":"Installing Authorization via Dell CSM Operator\n","excerpt":"Installing Authorization via Dell CSM Operator\n","ref":"/csm-docs/v2/deployment/csmoperator/modules/authorization/","tags":"","title":"Authorization"},{"body":" The CSM Authorization karavictl CLI is no longer actively maintained or supported. It will be deprecated in CSM 2.0.\nInstall CSM Authorization via Dell CSM Operator The CSM Authorization module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nPrerequisite Execute kubectl create namespace authorization to create the authorization namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘authorization’.\nInstall cert-manager CRDs kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.crds.yaml\nPrepare samples/authorization/config.yaml provided here which contains the JWT signing secret. The following table lists the configuration parameters.\nParameter Description Required Default web.jwtsigningsecret String used to sign JSON Web Tokens true secret Example:\nweb: jwtsigningsecret: randomString123 After editing the file, run this command to create a secret called karavi-config-secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/csm-authorization/config.yaml\nUse this command to replace or update the secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/csm-authorization/config.yaml -o yaml --dry-run=client | kubectl replace -f -\nCreate the karavi-storage-secret using the file provided here to store storage system credentials.\nUse this command to create the secret:\nkubectl create -f samples/authorization/karavi-storage-secret.yaml\nPrepare a storage class for Redis to use for persistence. If not supplied, the default storage class in your environment is used.\nExample, if using CSM Authorization for PowerScale:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: isilon provisioner: csi-isilon.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: # The name of the access zone a volume can be created in # Optional: true # Default value: default value specified in values.yaml # Examples: System, zone1 AccessZone: System # The base path for the volumes to be created on PowerScale cluster. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Optional: true # Default value: value specified in values.yaml for isiPath # Examples: /ifs/data/csi, /ifs/engineering IsiPath: /ifs/data/csi # The permissions for isi volume directory path # This value overrides the isiVolumePathPermissions attribute of corresponding cluster config in secret, if present # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" #IsiVolumePathPermissions: \"0777\" # AccessZone groupnet service IP. Update AzServiceIP if different than endpoint. # Optional: true # Default value: endpoint of the cluster ClusterName #AzServiceIP : 192.168.2.1 # When a PVC is being created, this parameter determines, when a node mounts the PVC, # whether to add the k8s node to the \"Root clients\" field or \"Clients\" field of the NFS export # Allowed values: # \"true\": adds k8s node to the \"Root clients\" field of the NFS export # \"false\": adds k8s node to the \"Clients\" field of the NFS export # Optional: true # Default value: \"false\" RootClientEnabled: \"false\" # Name of PowerScale cluster, where pv will be provisioned. # This name should match with name of one of the cluster configs in isilon-creds secret. # If this parameter is not specified, then default cluster config in isilon-creds secret # will be considered if available. # Optional: true #ClusterName: \u003ccluster_name\u003e # Sets the filesystem type which will be used to format the new volume # Optional: true # Default value: None #csi.storage.k8s.io/fstype: \"nfs\" # volumeBindingMode controls when volume binding and dynamic provisioning should occur. # Allowed values: # Immediate: indicates that volume binding and dynamic provisioning occurs once the # PersistentVolumeClaim is created # WaitForFirstConsumer: will delay the binding and provisioning of a PersistentVolume # until a Pod using the PersistentVolumeClaim is created # Default value: Immediate volumeBindingMode: Immediate # allowedTopologies helps scheduling pods on worker nodes which match all of below expressions. # If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologies # Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server #allowedTopologies: # - matchLabelExpressions: # - key: csi-isilon.dellemc.com/\u003cISILON_IP\u003e # values: # - csi-isilon.dellemc.com # specify additional mount options for when a Persistent Volume is being mounted on a node. # To mount volume with NFSv4, specify mount option vers=4. Make sure NFSv4 is enabled on the Isilon Cluster #mountOptions: [\"\u003cmountOption1\u003e\", \"\u003cmountOption2\u003e\", ..., \"\u003cmountOptionN\u003e\"] Save the file and create it by using kubectl create -f \u003cinput_file.yaml\u003e.\nInstall CSM Authorization Proxy Server Follow all the prerequisites.\nCreate a CR (Custom Resource) for Authorization using the sample file provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in the CR. This table lists the primary configurable parameters of the Authorization Proxy Server and their default values:\nParameter Description Required Default authorization This section configures the CSM-Authorization components. - - PROXY_HOST The hostname to configure the self-signed certificate (if applicable), and the proxy, tenant, role, and storage service Ingresses. Yes csm-authorization.com PROXY_INGRESS_CLASSNAME The ingressClassName of the proxy-service Ingress. Yes nginx PROXY_INGRESS_HOSTS Additional host rules to be applied to the proxy-service Ingress. No authorization-ingress-nginx-controller.authorization.svc.cluster.local TENANT_INGRESS_CLASSNAME The ingressClassName of the tenant-service Ingress. Yes nginx ROLE_INGRESS_CLASSNAME The ingressClassName of the role-service Ingress. Yes nginx STORAGE_INGRESS_CLASSNAME The ingressClassName of the storage-service Ingress. Yes nginx REDIS_STORAGE_CLASS The storage class for Redis to use for persistence. If not supplied, the default storage class is used. Yes - ingress-nginx This section configures the enablement of the NGINX Ingress Controller. - - enabled Enable/Disable deployment of the NGINX Ingress Controller. Set to false if you already have an Ingress Controller installed. No true cert-manager This section configures the enablement of cert-manager. - - enabled Enable/Disable deployment of cert-manager. Set to false if you already have cert-manager installed. No true Optional: To enable reporting of trace data with Zipkin, use the csm-config-params configMap in the sample CR or dynamically by editing the configMap.\nAdd the Zipkin values to the configMap:\nZIPKIN_URI: \"http://PROXY_HOST:9411/api/v2/spans\" ZIPKIN_PROBABILITY: \"1.0\" Execute this command to create the Authorization CR:\nkubectl create -f samples/authorization/csm_authorization_proxy_server.yaml\nNote:\nThis command will deploy the Authorization Proxy Server in the namespace specified in the input YAML file. Create the karavi-auth-tls secret using your own certificate or by using a self-signed certificate generated via cert-manager.\nIf using your own certificate that is valid for each Ingress hostname, use this command to create the karavi-auth-tls secret:\nkubectl create secret tls karavi-auth-tls -n authorization --key \u003clocation-of-private-key-file\u003e --cert \u003clocation-of-certificate-file\u003e\nIf using a self-signed certificate, prepare samples/authorization/certificate.yaml provided here. An entry for each hostname specified in the CR must be added under dnsNames for the certificate to be valid for each Ingress.\nUse this command to create the karavi-auth-tls secret:\nkubectl create -f samples/authorization/certificate.yaml\nVerify Installation of the CSM Authorization Proxy Server Once the Authorization CR is created, you can verify the installation as mentioned below:\nkubectl describe csm/\u003cname-of-custom-resource\u003e -n \u003cnamespace\u003e\nInstall Karavictl Follow the instructions available in CSM Authorization for Installing karavictl.\nConfiguring the CSM Authorization Proxy Server Follow the instructions available in CSM Authorization for Configuring the CSM Authorization Proxy Server.\nConfiguring a Dell CSI Driver with CSM Authorization Follow the instructions available in CSM Authorization for Configuring a Dell CSI Driver with CSM for Authorization.\n","categories":"","description":"Pre-requisite for Installing Authorization via Dell CSM Operator\n","excerpt":"Pre-requisite for Installing Authorization via Dell CSM Operator\n","ref":"/csm-docs/v3/deployment/csmoperator/modules/authorization/","tags":"","title":"Authorization"},{"body":"Cert-CSI is a tool to validate Dell CSI Drivers. It contains various test suites to validate the drivers.\nInstallation There are three methods of installing cert-csi.\nDownload the executable from the latest GitHub release. Pull the container image from DockerHub. Build the exectuable or container image locally. The exectuable from the GitHub Release only supports Linux. For non-Linux users, you must build the cert-csi executable locally.\nDownload Release (Linux) Download the latest release of the cert-csi zip file. curl -LO https://github.com/dell/cert-csi/releases/download/v1.3.1/cert-csi-v1.3.1.zip Unzip the file. unzip cert-csi-v1.3.1.zip chmod +x ./cert-csi-v1.3.1 Install cert-csi-v1.3.1 as cert-csi. sudo install -o root -g root -m 0755 cert-csi-v1.3.1 /usr/local/bin/cert-csi If you do not have root access on the target system, you can still install cert-csi to the ~/.local/bin directory:\nchmod +x cert-csi-v1.3.1 mkdir -p ~/.local/bin mv ./cert-csi-v1.3.1 ~/.local/bin/cert-csi # and then append (or prepend) ~/.local/bin to $PATH Pull The Container Image Docker Podman docker pull dellemc/cert-csi:v1.3.1 podman pull dellemc/cert-csi:v1.3.1 Building Locally Prerequisites Git Go (If buidling the executable) Podman or Docker (If building the container image) Clone the repository git clone -b \"v1.3.1\" https://github.com/dell/cert-csi.git \u0026\u0026 cd cert-csi Build cert-csi Executable Container Image make build # the cert-csi executable will be in the working directory chmod +x ./cert-csi # if building on *nix machine # uses podman if available, otherwise uses docker. The resulting image is tagged cert-csi:latest make docker Optional If you want to collect csi-driver resource usage metrics, then please provide the namespace where it can be found and install the metric-server using this command (kubectl is required):\nmake install-ms Running Cert-CSI Executable Docker Podman cert-csi --help docker run --rm -it -v ~/.kube/config:/root/.kube/config dellemc/cert-csi:v1.3.1 --help podman run --rm -it -v ~/.kube/config:/root/.kube/config dellemc/cert-csi:v1.3.1 --help The following sections showing how to execute the various test suites use the executable for brevity. For executions requiring special behavior, such as mounting file arguments into the container image, it will be noted for the relevant command.\nLog files are located in the logs directory in the working directory of cert-csi.\nReport files are located in the default $HOME/.cert-csi/reports directory.\nDatabase (SQLite) file for test suites is \u003cstorage-class-name\u003e.db in the working directory of cert-csi.\nDatabase (SQLite) file for functional test suites is cert-csi-functional.db in the working directory of cert-csi.\nNOTE: If using the container image, these files will be inside the container. If you are interested in these files, it is recommended to use the exectuable.\nRun All Test Suites You can use cert-csi to launch a test run against multiple storage classes to check if the driver adheres to advertised capabilities.\nPreparing Config To run the test suites you need to provide .yaml config with storage classes and their capabilities. You can use example-certify-config.yaml as an example.\nTemplate:\nstorageClasses: - name: # storage-class-name (ex. powerstore) minSize: # minimal size for your sc (ex. 1Gi) rawBlock: # is Raw Block supported (true or false) expansion: # is volume expansion supported (true or false) clone: # is volume cloning supported (true or false) snapshot: # is volume snapshotting supported (true or false) RWX: # is ReadWriteMany volume access mode supported for non RawBlock volumes (true or false) volumeHealth: # set this to enable the execution of the VolumeHealthMetricsSuite (true or false) # Make sure to enable healthMonitor for the driver's controller and node pods before running this suite. It is recommended to use a smaller interval time for this sidecar and pass the required arguments. VGS: # set this to enable the execution of the VolumeGroupSnapSuite (true or false) # Additionally, make sure to provide the necessary required arguments such as volumeSnapshotClass, vgs-volume-label, and any others as needed. RWOP: # set this to enable the execution of the MultiAttachSuite with the AccessMode set to ReadWriteOncePod (true or false) ephemeral: # if exists, then run EphemeralVolumeSuite. See the Ephemeral Volumes suite section for example Volume Attributes driver: # driver name for EphemeralVolumeSuite (e.g., csi-vxflexos.dellemc.com) fstype: # fstype for EphemeralVolumeSuite volumeAttributes: # volume attrs for EphemeralVolumeSuite. attr1: # volume attr for EphemeralVolumeSuite attr2: # volume attr for EphemeralVolumeSuite capacityTracking: driverNamespace: # namepsace where driver is installed pollInterval: # duration to poll capacity (e.g., 2m) Driver specific examples:\nCSI PowerFlex CSI PowerScale CSI PowerStore CSI Unity storageClasses: - name: vxflexos minSize: 8Gi rawBlock: true expansion: true clone: true snapshot: true RWX: false ephemeral: driver: csi-powerstore.dellemc.com fstype: ext4 volumeAttributes: volumeName: \"my-ephemeral-vol\" size: \"8Gi\" storagepool: \"sample\" systemID: \"sample\" - name: vxflexos-nfs minSize: 8Gi rawBlock: false expansion: true clone: true snapshot: true RWX: true RWOP: true ephemeral: driver: csi-vxflexos.dellemc.com fstype: \"nfs\" volumeAttributes: volumeName: \"my-ephemeral-vol\" size: \"8Gi\" storagepool: \"sample\" systemID: \"sample\" capacityTracking: driverNamespace: powerstore pollInterval: 2m storageClasses: - name: isilon minSize: 8Gi rawBlock: false expansion: true clone: true snapshot: true RWX: false ephemeral: driver: csi-isilon.dellemc.com fstype: nfs volumeAttributes: size: \"10Gi\" ClusterName: \"sample\" AccessZone: \"sample\" IsiPath: \"/ifs/data/sample\" IsiVolumePathPermissions: \"0777\" AzServiceIP: \"192.168.2.1\" storageClasses: - name: powerstore minSize: 5Gi rawBlock: true expansion: true clone: true snapshot: true RWX: false ephemeral: driver: csi-powerstore.dellemc.com fstype: ext4 volumeAttributes: arrayID: \"arrayid\" protocol: iSCSI size: 5Gi - name: powerstore-nfs minSize: 5Gi rawBlock: false expansion: true clone: true snapshot: true RWX: true RWOP: true ephemeral: driver: csi-powerstore.dellemc.com fstype: \"nfs\" volumeAttributes: arrayID: \"arrayid\" protocol: NFS size: 5Gi nasName: \"nas-server\" capacityTracking: driverNamespace: powerstore pollInterval: 2m storageClasses: - name: unity-iscsi minSize: 3Gi rawBlock: true expansion: true clone: false snapshot: true RWX: false ephemeral: driver: csi-unity.dellemc.com fstype: ext4 volumeAttributes: arrayId: \"array-id\" storagePool: pool-name protocol: NFS size: 5Gi - name: unity-nfs minSize: 3Gi rawBlock: false expansion: true clone: false snapshot: true RWX: true RWOP: true ephemeral: driver: csi-unity.dellemc.com fstype: \"nfs\" volumeAttributes: arrayId: \"array-id\" storagePool: pool-name protocol: NFS size: 5Gi nasServer: \"nas-server\" nasName: \"nas-name\" capacityTracking: driverNamespace: unity pollInterval: 2m Launching Test Run Executes the VolumeIO suite. Executes the Scaling suite. If storageClasses.clone is true, executes the Volume Cloning suite. If storageClasses.expansion is true, executes the Volume Expansion suite. If storageClasses.expansion is true and storageClasses.rawBlock is true, executes the Volume Expansion suite with raw block volumes. If storageClasses.snapshot is true, exeuctes the Snapshot suite and the Replication suite. If storageClasses.rawBlock is true, executes the Multi-Attach Volume suite with raw block volumes. If storageClasses.rwx is true, executes the Multi-Attach Volume suite. (Storgae Class must be NFS.) If storageClasses.volumeHealth is true, executes the Volume Health Metrics suite. If storageClasses.rwop is true, executes the Multi-Attach Volume suite with the volume access mode ReadWriteOncePod. If storageClasses.ephemeral exists, executes the Ephemeral Volumes suite. If storageClasses.vgs is true, executes the Volume Group Snapshot suite. If storageClasses.capacityTracking exists, exeuctes the Storage Class Capacity Tracking suite. NOTE: For testing/debugging purposes, it can be useful to use the --no-cleanup so resources do not get deleted.\nNOTE: If you are using CSI PowerScale with SmartQuotas disabled, the Volume Expansion suite is expected to timeout due to the way PowerScale provisions storage. Set storageClasses.expansion to false to skip this suite.\ncert-csi certify --cert-config \u003cpath-to-config\u003e --vsc \u003cvolume-snapshot-class\u003e Withold the --vsc argument if Snapshot capabilities are disabled.\ncert-csi certify --cert-config \u003cpath-to-config\u003e Optional Params: --vsc: volume snapshot class, required if you specified snapshot capability Run cert-csi certify -h for more options.\nIf you are using the container image, the cert-config file must be mounted into the container. Assuming your cert-config file is /home/user/example-certify-config.yaml, here are examples of how to exeucte this suite with the container image.\nDocker Podman docker run --rm -it -v ~/.kube/config:/root/.kube/config -v /home/user/example-certify-config.yaml:/example-certify-config.yaml dellemc/cert-csi:v1.3.1 certify --cert-config /example-certify-config.yaml --vsc \u003cvolume-snapshot-class\u003e podman run --rm -it -v ~/.kube/config:/root/.kube/config -v /home/user/example-certify-config.yaml:/example-certify-config.yaml dellemc/cert-csi:v1.3.1 certify --cert-config /example-certify-config.yaml --vsc \u003cvolume-snapshot-class\u003e Running Invidual Test Suites NOTE: For testing/debugging purposes, it can useful to use the --no-cleanup flag so resources do not get deleted.\nVolume I/O Creates the namespace volumeio-test-* where resources will be created. Creates Persistent Volume Claims. If the specified storage class binding mode is not WaitForFirstConsumer, waits for Persistent Volume Claims to be bound to Persistent Volumes. For each Persistent Volume Claim, executes the following workflow concurrently: Creates a Pod to consume the Persistent Volume Claim. Writes data to the volume and verifies the checksum of the data. Deletes the Pod. Waits for the associated Volume Attachment to be deleted. cert-csi test vio --sc \u003cstorage class\u003e Run cert-csi test vio -h for more options.\nScalability Creates the namespace scale-test-* where resources will be created. Creates a StatefulSet. Scales up the StatefulSet. Scales down the StatefulSet to zero. cert-csi test scaling --sc \u003cstorage class\u003e Run cert-csi test scaling -h for more options.\nSnapshots Creates the namespace snap-test-* where resources will be created. Creates Persistent Volume Claim. If the specified storage class binding mode is not WaitForFirstConsumer, waits for Persistent Volume Claim to be bound to Persistent Volumes. Create Pod to consume the Persistent Volume Claim. Writes data to the volume. Deletes the Pod. Creates a Volume Snapshot from the Persistent Volume Claim. Waits for the Volume Snapshot to be Ready. Creates a new Persistent Volume Claim from the Volume Snapshot. Creates a new Pod to consume the new Persistent Volume Claim. Verifies the checksum of the data. cert-csi test snap --sc \u003cstorage class\u003e --vsc \u003cvolume snapshot class\u003e Run cert-csi test snap -h for more options.\nVolume Group Snapshots Creates the namespace vgs-snap-test-* where resources will be created. Creates Persistent Volume Claims. If the specified storage class binding mode is not WaitForFirstConsumer, waits for Persistent Volume Claim to be bound to Persistent Volumes. Create Pods to consume the Persistent Volume Claims. Creates Volume Group Snapshot. Waits for Volume Group Snapshot state to be COMPLETE. Note: Volume Group Snapshots are only supported by CSI PowerFlex and CSI PowerStore.\nMulti-Attach Volume Creates the namespace mas-test-* where resources will be created. Creates Persistent Volume Claim. Creates Pod to consume the Persistent Volume Claim. Waits for Pod to be in the Ready state. Creates additional Pods to consume the same Persistent Volume Claim. Watis for Pods to be in the Ready state. Writes data to the volumes on the Pods and verifies checksum of the data. cert-csi test multi-attach-vol --sc \u003cstorage class\u003e The storage class must be an NFS storage class. Otherwise, raw block volumes must be used.\ncert-csi test multi-attach-vol --sc \u003cstorage class\u003e --block Run cert-csi test multi-attach-vol -h for more options.\nReplication Creates the namespace replication-suite-* where resources will be created. Creates Persistent Volume Claims. Create Pods to consume the Persistent Volume Claims. Waits for Pods to be in the Ready state. Creates a Volume Snapshot from each Persistent Volume Claim. Waits for the Volume Snapshots to be Ready. Creates Persistent Volume Claims from the Volume Snapshots. Creates Pods to consume the Persistent Volume Claims. Waits for Pods to be in the Ready state. Verifies the replication group name on ersistent Volume Claims. cert-csi test replication --sc \u003cstorage class\u003e --vsc \u003csnapshot class\u003e Run cert-csi test replication -h for more options.\nVolume Cloning Creates the namespace clonevolume-suite-* where resources will be created. Creates Persistent Volume Claims. Create Pods to consume the Persistent Volume Claims. Waits for Pods to be in the Ready state. Creates Persistent Volume Claims with the source volume being from the volumes in step 2. Create Pods to consume the Persistent Volume Claims. Waits for Pods to be in the Ready state. cert-csi test clone-volume --sc \u003cstorage class\u003e Run cert-csi test clone-volume -h for more options.\nVolume Expansion Creates the namespace volume-expansion-suite-* where resources will be created. Creates Persistent Volume Claims. Create Pods to consume the Persistent Volume Claims. Waits for Pods to be in the Ready state. Expands the size in the Persistent Volume Claims. Verifies that the volumes mounted to the Pods were expanded. Raw block volumes cannot be verified since there is no filesystem.\nIf you are using CSI PowerScale with SmartQuotas disabled, the Volume Expansion suite is expected to timeout due to the way PowerScale provisions storage.\ncert-csi test expansion --sc \u003cstorage class\u003e Run cert-csi test expansion -h for more options.\nBlocksnap suite Creates the namespace block-snap-test-* where resources will be created. Creates Persistent Volume Claim. If the specified storage class binding mode is not WaitForFirstConsumer, waits for Persistent Volume Claim to be bound to Persistent Volumes. Creates Pod to consume the Persistent Volume Claim. Writes data to the volume. Creates a Volume Snapshot from the Persistent Volume Claim. Waits for the Volume Snapshot to be Ready. Create a Persistent Volume Claim with raw block volume mode from the Volume Snapshot. Creates Pod to consume the Persistent Volume Claim. Mounts the raw block volume and verifes the checksum of the data. cert-csi test blocksnap --sc \u003cstorageClass\u003e --vsc \u003csnapshotclass\u003e Run cert-csi test blocksnap -h for more options.\nVolume Health Metrics Creates the namespace volume-health-metrics-* where resources will be created. Creates Persistent Volume Claim. Creates Pod to consume the Persistent Volume Claim. Waits for Pod to be in the Ready state. Veries that ControllerGetVolume and NodeGetVolumeStats are being executed in the controller and node pods, respectively. cert-csi test volumehealthmetrics --sc \u003cstorage-class\u003e --driver-ns \u003cdriver-namespace\u003e Run cert-csi test volumehealthmetrics -h for more options.\nNote: Make sure to enable healthMonitor for the driver’s controller and node pods before running this suite. It is recommended to use a smaller interval time for this sidecar.\nEphemeral Volumes Creates namespace functional-test where resources will be created. Creates Pods with one ephemeral inline volume each. Waits for Pods to be in the Ready state. Writes data to the volume on each Pod. Verifies the checksum of the data. cert-csi test ephemeral-volume --driver \u003cdriver-name\u003e --attr ephemeral-config.properties Run cert-csi test ephemeral-volume -h for more options.\n--driver is the name of a CSI Driver from the output of kubectl get csidriver (e.g, csi-vxflexos.dellemc.com). This suite does not delete resources on success.\nIf you are using the container image, the attr file must be mounted into the container. Assuming your attr file is /home/user/ephemeral-config.properties, here are examples of how to exeucte this suite with the container image.\nDocker Podman docker run --rm -it -v ~/.kube/config:/root/.kube/config -v /home/user/ephemeral-config.properties:/ephemeral-config.properties dellemc/cert-csi:v1.3.1 test ephemeral-volume --driver \u003cdriver-name\u003e --attr /ephemeral-config.properties podman run --rm -it -v ~/.kube/config:/root/.kube/config -v /home/user/ephemeral-config.properties:/ephemeral-config.properties dellemc/cert-csi:v1.3.1 test ephemeral-volume --driver \u003cdriver-name\u003e --attr /ephemeral-config.properties Sample ephemeral-config.properties (key/value pair) CSI PowerFlex CSI PowerScale CSI PowerStore CSI Unity volumeName: \"my-ephemeral-vol\" size: \"10Gi\" storagepool: \"sample\" systemID: \"sample\" size: \"10Gi\" ClusterName: \"sample\" AccessZone: \"sample\" IsiPath: \"/ifs/data/sample\" IsiVolumePathPermissions: \"0777\" AzServiceIP: \"192.168.2.1\" size: \"10Gi\" arrayID: \"sample\" nasName: \"sample\" nfsAcls: \"0777\" size: \"10Gi\" arrayID: \"sample\" protocol: iSCSI thinProvisioned: \"true\" isDataReductionEnabled: \"false\" tieringPolicy: \"1\" storagePool: pool_2 nasName: \"sample\" Storage Capacity Tracking Creates namespace functional-test where resources will be created. Creates a duplicate of the provided storge class using prefix capacity-tracking. Waits for the associated CSIStorageCapacity object to be created. Deletes the duplicate storge class. Waits for the associated CSIStorageCapacity to be deleted. Sets the capacity of the CSIStorageCapacity of the provided storage class to zero. Creates Pod with a volume using the provided storage class. Verifies that the Pod is in the Pending state. Waits for storage capacity to be polled by the driver. Waits for Pod to be Running. Storage class must use volume binding mode WaitForFirstConsumer.\nThis suite does not delete resources on success.\ncert-csi functional-test capacity-tracking --sc \u003cstorage-class\u003e --drns \u003cdriver-namespace\u003e Run cert-csi test capacity-tracking -h for more options.\nRunning Longevity mode cert-csi test \u003csuite-name\u003e --sc \u003cstorage class\u003e --longevity \u003cnumber of iterations\u003e Use configurable container images To use custom images for creating containers pass an image config YAML file as an argument. The YAML file should have linux(test) and postgres images name with their corresponding image URL. For example\nExample:\nimages: - test: \"docker.io/centos:centos7\" # change this to your url postgres: \"docker.io/bitnami/postgresql:11.8.0-debian-10-r72\" # change this to your url To use this feature, run cert-csi with the option --image-config /path/to/config.yaml along with any other arguments.\nKubernetes End-To-End Tests All Kubernetes end to end tests require that you provide the driver config based on the storage class you want to test and the version of the kubernetes you want to test against. These are the mandatory parameters that you can provide in command like..\n--driver-config \u003cpath of driver config file\u003e and --version \"v1.25.0\" Running kubernetes end-to-end tests To run kubernetes end-to-end tests, run the command:\ncert-csi k8s-e2e --config \u003ckube config\u003e --driver-config \u003cpath to driver config\u003e --focus \u003cregx pattern to focus Ex: \"External.Storage.*\" \u003e --timeout \u003ctimeout Ex: \"2h\"\u003e --version \u003c version of k8s Ex: \"v1.25.0\"\u003e --skip-tests \u003cskip these steps mentioned in file\u003e --skip \u003cregx pattern to skip tests Ex:\"Generic Ephemeral-volume|(block volmode)\"\u003e Kubernetes end-to-end reporting All the reports generated by kubernetes end-to-end tests will be under $HOME/reports directory by default if user doesn’t mention the report path. Kubernetes end to end tests Execution log file will be placed under $HOME/reports/execution_[storage class name].log Cert-CSI logs will be present in the execution directory info.log , error.log Test config files format driver-config ignore-tests Example Commands cert-csi k8s-e2e --config \"/root/.kube/config\" --driver-config \"/root/e2e_config/config-nfs.yaml\" --focus \"External.Storage.*\" --timeout \"2h\" --version \"v1.25.0\" --skip-tests \"/root/e2e_config/ignore.yaml\" ./cert-csi k8s-e2e --config \"/root/.kube/config\" --driver-config \"/root/e2e_config/config-iscsi.yaml\" --focus \"External.Storage.*\" --timeout \"2h\" --version \"v1.25.0\" --focus-file \"capacity.go\" Interacting with DB Generating report from runs without running tests To generate test report from the database, run the command:\ncert-csi --db \u003cpath/to/.db\u003e report --testrun \u003ctest-run-name\u003e --html --txt Report types: --html: performance html report --txt: performance txt report --xml: junit compatible xml report, contains basic run infomation --tabular: tidy html report with basic run information To generate tabular report from the database, run the command:\ncert-csi -db ./cert-csi-functional.db functional-report -tabular To generate XML report from the database, run the command:\ncert-csi -db ./cert-csi-functional.db functional-report -xml Customizing report folder To specify test report folder path, use –path option as follows:\ncert-csi --db \u003cpath/to/.db\u003e report --testrun \u003ctest-run-name\u003e --path \u003cpath-to-folder\u003e Options: --path: path to folder where reports will be created (if not specified ~/.cert-csi/ will be used) Generating report from multiple databases and test runs To generate report from multiple databases, run the command:\ncert-csi report --tr \u003cdb-path\u003e:\u003ctest-run-name\u003e --tr ... --tabular --xml Supported report types: --xml --tabular Listing all known test runs To list all test runs, run the command:\ncert-csi --db \u003cpath/to/.db\u003e list test-runs Other options Customizing report folder To specify test report folder path, use –path option as follows:\ncert-csi \u003ccommand\u003e --path \u003cpath-to-folder\u003e Commands: test \u003cany-subcommand\u003e certify report Running with enabled driver resource usage metrics To run tests with driver resource usage metrics enabled, run the command:\ncert-csi test \u003ctest suite\u003e --sc \u003cstorage class\u003e \u003c...\u003e --ns \u003cdriver namespace\u003e Running custom hooks from program To run tests with custom hooks, run the command:\ncert-csi test \u003ctest suite\u003e --sc \u003cstorage class\u003e \u003c...\u003e --sh ./hooks/start.sh --rh ./hooks/ready.sh --fh ./hooks/finish.sh Screenshots Running provisioning test You can interrupt the application by sending an interruption signal (for example pressing Ctrl + C). It will stop polling and try to cleanup resources.\nRunning scaling test Listing available test runs Running longevity mode Multi DB Tabular report example Text report example\nTabular Report example\nHTML report example Resource usage example chart ","categories":"","description":"Tool to validate Dell CSI Drivers","excerpt":"Tool to validate Dell CSI Drivers","ref":"/csm-docs/docs/csidriver/installation/test/certcsi/","tags":"","title":"Cert-CSI"},{"body":"Cert-CSI is a tool to validate Dell CSI Drivers. It contains various test suites to validate the drivers.\nInstallation To install this tool you can download one of binary files located in RELEASES\nYou can build the tool by cloning the repository and running this command:\nmake build You can also build a docker container image by running this command:\ndocker build -t cert-csi . If you want to collect csi-driver resource usage metrics, then please provide the namespace where it can be found and install the metric-server using this command (kubectl is required):\nmake install-ms [FOR UNIX] If you want to build and install the tool to your $PATH and enable the auto-completion feature, then run this command:\nmake install-nix Alternatively, you can install the metric-server by following the instructions at https://github.com/kubernetes-incubator/metrics-server\nRunning Cert-CSI To get information on how to use the program, you can use built-in help. If you’re using a UNIX-like system and enabled auto-completion feature while installing the tool, then you can use shell’s built-in auto-completion to navigate through program’s subcommands and flags interactively by just pressing TAB.\nTo run cert-csi, you have to point your environment to a kube cluster. This allows you to receive dynamically formatted suggestions from your cluster. For example if you press TAB while passing –storageclass (or –sc) argument, the tool will parse all existing Storage Classes from your cluster and suggest them as an input for you.\nTo run a docker container your command should look something like this\ndocker run --rm -it -v ~/.kube/config:/root/.kube/config -v $(pwd):/app/cert-csi cert-csi \u003cusual program arguments\u003e Driver Certification You can use cert-csi to launch a certification test run against multiple storage classes to check if the driver adheres to advertised capabilities.\nPreparing Config To run the certification test you need to provide .yaml config with storage classes and their capabilities. You can use example-certify-config.yaml as an example.\nExample:\nstorageClasses: - name: # storage-class-name (ex. powerstore) minSize: # minimal size for your sc (ex. 1Gi) rawBlock: # is Raw Block supported (true or false) expansion: # is volume expansion supported (true or false) clone: # is volume cloning supported (true or false) snapshot: # is volume snapshotting supported (true or false) RWX: # is ReadWriteMany volume access mode supported for non RawBlock volumes (true or false) volumeHealth: false # set this to enable the execution of the VolumeHealthMetricsSuite. # Make sure to enable healthMonitor for the driver's controller and node pods before running this suite. It is recommended to use a smaller interval time for this sidecar and pass the required arguments. VGS: false # set this to enable the execution of the VolumeGroupSnapSuite. # Additionally, make sure to provide the necessary required arguments such as volumeSnapshotClass, vgs-volume-label, and any others as needed. RWOP: false # set this to enable the execution of the MultiAttachSuite with the AccessMode set to ReadWriteOncePod. ephemeral: # if exists, then run EphemeralVolumeSuite driver: # driver name for EphemeralVolumeSuite fstype: # fstype for EphemeralVolumeSuite volumeAttributes: # volume attrs for EphemeralVolumeSuite. attr1: # volume attr for EphemeralVolumeSuite attr2: # volume attr for EphemeralVolumeSuite Launching Certification Test Run After preparing a certification configuration file, you can launch certification by running\ncert-csi certify --cert-config \u003cpath-to-config\u003e Optional Params: --vsc: volume snapshot class, required if you specified snapshot capability --timeout: set the timeout value for certification suites --no-metrics: disables metrics aggregation (set if you encounter k8s performance issues) --path: path to folder where reports will be created (if not specified ~/.cert-csi/ will be used) Functional Tests Running Individual Suites Volume/PVC Creation To run volume or PVC creation test suite, run the command:\ncert-csi functional-test volume-creation --sc \u003cstorage class\u003e -n 5 Optional Params: --custom-name : To give custom name for PVC while creating only 1 PVC --size : To give custom size, possible values for size in Gi/Mi --access-mode : To set custom access-modes, possible values - ReadWriteOnce,ReadOnlyMany and ReadWriteMany --block : To create raw block volumes Provisioning/Pod creation To run volume provisioning or pod creation test suite, run the command:\ncert-csi functional-test provisioning --sc \u003cstorage class\u003e Optional Params: --volumeNumber : number of volumes to attach to each pod --podNumber : number of pod to create --podName : To give custom name for pod while creating only 1 pod --block : To create raw block volumes and attach it to pods --vol-access-mode: To set volume access modes Running Volume Deletion suite To run volume delete test suite, run the command:\ncert-csi functional-test volume-deletion --pvc-name value : PVC name to delete --pvc-namespace : PVC namespace where PVC is present Running Pod Deletion suite To run pod deletion test suite, run the command:\ncert-csi functional-test pod-deletion --pod-name : Pod name to delete --pod-namespace : Pod namespace where pod is present Running Cloned Volume deletion suite To run cloned volume deletion test suite, run the command:\ncert-csi functional-test clone-volume-deletion --clone-volume-name : Volume name to delete Multi Attach Volume Tests To run multi-attach volume test suite, run the command:\ncert-csi functional-test multi-attach-vol --sc \u003cstorage-class\u003e --pods : Number of pods to create --block : To create raw block volume Ephemeral volumes suite To run ephemeral volume test suite, run the command:\ncert-csi functional-test ephemeral-volume --driver \u003cdriver-name\u003e --attr ephemeral-config.properties --pods : Number of pods to create --pod-name : To create pods with custom name --attr : CSI volume attributes file name --fs-type: FS Type can be specified Sample ephemeral-config.properties (key/value pair) arrayId=arr1 protocol=iSCSI size=5Gi Storage Capacity Tracking Suite To run storage capacity tracking test suite, run the command:\ncert-csi functional-test capacity-tracking --sc \u003cstorage-class\u003e --drns \u003cdriver-namespace\u003e --pi \u003cpoll-interval\u003e Optional Params: --vs : volume size to be created Other Options Generating tabular report from DB To generate tabular report from the database, run the command:\ncert-csi -db \u003cdb_path\u003e functional-report -tabular Example: cert-csi -db ./test.db functional-report -tabular Note: DB is mandatory parameter\nGenerating XML report from DB To generate XML report from the database, run the command:\ncert-csi -db \u003cdb_path\u003e functional-report -xml Example: cert-csi -db ./test.db functional-report -xml Note: DB is mandatory parameter\nIncluding Array configuration file # Array properties sample (array-config.properties) arrayIPs: 192.168.1.44 name: Unity user: root password: test-password arrayIds: arr-1 Screenshots Tabular Report example\nKubernetes End-To-End Tests All Kubernetes end to end tests require that you provide the driver config based on the storage class you want to test and the version of the kubernetes you want to test against. These are the mandatory parameters that you can provide in command like..\n--driver-config \u003cpath of driver config file\u003e and --version \"v1.25.0\" Running kubernetes end-to-end tests To run kubernetes end-to-end tests, run the command:\ncert-csi k8s-e2e --config \u003ckube config\u003e --driver-config \u003cpath to driver config\u003e --focus \u003cregx pattern to focus Ex: \"External.Storage.*\" \u003e --timeout \u003ctimeout Ex: \"2h\"\u003e --version \u003c version of k8s Ex: \"v1.25.0\"\u003e --skip-tests \u003cskip these steps mentioned in file\u003e --skip \u003cregx pattern to skip tests Ex:\"Generic Ephemeral-volume|(block volmode)\"\u003e Kubernetes end-to-end reporting All the reports generated by kubernetes end-to-end tests will be under $HOME/reports directory by default if user doesn’t mention the report path. Kubernetes end to end tests Execution log file will be placed under $HOME/reports/execution_[storage class name].log Cert-CSI logs will be present in the execution directory info.log , error.log Test config files format driver-config ignore-tests Example Commands cert-csi k8s-e2e --config \"/root/.kube/config\" --driver-config \"/root/e2e_config/config-nfs.yaml\" --focus \"External.Storage.*\" --timeout \"2h\" --version \"v1.25.0\" --skip-tests \"/root/e2e_config/ignore.yaml\" ./cert-csi k8s-e2e --config \"/root/.kube/config\" --driver-config \"/root/e2e_config/config-iscsi.yaml\" --focus \"External.Storage.*\" --timeout \"2h\" --version \"v1.25.0\" --focus-file \"capacity.go\" Performance Tests All performance tests require that you provide a storage class that you want to test. You can provide multiple storage classes in one command. For example,\n... --sc \u003csc1\u003e --sc \u003csc2\u003e ... Running Individual Suites Running Volume Creation test suite To run volume creation test suite, run the command:\ncert-csi test volume-creation --sc \u003cstorage class\u003e -n 25 Running Provisioning test suite To run volume provisioning test suite, run the command:\ncert-csi test provisioning --sc \u003cstorage class\u003e --podNum 1 --volNum 10 Running Scalability test suite To run scalability test suite, run the command:\ncert-csi test scaling --sc \u003cstorage class\u003e --replicas 5 Running VolumeIO test suite To run volumeIO test suite, run the command:\ncert-csi test vio --sc \u003cstorage class\u003e --chainNumber 5 --chainLength 20 Running Snap test suite To run volume snapshot test suite, run the command:\ncert-csi test snap --sc \u003cstorage class\u003e --vsc \u003cvolume snapshot class\u003e Running Multi-attach volume suite To run multi-attach volume test suite, run the command:\ncert-csi test multi-attach-vol --sc \u003cstorage class\u003e --podNum 3 cert-csi test multi-attach-vol --sc \u003cstorage class\u003e --podNum 3 --block # to use raw block volumes Running Replication test suite To run replication test suite, run the command:\ncert-csi test replication --sc \u003cstorage class\u003e --pn 1 --vn 5 --vsc \u003csnapshot class\u003e Running Volume Cloning test suite To run volume cloning test suite, run the command:\ncert-csi test clone-volume --sc \u003cstorage class\u003e --pn 1 --vn 5 Running Volume Expansion test suite To run volume expansion test, run the command:\ncert-csi test expansion --sc \u003cstorage class\u003e --pn 1 --vn 5 --iSize 8Gi --expSize 16Gi cert-csi test expansion --sc \u003cstorage class\u003e --pn 1 --vn 5 # `iSize` and `expSize` default to 3Gi and 6Gi respectively cert-csi test expansion --sc \u003cstorage class\u003e --pn 1 --vn 5 --block # to create block volumes Running Blocksnap suite To run block snapshot test suite, run the command:\ncert-csi test blocksnap --sc \u003cstorageClass\u003e --vsc \u003csnapshotclass\u003e Volume Health Metric Suite To run the volume health metric test suite, run the command:\ncert-csi test volumehealthmetrics --sc \u003cstorage-class\u003e --driver-ns \u003cdriver-namespace\u003e --podNum \u003cnumber-of-pods\u003e --volNum \u003cnumber-of-volumes\u003e Note: Make sure to enable healthMonitor for the driver’s controller and node pods before running this suite. It is recommended to use a smaller interval time for this sidecar.\nEphemeral volumes suite To run the ephemeral volume test suite, run the command:\ncert-csi test ephemeral-volume --driver \u003cdriver-name\u003e --attr ephemeral-config.properties --pods : Number of pods to create --pod-name : Create pods with custom name --attr : File name for the CSI volume attributes file (required) --fs-type: FS Type Sample ephemeral-config.properties (key/value pair) arrayId=arr1 protocol=iSCSI size=5Gi Running Longevity mode To run longevity test suite, run the command:\ncert-csi test \u003cany of previous tests\u003e --sc \u003cstorage class\u003e --longevity \u003cnumber of iterations\u003e Interacting with DB Generating report from runs without running tests To generate test report from the database, run the command:\ncert-csi --db \u003cpath/to/.db\u003e report --testrun \u003ctest-run-name\u003e --html --txt Report types: --html: performance html report --txt: performance txt report --xml: junit compatible xml report, contains basic run infomation --tabular: tidy html report with basic run information Customizing report folder To specify test report folder path, use –path option as follows:\ncert-csi --db \u003cpath/to/.db\u003e report --testrun \u003ctest-run-name\u003e --path \u003cpath-to-folder\u003e Options: --path: path to folder where reports will be created (if not specified ~/.cert-csi/ will be used) Generating report from multiple databases and test runs To generate report from multiple databases, run the command:\ncert-csi report --tr \u003cdb-path\u003e:\u003ctest-run-name\u003e --tr ... --tabular --xml Supported report types: --xml --tabular Listing all known test runs To list all test runs, run the command:\ncert-csi --db \u003cpath/to/.db\u003e list test-runs Other options Customizing report folder To specify test report folder path, use –path option as follows:\ncert-csi \u003ccommand\u003e --path \u003cpath-to-folder\u003e Commands: test \u003cany-subcommand\u003e certify report Running with enabled driver resource usage metrics To run tests with driver resource usage metrics enabled, run the command:\ncert-csi test \u003ctest suite\u003e --sc \u003cstorage class\u003e \u003c...\u003e --ns \u003cdriver namespace\u003e Running custom hooks from program To run tests with custom hooks, run the command:\ncert-csi test \u003ctest suite\u003e --sc \u003cstorage class\u003e \u003c...\u003e --sh ./hooks/start.sh --rh ./hooks/ready.sh --fh ./hooks/finish.sh Screenshots Running provisioning test You can interrupt the application by sending an interruption signal (for example pressing Ctrl + C). It will stop polling and try to cleanup resources.\nRunning scaling test Listing available test runs Running longevity mode Multi DB Tabular report example Text report example\nHTML report example Resource usage example chart ","categories":"","description":"Tool to validate Dell CSI Drivers","excerpt":"Tool to validate Dell CSI Drivers","ref":"/csm-docs/v1/csidriver/installation/test/certcsi/","tags":"","title":"Cert-CSI"},{"body":"Cert-CSI is a tool to validate Dell CSI Drivers. It contains various test suites to validate the drivers.\nInstallation To install this tool you can download one of binary files located in RELEASES\nYou can build the tool by cloning the repository and running this command:\nmake build You can also build a docker container image by running this command:\ndocker build -t cert-csi . If you want to collect csi-driver resource usage metrics, then please provide the namespace where it can be found and install the metric-server using this command (kubectl is required):\nmake install-ms [FOR UNIX] If you want to build and install the tool to your $PATH and enable the auto-completion feature, then run this command:\nmake install-nix Alternatively, you can install the metric-server by following the instructions at https://github.com/kubernetes-incubator/metrics-server\nRunning Cert-CSI To get information on how to use the program, you can use built-in help. If you’re using a UNIX-like system and enabled auto-completion feature while installing the tool, then you can use shell’s built-in auto-completion to navigate through program’s subcommands and flags interactively by just pressing TAB.\nTo run cert-csi, you have to point your environment to a kube cluster. This allows you to receive dynamically formatted suggestions from your cluster. For example if you press TAB while passing –storageclass (or –sc) argument, the tool will parse all existing Storage Classes from your cluster and suggest them as an input for you.\nTo run a docker container your command should look something like this\ndocker run --rm -it -v ~/.kube/config:/root/.kube/config -v $(pwd):/app/cert-csi cert-csi \u003cusual program arguments\u003e Driver Certification You can use cert-csi to launch a certification test run against multiple storage classes to check if the driver adheres to advertised capabilities.\nPreparing Config To run the certification test you need to provide .yaml config with storage classes and their capabilities. You can use example-certify-config.yaml as an example.\nExample:\nstorageClasses: - name: # storage-class-name (ex. powerstore) minSize: # minimal size for your sc (ex. 1Gi) rawBlock: # is Raw Block supported (true or false) expansion: # is volume expansion supported (true or false) clone: # is volume cloning supported (true or false) snapshot: # is volume snapshotting supported (true or false) RWX: # is ReadWriteMany volume access mode supported for non RawBlock volumes (true or false) ephemeral: # if exists, then run EphemeralVolumeSuite driver: # driver name for EphemeralVolumeSuite fstype: # fstype for EphemeralVolumeSuite volumeAttributes: # volume attrs for EphemeralVolumeSuite. attr1: # volume attr for EphemeralVolumeSuite attr2: # volume attr for EphemeralVolumeSuite Launching Certification Test Run After preparing a certification configuration file, you can launch certification by running\ncert-csi certify --cert-config \u003cpath-to-config\u003e Optional Params: --vsc: volume snapshot class, required if you specified snapshot capability --timeout: set the timeout value for certification suites --no-metrics: disables metrics aggregation (set if you encounter k8s performance issues) --path: path to folder where reports will be created (if not specified ~/.cert-csi/ will be used) Functional Tests Running Individual Suites Volume/PVC Creation To run volume or PVC creation test suite, run the command:\ncert-csi functional-test volume-creation --sc \u003cstorage class\u003e -n 5 Optional Params: --custom-name : To give custom name for PVC while creating only 1 PVC --size : To give custom size, possible values for size in Gi/Mi --access-mode : To set custom access-modes, possible values - ReadWriteOnce,ReadOnlyMany and ReadWriteMany --block : To create raw block volumes Provisioning/Pod creation To run volume provisioning or pod creation test suite, run the command:\ncert-csi functional-test provisioning --sc \u003cstorage class\u003e Optional Params: --volumeNumber : number of volumes to attach to each pod --podNumber : number of pod to create --podName : To give custom name for pod while creating only 1 pod --block : To create raw block volumes and attach it to pods --vol-access-mode: To set volume access modes Running Volume Deletion suite To run volume delete test suite, run the command:\ncert-csi functional-test volume-deletion --pvc-name value : PVC name to delete --pvc-namespace : PVC namespace where PVC is present Running Pod Deletion suite To run pod deletion test suite, run the command:\ncert-csi functional-test pod-deletion --pod-name : Pod name to delete --pod-namespace : Pod namespace where pod is present Running Cloned Volume deletion suite To run cloned volume deletion test suite, run the command:\ncert-csi functional-test clone-volume-deletion --clone-volume-name : Volume name to delete Multi Attach Volume Tests To run multi-attach volume test suite, run the command:\ncert-csi functional-test multi-attach-vol --sc \u003cstorage-class\u003e --pods : Number of pods to create --block : To create raw block volume Ephemeral volumes suite To run ephemeral volume test suite, run the command:\ncert-csi functional-test ephemeral-volume --driver \u003cdriver-name\u003e --attr ephemeral-config.properties --pods : Number of pods to create --pod-name : To create pods with custom name --attr : CSI volume attributes file name --fs-type: FS Type can be specified Sample ephemeral-config.properties (key/value pair) arrayId=arr1 protocol=iSCSI size=5Gi Storage Capacity Tracking Suite To run storage capacity tracking test suite, run the command:\ncert-csi functional-test capacity-tracking --sc \u003cstorage-class\u003e --drns \u003cdriver-namespace\u003e --pi \u003cpoll-interval\u003e Optional Params: --vs : volume size to be created Other Options Generating tabular report from DB To generate tabular report from the database, run the command:\ncert-csi -db \u003cdb_path\u003e functional-report -tabular Example: cert-csi -db ./test.db functional-report -tabular Note: DB is mandatory parameter\nGenerating XML report from DB To generate XML report from the database, run the command:\ncert-csi -db \u003cdb_path\u003e functional-report -xml Example: cert-csi -db ./test.db functional-report -xml Note: DB is mandatory parameter\nIncluding Array configuration file # Array properties sample (array-config.properties) arrayIPs: 192.168.1.44 name: Unity user: root password: test-password arrayIds: arr-1 Screenshots Tabular Report example\nKubernetes End-To-End Tests All Kubernetes end to end tests require that you provide the driver config based on the storage class you want to test and the version of the kubernetes you want to test against. These are the mandatory parameters that you can provide in command like..\n--driver-config \u003cpath of driver config file\u003e and --version \"v1.25.0\" Running kubernetes end-to-end tests To run kubernetes end-to-end tests, run the command:\ncert-csi k8s-e2e --config \u003ckube config\u003e --driver-config \u003cpath to driver config\u003e --focus \u003cregx pattern to focus Ex: \"External.Storage.*\" \u003e --timeout \u003ctimeout Ex: \"2h\"\u003e --version \u003c version of k8s Ex: \"v1.25.0\"\u003e --skip-tests \u003cskip these steps mentioned in file\u003e --skip \u003cregx pattern to skip tests Ex:\"Generic Ephemeral-volume|(block volmode)\"\u003e Kubernetes end-to-end reporting All the reports generated by kubernetes end-to-end tests will be under $HOME/reports directory by default if user doesn’t mention the report path. Kubernetes end to end tests Execution log file will be placed under $HOME/reports/execution_[storage class name].log Cert-CSI logs will be present in the execution directory info.log , error.log Test config files format driver-config ignore-tests Example Commands cert-csi k8s-e2e --config \"/root/.kube/config\" --driver-config \"/root/e2e_config/config-nfs.yaml\" --focus \"External.Storage.*\" --timeout \"2h\" --version \"v1.25.0\" --skip-tests \"/root/e2e_config/ignore.yaml\" ./cert-csi k8s-e2e --config \"/root/.kube/config\" --driver-config \"/root/e2e_config/config-iscsi.yaml\" --focus \"External.Storage.*\" --timeout \"2h\" --version \"v1.25.0\" --focus-file \"capacity.go\" Performance Tests All performance tests require that you provide a storage class that you want to test. You can provide multiple storage classes in one command. For example,\n... --sc \u003csc1\u003e --sc \u003csc2\u003e ... Running Individual Suites Running Volume Creation test suite To run volume creation test suite, run the command:\ncert-csi test volume-creation --sc \u003cstorage class\u003e -n 25 Running Provisioning test suite To run volume provisioning test suite, run the command:\ncert-csi test provisioning --sc \u003cstorage class\u003e --podNum 1 --volNum 10 Running Scalability test suite To run scalability test suite, run the command:\ncert-csi test scaling --sc \u003cstorage class\u003e --replicas 5 Running VolumeIO test suite To run volumeIO test suite, run the command:\ncert-csi test vio --sc \u003cstorage class\u003e --chainNumber 5 --chainLength 20 Running Snap test suite To run volume snapshot test suite, run the command:\ncert-csi test snap --sc \u003cstorage class\u003e --vsc \u003cvolume snapshot class\u003e Running Multi-attach volume suite To run multi-attach volume test suite, run the command:\ncert-csi test multi-attach-vol --sc \u003cstorage class\u003e --podNum 3 cert-csi test multi-attach-vol --sc \u003cstorage class\u003e --podNum 3 --block # to use raw block volumes Running Replication test suite To run replication test suite, run the command:\ncert-csi test replication --sc \u003cstorage class\u003e --pn 1 --vn 5 --vsc \u003csnapshot class\u003e Running Volume Cloning test suite To run volume cloning test suite, run the command:\ncert-csi test clone-volume --sc \u003cstorage class\u003e --pn 1 --vn 5 Running Volume Expansion test suite To run volume expansion test, run the command:\ncert-csi test expansion --sc \u003cstorage class\u003e --pn 1 --vn 5 --iSize 8Gi --expSize 16Gi cert-csi test expansion --sc \u003cstorage class\u003e --pn 1 --vn 5 # `iSize` and `expSize` default to 3Gi and 6Gi respectively cert-csi test expansion --sc \u003cstorage class\u003e --pn 1 --vn 5 --block # to create block volumes Running Blocksnap suite To run block snapshot test suite, run the command:\ncert-csi test blocksnap --sc \u003cstorageClass\u003e --vsc \u003csnapshotclass\u003e Running Longevity mode To run longevity test suite, run the command:\ncert-csi test \u003cany of previous tests\u003e --sc \u003cstorage class\u003e --longevity \u003cnumber of iterations\u003e Interacting with DB Generating report from runs without running tests To generate test report from the database, run the command:\ncert-csi --db \u003cpath/to/.db\u003e report --testrun \u003ctest-run-name\u003e --html --txt Report types: --html: performance html report --txt: performance txt report --xml: junit compatible xml report, contains basic run infomation --tabular: tidy html report with basic run information Customizing report folder To specify test report folder path, use –path option as follows:\ncert-csi --db \u003cpath/to/.db\u003e report --testrun \u003ctest-run-name\u003e --path \u003cpath-to-folder\u003e Options: --path: path to folder where reports will be created (if not specified ~/.cert-csi/ will be used) Generating report from multiple databases and test runs To generate report from multiple databases, run the command:\ncert-csi report --tr \u003cdb-path\u003e:\u003ctest-run-name\u003e --tr ... --tabular --xml Supported report types: --xml --tabular Listing all known test runs To list all test runs, run the command:\ncert-csi --db \u003cpath/to/.db\u003e list test-runs Other options Customizing report folder To specify test report folder path, use –path option as follows:\ncert-csi \u003ccommand\u003e --path \u003cpath-to-folder\u003e Commands: test \u003cany-subcommand\u003e certify report Running with enabled driver resource usage metrics To run tests with driver resource usage metrics enabled, run the command:\ncert-csi test \u003ctest suite\u003e --sc \u003cstorage class\u003e \u003c...\u003e --ns \u003cdriver namespace\u003e Running custom hooks from program To run tests with custom hooks, run the command:\ncert-csi test \u003ctest suite\u003e --sc \u003cstorage class\u003e \u003c...\u003e --sh ./hooks/start.sh --rh ./hooks/ready.sh --fh ./hooks/finish.sh Screenshots Running provisioning test You can interrupt the application by sending an interruption signal (for example pressing Ctrl + C). It will stop polling and try to cleanup resources.\nRunning scaling test Listing available test runs Running longevity mode Multi DB Tabular report example Text report example\nHTML report example Resource usage example chart ","categories":"","description":"Tool to validate Dell CSI Drivers","excerpt":"Tool to validate Dell CSI Drivers","ref":"/csm-docs/v2/csidriver/installation/test/certcsi/","tags":"","title":"Cert-CSI"},{"body":"Cert-CSI is a tool to validate Dell CSI Drivers. It contains various test suites to validate the drivers.\nInstallation To install this tool you can download one of binary files located in RELEASES\nYou can build the tool by cloning the repository and running this command:\nmake build You can also build a docker container image by running this command:\ndocker build -t cert-csi . If you want to collect csi-driver resource usage metrics, then please provide the namespace where it can be found and install the metric-server using this command (kubectl is required):\nmake install-ms [FOR UNIX] If you want to build and install the tool to your $PATH and enable the auto-completion feature, then run this command:\nmake install-nix Alternatively, you can install the metric-server by following the instructions at https://github.com/kubernetes-incubator/metrics-server\nRunning Cert-CSI To get information on how to use the program, you can use built-in help. If you’re using a UNIX-like system and enabled auto-completion feature while installing the tool, then you can use shell’s built-in auto-completion to navigate through program’s subcommands and flags interactively by just pressing TAB.\nTo run cert-csi, you have to point your environment to a kube cluster. This allows you to receive dynamically formatted suggestions from your cluster. For example if you press TAB while passing –storageclass (or –sc) argument, the tool will parse all existing Storage Classes from your cluster and suggest them as an input for you.\nTo run a docker container your command should look something like this docker run --rm -it -v ~/.kube/config:/root/.kube/config -v $(pwd):/app/cert-csi cert-csi \u003cusual program arguments\u003e\nDriver Certification You can use cert-csi to launch a certification test run against multiple storage classes to check if the driver adheres to advertised capabilities.\nPreparing Config To run the certification test you need to provide .yaml config with storage classes and their capabilities. You can use example-certify-config.yaml as an example.\nExample:\nstorageClasses: - name: # storage-class-name (ex. powerstore) minSize: # minimal size for your sc (ex. 1Gi) rawBlock: # is Raw Block supported (true or false) expansion: # is volume expansion supported (true or false) clone: # is volume cloning supported (true or false) snapshot: # is volume snapshotting supported (true or false) RWX: # is ReadWriteMany volume access mode supported for non RawBlock volumes (true or false) volumeHealth: false # set this to enable the execution of the VolumeHealthMetricsSuite. # Make sure to enable healthMonitor for the driver's controller and node pods before running this suite. It is recommended to use a smaller interval time for this sidecar and pass the required arguments. VGS: false # set this to enable the execution of the VolumeGroupSnapSuite. # Additionally, make sure to provide the necessary required arguments such as volumeSnapshotClass, vgs-volume-label, and any others as needed. RWOP: false # set this to enable the execution of the MultiAttachSuite with the AccessMode set to ReadWriteOncePod. ephemeral: # if exists, then run EphemeralVolumeSuite driver: # driver name for EphemeralVolumeSuite fstype: # fstype for EphemeralVolumeSuite volumeAttributes: # volume attrs for EphemeralVolumeSuite. attr1: # volume attr for EphemeralVolumeSuite attr2: # volume attr for EphemeralVolumeSuite Launching Certification Test Run After preparing a certification configuration file, you can launch certification by running\ncert-csi certify --cert-config \u003cpath-to-config\u003e Optional Params: --vsc: volume snapshot class, required if you specified snapshot capability --timeout: set the timeout value for certification suites --no-metrics: disables metrics aggregation (set if you encounter k8s performance issues) --path: path to folder where reports will be created (if not specified ~/.cert-csi/ will be used) Functional Tests Running Individual Suites Volume/PVC Creation To run volume or PVC creation test suite, run the command:\ncert-csi functional-test volume-creation --sc \u003cstorage class\u003e -n 5 Optional Params: --custom-name : To give custom name for PVC while creating only 1 PVC --size : To give custom size, possible values for size in Gi/Mi --access-mode : To set custom access-modes, possible values - ReadWriteOnce,ReadOnlyMany and ReadWriteMany --block : To create raw block volumes Provisioning/Pod creation To run volume provisioning or pod creation test suite, run the command:\ncert-csi functional-test provisioning --sc \u003cstorage class\u003e Optional Params: --volumeNumber : number of volumes to attach to each pod --podNumber : number of pod to create --podName : To give custom name for pod while creating only 1 pod --block : To create raw block volumes and attach it to pods --vol-access-mode: To set volume access modes Running Volume Deletion suite To run volume delete test suite, run the command:\ncert-csi functional-test volume-deletion --pvc-name value : PVC name to delete --pvc-namespace : PVC namespace where PVC is present Running Pod Deletion suite To run pod deletion test suite, run the command:\ncert-csi functional-test pod-deletion --pod-name : Pod name to delete --pod-namespace : Pod namespace where pod is present Running Cloned Volume deletion suite To run cloned volume deletion test suite, run the command:\ncert-csi functional-test clone-volume-deletion --clone-volume-name : Volume name to delete Multi Attach Volume Tests To run multi-attach volume test suite, run the command:\ncert-csi functional-test multi-attach-vol --sc \u003cstorage-class\u003e --pods : Number of pods to create --block : To create raw block volume Ephemeral volumes suite To run ephemeral volume test suite, run the command:\ncert-csi functional-test ephemeral-volume --driver \u003cdriver-name\u003e --attr ephemeral-config.properties --pods : Number of pods to create --pod-name : To create pods with custom name --attr : CSI volume attributes file name --fs-type: FS Type can be specified Sample ephemeral-config.properties (key/value pair) arrayId=arr1 protocol=iSCSI size=5Gi Storage Capacity Tracking Suite To run storage capacity tracking test suite, run the command:\ncert-csi functional-test capacity-tracking --sc \u003cstorage-class\u003e --drns \u003cdriver-namespace\u003e --pi \u003cpoll-interval\u003e Optional Params: --vs : volume size to be created Other Options Generating tabular report from DB To generate tabular report from the database, run the command:\ncert-csi -db \u003cdb_path\u003e functional-report -tabular Example: cert-csi -db ./test.db functional-report -tabular Note: DB is mandatory parameter\nGenerating XML report from DB To generate XML report from the database, run the command:\ncert-csi -db \u003cdb_path\u003e functional-report -xml Example: cert-csi -db ./test.db functional-report -xml Note: DB is mandatory parameter\nIncluding Array configuration file # Array properties sample (array-config.properties) arrayIPs: 192.168.1.44 name: Unity user: root password: test-password arrayIds: arr-1 Screenshots Tabular Report example\nKubernetes End-To-End Tests All Kubernetes end to end tests require that you provide the driver config based on the storage class you want to test and the version of the kubernetes you want to test against. These are the mandatory parameters that you can provide in command like.. --driver-config \u003cpath of driver config file\u003e and --version \"v1.25.0\"\nRunning kubernetes end-to-end tests To run kubernetes end-to-end tests, run the command:\ncert-csi k8s-e2e --config \u003ckube config\u003e --driver-config \u003cpath to driver config\u003e --focus \u003cregx pattern to focus Ex: \"External.Storage.*\" \u003e --timeout \u003ctimeout Ex: \"2h\"\u003e --version \u003c version of k8s Ex: \"v1.25.0\"\u003e --skip-tests \u003cskip these steps mentioned in file\u003e --skip \u003cregx pattern to skip tests Ex:\"Generic Ephemeral-volume|(block volmode)\"\u003e Kubernetes end-to-end reporting All the reports generated by kubernetes end-to-end tests will be under $HOME/reports directory by default if user doesn’t mention the report path. Kubernetes end to end tests Execution log file will be placed under $HOME/reports/execution_[storage class name].log Cert-CSI logs will be present in the execution directory info.log , error.log Test config files format driver-config ignore-tests Example Commands cert-csi k8s-e2e --config \"/root/.kube/config\" --driver-config \"/root/e2e_config/config-nfs.yaml\" --focus \"External.Storage.*\" --timeout \"2h\" --version \"v1.25.0\" --skip-tests \"/root/e2e_config/ignore.yaml\" ./cert-csi k8s-e2e --config \"/root/.kube/config\" --driver-config \"/root/e2e_config/config-iscsi.yaml\" --focus \"External.Storage.*\" --timeout \"2h\" --version \"v1.25.0\" --focus-file \"capacity.go\" Performance Tests All performance tests require that you provide a storage class that you want to test. You can provide multiple storage classes in one command. For example, ... --sc \u003csc1\u003e --sc \u003csc2\u003e ...\nRunning Individual Suites Running Volume Creation test suite To run volume creation test suite, run the command:\ncert-csi test volume-creation --sc \u003cstorage class\u003e -n 25 Running Provisioning test suite To run volume provisioning test suite, run the command:\ncert-csi test provisioning --sc \u003cstorage class\u003e --podNum 1 --volNum 10 Running Scalability test suite To run scalability test suite, run the command:\ncert-csi test scaling --sc \u003cstorage class\u003e --replicas 5 Running VolumeIO test suite To run volumeIO test suite, run the command:\ncert-csi test vio --sc \u003cstorage class\u003e --chainNumber 5 --chainLength 20 Running Snap test suite To run volume snapshot test suite, run the command:\ncert-csi test snap --sc \u003cstorage class\u003e --vsc \u003cvolume snapshot class\u003e Running Multi-attach volume suite To run multi-attach volume test suite, run the command:\ncert-csi test multi-attach-vol --sc \u003cstorage class\u003e --podNum 3 cert-csi test multi-attach-vol --sc \u003cstorage class\u003e --podNum 3 --block # to use raw block volumes Running Replication test suite To run replication test suite, run the command:\ncert-csi test replication --sc \u003cstorage class\u003e --pn 1 --vn 5 --vsc \u003csnapshot class\u003e Running Volume Cloning test suite To run volume cloning test suite, run the command:\ncert-csi test clone-volume --sc \u003cstorage class\u003e --pn 1 --vn 5 Running Volume Expansion test suite To run volume expansion test, run the command:\ncert-csi test expansion --sc \u003cstorage class\u003e --pn 1 --vn 5 --iSize 8Gi --expSize 16Gi cert-csi test expansion --sc \u003cstorage class\u003e --pn 1 --vn 5 # `iSize` and `expSize` default to 3Gi and 6Gi respectively cert-csi test expansion --sc \u003cstorage class\u003e --pn 1 --vn 5 --block # to create block volumes Running Blocksnap suite To run block snapshot test suite, run the command:\ncert-csi test blocksnap --sc \u003cstorageClass\u003e --vsc \u003csnapshotclass\u003e Volume Health Metric Suite To run the volume health metric test suite, run the command:\ncert-csi test volumehealthmetrics --sc \u003cstorage-class\u003e --driver-ns \u003cdriver-namespace\u003e --podNum \u003cnumber-of-pods\u003e --volNum \u003cnumber-of-volumes\u003e Note: Make sure to enable healthMonitor for the driver’s controller and node pods before running this suite. It is recommended to use a smaller interval time for this sidecar.\nEphemeral volumes suite To run the ephemeral volume test suite, run the command:\ncert-csi test ephemeral-volume --driver \u003cdriver-name\u003e --attr ephemeral-config.properties --pods : Number of pods to create --pod-name : Create pods with custom name --attr : File name for the CSI volume attributes file (required) --fs-type: FS Type Sample ephemeral-config.properties (key/value pair) arrayId=arr1 protocol=iSCSI size=5Gi Running Longevity mode To run longevity test suite, run the command:\ncert-csi test \u003cany of previous tests\u003e --sc \u003cstorage class\u003e --longevity \u003cnumber of iterations\u003e Interacting with DB Generating report from runs without running tests To generate test report from the database, run the command:\ncert-csi --db \u003cpath/to/.db\u003e report --testrun \u003ctest-run-name\u003e --html --txt Report types: --html: performance html report --txt: performance txt report --xml: junit compatible xml report, contains basic run infomation --tabular: tidy html report with basic run information Customizing report folder To specify test report folder path, use –path option as follows:\ncert-csi --db \u003cpath/to/.db\u003e report --testrun \u003ctest-run-name\u003e --path \u003cpath-to-folder\u003e Options: --path: path to folder where reports will be created (if not specified ~/.cert-csi/ will be used) Generating report from multiple databases and test runs To generate report from multiple databases, run the command:\ncert-csi report --tr \u003cdb-path\u003e:\u003ctest-run-name\u003e --tr ... --tabular --xml Supported report types: --xml --tabular Listing all known test runs To list all test runs, run the command:\ncert-csi --db \u003cpath/to/.db\u003e list test-runs Other options Customizing report folder To specify test report folder path, use –path option as follows:\ncert-csi \u003ccommand\u003e --path \u003cpath-to-folder\u003e Commands: test \u003cany-subcommand\u003e certify report Running with enabled driver resource usage metrics To run tests with driver resource usage metrics enabled, run the command:\ncert-csi test \u003ctest suite\u003e --sc \u003cstorage class\u003e \u003c...\u003e --ns \u003cdriver namespace\u003e Running custom hooks from program To run tests with custom hooks, run the command:\ncert-csi test \u003ctest suite\u003e --sc \u003cstorage class\u003e \u003c...\u003e --sh ./hooks/start.sh --rh ./hooks/ready.sh --fh ./hooks/finish.sh Screenshots Running provisioning test You can interrupt the application by sending an interruption signal (for example pressing Ctrl + C). It will stop polling and try to cleanup resources.\nRunning scaling test Listing available test runs Running longevity mode Multi DB Tabular report example Text report example\nHTML report example Resource usage example chart ","categories":"","description":"Tool to validate Dell CSI Drivers","excerpt":"Tool to validate Dell CSI Drivers","ref":"/csm-docs/v3/csidriver/installation/test/certcsi/","tags":"","title":"Cert-CSI"},{"body":"CR Sample Files CSI Operator CSM Operator PowerScale isilon_v270_k8s_127.yaml storage_csm_powerscale_v290.yaml PowerMax powermax_v270_k8s_127.yaml storage_csm_powermax_v290.yaml PowerStore powerstore_v270_k8s_127.yaml storage_csm_powerstore_v290.yaml Unity XT unity_v270_k8s_127.yaml storage_csm_unity_v290.yaml PowerFlex vxflex_v270_k8s_127.yaml storage_csm_powerflex_v290.yaml NOTE: Sample files refer to the latest version for each platform. If you do not want to upgrade, please find your preferred version in the csm-operator repository.\nMigration Steps Save the CR yaml file of the current CSI driver to preserve the settings. Use the following commands in your cluster to get the CR: kubectl -n \u003cnamespace\u003e get \u003cCRD_kind\u003e kubectl -n \u003cnamespace\u003e get \u003cCRD_kind\u003e/\u003cCR_name\u003e -o yaml Example for CSI Unity:\nkubectl -n openshift-operators get CSIUnity kubectl -n openshift-operators get CSIUnity/test-unity -o yaml Map and update the settings from the CR in step 1 to the relevant CSM Operator CR As the yaml content may differ, ensure the values held in the step 1 CR backup are present in the new CR before installing the new driver. CR Samples table provided above can be used to compare and map the differences in attributes between Dell CSI Operator and CSM Operator CRs Ex: spec.driver.fsGroupPolicy in CSI Operator maps to spec.driver.csiDriverSpec.fSGroupPolicy in CSM Operator Retain (or do not delete) the secret, namespace, storage classes, and volume snapshot classes from the original deployment as they will be re-used in the CSM operator deployment Uninstall the CR from the CSI Operator kubectl delete \u003cdriver_type\u003e/\u003cdriver_name\u003e -n \u003cdriver_namespace\u003e Uninstall the CSI Operator itself Instructions can be found here Install the CSM Operator Instructions can be found here Install the CR updated in step 2 Instructions can be found here NOTE: Uninstallation of the driver and the Operator is non-disruptive for mounted volumes. Nonetheless you can not create new volume, snapshot or move a Pod.\nOpenShift Web Console Migration Steps Save the CR yaml file of the current CSI driver to preserve the settings (for use in step 6). Use the following commands in your cluster to get the CR: kubectl -n \u003cnamespace\u003e get \u003cCRD_kind\u003e kubectl -n \u003cnamespace\u003e get \u003cCRD_kind\u003e/\u003cCR_name\u003e -o yaml Example for CSI Unity:\nkubectl -n openshift-operators get CSIUnity kubectl -n openshift-operators get CSIUnity/test-unity -o yaml Retain (or do not delete) the secret, namespace, storage classes, and volume snapshot classes from the original deployment as they will be re-used in the CSM operator deployment Delete the CSI driver through the CSI Operator in the OpenShift Web Console Find the CSI operator under Operators -\u003e Installed Operators Select the Dell CSI Operator and find your installed CSI driver under All instances Uninstall the CSI Operator in the OpenShift Web Console Install the CSM Operator in the OpenShift Web Console Search for Dell in the OperatorHub Select Dell Container Storage Modules and install Install the CSI driver through the CSM Operator in the OpenShift Web Console Select Create instance under the provided Container Storage Module API Use the CR backup from step 1 to manually map desired settings to the new CSI driver As the yaml content may differ, ensure the values held in the step 1 CR backup are present in the new CR before installing the new driver Ex: spec.driver.fsGroupPolicy in PowerMax 2.7 for CSI Operator maps to spec.driver.csiDriverSpec.fSGroupPolicy in PowerMax 2.7 for CSM Operator NOTE: Uninstallation of the driver and the Operator is non-disruptive for mounted volumes. Nonetheless you can not create new volume, snapshot or move a Pod.\nTesting To test that the new installation is working, please follow the steps outlined here for your specific driver.\n","categories":"","description":"Migrating from CSI Operator to CSM Operator\n","excerpt":"Migrating from CSI Operator to CSM Operator\n","ref":"/csm-docs/docs/csidriver/installation/operator/operator_migration/","tags":"","title":"CSI to CSM Operator Migration"},{"body":"CR Sample Files CSI Operator CSM Operator PowerScale isilon_v270_k8s_127.yaml storage_csm_powerscale_v280.yaml PowerMax powermax_v270_k8s_127.yaml storage_csm_powermax_v280.yaml PowerStore powerstore_v270_k8s_127.yaml storage_csm_powerstore_v290.yaml Unity XT unity_v270_k8s_127.yaml storage_csm_unity_v280.yaml PowerFlex vxflex_v270_k8s_127.yaml storage_csm_powerflex_v290.yaml NOTE: Sample files refer to the latest version for each platform. If you do not want to upgrade, please find your preferred version in the csm-operator repository.\nMigration Steps Save the CR yaml file of the current CSI driver to preserve the settings. Use the following commands in your cluster to get the CR: kubectl -n \u003cnamespace\u003e get \u003cCRD_kind\u003e kubectl -n \u003cnamespace\u003e get \u003cCRD_kind\u003e/\u003cCR_name\u003e -o yaml Example for CSI Unity:\nkubectl -n openshift-operators get CSIUnity kubectl -n openshift-operators get CSIUnity/test-unity -o yaml Map and update the settings from the CR in step 1 to the relevant CSM Operator CR As the yaml content may differ, ensure the values held in the step 1 CR backup are present in the new CR before installing the new driver Ex: spec.driver.fsGroupPolicy in PowerMax 2.6 for CSI Operator maps to spec.driver.csiDriverSpec.fSGroupPolicy in PowerMax 2.6 for CSM Operator Retain (or do not delete) the secret, namespace, storage classes, and volume snapshot classes from the original deployment as they will be re-used in the CSM operator deployment Uninstall the CR from the CSI Operator kubectl delete \u003cdriver_type\u003e/\u003cdriver_name\u003e -n \u003cdriver_namespace\u003e Uninstall the CSI Operator itself Instructions can be found here Install the CSM Operator Instructions can be found here Install the CR updated in step 2 Instructions can be found here NOTE: Uninstallation of the driver and the Operator is non-disruptive for mounted volumes. Nonetheless you can not create new volume, snapshot or move a Pod.\nOpenShift Web Console Migration Steps Save the CR yaml file of the current CSI driver to preserve the settings (for use in step 6). Use the following commands in your cluster to get the CR: kubectl -n \u003cnamespace\u003e get \u003cCRD_kind\u003e kubectl -n \u003cnamespace\u003e get \u003cCRD_kind\u003e/\u003cCR_name\u003e -o yaml Example for CSI Unity:\nkubectl -n openshift-operators get CSIUnity kubectl -n openshift-operators get CSIUnity/test-unity -o yaml Retain (or do not delete) the secret, namespace, storage classes, and volume snapshot classes from the original deployment as they will be re-used in the CSM operator deployment Delete the CSI driver through the CSI Operator in the OpenShift Web Console Find the CSI operator under Operators -\u003e Installed Operators Select the Dell CSI Operator and find your installed CSI driver under All instances Uninstall the CSI Operator in the OpenShift Web Console Install the CSM Operator in the OpenShift Web Console Search for Dell in the OperatorHub Select Dell Container Storage Modules and install Install the CSI driver through the CSM Operator in the OpenShift Web Console Select Create instance under the provided Container Storage Module API Use the CR backup from step 1 to manually map desired settings to the new CSI driver As the yaml content may differ, ensure the values held in the step 1 CR backup are present in the new CR before installing the new driver Ex: spec.driver.fsGroupPolicy in PowerMax 2.6 for CSI Operator maps to spec.driver.csiDriverSpec.fSGroupPolicy in PowerMax 2.6 for CSM Operator NOTE: Uninstallation of the driver and the Operator is non-disruptive for mounted volumes. Nonetheless you can not create new volume, snapshot or move a Pod.\nTesting To test that the new installation is working, please follow the steps outlined here for your specific driver.\n","categories":"","description":"Migrating from CSI Operator to CSM Operator\n","excerpt":"Migrating from CSI Operator to CSM Operator\n","ref":"/csm-docs/v1/csidriver/installation/operator/operator_migration/","tags":"","title":"CSI to CSM Operator Migration"},{"body":" Before installing the drivers, Dell CSI Operator tries to validate the Custom Resource being created. If some mandatory environment variables are missing or there is a type mismatch, then the Operator will report an error during the reconciliation attempts.\nBecause of this, the status of the Custom Resource will change to “Failed” and the error captured in the “ErrorMessage” field in the status.\nFor example - If the PowerMax driver was installed in the namespace test-powermax and has the name powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the Custom Resource details.\nIf there was an error while installing the driver, then you would see a status like this:\nstatus: status: errorMessage: mandatory Env - X_CSI_K8S_CLUSTER_PREFIX not specified in user spec state: Failed The state of the Custom Resource can also change to Failed because of any other prohibited updates or any failure while installing the driver. In order to recover from this failure, fix the error in the manifest and update/patch the Custom Resource\nAfter an update to the driver, the controller pod may not have the latest desired specification.\nThis happens when the controller pod was in a failed state before applying the update. Even though the Dell CSI Operator updates the pod template specification for the StatefulSet, the StatefulSet controller does not apply the update to the pod. This happens because of the unique nature of StatefulSets where the controller tries to retain the last known working state.\nTo get around this problem, the Dell CSI Operator forces an update of the pod specification by deleting the older pod. In case the Dell CSI Operator fails to do so, delete the controller pod to force an update of the controller pod specification\nThe Status of the CSI Driver Custom Resource shows the state of the driver pods after installation. This state will not be updated automatically if there are any changes to the driver pods outside any Operator operations.\nAt times because of inconsistencies in fetching data from the Kubernetes cache, the state of some driver pods may not be updated correctly in the status. To force an update of the state, you can update the Custom Resource forcefully by setting forceUpdate to true. If all the driver pods are in the Available State, then the state of the Custom Resource will be updated as Running\n","categories":"","description":"Troubleshooting Dell CSI Operator","excerpt":"Troubleshooting Dell CSI Operator","ref":"/csm-docs/v1/csidriver/troubleshooting/operator/","tags":"","title":"Dell CSI Operator"},{"body":" Before installing the drivers, Dell CSI Operator tries to validate the Custom Resource being created. If some mandatory environment variables are missing or there is a type mismatch, then the Operator will report an error during the reconciliation attempts.\nBecause of this, the status of the Custom Resource will change to “Failed” and the error captured in the “ErrorMessage” field in the status.\nFor example - If the PowerMax driver was installed in the namespace test-powermax and has the name powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the Custom Resource details.\nIf there was an error while installing the driver, then you would see a status like this:\nstatus: status: errorMessage: mandatory Env - X_CSI_K8S_CLUSTER_PREFIX not specified in user spec state: Failed The state of the Custom Resource can also change to Failed because of any other prohibited updates or any failure while installing the driver. In order to recover from this failure, fix the error in the manifest and update/patch the Custom Resource\nAfter an update to the driver, the controller pod may not have the latest desired specification.\nThis happens when the controller pod was in a failed state before applying the update. Even though the Dell CSI Operator updates the pod template specification for the StatefulSet, the StatefulSet controller does not apply the update to the pod. This happens because of the unique nature of StatefulSets where the controller tries to retain the last known working state.\nTo get around this problem, the Dell CSI Operator forces an update of the pod specification by deleting the older pod. In case the Dell CSI Operator fails to do so, delete the controller pod to force an update of the controller pod specification\nThe Status of the CSI Driver Custom Resource shows the state of the driver pods after installation. This state will not be updated automatically if there are any changes to the driver pods outside any Operator operations.\nAt times because of inconsistencies in fetching data from the Kubernetes cache, the state of some driver pods may not be updated correctly in the status. To force an update of the state, you can update the Custom Resource forcefully by setting forceUpdate to true. If all the driver pods are in the Available State, then the state of the Custom Resource will be updated as Running\n","categories":"","description":"Troubleshooting Dell CSI Operator","excerpt":"Troubleshooting Dell CSI Operator","ref":"/csm-docs/v2/csidriver/troubleshooting/operator/","tags":"","title":"Dell CSI Operator"},{"body":" Before installing the drivers, Dell CSI Operator tries to validate the Custom Resource being created. If some mandatory environment variables are missing or there is a type mismatch, then the Operator will report an error during the reconciliation attempts.\nBecause of this, the status of the Custom Resource will change to “Failed” and the error captured in the “ErrorMessage” field in the status.\nFor example - If the PowerMax driver was installed in the namespace test-powermax and has the name powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the Custom Resource details.\nIf there was an error while installing the driver, then you would see a status like this:\nstatus: status: errorMessage: mandatory Env - X_CSI_K8S_CLUSTER_PREFIX not specified in user spec state: Failed The state of the Custom Resource can also change to Failed because of any other prohibited updates or any failure while installing the driver. In order to recover from this failure, fix the error in the manifest and update/patch the Custom Resource\nAfter an update to the driver, the controller pod may not have the latest desired specification.\nThis happens when the controller pod was in a failed state before applying the update. Even though the Dell CSI Operator updates the pod template specification for the StatefulSet, the StatefulSet controller does not apply the update to the pod. This happens because of the unique nature of StatefulSets where the controller tries to retain the last known working state.\nTo get around this problem, the Dell CSI Operator forces an update of the pod specification by deleting the older pod. In case the Dell CSI Operator fails to do so, delete the controller pod to force an update of the controller pod specification\nThe Status of the CSI Driver Custom Resource shows the state of the driver pods after installation. This state will not be updated automatically if there are any changes to the driver pods outside any Operator operations.\nAt times because of inconsistencies in fetching data from the Kubernetes cache, the state of some driver pods may not be updated correctly in the status. To force an update of the state, you can update the Custom Resource forcefully by setting forceUpdate to true. If all the driver pods are in the Available State, then the state of the Custom Resource will be updated as Running\n","categories":"","description":"Troubleshooting Dell CSI Operator","excerpt":"Troubleshooting Dell CSI Operator","ref":"/csm-docs/v3/csidriver/troubleshooting/operator/","tags":"","title":"Dell CSI Operator"},{"body":" Welcome to Dell Technologies Container Storage Modules documentation! Learn More ","categories":"","description":"","excerpt":" Welcome to Dell Technologies Container Storage Modules documentation! …","ref":"/csm-docs/","tags":"","title":"Dell Technologies"},{"body":" This document version is no longer actively maintained. The site that you are currently viewing is an archived snapshot. For up-to-date documentation, see the latest version The CSM Authorization RPM will be deprecated in a future release. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nThe Dell Technologies (Dell) Container Storage Modules (CSM) enables simple and consistent integration and automation experiences, extending enterprise storage capabilities to Kubernetes for cloud-native stateful applications. It reduces management complexity so developers can independently consume enterprise storage with ease and automate daily operations such as provisioning, snapshotting, replication, observability, authorization, application mobility, encryption, and resiliency.\nCSM is made up of multiple components including modules (enterprise capabilities), CSI drivers (storage enablement), and other related applications (deployment, feature controllers, etc).\nAuthorization CSM for Authorization provides storage and Kubernetes administrators the ability to apply RBAC for Dell CSI Drivers. It does this by deploying a proxy between the CSI driver and the storage system to enforce role-based access and usage rules.\n…Learn more Supports PowerFlex PowerScale PowerMax Replication CSM for Replication project aims to bring Replication \u0026 Disaster Recovery capabilities of Dell Storage Arrays to Kubernetes clusters. It helps you replicate groups of volumes and can provide you a way to restart applications in case of both planned and unplanned migration. …Learn more Supports PowerFlex PowerStore PowerScale PowerMax Resiliency CSM for Resiliency is designed to make Kubernetes Applications, including those that utilize persistent storage, more resilient to various failures. …Learn more Supports PowerFlex PowerScale Unity Observability CSM for Observability provides visibility on the capacity of the volumes/file shares that is being managed with Dell CSM CSI (Container Storage Interface) drivers along with their performance in terms of bandwidth, IOPS, and response time. …Learn more Supports PowerFlex PowerStore PowerScale PowerMax Application Mobility Container Storage Modules for Application Mobility provide Kubernetes administrators the ability to clone their stateful application workloads and application data to other clusters, either on-premise or in the cloud. …Learn more Supports all platforms Encryption Encryption provides the capability to encrypt user data residing on volumes created by Dell CSI Drivers. …Learn more Supports PowerScale License The tech-preview releases of Application Mobility and Encryption require a license. Request a license using the Container Storage Modules License Request by providing the requested details. …Learn more Required for Application Mobility \u0026 Encryption CSM Modules Support Matrix for Dell CSI Drivers CSM Module CSI PowerFlex v2.8.0 CSI PowerScale v2.8.0 CSI PowerStore v2.8.0 CSI PowerMax v2.8.0 CSI Unity XT v2.8.0 Authorization v1.8.0 ✔️ ✔️ ❌ ✔️ ❌ Observability v1.6.0 ✔️ ✔️ ✔️ ✔️ ❌ Replication v1.6.0 ✔️ ✔️ ✔️ ✔️ ❌ Resiliency v1.7.0 ✔️ ✔️ ✔️ ❌ ✔️ Encryption v0.4.0 ❌ ✔️ ❌ ❌ ❌ Application Mobility v0.4.0 ✔️ ✔️ ✔️ ✔️ ✔️ ","categories":"","description":"","excerpt":" This document version is no longer actively maintained. The site that …","ref":"/csm-docs/v1/","tags":"","title":"Documentation"},{"body":" This document version is no longer actively maintained. The site that you are currently viewing is an archived snapshot. For up-to-date documentation, see the latest version CSM 1.7.1 is applicable to helm based installations of PowerFlex driver.\nThe Dell Technologies (Dell) Container Storage Modules (CSM) enables simple and consistent integration and automation experiences, extending enterprise storage capabilities to Kubernetes for cloud-native stateful applications. It reduces management complexity so developers can independently consume enterprise storage with ease and automate daily operations such as provisioning, snapshotting, replication, observability, authorization, application mobility, encryption, and resiliency.\nCSM is made up of multiple components including modules (enterprise capabilities), CSI drivers (storage enablement), and other related applications (deployment, feature controllers, etc).\nAuthorization CSM for Authorization provides storage and Kubernetes administrators the ability to apply RBAC for Dell CSI Drivers. It does this by deploying a proxy between the CSI driver and the storage system to enforce role-based access and usage rules.\n…Learn more Supports PowerFlex PowerScale PowerMax Replication CSM for Replication project aims to bring Replication \u0026 Disaster Recovery capabilities of Dell Storage Arrays to Kubernetes clusters. It helps you replicate groups of volumes and can provide you a way to restart applications in case of both planned and unplanned migration. …Learn more Supports PowerFlex PowerStore PowerScale PowerMax Resiliency CSM for Resiliency is designed to make Kubernetes Applications, including those that utilize persistent storage, more resilient to various failures. …Learn more Supports PowerFlex PowerScale Unity Observability CSM for Observability provides visibility on the capacity of the volumes/file shares that is being managed with Dell CSM CSI (Container Storage Interface) drivers along with their performance in terms of bandwidth, IOPS, and response time. …Learn more Supports PowerFlex PowerStore PowerScale PowerMax Application Mobility Container Storage Modules for Application Mobility provide Kubernetes administrators the ability to clone their stateful application workloads and application data to other clusters, either on-premise or in the cloud. …Learn more Supports all platforms Encryption Encryption provides the capability to encrypt user data residing on volumes created by Dell CSI Drivers. …Learn more Supports PowerScale License The tech-preview releases of Application Mobility and Encryption require a license. Request a license using the Container Storage Modules License Request by providing the requested details. …Learn more Required for Application Mobility \u0026 Encryption CSM Modules Support Matrix for Dell CSI Drivers CSM Module CSI PowerFlex v2.7.1 CSI PowerScale v2.7.0 CSI PowerStore v2.7.0 CSI PowerMax v2.7.0 CSI Unity XT v2.7.0 Authorization v1.7.0 ✔️ ✔️ ❌ ✔️ ❌ Observability v1.5.0 ✔️ ✔️ ✔️ ✔️ ❌ Replication v1.5.0 ✔️ ✔️ ✔️ ✔️ ❌ Resiliency v1.6.0 ✔️ ✔️ ✔️ ❌ ✔️ Encryption v0.4.0 ❌ ✔️ ❌ ❌ ❌ Application Mobility v0.4.0 ✔️ ✔️ ✔️ ✔️ ✔️ ","categories":"","description":"","excerpt":" This document version is no longer actively maintained. The site that …","ref":"/csm-docs/v2/","tags":"","title":"Documentation"},{"body":" This document version is no longer actively maintained. The site that you are currently viewing is an archived snapshot. For up-to-date documentation, see the latest version\nThe Dell Technologies (Dell) Container Storage Modules (CSM) enables simple and consistent integration and automation experiences, extending enterprise storage capabilities to Kubernetes for cloud-native stateful applications. It reduces management complexity so developers can independently consume enterprise storage with ease and automate daily operations such as provisioning, snapshotting, replication, observability, authorization, application mobility, encryption, and resiliency.\nCSM is made up of multiple components including modules (enterprise capabilities), CSI drivers (storage enablement), and other related applications (deployment, feature controllers, etc).\nAuthorization CSM for Authorization provides storage and Kubernetes administrators the ability to apply RBAC for Dell CSI Drivers. It does this by deploying a proxy between the CSI driver and the storage system to enforce role-based access and usage rules.\n…Learn more Supports PowerFlex PowerScale PowerMax Replication CSM for Replication project aims to bring Replication \u0026 Disaster Recovery capabilities of Dell Storage Arrays to Kubernetes clusters. It helps you replicate groups of volumes and can provide you a way to restart applications in case of both planned and unplanned migration. …Learn more Supports PowerFlex PowerStore PowerScale PowerMax Resiliency CSM for Resiliency is designed to make Kubernetes Applications, including those that utilize persistent storage, more resilient to various failures. …Learn more Supports PowerFlex PowerScale Unity Observability CSM for Observability provides visibility on the capacity of the volumes/file shares that is being managed with Dell CSM CSI (Container Storage Interface) drivers along with their performance in terms of bandwidth, IOPS, and response time. …Learn more Supports PowerFlex PowerStore PowerScale PowerMax Application Mobility Container Storage Modules for Application Mobility provide Kubernetes administrators the ability to clone their stateful application workloads and application data to other clusters, either on-premise or in the cloud. …Learn more Supports all platforms Encryption Encryption provides the capability to encrypt user data residing on volumes created by Dell CSI Drivers. …Learn more Supports PowerScale License The tech-preview releases of Application Mobility and Encryption require a license. Request a license using the Container Storage Modules License Request by providing the requested details. …Learn more Required for Application Mobility \u0026 Encryption CSM Modules Support Matrix for Dell CSI Drivers CSM Module CSI PowerFlex v2.6.0 CSI PowerScale v2.6.1 CSI PowerStore v2.6.0 CSI PowerMax v2.6.0 CSI Unity XT v2.6.0 Authorization v1.6.0 ✔️ ✔️ ❌ ✔️ ❌ Observability v1.5.0 ✔️ ✔️ ✔️ ✔️ ❌ Replication v1.4.0 ✔️ ✔️ ✔️ ✔️ ❌ Resiliency v1.5.0 ✔️ ✔️ ✔️ ❌ ✔️ Encryption v0.3.0 ❌ ✔️ ❌ ❌ ❌ Application Mobility v0.3.0 ✔️ ✔️ ✔️ ✔️ ✔️ ","categories":"","description":"","excerpt":" This document version is no longer actively maintained. The site that …","ref":"/csm-docs/v3/","tags":"","title":"Documentation"},{"body":"Roles Role data is stored in the common Config Map.\nSteps to execute in the existing Authorization deployment Save the role data by saving the common configMap to a file. kubectl -n \u003cauthorization-namespace\u003e get configMap common -o yaml \u003e roles.yaml Steps to execute in the Authorization deployment to restore Delete the existing common configMap. kubectl -n \u003cauthorization-namespace\u003e delete configMap common Apply the file containing the backed-up role data. kubectl apply -f roles.yaml Restart the proxy-server deployment. kubectl -n \u003cauthorization-namespace\u003e rollout restart deploy/proxy-server deployment.apps/proxy-server restarted Storage Storage data is stored in the karavi-storage-secret Secret.\nSteps to execute in the existing Authorization deployment Save the storage data by saving the karavi-storage-secret Secret to a file. kubectl -n \u003cauthorization-namespace\u003e get secret karavi-storage-secret -o yaml \u003e storage.yaml Steps to execute in the Authorization deployment to restore Delete the existing karavi-storage-secret secret. kubectl -n \u003cauthorization-namespace\u003e delete secret karavi-storage-secret Apply the file containing the storage data created in step 1. kubectl apply -f storage.yaml Restart the proxy-server deployment. kubectl -n \u003cauthorization-namespace\u003e rollout restart deploy/proxy-server deployment.apps/proxy-server restarted Tenants, Quota, and Volume ownership Redis is used to store application data regarding tenants, quota, and volume ownership with the Storage Class specified in the redis.storageClass parameter in the values file, or with the default Storage Class if that parameter was not specified.\nThe Persistent Volume for Redis is dynamically provisioned by this Storage Class with the redis-primary-pv-claim Persistent Volume Claim. See the example.\nkubectl get persistentvolume NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE k8s-ab74921ab9 8Gi RWO Delete Bound authorization/redis-primary-pv-claim \u003cstorage-class\u003e 112m Steps to execute in the existing Authorization deployment Create a backup of this volume, typically via snapshot and/or replication, and create a Persistent Volume Claim using this backup by following the Storage Class’s provisioner documentation. Steps to execute in the Authorization deployment to restore Edit the redis-primary Deployment to use the Persistent Volume Claim associated with the backup by running: kubectl -n \u003cauthorization-namespace\u003e edit deploy/redis-primary The Deployment has a volumes field that should look like this:\nvolumes: - name: redis-primary-volume persistentVolumeClaim: claimName: redis-primary-pv-claim Replace the value of claimName with the name of the Persisent Volume Claim associated with the backup. If the new Persisent Volume Claim name is redis-backup, you would edit the deployment to look like this:\nvolumes: - name: redis-primary-volume persistentVolumeClaim: claimName: redis-backup Once saved, Redis will now use the backup volume.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization Helm backup and restore\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/authorization/backup-and-restore/helm/","tags":"","title":"Helm"},{"body":" The CSM Authorization karavictl CLI is no longer actively maintained or supported. It will be deprecated in CSM 2.0.\nCSM Authorization can be installed by using the provided Helm v3 charts on Kubernetes platforms.\nThe following CSM Authorization components are installed in the specified namespace:\nproxy-service, which forwards requests from the CSI Driver to the backend storage array tenant-service, which configures tenants, role bindings, and generates JSON Web Tokens role-service, which configures roles for tenants to be bound to storage-service, which configures backend storage arrays for the proxy-server to foward requests to The following third-party components are installed in the specified namespace:\nredis, which stores data regarding tenants and their volume ownership, quota, and revokation status redis-commander, a web management tool for Redis The following third-party components are optionally installed in the specified namespace:\ncert-manager, which optionally provides a self-signed certificate to configure the CSM Authorization Ingresses nginx-ingress-controller, which fulfills the CSM Authorization Ingresses Install CSM Authorization Steps\nRun git clone https://github.com/dell/helm-charts.git to clone the git repository.\nEnsure that you have created a namespace where you want to install CSM Authorization. You can run kubectl create namespace authorization to create a new one.\nPrepare samples/csm-authorization/config.yaml which contains the JWT signing secret. The following table lists the configuration parameters.\nParameter Description Required Default web.jwtsigningsecret String used to sign JSON Web Tokens true secret Example:\nweb: jwtsigningsecret: randomString123 After editing the file, run the following command to create a secret called karavi-config-secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/csm-authorization/config.yaml Use the following command to replace or update the secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/csm-authorization/config.yaml -o yaml --dry-run=client | kubectl replace -f - Copy the default values.yaml file cp charts/csm-authorization/values.yaml myvalues.yaml\nLook over all the fields in myvalues.yaml and fill in/adjust any as needed.\nParameter Description Required Default ingress-nginx This section configures the enablement of the NGINX Ingress Controller. - - enabled Enable/Disable deployment of the NGINX Ingress Controller. Set to false if you already have an Ingress Controller installed. No true cert-manager This section configures the enablement of cert-manager. - - enabled Enable/Disable deployment of cert-manager. Set to false if you already have cert-manager installed. No true authorization This section configures the CSM-Authorization components. - - authorization.images.proxyService The image to use for the proxy-service. Yes dellemc/csm-authorization-proxy:nightly authorization.images.tenantService The image to use for the tenant-service. Yes dellemc/csm-authorization-tenant:nightly authorization.images.roleService The image to use for the role-service. Yes dellemc/csm-authorization-proxy:nightly authorization.images.storageService The image to use for the storage-service. Yes dellemc/csm-authorization-storage:nightly authorization.images.opa The image to use for Open Policy Agent. Yes openpolicyagent/opa authorization.images.opaKubeMgmt The image to use for Open Policy Agent kube-mgmt. Yes openpolicyagent/kube-mgmt:0.11 authorization.hostname The hostname to configure the self-signed certificate (if applicable) and the proxy Ingress. Yes csm-authorization.com authorization.logLevel CSM Authorization log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes debug authorization.zipkin.collectoruri The URI of the Zipkin instance to export traces. No - authorization.zipkin.probability The ratio of traces to export. No - authorization.proxyServerIngress.ingressClassName The ingressClassName of the proxy-service Ingress. Yes - authorization.proxyServerIngress.hosts Additional host rules to be applied to the proxy-service Ingress. No - authorization.proxyServerIngress.annotations Additional annotations for the proxy-service Ingress. No - authorization.roleServiceIngress.ingressClassName The ingressClassName of the role-service Ingress. Yes - authorization.roleServiceIngress.hosts Additional host rules to be applied to the role-service Ingress. No - authorization.roleServiceIngress.annotations Additional annotations for the role-service Ingress. No - redis This section configures Redis. - - redis.images.redis The image to use for Redis. Yes redis:6.0.8-alpine redis.images.commander The image to use for Redis Commander. Yes rediscommander/redis-commander:latest redis.storageClass The storage class for Redis to use for persistence. If not supplied, the default storage class is used. No - Install the driver using helm: To install CSM Authorization with the service Ingresses using your own certificate, run:\nhelm -n authorization install authorization -f myvalues.yaml charts/csm-authorization \\ --set-file authorization.certificate=\u003clocation-of-certificate-file\u003e \\ --set-file authorization.privateKey=\u003clocation-of-private-key-file\u003e To install CSM Authorization with the service Ingresses using a self-signed certificate generated via cert-manager, run:\nhelm -n authorization install authorization -f myvalues.yaml charts/csm-authorization Install Karavictl Download the latest release of karavictl curl -LO https://github.com/dell/karavi-authorization/releases/latest/download/karavictl Install karavictl sudo install -o root -g root -m 0755 karavictl /usr/local/bin/karavictl If you do not have root access on the target system, you can still install karavictl to the ~/.local/bin directory:\nchmod +x karavictl mkdir -p ~/.local/bin mv ./karavictl ~/.local/bin/karavictl # and then append (or prepend) ~/.local/bin to $PATH Karavictl commands and intended use can be found here.\nConfiguring the CSM Authorization Proxy Server The first part of CSM for Authorization deployment is to configure the proxy server. This is controlled by the Storage Administrator.\nConfiguration is achieved by using karavictl to connect to the proxy service. In this example, we will be referencing an installation using csm-authorization.com as the authorization.hostname value and the NGINX Ingress Controller accessed via the cluster’s master node.\nRun kubectl -n authorization get ingress and kubectl -n authorization get service to see the Ingress rules for these services and the exposed port for accessing these services via the LoadBalancer. For example:\nkubectl -n authorization get ingress NAME CLASS HOSTS ADDRESS PORTS AGE proxy-server nginx csm-authorization.com 00, 000 86s kubectl -n auth get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE authorization-cert-manager ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 28s authorization-cert-manager-webhook ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s authorization-ingress-nginx-controller LoadBalancer 00.000.000.000 \u003cpending\u003e 00:00000/TCP,000:00000/TCP 27s authorization-ingress-nginx-controller-admission ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s proxy-server ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 28s redis ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 28s redis-commander ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s role-service ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s storage-service ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s tenant-service ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 28s On the machine running karavictl, the /etc/hosts file needs to be updated with the Ingress hosts for the proxy, storage, and role services. For example:\n\u003cmaster_node_ip\u003e csm-authorization.com Please continue following the steps outlined in the proxy server configuration.\nConfiguring a Dell CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the supported CSI drivers. This is controlled by the Kubernetes tenant admin.\nPlease follow the steps outlined in PowerFlex, PowerMax, or PowerScale to configure the CSI Driver to work with the Authorization sidecar.\nUpdating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically:\nParameter Type Default Description web.jwtsigningsecret String “secret” The secret used to sign JWT tokens Updating configuration parameters can be done by editing the karavi-config-secret. The secret can be queried using k3s and kubectl like so:\nkubectl -n authorization get secret/karavi-config-secret To update parameters, you must edit the base64 encoded data in the secret. The karavi-config-secret data can be decoded like so:\nkubectl -n authorization get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d Save the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so:\ncat \u003cfile\u003e | base64 Copy the new, encoded data and edit the karavi-config-secret with the new data. Run this command to edit the secret:\nkubectl -n karavi edit secret/karavi-config-secret Replace the data in config.yaml under the data field with your new, encoded data. Save the changes and CSM Authorization will read the changed secret.\nNote: If you are updating the signing secret, the tenants need to be updated with new tokens via the karavictl generate token command.\nCSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the karavi-config-secret but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM Authorization logging settings during runtime, run the below command, make your changes, and save the updated configMap data.\nkubectl -n authorization edit configmap/csm-config-params This edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion:\nkubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params Using PowerFlex as an example, kubectl -n vxflexos edit configmap/vxflexos-config-params can be used to update the logging level of the sidecar-proxy and the driver.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization Helm deployment\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/authorization/deployment/helm/","tags":"","title":"Helm"},{"body":"Roles Role data is stored in the common Config Map.\nSteps to execute in the existing Authorization deployment Save the role data by saving the common configMap to a file. kubectl -n \u003cauthorization-namespace\u003e get configMap common -o yaml \u003e roles.yaml Steps to execute in the Authorization deployment to restore Delete the existing common configMap. kubectl -n \u003cauthorization-namespace\u003e delete configMap common Apply the file containing the backed-up role data. kubectl apply -f roles.yaml Restart the proxy-server deployment. kubectl -n \u003cauthorization-namespace\u003e rollout restart deploy/proxy-server deployment.apps/proxy-server restarted Storage Storage data is stored in the karavi-storage-secret Secret.\nSteps to execute in the existing Authorization deployment Save the storage data by saving the karavi-storage-secret Secret to a file. kubectl -n \u003cauthorization-namespace\u003e get secret karavi-storage-secret -o yaml \u003e storage.yaml Steps to execute in the Authorization deployment to restore Delete the existing karavi-storage-secret secret. kubectl -n \u003cauthorization-namespace\u003e delete secret karavi-storage-secret Apply the file containing the storage data created in step 1. kubectl apply -f storage.yaml Restart the proxy-server deployment. kubectl -n \u003cauthorization-namespace\u003e rollout restart deploy/proxy-server deployment.apps/proxy-server restarted Tenants, Quota, and Volume ownership Redis is used to store application data regarding tenants, quota, and volume ownership with the Storage Class specified in the redis.storageClass parameter in the values file, or with the default Storage Class if that parameter was not specified.\nThe Persistent Volume for Redis is dynamically provisioned by this Storage Class with the redis-primary-pv-claim Persistent Volume Claim. See the example.\nkubectl get persistentvolume NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE k8s-ab74921ab9 8Gi RWO Delete Bound authorization/redis-primary-pv-claim \u003cstorage-class\u003e 112m Steps to execute in the existing Authorization deployment Create a backup of this volume, typically via snapshot and/or replication, and create a Persistent Volume Claim using this backup by following the Storage Class’s provisioner documentation. Steps to execute in the Authorization deployment to restore Edit the redis-primary Deployment to use the Persistent Volume Claim associated with the backup by running: kubectl -n \u003cauthorization-namespace\u003e edit deploy/redis-primary The Deployment has a volumes field that should look like this:\nvolumes: - name: redis-primary-volume persistentVolumeClaim: claimName: redis-primary-pv-claim Replace the value of claimName with the name of the Persisent Volume Claim associated with the backup. If the new Persisent Volume Claim name is redis-backup, you would edit the deployment to look like this:\nvolumes: - name: redis-primary-volume persistentVolumeClaim: claimName: redis-backup Once saved, Redis will now use the backup volume.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization Helm backup and restore\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v1/authorization/backup-and-restore/helm/","tags":"","title":"Helm"},{"body":" The CSM Authorization karavictl CLI is no longer actively maintained or supported. It will be deprecated in CSM 2.0.\nCSM Authorization can be installed by using the provided Helm v3 charts on Kubernetes platforms.\nThe following CSM Authorization components are installed in the specified namespace:\nproxy-service, which forwards requests from the CSI Driver to the backend storage array tenant-service, which configures tenants, role bindings, and generates JSON Web Tokens role-service, which configures roles for tenants to be bound to storage-service, which configures backend storage arrays for the proxy-server to foward requests to The following third-party components are installed in the specified namespace:\nredis, which stores data regarding tenants and their volume ownership, quota, and revokation status redis-commander, a web management tool for Redis The following third-party components are optionally installed in the specified namespace:\ncert-manager, which optionally provides a self-signed certificate to configure the CSM Authorization Ingresses nginx-ingress-controller, which fulfills the CSM Authorization Ingresses Install CSM Authorization Steps\nRun git clone https://github.com/dell/helm-charts.git to clone the git repository.\nEnsure that you have created a namespace where you want to install CSM Authorization. You can run kubectl create namespace authorization to create a new one.\nPrepare samples/csm-authorization/config.yaml which contains the JWT signing secret. The following table lists the configuration parameters.\nParameter Description Required Default web.jwtsigningsecret String used to sign JSON Web Tokens true secret Example:\nweb: jwtsigningsecret: randomString123 After editing the file, run the following command to create a secret called karavi-config-secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/csm-authorization/config.yaml Use the following command to replace or update the secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/csm-authorization/config.yaml -o yaml --dry-run=client | kubectl replace -f - Copy the default values.yaml file cp charts/csm-authorization/values.yaml myvalues.yaml\nLook over all the fields in myvalues.yaml and fill in/adjust any as needed.\nParameter Description Required Default ingress-nginx This section configures the enablement of the NGINX Ingress Controller. - - enabled Enable/Disable deployment of the NGINX Ingress Controller. Set to false if you already have an Ingress Controller installed. No true cert-manager This section configures the enablement of cert-manager. - - enabled Enable/Disable deployment of cert-manager. Set to false if you already have cert-manager installed. No true authorization This section configures the CSM-Authorization components. - - authorization.images.proxyService The image to use for the proxy-service. Yes dellemc/csm-authorization-proxy:nightly authorization.images.tenantService The image to use for the tenant-service. Yes dellemc/csm-authorization-tenant:nightly authorization.images.roleService The image to use for the role-service. Yes dellemc/csm-authorization-proxy:nightly authorization.images.storageService The image to use for the storage-service. Yes dellemc/csm-authorization-storage:nightly authorization.images.opa The image to use for Open Policy Agent. Yes openpolicyagent/opa authorization.images.opaKubeMgmt The image to use for Open Policy Agent kube-mgmt. Yes openpolicyagent/kube-mgmt:0.11 authorization.hostname The hostname to configure the self-signed certificate (if applicable) and the proxy Ingress. Yes csm-authorization.com authorization.logLevel CSM Authorization log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes debug authorization.zipkin.collectoruri The URI of the Zipkin instance to export traces. No - authorization.zipkin.probability The ratio of traces to export. No - authorization.proxyServerIngress.ingressClassName The ingressClassName of the proxy-service Ingress. Yes - authorization.proxyServerIngress.hosts Additional host rules to be applied to the proxy-service Ingress. No - authorization.proxyServerIngress.annotations Additional annotations for the proxy-service Ingress. No - authorization.roleServiceIngress.ingressClassName The ingressClassName of the role-service Ingress. Yes - authorization.roleServiceIngress.hosts Additional host rules to be applied to the role-service Ingress. No - authorization.roleServiceIngress.annotations Additional annotations for the role-service Ingress. No - redis This section configures Redis. - - redis.images.redis The image to use for Redis. Yes redis:6.0.8-alpine redis.images.commander The image to use for Redis Commander. Yes rediscommander/redis-commander:latest redis.storageClass The storage class for Redis to use for persistence. If not supplied, the default storage class is used. No - Install the driver using helm: To install CSM Authorization with the service Ingresses using your own certificate, run:\nhelm -n authorization install authorization -f myvalues.yaml charts/csm-authorization \\ --set-file authorization.certificate=\u003clocation-of-certificate-file\u003e \\ --set-file authorization.privateKey=\u003clocation-of-private-key-file\u003e To install CSM Authorization with the service Ingresses using a self-signed certificate generated via cert-manager, run:\nhelm -n authorization install authorization -f myvalues.yaml charts/csm-authorization Install Karavictl Download the latest release of karavictl curl -LO https://github.com/dell/karavi-authorization/releases/latest/download/karavictl Install karavictl sudo install -o root -g root -m 0755 karavictl /usr/local/bin/karavictl If you do not have root access on the target system, you can still install karavictl to the ~/.local/bin directory:\nchmod +x karavictl mkdir -p ~/.local/bin mv ./karavictl ~/.local/bin/karavictl # and then append (or prepend) ~/.local/bin to $PATH Karavictl commands and intended use can be found here.\nConfiguring the CSM Authorization Proxy Server The first part of CSM for Authorization deployment is to configure the proxy server. This is controlled by the Storage Administrator.\nConfiguration is achieved by using karavictl to connect to the proxy service. In this example, we will be referencing an installation using csm-authorization.com as the authorization.hostname value and the NGINX Ingress Controller accessed via the cluster’s master node.\nRun kubectl -n authorization get ingress and kubectl -n authorization get service to see the Ingress rules for these services and the exposed port for accessing these services via the LoadBalancer. For example:\nkubectl -n authorization get ingress NAME CLASS HOSTS ADDRESS PORTS AGE proxy-server nginx csm-authorization.com 00, 000 86s kubectl -n auth get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE authorization-cert-manager ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 28s authorization-cert-manager-webhook ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s authorization-ingress-nginx-controller LoadBalancer 00.000.000.000 \u003cpending\u003e 00:00000/TCP,000:00000/TCP 27s authorization-ingress-nginx-controller-admission ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s proxy-server ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 28s redis ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 28s redis-commander ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s role-service ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s storage-service ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s tenant-service ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 28s On the machine running karavictl, the /etc/hosts file needs to be updated with the Ingress hosts for the proxy, storage, and role services. For example:\n\u003cmaster_node_ip\u003e csm-authorization.com Please continue following the steps outlined in the proxy server configuration.\nConfiguring a Dell CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the supported CSI drivers. This is controlled by the Kubernetes tenant admin.\nPlease follow the steps outlined in PowerFlex, PowerMax, or PowerScale to configure the CSI Driver to work with the Authorization sidecar.\nUpdating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically:\nParameter Type Default Description web.jwtsigningsecret String “secret” The secret used to sign JWT tokens Updating configuration parameters can be done by editing the karavi-config-secret. The secret can be queried using k3s and kubectl like so:\nkubectl -n authorization get secret/karavi-config-secret To update parameters, you must edit the base64 encoded data in the secret. The karavi-config-secret data can be decoded like so:\nkubectl -n authorization get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d Save the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so:\ncat \u003cfile\u003e | base64 Copy the new, encoded data and edit the karavi-config-secret with the new data. Run this command to edit the secret:\nkubectl -n karavi edit secret/karavi-config-secret Replace the data in config.yaml under the data field with your new, encoded data. Save the changes and CSM Authorization will read the changed secret.\nNote: If you are updating the signing secret, the tenants need to be updated with new tokens via the karavictl generate token command.\nCSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the karavi-config-secret but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM Authorization logging settings during runtime, run the below command, make your changes, and save the updated configMap data.\nkubectl -n authorization edit configmap/csm-config-params This edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion:\nkubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params Using PowerFlex as an example, kubectl -n vxflexos edit configmap/vxflexos-config-params can be used to update the logging level of the sidecar-proxy and the driver.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization Helm deployment\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v1/authorization/deployment/helm/","tags":"","title":"Helm"},{"body":"Roles Role data is stored in the common Config Map.\nSteps to execute in the existing Authorization deployment Save the role data by saving the common configMap to a file. kubectl -n \u003cauthorization-namespace\u003e get configMap common -o yaml \u003e roles.yaml Steps to execute in the Authorization deployment to restore Delete the existing common configMap. kubectl -n \u003cauthorization-namespace\u003e delete configMap common Apply the file containing the backed-up role data. kubectl apply -f roles.yaml Restart the proxy-server deployment. kubectl -n \u003cauthorization-namespace\u003e rollout restart deploy/proxy-server deployment.apps/proxy-server restarted Storage Storage data is stored in the karavi-storage-secret Secret.\nSteps to execute in the existing Authorization deployment Save the storage data by saving the karavi-storage-secret Secret to a file. kubectl -n \u003cauthorization-namespace\u003e get secret karavi-storage-secret -o yaml \u003e storage.yaml Steps to execute in the Authorization deployment to restore Delete the existing karavi-storage-secret secret. kubectl -n \u003cauthorization-namespace\u003e delete secret karavi-storage-secret Apply the file containing the storage data created in step 1. kubectl apply -f storage.yaml Restart the proxy-server deployment. kubectl -n \u003cauthorization-namespace\u003e rollout restart deploy/proxy-server deployment.apps/proxy-server restarted Tenants, Quota, and Volume ownership Redis is used to store application data regarding tenants, quota, and volume ownership with the Storage Class specified in the redis.storageClass parameter in the values file, or with the default Storage Class if that parameter was not specified.\nThe Persistent Volume for Redis is dynamically provisioned by this Storage Class with the redis-primary-pv-claim Persistent Volume Claim. See the example.\nkubectl get persistentvolume NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE k8s-ab74921ab9 8Gi RWO Delete Bound authorization/redis-primary-pv-claim \u003cstorage-class\u003e 112m Steps to execute in the existing Authorization deployment Create a backup of this volume, typically via snapshot and/or replication, and create a Persistent Volume Claim using this backup by following the Storage Class’s provisioner documentation. Steps to execute in the Authorization deployment to restore Edit the redis-primary Deployment to use the Persistent Volume Claim associated with the backup by running: kubectl -n \u003cauthorization-namespace\u003e edit deploy/redis-primary The Deployment has a volumes field that should look like this:\nvolumes: - name: redis-primary-volume persistentVolumeClaim: claimName: redis-primary-pv-claim Replace the value of claimName with the name of the Persisent Volume Claim associated with the backup. If the new Persisent Volume Claim name is redis-backup, you would edit the deployment to look like this:\nvolumes: - name: redis-primary-volume persistentVolumeClaim: claimName: redis-backup Once saved, Redis will now use the backup volume.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization Helm backup and restore\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v2/authorization/backup-and-restore/helm/","tags":"","title":"Helm"},{"body":"CSM Authorization can be installed by using the provided Helm v3 charts on Kubernetes platforms.\nThe following CSM Authorization components are installed in the specified namespace:\nproxy-service, which forwards requests from the CSI Driver to the backend storage array tenant-service, which configures tenants, role bindings, and generates JSON Web Tokens role-service, which configures roles for tenants to be bound to storage-service, which configures backend storage arrays for the proxy-server to foward requests to The following third-party components are installed in the specified namespace:\nredis, which stores data regarding tenants and their volume ownership, quota, and revokation status redis-commander, a web management tool for Redis The following third-party components are optionally installed in the specified namespace:\ncert-manager, which optionally provides a self-signed certificate to configure the CSM Authorization Ingresses nginx-ingress-controller, which fulfills the CSM Authorization Ingresses Install CSM Authorization Steps\nRun git clone https://github.com/dell/helm-charts.git to clone the git repository.\nEnsure that you have created a namespace where you want to install CSM Authorization. You can run kubectl create namespace authorization to create a new one.\nPrepare samples/csm-authorization/config.yaml which contains the JWT signing secret. The following table lists the configuration parameters.\nParameter Description Required Default web.jwtsigningsecret String used to sign JSON Web Tokens true secret Example:\nweb: jwtsigningsecret: randomString123 After editing the file, run the following command to create a secret called karavi-config-secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/csm-authorization/config.yaml Use the following command to replace or update the secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/csm-authorization/config.yaml -o yaml --dry-run=client | kubectl replace -f - Copy the default values.yaml file cp charts/csm-authorization/values.yaml myvalues.yaml\nLook over all the fields in myvalues.yaml and fill in/adjust any as needed.\nParameter Description Required Default ingress-nginx This section configures the enablement of the NGINX Ingress Controller. - - enabled Enable/Disable deployment of the NGINX Ingress Controller. Set to false if you already have an Ingress Controller installed. No true cert-manager This section configures the enablement of cert-manager. - - enabled Enable/Disable deployment of cert-manager. Set to false if you already have cert-manager installed. No true authorization This section configures the CSM-Authorization components. - - authorization.images.proxyService The image to use for the proxy-service. Yes dellemc/csm-authorization-proxy:nightly authorization.images.tenantService The image to use for the tenant-service. Yes dellemc/csm-authorization-tenant:nightly authorization.images.roleService The image to use for the role-service. Yes dellemc/csm-authorization-proxy:nightly authorization.images.storageService The image to use for the storage-service. Yes dellemc/csm-authorization-storage:nightly authorization.images.opa The image to use for Open Policy Agent. Yes openpolicyagent/opa authorization.images.opaKubeMgmt The image to use for Open Policy Agent kube-mgmt. Yes openpolicyagent/kube-mgmt:0.11 authorization.hostname The hostname to configure the self-signed certificate (if applicable) and the proxy Ingress. Yes csm-authorization.com authorization.logLevel CSM Authorization log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes debug authorization.zipkin.collectoruri The URI of the Zipkin instance to export traces. No - authorization.zipkin.probability The ratio of traces to export. No - authorization.proxyServerIngress.ingressClassName The ingressClassName of the proxy-service Ingress. Yes - authorization.proxyServerIngress.hosts Additional host rules to be applied to the proxy-service Ingress. No - authorization.proxyServerIngress.annotations Additional annotations for the proxy-service Ingress. No - authorization.roleServiceIngress.ingressClassName The ingressClassName of the role-service Ingress. Yes - authorization.roleServiceIngress.hosts Additional host rules to be applied to the role-service Ingress. No - authorization.roleServiceIngress.annotations Additional annotations for the role-service Ingress. No - redis This section configures Redis. - - redis.images.redis The image to use for Redis. Yes redis:6.0.8-alpine redis.images.commander The image to use for Redis Commander. Yes rediscommander/redis-commander:latest redis.storageClass The storage class for Redis to use for persistence. If not supplied, the default storage class is used. No - Install the driver using helm: To install CSM Authorization with the service Ingresses using your own certificate, run:\nhelm -n authorization install authorization -f myvalues.yaml charts/csm-authorization \\ --set-file authorization.certificate=\u003clocation-of-certificate-file\u003e \\ --set-file authorization.privateKey=\u003clocation-of-private-key-file\u003e To install CSM Authorization with the service Ingresses using a self-signed certificate generated via cert-manager, run:\nhelm -n authorization install authorization -f myvalues.yaml charts/csm-authorization Install Karavictl Download the latest release of karavictl curl -LO https://github.com/dell/karavi-authorization/releases/latest/download/karavictl Install karavictl sudo install -o root -g root -m 0755 karavictl /usr/local/bin/karavictl If you do not have root access on the target system, you can still install karavictl to the ~/.local/bin directory:\nchmod +x karavictl mkdir -p ~/.local/bin mv ./karavictl ~/.local/bin/karavictl # and then append (or prepend) ~/.local/bin to $PATH Karavictl commands and intended use can be found here.\nConfiguring the CSM Authorization Proxy Server The first part of CSM for Authorization deployment is to configure the proxy server. This is controlled by the Storage Administrator.\nConfiguration is achieved by using karavictl to connect to the proxy service. In this example, we will be referencing an installation using csm-authorization.com as the authorization.hostname value and the NGINX Ingress Controller accessed via the cluster’s master node.\nRun kubectl -n authorization get ingress and kubectl -n authorization get service to see the Ingress rules for these services and the exposed port for accessing these services via the LoadBalancer. For example:\nkubectl -n authorization get ingress NAME CLASS HOSTS ADDRESS PORTS AGE proxy-server nginx csm-authorization.com 00, 000 86s kubectl -n auth get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE authorization-cert-manager ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 28s authorization-cert-manager-webhook ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s authorization-ingress-nginx-controller LoadBalancer 00.000.000.000 \u003cpending\u003e 00:00000/TCP,000:00000/TCP 27s authorization-ingress-nginx-controller-admission ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s proxy-server ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 28s redis ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 28s redis-commander ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s role-service ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s storage-service ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s tenant-service ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 28s On the machine running karavictl, the /etc/hosts file needs to be updated with the Ingress hosts for the proxy, storage, and role services. For example:\n\u003cmaster_node_ip\u003e csm-authorization.com Please continue following the steps outlined in the proxy server configuration.\nConfiguring a Dell CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the supported CSI drivers. This is controlled by the Kubernetes tenant admin.\nPlease follow the steps outlined in PowerFlex, PowerMax, or PowerScale to configure the CSI Driver to work with the Authorization sidecar.\nUpdating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically:\nParameter Type Default Description web.jwtsigningsecret String “secret” The secret used to sign JWT tokens Updating configuration parameters can be done by editing the karavi-config-secret. The secret can be queried using k3s and kubectl like so:\nkubectl -n authorization get secret/karavi-config-secret To update parameters, you must edit the base64 encoded data in the secret. The karavi-config-secret data can be decoded like so:\nkubectl -n authorization get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d Save the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so:\ncat \u003cfile\u003e | base64 Copy the new, encoded data and edit the karavi-config-secret with the new data. Run this command to edit the secret:\nkubectl -n karavi edit secret/karavi-config-secret Replace the data in config.yaml under the data field with your new, encoded data. Save the changes and CSM Authorization will read the changed secret.\nNote: If you are updating the signing secret, the tenants need to be updated with new tokens via the karavictl generate token command.\nCSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the karavi-config-secret but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM Authorization logging settings during runtime, run the below command, make your changes, and save the updated configMap data.\nkubectl -n authorization edit configmap/csm-config-params This edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion:\nkubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params Using PowerFlex as an example, kubectl -n vxflexos edit configmap/vxflexos-config-params can be used to update the logging level of the sidecar-proxy and the driver.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization Helm deployment\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v2/authorization/deployment/helm/","tags":"","title":"Helm"},{"body":"Roles Role data is stored in the common Config Map.\nSteps to execute in the existing Authorization deployment Save the role data by saving the common configMap to a file. kubectl -n \u003cauthorization-namespace\u003e get configMap common -o yaml \u003e roles.yaml Steps to execute in the Authorization deployment to restore Delete the existing common configMap. kubectl -n \u003cauthorization-namespace\u003e delete configMap common Apply the file containing the backed-up role data. kubectl apply -f roles.yaml Restart the proxy-server deployment. kubectl -n \u003cauthorization-namespace\u003e rollout restart deploy/proxy-server deployment.apps/proxy-server restarted Storage Storage data is stored in the karavi-storage-secret Secret.\nSteps to execute in the existing Authorization deployment Save the storage data by saving the karavi-storage-secret Secret to a file. kubectl -n \u003cauthorization-namespace\u003e get secret karavi-storage-secret -o yaml \u003e storage.yaml Steps to execute in the Authorization deployment to restore Delete the existing karavi-storage-secret secret. kubectl -n \u003cauthorization-namespace\u003e delete secret karavi-storage-secret Apply the file containing the storage data created in step 1. kubectl apply -f storage.yaml Restart the proxy-server deployment. kubectl -n \u003cauthorization-namespace\u003e rollout restart deploy/proxy-server deployment.apps/proxy-server restarted Tenants, Quota, and Volume ownership Redis is used to store application data regarding tenants, quota, and volume ownership with the Storage Class specified in the redis.storageClass parameter in the values file, or with the default Storage Class if that parameter was not specified.\nThe Persistent Volume for Redis is dynamically provisioned by this Storage Class with the redis-primary-pv-claim Persistent Volume Claim. See the example.\nkubectl get persistentvolume NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE k8s-ab74921ab9 8Gi RWO Delete Bound authorization/redis-primary-pv-claim \u003cstorage-class\u003e 112m Steps to execute in the existing Authorization deployment Create a backup of this volume, typically via snapshot and/or replication, and create a Persistent Volume Claim using this backup by following the Storage Class’s provisioner documentation. Steps to execute in the Authorization deployment to restore Edit the redis-primary Deployment to use the Persistent Volume Claim associated with the backup by running: kubectl -n \u003cauthorization-namespace\u003e edit deploy/redis-primary\nThe Deployment has a volumes field that should look like this:\nvolumes: - name: redis-primary-volume persistentVolumeClaim: claimName: redis-primary-pv-claim Replace the value of claimName with the name of the Persisent Volume Claim associated with the backup. If the new Persisent Volume Claim name is redis-backup, you would edit the deployment to look like this:\nvolumes: - name: redis-primary-volume persistentVolumeClaim: claimName: redis-backup Once saved, Redis will now use the backup volume.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization Helm backup and restore\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v3/authorization/backup-and-restore/helm/","tags":"","title":"Helm"},{"body":" The CSM Authorization karavictl CLI is no longer actively maintained or supported. It will be deprecated in CSM 2.0.\nCSM Authorization can be installed by using the provided Helm v3 charts on Kubernetes platforms.\nThe following CSM Authorization components are installed in the specified namespace:\nproxy-service, which forwards requests from the CSI Driver to the backend storage array tenant-service, which configures tenants, role bindings, and generates JSON Web Tokens role-service, which configures roles for tenants to be bound to storage-service, which configures backend storage arrays for the proxy-server to foward requests to The following third-party components are installed in the specified namespace:\nredis, which stores data regarding tenants and their volume ownership, quota, and revokation status redis-commander, a web management tool for Redis The following third-party components are optionally installed in the specified namespace:\ncert-manager, which optionally provides a self-signed certificate to configure the CSM Authorization Ingresses nginx-ingress-controller, which fulfills the CSM Authorization Ingresses Install CSM Authorization Steps\nRun git clone https://github.com/dell/helm-charts.git to clone the git repository.\nEnsure that you have created a namespace where you want to install CSM Authorization. You can run kubectl create namespace authorization to create a new one.\nPrepare samples/csm-authorization/config.yaml which contains the JWT signing secret. The following table lists the configuration parameters.\nParameter Description Required Default web.jwtsigningsecret String used to sign JSON Web Tokens true secret Example:\nweb: jwtsigningsecret: randomString123 After editing the file, run the following command to create a secret called karavi-config-secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/csm-authorization/config.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/csm-authorization/config.yaml -o yaml --dry-run=client | kubectl replace -f -\nCopy the default values.yaml file cp charts/csm-authorization/values.yaml myvalues.yaml\nLook over all the fields in myvalues.yaml and fill in/adjust any as needed.\nParameter Description Required Default ingress-nginx This section configures the enablement of the NGINX Ingress Controller. - - enabled Enable/Disable deployment of the NGINX Ingress Controller. Set to false if you already have an Ingress Controller installed. No true cert-manager This section configures the enablement of cert-manager. - - enabled Enable/Disable deployment of cert-manager. Set to false if you already have cert-manager installed. No true authorization This section configures the CSM-Authorization components. - - authorization.images.proxyService The image to use for the proxy-service. Yes dellemc/csm-authorization-proxy:nightly authorization.images.tenantService The image to use for the tenant-service. Yes dellemc/csm-authorization-tenant:nightly authorization.images.roleService The image to use for the role-service. Yes dellemc/csm-authorization-proxy:nightly authorization.images.storageService The image to use for the storage-service. Yes dellemc/csm-authorization-storage:nightly authorization.images.opa The image to use for Open Policy Agent. Yes openpolicyagent/opa authorization.images.opaKubeMgmt The image to use for Open Policy Agent kube-mgmt. Yes openpolicyagent/kube-mgmt:0.11 authorization.hostname The hostname to configure the self-signed certificate (if applicable) and the proxy, tenant, role, and storage service Ingresses. Yes csm-authorization.com authorization.logLevel CSM Authorization log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes debug authorization.zipkin.collectoruri The URI of the Zipkin instance to export traces. No - authorization.zipkin.probability The ratio of traces to export. No - authorization.proxyServerIngress.ingressClassName The ingressClassName of the proxy-service Ingress. Yes - authorization.proxyServerIngress.hosts Additional host rules to be applied to the proxy-service Ingress. No - authorization.proxyServerIngress.annotations Additional annotations for the proxy-service Ingress. No - authorization.tenantServiceIngress.ingressClassName The ingressClassName of the tenant-service Ingress. Yes - authorization.tenantServiceIngress.hosts Additional host rules to be applied to the tenant-service Ingress. No - authorization.tenantServiceIngress.annotations Additional annotations for the tenant-service Ingress. No - authorization.roleServiceIngress.ingressClassName The ingressClassName of the role-service Ingress. Yes - authorization.roleServiceIngress.hosts Additional host rules to be applied to the role-service Ingress. No - authorization.roleServiceIngress.annotations Additional annotations for the role-service Ingress. No - authorization.storageServiceIngress.ingressClassName The ingressClassName of the storage-service Ingress. Yes - authorization.storageServiceIngress.hosts Additional host rules to be applied to the storage-service Ingress. No - authorization.storageServiceIngress.annotations Additional annotations for the storage-service Ingress. No - redis This section configures Redis. - - redis.images.redis The image to use for Redis. Yes redis:6.0.8-alpine redis.images.commander The image to use for Redis Commander. Yes rediscommander/redis-commander:latest redis.storageClass The storage class for Redis to use for persistence. If not supplied, the default storage class is used. No - Note:\nThe tenant, role, and storage services use GRPC. If the Ingress Controller requires annotations to support GRPC, they must be supplied. Install the driver using helm: To install CSM Authorization with the service Ingresses using your own certificate, run:\nhelm -n authorization install authorization -f myvalues.yaml charts/csm-authorization \\ --set-file authorization.certificate=\u003clocation-of-certificate-file\u003e \\ --set-file authorization.privateKey=\u003clocation-of-private-key-file\u003e To install CSM Authorization with the service Ingresses using a self-signed certificate generated via cert-manager, run:\nhelm -n authorization install authorization -f myvalues.yaml charts/csm-authorization Install Karavictl Download the latest release of karavictl curl -LO https://github.com/dell/karavi-authorization/releases/latest/download/karavictl Install karavictl sudo install -o root -g root -m 0755 karavictl /usr/local/bin/karavictl If you do not have root access on the target system, you can still install karavictl to the ~/.local/bin directory:\nchmod +x karavictl mkdir -p ~/.local/bin mv ./karavictl ~/.local/bin/karavictl # and then append (or prepend) ~/.local/bin to $PATH Karavictl commands and intended use can be found here.\nConfiguring the CSM Authorization Proxy Server The first part of CSM for Authorization deployment is to configure the proxy server. This is controlled by the Storage Administrator.\nConfiguration is achieved by using karavictl to connect to the storage, tenant, and role services. In this example, we will be referencing an installation using csm-authorization.com as the authorization.hostname value and the NGINX Ingress Controller accessed via the cluster’s master node.\nRun kubectl -n authorization get ingress and kubectl -n authorization get service to see the Ingress rules for these services and the exposed port for accessing these services via the LoadBalancer. For example:\n# kubectl -n authorization get ingress NAME CLASS HOSTS ADDRESS PORTS AGE proxy-server nginx csm-authorization.com 00, 000 86s role-service nginx role.csm-authorization.com 00, 000 86s storage-service nginx storage.csm-authorization.com 00, 000 86s tenant-service nginx tenant.csm-authorization.com 00, 000 86s # kubectl -n auth get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE authorization-cert-manager ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 28s authorization-cert-manager-webhook ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s authorization-ingress-nginx-controller LoadBalancer 00.000.000.000 \u003cpending\u003e 00:00000/TCP,000:00000/TCP 27s authorization-ingress-nginx-controller-admission ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s proxy-server ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 28s redis ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 28s redis-commander ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s role-service ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s storage-service ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 27s tenant-service ClusterIP 00.000.000.000 \u003cnone\u003e 000/TCP 28s On the machine running karavictl, the /etc/hosts file needs to be updated with the Ingress hosts for the storage, tenant, and role services. For example:\n\u003cmaster_node_ip\u003e tenant.csm-authorization.com \u003cmaster_node_ip\u003e role.csm-authorization.com \u003cmaster_node_ip\u003e storage.csm-authorization.com The port that exposes these services is 30016.\nPlease continue following the steps outlined in the proxy server configuration.\nConfiguring a Dell CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the supported CSI drivers. This is controlled by the Kubernetes tenant admin.\nPlease follow the steps outlined in PowerFlex, PowerMax, or PowerScale to configure the CSI Driver to work with the Authorization sidecar.\nUpdating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically:\nParameter Type Default Description web.jwtsigningsecret String “secret” The secret used to sign JWT tokens Updating configuration parameters can be done by editing the karavi-config-secret. The secret can be queried using k3s and kubectl like so:\nkubectl -n authorization get secret/karavi-config-secret\nTo update parameters, you must edit the base64 encoded data in the secret. The karavi-config-secret data can be decoded like so:\nkubectl -n authorization get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d\nSave the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so:\ncat \u003cfile\u003e | base64\nCopy the new, encoded data and edit the karavi-config-secret with the new data. Run this command to edit the secret:\nkubectl -n karavi edit secret/karavi-config-secret\nReplace the data in config.yaml under the data field with your new, encoded data. Save the changes and CSM Authorization will read the changed secret.\nNote: If you are updating the signing secret, the tenants need to be updated with new tokens via the karavictl generate token command.\nCSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the karavi-config-secret but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM Authorization logging settings during runtime, run the below command, make your changes, and save the updated configMap data.\nkubectl -n authorization edit configmap/csm-config-params This edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion:\nkubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params Using PowerFlex as an example, kubectl -n vxflexos edit configmap/vxflexos-config-params can be used to update the logging level of the sidecar-proxy and the driver.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization Helm deployment\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v3/authorization/deployment/helm/","tags":"","title":"Helm"},{"body":"The CSM Replication module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nTo install CSM Replication via the Dell CSM Operator, follow the instructions here.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Replication Operator deployment\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/replication/deployment/install-operator/","tags":"","title":"Installation using Operator"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Mirantis Kubernetes Engine (MKE).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn MKE-based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on MKE-backed clusters may run any of the OS which we support with upstream clusters.\nMKE UI Examples ","categories":"","description":"About Mirantis Kubernetes Engine","excerpt":"About Mirantis Kubernetes Engine","ref":"/csm-docs/v1/csidriver/partners/docker/","tags":"","title":"MKE"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Mirantis Kubernetes Engine (MKE).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn MKE-based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on MKE-backed clusters may run any of the OS which we support with upstream clusters.\nMKE UI Examples ","categories":"","description":"About Mirantis Kubernetes Engine","excerpt":"About Mirantis Kubernetes Engine","ref":"/csm-docs/v2/csidriver/partners/docker/","tags":"","title":"MKE"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Mirantis Kubernetes Engine (MKE).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn MKE-based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on MKE-backed clusters may run any of the OS which we support with upstream clusters.\nMKE UI Examples ","categories":"","description":"About Mirantis Kubernetes Engine","excerpt":"About Mirantis Kubernetes Engine","ref":"/csm-docs/v3/csidriver/partners/docker/","tags":"","title":"MKE"},{"body":"The CSM Observability module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. Dell CSM Operator will deploy CSM Observability, including topology service, Otel collector, and metrics services.\nPrerequisites Create a namespace karavi kubectl create namespace karavi Enable Observability module and components in sample manifests. If cert-manager has already been installed, don’t enable it. Observability will deploy with self-signed certificates by default. If you want to have custom certificates created instead, please generate certificates and private keys, encode them in base64, and insert them into the sample file as shown below for whichever components you are enabling. If none of the pods deploy, check the operator logs to see if there is an error with the certificates. If the pods deploy but the karavi pods never complete, check the cert-manager controller logs to see if there are issues with certificate creation. # observability: allows to configure observability - name: observability ... components: - name: topology ... # certificate: base64-encoded certificate for cert/private-key pair -- add cert here to use custom certificates # for self-signed certs, leave empty string # Allowed values: string certificate: \"\u003cINSERT BASE64-ENCODED TOPOLOGY CERTIFICATE HERE\u003e\" # privateKey: base64-encoded private key for cert/private-key pair -- add private key here to use custom certificates # for self-signed certs, leave empty string # Allowed values: string privateKey: \"\u003cINSERT BASE64-ENCODED TOPOLOGY PRIVATE KEY HERE\u003e\" ... - name: otel-collector ... # certificate: base64-encoded certificate for cert/private-key pair -- add cert here to use custom certificates # for self-signed certs, leave empty string # Allowed values: string certificate: \"\u003cINSERT BASE64-ENCODED OTEL-COLLECTOR CERTIFICATE HERE\u003e\" # privateKey: base64-encoded private key for cert/private-key pair -- add private key here to use custom certificates # for self-signed certs, leave empty string # Allowed values: string privateKey: \"\u003cINSERT BASE64-ENCODED OTEL-COLLECTOR PRIVATE KEY HERE\u003e\" ... Notes: If you enable metrics-powerscale or metrics-powerflex, you must enable otel-collector as well. otel-collector cannot be enabled without a metrics component also enabled. If you are deploying multiple drivers, only enable topology, otel-collector, and cert-manager in the first driver. For subsequent drivers, only enable the metrics component. When deleting a deployment with multiple drivers, the driver that was created first must be deleted last. Install Observability Once you have prepared the sample file(s) (one per driver being installed), deploy by running kubectl apply -f \u003cSAMPLE FILE\u003e on the sample file. ","categories":"","description":"Installing Observability via Dell CSM Operator\n","excerpt":"Installing Observability via Dell CSM Operator\n","ref":"/csm-docs/docs/deployment/csmoperator/modules/observability/","tags":"","title":"Observability"},{"body":"The CSM Observability module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. Dell CSM Operator will deploy CSM Observability, including topology service, Otel collector, and metrics services.\nPrerequisites Create a namespace karavi kubectl create namespace karavi Enable Observability module and components in sample manifests. If cert-manager has already been installed, don’t enable it. Scenario 1: Deploy one supported CSI Driver and enable Observability module\nIf you enable metrics-powerscale or metrics-powerflex, must enable otel-collector as well. Scenario 2: Deploy multiple supported CSI Drivers and enable Observability module\nWhen deploying the first driver, enable all components of Observability module in the CR. For the following drivers, only enable the metrics service, and remove topology and otel-collector sections from the CR. The CR created at first must be deleted at last. Note: pods in the karavi namespace will be in the ContainerCreating state until certificates are successfully created as described in the next step.\nCreate certificates\nNote: you may need to wait for the cert-manager pods to be 60-90 seconds old to successfully create certificates without an x509 error. See the cert-manager documentation for more information.\nOption 1: Self-signed certificates\nA Sample certificates manifest can be found at samples/observability/selfsigned-cert.yaml. Create certificates kubectl create -f selfsigned-cert.yaml Option 2: Custom certificates\nReplace tls.crt and tls.key with actual base64-encoded certificate and private key in samples/observability/custom-cert.yaml. Create certificates kubectl create -f custom-cert.yaml ","categories":"","description":"Installing Observability via Dell CSM Operator\n","excerpt":"Installing Observability via Dell CSM Operator\n","ref":"/csm-docs/v1/deployment/csmoperator/modules/observability/","tags":"","title":"Observability"},{"body":"The CSM Observability module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. Dell CSM Operator will deploy CSM Observability, including topology service, Otel collector, and metrics services.\nPrerequisites Create a namespace karavi kubectl create namespace karavi Install cert-manager with Helm Add the Helm repository helm repo add jetstack https://charts.jetstack.io Update your local Helm chart repository cache helm repo update Install cert-manager in the namespace karavi helm install \\ cert-manager jetstack/cert-manager \\ --namespace karavi \\ --version v1.10.0 \\ --set installCRDs=true Verify installation kubectl get pod -n karavi NAME READY STATUS RESTARTS AGE cert-manager-7b45d477c8-z28sq 1/1 Running 0 2m2s cert-manager-cainjector-86f7f4749-mdz7c 1/1 Running 0 2m2s cert-manager-webhook-66c85f8577-c7hxx 1/1 Running 0 2m2s Create certificates Option 1: Self-signed certificates\nA Sample certificates manifest can be found at samples/observability/selfsigned-cert.yaml. Create certificates kubectl create -f selfsigned-cert.yaml Option 2: Custom certificates\nReplace tls.crt and tls.key with actual base64-encoded certificate and private key in samples/observability/custom-cert.yaml. Create certificates kubectl create -f custom-cert.yaml Enable Observability module and components in sample manifests Scenario 1: Deploy one supported CSI Driver and enable Observability module\nIf you enable metrics-powerscale or metrics-powerflex, must enable otel-collector as well. Scenario 2: Deploy multiple supported CSI Drivers and enable Observability module\nWhen deploying the first driver, enable all components of Observability module in the CR. For the following drivers, only enable the metrics service, and remove topology and otel-collector sections from the CR. The CR created at first must be deleted at last. ","categories":"","description":"Installing Observability via Dell CSM Operator\n","excerpt":"Installing Observability via Dell CSM Operator\n","ref":"/csm-docs/v2/deployment/csmoperator/modules/observability/","tags":"","title":"Observability"},{"body":"The CSM Observability module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. Dell CSM Operator will deploy CSM Observability, including topology service, Otel collector, and metrics services.\nPrerequisites Create a namespace karavi kubectl create namespace karavi Install cert-manager with Helm Add the Helm repository helm repo add jetstack https://charts.jetstack.io Update your local Helm chart repository cache helm repo update Install cert-manager in the namespace karavi helm install \\ cert-manager jetstack/cert-manager \\ --namespace karavi \\ --version v1.10.0 \\ --set installCRDs=true Verify installation $ kubectl get pod -n karavi NAME READY STATUS RESTARTS AGE cert-manager-7b45d477c8-z28sq 1/1 Running 0 2m2s cert-manager-cainjector-86f7f4749-mdz7c 1/1 Running 0 2m2s cert-manager-webhook-66c85f8577-c7hxx 1/1 Running 0 2m2s Create certificates Option 1: Self-signed certificates\nA Sample certificates manifest can be found at samples/observability/selfsigned-cert.yaml. Create certificates kubectl create -f selfsigned-cert.yaml Option 2: Custom certificates\nReplace tls.crt and tls.key with actual base64-encoded certificate and private key in samples/observability/custom-cert.yaml. Create certificates kubectl create -f custom-cert.yaml Enable Observability module and components in sample manifests Scenario 1: Deploy one supported CSI Driver and enable Observability module\nIf you enable metrics-powerscale or metrics-powerflex, must enable otel-collector as well. Scenario 2: Deploy multiple supported CSI Drivers and enable Observability module\nWhen deploying the first driver, enable all components of Observability module in the CR. For the following drivers, only enable the metrics service, and remove topology and otel-collector sections from the CR. The CR created at first must be deleted at last. ","categories":"","description":"Pre-requisite for Installing Observability via Dell CSM Operator\n","excerpt":"Pre-requisite for Installing Observability via Dell CSM Operator\n","ref":"/csm-docs/v3/deployment/csmoperator/modules/observability/","tags":"","title":"Observability"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell CSI Storage Providers, via either Helm or the Dell CSM Operator.\nThis includes the following drivers:\nPowerFlex PowerMax PowerScale PowerStore Unity XT As well as the Dell CSM Operator.\nDell CSM Operator Directions for offline installation can be found here. Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\nOne Linux-based system, with Internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry If one Linux system has both Internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\nDependency Usage docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry. One of these will be required on both the system building the offline bundle as well as the system preparing for installation. Tested version(s) are docker 19.03+ and podman 1.6.4+ git git will be used to manually clone one of the above repositories in order to create an offline bundle. This is only needed on the system preparing the offline bundle. Tested version(s) are git 1.8+ but any version should work. Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\nBuild an offline bundle Unpacking the offline bundle created in Step 1 and preparing for installation Perform either a Helm installation or Operator installation using the files obtained after unpacking in Step 2 NOTE: It is recommended to use the same build tool for packing and unpacking of images (either docker or podman).\nBuilding an offline bundle This needs to be performed on a Linux system with access to the Internet as a git repo will need to be cloned, and container images pulled from public registries.\nTo build an offline bundle, the following steps are needed:\nPerform a git clone of the desired repository. For a helm-based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSM Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csm-offline-bundle.sh script will be found in the scripts directory The script will perform the following steps:\nDetermine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSM Operator configuration files (if run from a clone of the Dell CSM Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSM Operator:\ngit clone -b v1.4.3 https://github.com/dell/csm-operator.git cd csm-operator bash scripts/csm-offline-bundle.sh -c * * Building image manifest file Processing file /root/csm-operator/operatorconfig/driverconfig/common/default.yaml Processing file /root/csm-operator/bundle/manifests/dell-csm-operator.clusterserviceversion.yaml * * Pulling and saving container images dellemc/csi-isilon:v2.9.1 dellemc/csi-metadata-retriever:v1.6.1 dellemc/csipowermax-reverseproxy:v2.8.1 dellemc/csi-powermax:v2.9.1 dellemc/csi-powerstore:v2.9.1 dellemc/csi-unity:v2.8.1 dellemc/csi-vxflexos:v2.9.2 dellemc/csm-authorization-sidecar:v1.9.1 dellemc/csm-metrics-powerflex:v1.5.0 dellemc/csm-metrics-powerscale:v1.2.0 dellemc/csm-topology:v1.5.0 dellemc/dell-csi-replicator:v1.7.1 dellemc/dell-replication-controller:v1.7.0 dellemc/sdc:4.5 docker.io/dellemc/dell-csm-operator:v1.4.3 gcr.io/kubebuilder/kube-rbac-proxy:v0.8.0 nginxinc/nginx-unprivileged:1.20 otel/opentelemetry-collector:0.42.0 registry.k8s.io/sig-storage/csi-attacher:v4.3.0 registry.k8s.io/sig-storage/csi-external-health-monitor-controller:v0.9.0 registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0 registry.k8s.io/sig-storage/csi-provisioner:v3.5.0 registry.k8s.io/sig-storage/csi-resizer:v1.8.0 registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2 * * Copying necessary files /root/csm-operator/deploy /root/csm-operator/operatorconfig /root/csm-operator/samples /root/csm-operator/scripts /root/csm-operator/README.md /root/csm-operator/LICENSE * * Compressing release dell-csm-operator-bundle/ dell-csm-operator-bundle/deploy/ dell-csm-operator-bundle/deploy/operator.yaml dell-csm-operator-bundle/deploy/crds/ dell-csm-operator-bundle/deploy/crds/storage.dell.com_containerstoragemodules.yaml dell-csm-operator-bundle/deploy/olm/ dell-csm-operator-bundle/deploy/olm/operator_community.yaml ... ... dell-csm-operator-bundle/README.md dell-csm-operator-bundle/LICENSE * * Complete Offline bundle file is: /root/csm-operator/dell-csm-operator-bundle.tar.gz Unpacking the offline bundle and preparing for installation This needs to be performed on a Linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for the driver or Operator installation, the following steps need to be performed:\nCopy the offline bundle file created from the previous step to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option For Operator installs, the csm-offline-bundle.sh script will be found in the scripts directory The script will then perform the following steps:\nLoad the required container images into the local system Tag the images according to the user-supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images An example of preparing the bundle for installation for the Dell CSM Operator:\ntar xvfz dell-csm-operator-bundle.tar.gz dell-csm-operator-bundle/ dell-csm-operator-bundle/deploy/ dell-csm-operator-bundle/deploy/operator.yaml dell-csm-operator-bundle/deploy/crds/ dell-csm-operator-bundle/deploy/crds/storage.dell.com_containerstoragemodules.yaml dell-csm-operator-bundle/deploy/olm/ dell-csm-operator-bundle/deploy/olm/operator_community.yaml ... ... dell-csm-operator-bundle/README.md dell-csm-operator-bundle/LICENSE cd dell-csm-operator-bundle bash scripts/csm-offline-bundle.sh -p -r localregistry:5000/dell-csm-operator/ Preparing a offline bundle for installation * * Loading docker images Loaded image: docker.io/dellemc/csi-powerstore:v2.9.1 Loaded image: docker.io/dellemc/csi-isilon:v2.9.1 ... ... Loaded image: registry.k8s.io/sig-storage/csi-resizer:v1.8.0 Loaded image: registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2 * * Tagging and pushing images dellemc/csi-isilon:v2.8.0 -\u003e localregistry:5000/dell-csm-operator/csi-isilon:v2.8.0 dellemc/csi-metadata-retriever:v1.5.0 -\u003e localregistry:5000/dell-csm-operator/csi-metadata-retriever:v1.5.0 ... ... registry.k8s.io/sig-storage/csi-resizer:v1.8.0 -\u003e localregistry:5000/dell-csm-operator/csi-resizer:v1.8.0 registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2 -\u003e localregistry:5000/dell-csm-operator/csi-snapshotter:v6.2.2 * * Preparing files within /root/dell-csm-operator-bundle changing: dellemc/csi-isilon:v2.8.0 -\u003e localregistry:5000/dell-csm-operator/csi-isilon:v2.8.0 changing: dellemc/csi-metadata-retriever:v1.5.0 -\u003e localregistry:5000/dell-csm-operator/csi-metadata-retriever:v1.5.0 ... ... changing: registry.k8s.io/sig-storage/csi-resizer:v1.8.0 -\u003e localregistry:5000/dell-csm-operator/csi-resizer:v1.8.0 changing: registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2 -\u003e localregistry:5000/dell-csm-operator/csi-snapshotter:v6.2.2 * * Complete Perform either a Helm installation or Operator installation Now that the required images are available and the Helm Charts/Operator configuration updated, you can proceed by following the usual installation procedure as documented either via Helm or Operator.\nNOTES:\nOffline bundle installation is only supported with manual installs i.e. without using Operator Lifecycle Manager. Installation should be done using the files that are obtained after unpacking the offline bundle (dell-csm-operator-bundle.tar.gz) as the image tags in the manifests are modified to point to the internal registry. Offline bundle installs operator in default namespace via install.sh script. Make sure that the current context in kubeconfig file has the namespace set to default. ","categories":"","description":"Offline Installation of Dell CSI Storage Providers","excerpt":"Offline Installation of Dell CSI Storage Providers","ref":"/csm-docs/docs/csidriver/installation/offline/","tags":"","title":"Offline Installation of Dell CSI Storage Providers"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\nPowerFlex PowerMax PowerScale PowerStore Unity XT As well as the Dell CSI Operator\nDell CSI Operator Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\nOne Linux-based system, with Internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry If one Linux system has both Internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\nDependency Usage docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry. One of these will be required on both the system building the offline bundle as well as the system preparing for installation. Tested version(s) are docker 19.03+ and podman 1.6.4+ git git will be used to manually clone one of the above repositories in order to create an offline bundle. This is only needed on the system preparing the offline bundle. Tested version(s) are git 1.8+ but any version should work. Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\nBuild an offline bundle Unpacking the offline bundle created in Step 1 and preparing for installation Perform either a Helm installation or Operator installation using the files obtained after unpacking in Step 2 NOTE: It is recommended to use the same build tool for packing and unpacking of images (either docker or podman).\nBuilding an offline bundle This needs to be performed on a Linux system with access to the Internet as a git repo will need to be cloned, and container images pulled from public registries.\nTo build an offline bundle, the following steps are needed:\nPerform a git clone of the desired repository. For a helm-based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory The script will perform the following steps:\nDetermine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\ngit clone -b v1.12.0 https://github.com/dell/dell-csi-operator.git cd dell-csi-operator/scripts ./csi-offline-bundle.sh -c * * Pulling and saving container images dellemc/csi-isilon:v2.5.0 dellemc/csi-isilon:v2.6.0 dellemc/csi-isilon:v2.7.0 dellemc/csipowermax-reverseproxy:v2.4.0 dellemc/csi-powermax:v2.3.1 dellemc/csi-powermax:v2.4.0 dellemc/csi-powermax:v2.5.0 dellemc/csi-powerstore:v2.6.0 dellemc/csi-powerstore:v2.7.0 dellemc/csi-powerstore:v2.8.0 dellemc/csi-unity:v2.3.0 dellemc/csi-unity:v2.4.0 dellemc/csi-unity:v2.5.0 dellemc/csi-vxflexos:v2.6.0 dellemc/csi-vxflexos:v2.7.0 dellemc/csi-vxflexos:v2.8.0 dellemc/dell-csi-operator:v1.12.0 dellemc/sdc:3.6 dellemc/sdc:3.6.0.6 dellemc/sdc:3.6.1 docker.io/busybox:1.32.0 ... ... * * Copying necessary files /root/dell-csi-operator/driverconfig /root/dell-csi-operator/deploy /root/dell-csi-operator/samples /root/dell-csi-operator/scripts /root/dell-csi-operator/OLM.md /root/dell-csi-operator/README.md /root/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/driverconfig/ dell-csi-operator-bundle/driverconfig/config.yaml dell-csi-operator-bundle/driverconfig/isilon_v230_v121.json dell-csi-operator-bundle/driverconfig/isilon_v230_v122.json dell-csi-operator-bundle/driverconfig/isilon_v230_v123.json dell-csi-operator-bundle/driverconfig/isilon_v230_v124.json dell-csi-operator-bundle/driverconfig/isilon_v240_v121.json dell-csi-operator-bundle/driverconfig/isilon_v240_v122.json dell-csi-operator-bundle/driverconfig/isilon_v240_v123.json dell-csi-operator-bundle/driverconfig/isilon_v240_v124.json dell-csi-operator-bundle/driverconfig/isilon_v250_v123.json dell-csi-operator-bundle/driverconfig/isilon_v250_v124.json dell-csi-operator-bundle/driverconfig/isilon_v250_v125.json dell-csi-operator-bundle/driverconfig/powermax_v230_v121.json ... ... * * Complete Offline bundle file is: /root/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking the offline bundle and preparing for installation This needs to be performed on a Linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for the driver or Operator installation, the following steps need to be performed:\nCopy the offline bundle file created from the previous step to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option The script will then perform the following steps:\nLoad the required container images into the local system Tag the images according to the user-supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images An example of preparing the bundle for installation (192.168.75.40:5000 refers to an image registry accessible to Kubernetes/OpenShift):\ntar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md cd dell-csi-operator-bundle ./csi-offline-bundle.sh -p -r localregistry:5000/csi-operator Preparing a offline bundle for installation * * Loading docker images 5b1fa8e3e100: Loading layer [==================================================\u003e] 3.697MB/3.697MB e20ed4c73206: Loading layer [==================================================\u003e] 17.22MB/17.22MB Loaded image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.6.0 d72a74c56330: Loading layer [==================================================\u003e] 3.031MB/3.031MB f2d2ab12e2a7: Loading layer [==================================================\u003e] 48.08MB/48.08MB Loaded image: k8s.gcr.io/sig-storage/csi-snapshotter-v6.1.0 417cb9b79ade: Loading layer [==================================================\u003e] 3.062MB/3.062MB 61fefb35ccee: Loading layer [==================================================\u003e] 16.88MB/16.88MB Loaded image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.1 7a5b9c0b4b14: Loading layer [==================================================\u003e] 3.031MB/3.031MB 1555ad6e2d44: Loading layer [==================================================\u003e] 49.86MB/49.86MB Loaded image: k8s.gcr.io/sig-storage/csi-attacher-v4.0.0 2de1422d5d2d: Loading layer [==================================================\u003e] 54.56MB/54.56MB Loaded image: k8s.gcr.io/sig-storage/csi-resizer-v1.6.0 25a1c1010608: Loading layer [==================================================\u003e] 54.54MB/54.54MB Loaded image: k8s.gcr.io/sig-storage/csi-snapshotter-v6.0.1 07363fa84210: Loading layer [==================================================\u003e] 3.062MB/3.062MB 5227e51ea570: Loading layer [==================================================\u003e] 54.92MB/54.92MB Loaded image: k8s.gcr.io/sig-storage/csi-attacher-v3.5.0 cfb5cbeabdb2: Loading layer [==================================================\u003e] 55.38MB/55.38MB Loaded image: k8s.gcr.io/sig-storage/csi-resizer-v1.5.0 ... ... * * Tagging and pushing images dellemc/dell-csi-operator:v1.12.0 -\u003e localregistry:5000/csi-operator/dell-csi-operator:v1.12.0 dellemc/csi-isilon:v2.3.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.3.0 dellemc/csi-isilon:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.4.0 dellemc/csi-isilon:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.5.0 dellemc/csipowermax-reverseproxy:v2.4.0 -\u003e localregistry:5000/csi-operator/csipowermax-reverseproxy:v2.4.0 dellemc/csi-powermax:v2.3.1 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.3.1 dellemc/csi-powermax:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.4.0 dellemc/csi-powermax:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.5.0 dellemc/csi-powerstore:v2.6.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.6.0 dellemc/csi-powerstore:v2.7.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.7.0 dellemc/csi-powerstore:v2.8.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.8.0 dellemc/csi-unity:v2.3.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.3.0 dellemc/csi-unity:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.4.0 dellemc/csi-unity:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.5.0 dellemc/csi-vxflexos:v2.6.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.6.0 dellemc/csi-vxflexos:v2.7.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.7.0 dellemc/csi-vxflexos:v2.8.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.8.0 dellemc/sdc:3.6 -\u003e localregistry:5000/csi-operator/sdc:3.6 dellemc/sdc:3.6.0.6 -\u003e localregistry:5000/csi-operator/sdc:3.6.0.6 dellemc/sdc:3.6.1 -\u003e localregistry:5000/csi-operator/sdc:3.6.1 docker.io/busybox:1.32.0 -\u003e localregistry:5000/csi-operator/busybox:1.32.0 ... ... * * Preparing operator files within /root/dell-csi-operator-bundle changing: dellemc/dell-csi-operator:v1.12.0 -\u003e localregistry:5000/csi-operator/dell-csi-operator:v1.12.0 changing: dellemc/csi-isilon:v2.3.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.3.0 changing: dellemc/csi-isilon:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.4.0 changing: dellemc/csi-isilon:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.5.0 changing: dellemc/csipowermax-reverseproxy:v2.4.0 -\u003e localregistry:5000/csi-operator/csipowermax-reverseproxy:v2.4.0 changing: dellemc/csi-powermax:v2.3.1 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.3.1 changing: dellemc/csi-powermax:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.4.0 changing: dellemc/csi-powermax:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.5.0 changing: dellemc/csi-powerstore:v2.6.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.6.0 changing: dellemc/csi-powerstore:v2.7.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.7.0 changing: dellemc/csi-powerstore:v2.8.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.8.0 changing: dellemc/csi-unity:v2.3.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.3.0 changing: dellemc/csi-unity:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.4.0 changing: dellemc/csi-unity:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.5.0 changing: dellemc/csi-vxflexos:v2.6.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.6.0 changing: dellemc/csi-vxflexos:v2.7.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.7.0 changing: dellemc/csi-vxflexos:v2.8.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.8.0 changing: dellemc/sdc:3.6 -\u003e localregistry:5000/csi-operator/sdc:3.6 changing: dellemc/sdc:3.6.0.6 -\u003e localregistry:5000/csi-operator/sdc:3.6.0.6 changing: dellemc/sdc:3.6.1 -\u003e localregistry:5000/csi-operator/sdc:3.6.1 changing: docker.io/busybox:1.32.0 -\u003e localregistry:5000/csi-operator/busybox:1.32.0 ... ... * * Complete Perform either a Helm installation or Operator installation Now that the required images are available and the Helm Charts/Operator configuration updated, you can proceed by following the usual installation procedure as documented either via Helm or Operator.\nNOTES:\nOffline bundle installation is only supported with manual installs i.e. without using Operator Lifecycle Manager. Installation should be done using the files that are obtained after unpacking the offline bundle (dell-csi-operator-bundle.tar.gz) as the image tags in the manifests are modified to point to the internal registry. Offline bundle installs operator in default namespace via install.sh script. Make sure that the current context in kubeconfig file has the namespace set to default. ","categories":"","description":"Offline Installation of Dell CSI Storage Providers","excerpt":"Offline Installation of Dell CSI Storage Providers","ref":"/csm-docs/v1/csidriver/installation/offline/","tags":"","title":"Offline Installation of Dell CSI Storage Providers"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\nPowerFlex PowerMax PowerScale PowerStore Unity XT As well as the Dell CSI Operator\nDell CSI Operator Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\nOne Linux-based system, with internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry If one Linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\nDependency Usage docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry. One of these will be required on both the system building the offline bundle as well as the system preparing for installation. Tested version(s) are docker 19.03+ and podman 1.6.4+ git git will be used to manually clone one of the above repositories in order to create an offline bundle. This is only needed on the system preparing the offline bundle. Tested version(s) are git 1.8+ but any version should work. Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\nBuild an offline bundle Unpacking the offline bundle created in Step 1 and preparing for installation Perform either a Helm installation or Operator installation using the files obtained after unpacking in Step 2 NOTE: It is recommended to use the same build tool for packing and unpacking of images (either docker or podman).\nBuilding an offline bundle This needs to be performed on a Linux system with access to the internet as a git repo will need to be cloned, and container images pulled from public registries.\nTo build an offline bundle, the following steps are needed:\nPerform a git clone of the desired repository. For a helm-based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory The script will perform the following steps:\nDetermine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\ngit clone -b v1.12.0 https://github.com/dell/dell-csi-operator.git cd dell-csi-operator/scripts ./csi-offline-bundle.sh -c * * Pulling and saving container images dellemc/csi-isilon:v2.5.0 dellemc/csi-isilon:v2.6.0 dellemc/csi-isilon:v2.7.0 dellemc/csipowermax-reverseproxy:v2.4.0 dellemc/csi-powermax:v2.3.1 dellemc/csi-powermax:v2.4.0 dellemc/csi-powermax:v2.5.0 dellemc/csi-powerstore:v2.5.0 dellemc/csi-powerstore:v2.6.0 dellemc/csi-powerstore:v2.7.0 dellemc/csi-unity:v2.3.0 dellemc/csi-unity:v2.4.0 dellemc/csi-unity:v2.5.0 dellemc/csi-vxflexos:v2.5.0 dellemc/csi-vxflexos:v2.6.0 dellemc/csi-vxflexos:v2.7.0 dellemc/dell-csi-operator:v1.12.0 dellemc/sdc:3.6 dellemc/sdc:3.6.0.6 docker.io/busybox:1.32.0 ... ... * * Copying necessary files /root/dell-csi-operator/driverconfig /root/dell-csi-operator/deploy /root/dell-csi-operator/samples /root/dell-csi-operator/scripts /root/dell-csi-operator/OLM.md /root/dell-csi-operator/README.md /root/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/driverconfig/ dell-csi-operator-bundle/driverconfig/config.yaml dell-csi-operator-bundle/driverconfig/isilon_v230_v121.json dell-csi-operator-bundle/driverconfig/isilon_v230_v122.json dell-csi-operator-bundle/driverconfig/isilon_v230_v123.json dell-csi-operator-bundle/driverconfig/isilon_v230_v124.json dell-csi-operator-bundle/driverconfig/isilon_v240_v121.json dell-csi-operator-bundle/driverconfig/isilon_v240_v122.json dell-csi-operator-bundle/driverconfig/isilon_v240_v123.json dell-csi-operator-bundle/driverconfig/isilon_v240_v124.json dell-csi-operator-bundle/driverconfig/isilon_v250_v123.json dell-csi-operator-bundle/driverconfig/isilon_v250_v124.json dell-csi-operator-bundle/driverconfig/isilon_v250_v125.json dell-csi-operator-bundle/driverconfig/powermax_v230_v121.json ... ... * * Complete Offline bundle file is: /root/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking the offline bundle and preparing for installation This needs to be performed on a Linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for the driver or Operator installation, the following steps need to be performed:\nCopy the offline bundle file created from the previous step to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option The script will then perform the following steps:\nLoad the required container images into the local system Tag the images according to the user-supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images An example of preparing the bundle for installation (192.168.75.40:5000 refers to an image registry accessible to Kubernetes/OpenShift):\ntar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md cd dell-csi-operator-bundle ./csi-offline-bundle.sh -p -r localregistry:5000/csi-operator Preparing a offline bundle for installation * * Loading docker images 5b1fa8e3e100: Loading layer [==================================================\u003e] 3.697MB/3.697MB e20ed4c73206: Loading layer [==================================================\u003e] 17.22MB/17.22MB Loaded image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.6.0 d72a74c56330: Loading layer [==================================================\u003e] 3.031MB/3.031MB f2d2ab12e2a7: Loading layer [==================================================\u003e] 48.08MB/48.08MB Loaded image: k8s.gcr.io/sig-storage/csi-snapshotter-v6.1.0 417cb9b79ade: Loading layer [==================================================\u003e] 3.062MB/3.062MB 61fefb35ccee: Loading layer [==================================================\u003e] 16.88MB/16.88MB Loaded image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.1 7a5b9c0b4b14: Loading layer [==================================================\u003e] 3.031MB/3.031MB 1555ad6e2d44: Loading layer [==================================================\u003e] 49.86MB/49.86MB Loaded image: k8s.gcr.io/sig-storage/csi-attacher-v4.0.0 2de1422d5d2d: Loading layer [==================================================\u003e] 54.56MB/54.56MB Loaded image: k8s.gcr.io/sig-storage/csi-resizer-v1.6.0 25a1c1010608: Loading layer [==================================================\u003e] 54.54MB/54.54MB Loaded image: k8s.gcr.io/sig-storage/csi-snapshotter-v6.0.1 07363fa84210: Loading layer [==================================================\u003e] 3.062MB/3.062MB 5227e51ea570: Loading layer [==================================================\u003e] 54.92MB/54.92MB Loaded image: k8s.gcr.io/sig-storage/csi-attacher-v3.5.0 cfb5cbeabdb2: Loading layer [==================================================\u003e] 55.38MB/55.38MB Loaded image: k8s.gcr.io/sig-storage/csi-resizer-v1.5.0 ... ... * * Tagging and pushing images dellemc/dell-csi-operator:v1.12.0 -\u003e localregistry:5000/csi-operator/dell-csi-operator:v1.12.0 dellemc/csi-isilon:v2.3.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.3.0 dellemc/csi-isilon:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.4.0 dellemc/csi-isilon:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.5.0 dellemc/csipowermax-reverseproxy:v2.4.0 -\u003e localregistry:5000/csi-operator/csipowermax-reverseproxy:v2.4.0 dellemc/csi-powermax:v2.3.1 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.3.1 dellemc/csi-powermax:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.4.0 dellemc/csi-powermax:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.5.0 dellemc/csi-powerstore:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.5.0 dellemc/csi-powerstore:v2.6.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.6.0 dellemc/csi-powerstore:v2.7.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.7.0 dellemc/csi-unity:v2.3.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.3.0 dellemc/csi-unity:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.4.0 dellemc/csi-unity:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.5.0 dellemc/csi-vxflexos:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.5.0 dellemc/csi-vxflexos:v2.6.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.6.0 dellemc/csi-vxflexos:v2.7.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.7.0 dellemc/sdc:3.6 -\u003e localregistry:5000/csi-operator/sdc:3.6 dellemc/sdc:3.6.0.6 -\u003e localregistry:5000/csi-operator/sdc:3.6.0.6 docker.io/busybox:1.32.0 -\u003e localregistry:5000/csi-operator/busybox:1.32.0 ... ... * * Preparing operator files within /root/dell-csi-operator-bundle changing: dellemc/dell-csi-operator:v1.12.0 -\u003e localregistry:5000/csi-operator/dell-csi-operator:v1.12.0 changing: dellemc/csi-isilon:v2.3.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.3.0 changing: dellemc/csi-isilon:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.4.0 changing: dellemc/csi-isilon:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.5.0 changing: dellemc/csipowermax-reverseproxy:v2.4.0 -\u003e localregistry:5000/csi-operator/csipowermax-reverseproxy:v2.4.0 changing: dellemc/csi-powermax:v2.3.1 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.3.1 changing: dellemc/csi-powermax:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.4.0 changing: dellemc/csi-powermax:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.5.0 changing: dellemc/csi-powerstore:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.5.0 changing: dellemc/csi-powerstore:v2.6.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.6.0 changing: dellemc/csi-powerstore:v2.7.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.7.0 changing: dellemc/csi-unity:v2.3.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.3.0 changing: dellemc/csi-unity:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.4.0 changing: dellemc/csi-unity:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.5.0 changing: dellemc/csi-vxflexos:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.5.0 changing: dellemc/csi-vxflexos:v2.6.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.6.0 changing: dellemc/csi-vxflexos:v2.7.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.7.0 changing: dellemc/sdc:3.6 -\u003e localregistry:5000/csi-operator/sdc:3.6 changing: dellemc/sdc:3.6.0.6 -\u003e localregistry:5000/csi-operator/sdc:3.6.0.6 changing: docker.io/busybox:1.32.0 -\u003e localregistry:5000/csi-operator/busybox:1.32.0 ... ... * * Complete Perform either a Helm installation or Operator installation Now that the required images are available and the Helm Charts/Operator configuration updated, you can proceed by following the usual installation procedure as documented either via Helm or Operator.\nNOTES:\nOffline bundle installation is only supported with manual installs i.e. without using Operator Lifecycle Manager. Installation should be done using the files that are obtained after unpacking the offline bundle (dell-csi-operator-bundle.tar.gz) as the image tags in the manifests are modified to point to the internal registry. Offline bundle installs operator in default namespace via install.sh script. Make sure that the current context in kubeconfig file has the namespace set to default. ","categories":"","description":"Offline Installation of Dell CSI Storage Providers","excerpt":"Offline Installation of Dell CSI Storage Providers","ref":"/csm-docs/v2/csidriver/installation/offline/","tags":"","title":"Offline Installation of Dell CSI Storage Providers"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\nPowerFlex PowerMax PowerScale PowerStore Unity XT As well as the Dell CSI Operator\nDell CSI Operator Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\nOne Linux-based system, with internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry If one Linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\nDependency Usage docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry. One of these will be required on both the system building the offline bundle as well as the system preparing for installation. Tested version(s) are docker 19.03+ and podman 1.6.4+ git git will be used to manually clone one of the above repositories in order to create an offline bundle. This is only needed on the system preparing the offline bundle. Tested version(s) are git 1.8+ but any version should work. Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\nBuild an offline bundle Unpacking the offline bundle created in Step 1 and preparing for installation Perform either a Helm installation or Operator installation using the files obtained after unpacking in Step 2 NOTE: It is recommended to use the same build tool for packing and unpacking of images (either docker or podman).\nBuilding an offline bundle This needs to be performed on a Linux system with access to the internet as a git repo will need to be cloned, and container images pulled from public registries.\nTo build an offline bundle, the following steps are needed:\nPerform a git clone of the desired repository. For a helm-based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory The script will perform the following steps:\nDetermine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\ngit clone -b v1.11.0 https://github.com/dell/dell-csi-operator.git cd dell-csi-operator/scripts [root@user scripts]# ./csi-offline-bundle.sh -c * * Pulling and saving container images dellemc/csi-isilon:v2.4.0 dellemc/csi-isilon:v2.5.0 dellemc/csi-isilon:v2.6.0 dellemc/csipowermax-reverseproxy:v2.4.0 dellemc/csi-powermax:v2.3.1 dellemc/csi-powermax:v2.4.0 dellemc/csi-powermax:v2.5.0 dellemc/csi-powerstore:v2.3.0 dellemc/csi-powerstore:v2.4.0 dellemc/csi-powerstore:v2.5.0 dellemc/csi-unity:v2.3.0 dellemc/csi-unity:v2.4.0 dellemc/csi-unity:v2.5.0 dellemc/csi-vxflexos:v2.4.0 dellemc/csi-vxflexos:v2.5.0 dellemc/csi-vxflexos:v2.6.0 dellemc/dell-csi-operator:v1.11.0 dellemc/sdc:3.5.1.1-1 dellemc/sdc:3.6 dellemc/sdc:3.6.0.6 dellemc/sdc:3.6.1 docker.io/busybox:1.32.0 ... ... * * Copying necessary files /root/dell-csi-operator/driverconfig /root/dell-csi-operator/deploy /root/dell-csi-operator/samples /root/dell-csi-operator/scripts /root/dell-csi-operator/OLM.md /root/dell-csi-operator/README.md /root/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/driverconfig/ dell-csi-operator-bundle/driverconfig/config.yaml dell-csi-operator-bundle/driverconfig/isilon_v230_v121.json dell-csi-operator-bundle/driverconfig/isilon_v230_v122.json dell-csi-operator-bundle/driverconfig/isilon_v230_v123.json dell-csi-operator-bundle/driverconfig/isilon_v230_v124.json dell-csi-operator-bundle/driverconfig/isilon_v240_v121.json dell-csi-operator-bundle/driverconfig/isilon_v240_v122.json dell-csi-operator-bundle/driverconfig/isilon_v240_v123.json dell-csi-operator-bundle/driverconfig/isilon_v240_v124.json dell-csi-operator-bundle/driverconfig/isilon_v250_v123.json dell-csi-operator-bundle/driverconfig/isilon_v250_v124.json dell-csi-operator-bundle/driverconfig/isilon_v250_v125.json dell-csi-operator-bundle/driverconfig/powermax_v230_v121.json ... ... * * Complete Offline bundle file is: /root/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking the offline bundle and preparing for installation This needs to be performed on a Linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for the driver or Operator installation, the following steps need to be performed:\nCopy the offline bundle file created from the previous step to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option The script will then perform the following steps:\nLoad the required container images into the local system Tag the images according to the user-supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images An example of preparing the bundle for installation (192.168.75.40:5000 refers to an image registry accessible to Kubernetes/OpenShift):\ntar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md cd dell-csi-operator-bundle [root@user scripts]# ./csi-offline-bundle.sh -p -r localregistry:5000/csi-operator Preparing a offline bundle for installation * * Loading docker images 5b1fa8e3e100: Loading layer [==================================================\u003e] 3.697MB/3.697MB e20ed4c73206: Loading layer [==================================================\u003e] 17.22MB/17.22MB Loaded image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.6.0 d72a74c56330: Loading layer [==================================================\u003e] 3.031MB/3.031MB f2d2ab12e2a7: Loading layer [==================================================\u003e] 48.08MB/48.08MB Loaded image: k8s.gcr.io/sig-storage/csi-snapshotter-v6.1.0 417cb9b79ade: Loading layer [==================================================\u003e] 3.062MB/3.062MB 61fefb35ccee: Loading layer [==================================================\u003e] 16.88MB/16.88MB Loaded image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.1 7a5b9c0b4b14: Loading layer [==================================================\u003e] 3.031MB/3.031MB 1555ad6e2d44: Loading layer [==================================================\u003e] 49.86MB/49.86MB Loaded image: k8s.gcr.io/sig-storage/csi-attacher-v4.0.0 2de1422d5d2d: Loading layer [==================================================\u003e] 54.56MB/54.56MB Loaded image: k8s.gcr.io/sig-storage/csi-resizer-v1.6.0 25a1c1010608: Loading layer [==================================================\u003e] 54.54MB/54.54MB Loaded image: k8s.gcr.io/sig-storage/csi-snapshotter-v6.0.1 07363fa84210: Loading layer [==================================================\u003e] 3.062MB/3.062MB 5227e51ea570: Loading layer [==================================================\u003e] 54.92MB/54.92MB Loaded image: k8s.gcr.io/sig-storage/csi-attacher-v3.5.0 cfb5cbeabdb2: Loading layer [==================================================\u003e] 55.38MB/55.38MB Loaded image: k8s.gcr.io/sig-storage/csi-resizer-v1.5.0 ... ... * * Tagging and pushing images dellemc/dell-csi-operator:v1.11.0 -\u003e localregistry:5000/csi-operator/dell-csi-operator:v1.11.0 dellemc/csi-isilon:v2.3.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.3.0 dellemc/csi-isilon:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.4.0 dellemc/csi-isilon:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.5.0 dellemc/csipowermax-reverseproxy:v2.4.0 -\u003e localregistry:5000/csi-operator/csipowermax-reverseproxy:v2.4.0 dellemc/csi-powermax:v2.3.1 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.3.1 dellemc/csi-powermax:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.4.0 dellemc/csi-powermax:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.5.0 dellemc/csi-powerstore:v2.3.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.3.0 dellemc/csi-powerstore:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.4.0 dellemc/csi-powerstore:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.5.0 dellemc/csi-unity:v2.3.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.3.0 dellemc/csi-unity:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.4.0 dellemc/csi-unity:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.5.0 dellemc/csi-vxflexos:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.4.0 dellemc/csi-vxflexos:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.5.0 dellemc/csi-vxflexos:v2.6.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.6.0 dellemc/sdc:3.5.1.1-1 -\u003e localregistry:5000/csi-operator/sdc:3.5.1.1-1 dellemc/sdc:3.6 -\u003e localregistry:5000/csi-operator/sdc:3.6 dellemc/sdc:3.6.0.6 -\u003e localregistry:5000/csi-operator/sdc:3.6.0.6 dellemc/sdc:3.6.1 -\u003e localregistry:5000/csi-operator/sdc:3.6.1 docker.io/busybox:1.32.0 -\u003e localregistry:5000/csi-operator/busybox:1.32.0 ... ... * * Preparing operator files within /root/dell-csi-operator-bundle changing: dellemc/dell-csi-operator:v1.11.0 -\u003e localregistry:5000/csi-operator/dell-csi-operator:v1.11.0 changing: dellemc/csi-isilon:v2.3.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.3.0 changing: dellemc/csi-isilon:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.4.0 changing: dellemc/csi-isilon:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.5.0 changing: dellemc/csipowermax-reverseproxy:v2.4.0 -\u003e localregistry:5000/csi-operator/csipowermax-reverseproxy:v2.4.0 changing: dellemc/csi-powermax:v2.3.1 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.3.1 changing: dellemc/csi-powermax:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.4.0 changing: dellemc/csi-powermax:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.5.0 changing: dellemc/csi-powerstore:v2.3.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.3.0 changing: dellemc/csi-powerstore:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.4.0 changing: dellemc/csi-powerstore:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.5.0 changing: dellemc/csi-unity:v2.3.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.3.0 changing: dellemc/csi-unity:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.4.0 changing: dellemc/csi-unity:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.5.0 changing: dellemc/csi-vxflexos:v2.4.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.4.0 changing: dellemc/csi-vxflexos:v2.5.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.5.0 changing: dellemc/csi-vxflexos:v2.6.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.6.0 changing: dellemc/sdc:3.5.1.1-1 -\u003e localregistry:5000/csi-operator/sdc:3.5.1.1-1 changing: dellemc/sdc:3.6 -\u003e localregistry:5000/csi-operator/sdc:3.6 changing: dellemc/sdc:3.6.0.6 -\u003e localregistry:5000/csi-operator/sdc:3.6.0.6 changing: dellemc/sdc:3.6.1 -\u003e localregistry:5000/csi-operator/sdc:3.6.1 changing: docker.io/busybox:1.32.0 -\u003e localregistry:5000/csi-operator/busybox:1.32.0 ... ... * * Complete Perform either a Helm installation or Operator installation Now that the required images are available and the Helm Charts/Operator configuration updated, you can proceed by following the usual installation procedure as documented either via Helm or Operator.\nNOTES:\nOffline bundle installation is only supported with manual installs i.e. without using Operator Lifecycle Manager. Installation should be done using the files that are obtained after unpacking the offline bundle (dell-csi-operator-bundle.tar.gz) as the image tags in the manifests are modified to point to the internal registry. Offline bundle installs operator in default namespace via install.sh script. Make sure that the current context in kubeconfig file has the namespace set to default. ","categories":"","description":"Offline Installation of Dell CSI Storage Providers","excerpt":"Offline Installation of Dell CSI Storage Providers","ref":"/csm-docs/v3/csidriver/installation/offline/","tags":"","title":"Offline Installation of Dell CSI Storage Providers"},{"body":" To perform offline upgrade of the driver, please create an offline bundle as mentioned here. Once the bundle is created, please unpack the bundle by following the steps mentioned here. Please use the driver specific upgrade steps to upgrade. ","categories":"","description":"Offline Upgrade of Dell CSI Storage Providers","excerpt":"Offline Upgrade of Dell CSI Storage Providers","ref":"/csm-docs/docs/csidriver/upgradation/drivers/offline/","tags":"","title":"Offline Upgrade of Dell CSI Storage Providers"},{"body":" To perform offline upgrade of the driver, please create an offline bundle as mentioned here. Once the bundle is created, please unpack the bundle by following the steps mentioned here. Please use the driver specific upgrade steps to upgrade. ","categories":"","description":"Offline Upgrade of Dell CSI Storage Providers","excerpt":"Offline Upgrade of Dell CSI Storage Providers","ref":"/csm-docs/v1/csidriver/upgradation/drivers/offline/","tags":"","title":"Offline Upgrade of Dell CSI Storage Providers"},{"body":" To perform offline upgrade of the driver, please create an offline bundle as mentioned here. Once the bundle is created, please unpack the bundle by following the steps mentioned here. Please use the driver specific upgrade steps to upgrade. ","categories":"","description":"Offline Upgrade of Dell CSI Storage Providers","excerpt":"Offline Upgrade of Dell CSI Storage Providers","ref":"/csm-docs/v2/csidriver/upgradation/drivers/offline/","tags":"","title":"Offline Upgrade of Dell CSI Storage Providers"},{"body":" To perform offline upgrade of the driver, please create an offline bundle as mentioned here. Once the bundle is created, please unpack the bundle by following the steps mentioned here. Please use the driver specific upgrade steps to upgrade. ","categories":"","description":"Offline Upgrade of Dell CSI Storage Providers","excerpt":"Offline Upgrade of Dell CSI Storage Providers","ref":"/csm-docs/v3/csidriver/upgradation/drivers/offline/","tags":"","title":"Offline Upgrade of Dell CSI Storage Providers"},{"body":"The CSM Authorization module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nTo install CSM Authorization via the Dell CSM Operator, follow the instructions here.\nTo enable CSM Authorization with a supported Dell CSI Driver, follow the configuration steps for the relevant driver here.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization Operator deployment\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/authorization/deployment/operator/","tags":"","title":"Operator"},{"body":"The CSM Observability module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nTo install CSM Observability via the Dell CSM Operator, follow the instructions here.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Observability Operator deployment\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/observability/deployment/operator/","tags":"","title":"Operator"},{"body":"The CSM Resiliency module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nTo install CSM Resiliency via the Dell CSM Operator, follow the instructions here.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Resiliency Operator deployment\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/resiliency/deployment/operator/","tags":"","title":"Operator"},{"body":"The CSM Authorization module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nTo install CSM Authorization via the Dell CSM Operator, follow the instructions here.\nTo enable CSM Authorization with a supported Dell CSI Driver, follow the configuration steps for the relevant driver here.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization Operator deployment\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v1/authorization/deployment/operator/","tags":"","title":"Operator"},{"body":"Release Notes - Dell CSI Operator 1.12.0 The Dell CSI Operator is no longer actively maintained or supported. Dell CSI Operator has been replaced with Dell CSM Operator. If you are currently using Dell CSI Operator, refer to the operator migration documentation to migrate from Dell CSI Operator to Dell CSM Operator.\nNew Features/Changes Added support to Kubernetes 1.27 Added support to Openshift 4.12 Added Storage Capacity Tracking support for CSI-PowerScale Migrated image registry from k8s.gcr.io to registry.k8s.io Allow user to set Quota limit parameters from the PVC request in CSI PowerScale Note: There will be a delay in certification of Dell CSI Operator 1.12.0 and it will not be available for download from the Red Hat OpenShift certified catalog right away. The operator will still be available for download from the Red Hat OpenShift Community Catalog soon after the 1.12.0 release.\nFixed Issues CHAP is set to true in the CSI-PowerStore sample file in CSI Operator Vsphere credentials for vsphere secrets is expected when vsphere enable is set to false in CSI PowerMax Known Issues There are no known issues in this release.\nSupport The Dell CSI Operator image is available on Docker Hub and is officially supported by Dell. For any CSI operator and driver issues, questions or feedback, please follow our support process.\n","categories":"","description":"Release notes for Dell CSI Operator","excerpt":"Release notes for Dell CSI Operator","ref":"/csm-docs/v1/csidriver/release/operator/","tags":"","title":"Operator"},{"body":"The CSM Authorization module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nTo install CSM Authorization via the Dell CSM Operator, follow the instructions here.\nTo enable CSM Authorization with a supported Dell CSI Driver, follow the configuration steps for the relevant driver here.\n","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization Operator deployment\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v2/authorization/deployment/operator/","tags":"","title":"Operator"},{"body":"Release Notes - Dell CSI Operator 1.12.0 The Dell CSI Operator is no longer actively maintained or supported. Dell CSI Operator has been replaced with Dell CSM Operator. If you are currently using Dell CSI Operator, refer to the operator migration documentation to migrate from Dell CSI Operator to Dell CSM Operator.\nNew Features/Changes Added support to Kubernetes 1.27 Added support to Openshift 4.12 Added Storage Capacity Tracking support for CSI-PowerScale Migrated image registry from k8s.gcr.io to registry.k8s.io Allow user to set Quota limit parameters from the PVC request in CSI PowerScale Note: There will be a delay in certification of Dell CSI Operator 1.12.0 and it will not be available for download from the Red Hat OpenShift certified catalog right away. The operator will still be available for download from the Red Hat OpenShift Community Catalog soon after the 1.12.0 release.\nFixed Issues CHAP is set to true in the CSI-PowerStore sample file in CSI Operator Vsphere credentials for vsphere secrets is expected when vsphere enable is set to false in CSI PowerMax Known Issues There are no known issues in this release.\nSupport The Dell CSI Operator image is available on Docker Hub and is officially supported by Dell. For any CSI operator and driver issues, questions or feedback, please follow our support process.\n","categories":"","description":"Release notes for Dell CSI Operator","excerpt":"Release notes for Dell CSI Operator","ref":"/csm-docs/v2/csidriver/release/operator/","tags":"","title":"Operator"},{"body":"Release Notes - Dell CSI Operator 1.11.0 New Features/Changes Added support to Kubernetes 1.26 Added pre-approved GUIDs support for PowerFlex Updated Go version from 1.19 to 1.20 Note: There will be a delay in certification of Dell CSI Operator 1.11.0 and it will not be available for download from the Red Hat OpenShift certified catalog right away. The operator will still be available for download from the Red Hat OpenShift Community Catalog soon after the 1.11.0 release.\nFixed Issues Updated Powermax environment variables name for consistency Updated PowerMax vCenter to use secrets for its credentials Known Issues There are no known issues in this release.\nSupport The Dell CSI Operator image is available on Docker Hub and is officially supported by Dell. For any CSI operator and driver issues, questions or feedback, please follow our support process.\n","categories":"","description":"Release notes for Dell CSI Operator","excerpt":"Release notes for Dell CSI Operator","ref":"/csm-docs/v3/csidriver/release/operator/","tags":"","title":"Operator"},{"body":"Configuring PowerFlex CSI Driver with CSM for Authorization Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow these steps to configure the CSI Drivers to work with the Authorization sidecar:\nApply the secret containing the tenant token data into the driver namespace. It’s assumed that the Kubernetes administrator has the token secret manifest, generated by your storage administrator via Generate a Token, saved in /tmp/token.yaml.\n#It is assumed that array type powerflex has the namepace “vxflexos”.\nkubectl apply -f /tmp/token.yaml -n vxflexos Edit these parameters in samples/secret/karavi-authorization-config.json file in the CSI PowerFlex driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\nParameter Description Required Default username Username for connecting to the backend storage array. This parameter is ignored. No - password Password for connecting to to the backend storage array. This parameter is ignored. No - intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes - endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400 systemID System ID of the backend storage array. Yes \" \" skipCertificateValidation A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml Create the karavi-authorization-config secret using this command:\nkubectl -n vxflexos create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f - Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n vxflexos create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f - Otherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n vxflexos create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f - Prepare the driver configuration secret, applicable to your driver installation method, to communicate with the CSM Authorization sidecar.\nHelm\nRefer to the Install the Driver section to edit the parameters in samples/config.yaml to configure the driver to communicate with the CSM Authorization sidecar.\nUpdate endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate skipCertificateValidation to true.\nThe username and password can be any value since they will be ignored.\nExample:\n- username: \"ignored\" password: \"ignored\" systemID: \"ID2\" endpoint: \"https://localhost:9400\" skipCertificateValidation: true isDefault: true mdm: \"10.0.0.3,10.0.0.4\" Operator\nRefer to the Create Secret section to prepare config.yaml to configure the driver to communicate with the CSM Authorization sidecar.\nUpdate endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate skipCertificateValidation to true.\nThe username and password can be any value since they will be ignored.\nExample:\n- username: \"ignored\" password: \"ignored\" systemID: \"ID2\" endpoint: \"https://localhost:9400\" skipCertificateValidation: true isDefault: true mdm: \"10.0.0.3,10.0.0.4\" Enable CSM Authorization in the driver installation applicable to your installation method.\nHelm\nRefer to the Install the Driver section to edit the parameters in myvalues.yaml to enable CSM Authorization.\nUpdate authorization.enabled to true.\nUpdate images.authorization to the image of the CSM Authorization sidecar. In most cases, you can leave the default value.\nUpdate authorization.proxyHost to the hostname of the CSM Authorization Proxy Server.\nUpdate authorization.skipCertificateValidation to true or false depending on if you want to disable or enable certificate validation of the CSM Authorization Proxy Server.\nExample:\nauthorization: enabled: true # sidecarProxyImage: the container image used for the csm-authorization-sidecar. # Default value: dellemc/csm-authorization-sidecar:v1.9.0 sidecarProxyImage: dellemc/csm-authorization-sidecar:v1.9.0 # proxyHost: hostname of the csm-authorization server # Default value: None proxyHost: csm-authorization.com # skipCertificateValidation: certificate validation of the csm-authorization server # Allowed Values: # \"true\" - TLS certificate verification will be skipped # \"false\" - TLS certificate will be verified # Default value: \"true\" skipCertificateValidation: true Operator\nRefer to the Install Driver section to edit the parameters in the Custom Resource to enable CSM Authorization.\nUnder modules, enable the module named authorization:\nUpdate the enabled field to true.\nUpdate the image to the image of the CSM Authorization sidecar. In most cases, you can leave the default value.\nUpdate the PROXY_HOST environment value to the hostname of the CSM Authorization Proxy Server.\nUpdate the SKIP_CERTIFICATE_VALIDATION environment value to true or false depending on if you want to disable or enable certificate validation of the CSM Authorization Proxy Server.\nExample:\nmodules: # Authorization: enable csm-authorization for RBAC - name: authorization # enable: Enable/Disable csm-authorization enabled: true configVersion: v1.9.0 components: - name: karavi-authorization-proxy image: dellemc/csm-authorization-sidecar:v1.9.0 envs: # proxyHost: hostname of the csm-authorization server - name: \"PROXY_HOST\" value: \"csm-authorization.com\" # skipCertificateValidation: Enable/Disable certificate validation of the csm-authorization server - name: \"SKIP_CERTIFICATE_VALIDATION\" value: \"true\" Install the Dell CSI PowerFlex driver following the appropriate documenation for your installation method.\n(Optional) Install dellctl to perform Kubernetes administrator commands for additional capabilities (e.g., list volumes). Please refer to the dellctl documentation page for the installation steps and command list.\n","categories":"","description":"Enabling CSM Authorization for PowerFlex CSI Driver\n","excerpt":"Enabling CSM Authorization for PowerFlex CSI Driver\n","ref":"/csm-docs/docs/authorization/configuration/powerflex/","tags":"","title":"PowerFlex"},{"body":"The CSI Driver for Dell PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nPrerequisites The following are requirements that must be met before installing the CSI Driver for Dell PowerFlex:\nInstall Kubernetes or OpenShift (see supported versions) Install Helm 3.x Enable Zero Padding on PowerFlex Mount propagation is enabled on container runtime that is being used Install PowerFlex Storage Data Client If using Snapshot feature, satisfy all Volume Snapshot requirements A user must exist on the array with a role \u003e= FrontEndConfigure If enabling CSM for Authorization, please refer to the Authorization deployment steps first If multipath is configured, ensure CSI-PowerFlex volumes are blacklisted by multipathd. See troubleshooting section for details Install Helm 3.x Install Helm 3.x on the master node before you install the CSI Driver for Dell PowerFlex.\nSteps\nRun the command to install Helm 3.x.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Enable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. For more information to configure this setting, see Dell PowerFlex documentation.\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run the node portion of the CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment; for Red Hat CoreOS (RHCOS), RHEL 7.9, RHEL 8.6. On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions.\nNOTE: To install CSI driver for Powerflex with automated SDC deployment, you need below two packages on worker nodes.\nlibaio numactl-libs Optional: For a typical install, you will pull SDC kernel modules from the Dell FTP site, which is set up by default. Some users might want to mirror this repository to a local location. The PowerFlex KB article has instructions on how to do this.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\nDownload the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide: For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version. To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx Installation Wizard prerequisite, secret update: When the driver is installed using values generated by installation wizard, then the user needs to update the secret for driver by patching the MDM keys, as follows:\nSteps\necho -n '\u003cMDM_IPS\u003e' | base64 kubectl patch secret vxflexos-config -n vxflexos -p \"{\\\"data\\\": { \\\"MDM\\\": \\\"\u003cGENERATED_BASE64\u003e\\\"}}\" (Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\nInstall the Driver Steps\nRun git clone -b v2.9.2 https://github.com/dell/csi-powerflex.git to clone the git repository.\nA namespace for the driver is expected prior to running the command below. If one is not created already, you can run kubectl create namespace vxflexos to create a new one. Note that the namespace can be any user-defined name that follows the conventions for namespaces outlined by Kubernetes. In this example we assume that the namespace is ‘vxflexos’\nCollect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the scripts directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the values for these parameters as they must be entered into samples/secret.yaml.\nPrepare samples/secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\nParameter Description Required Default username Username for accessing PowerFlex system. If authorization is enabled, username will be ignored. true - password Password for accessing PowerFlex system. If authorization is enabled, password will be ignored. true - systemID PowerFlex system name or ID. true - allSystemNames List of previous names of powerflex array if used for PV create false - endpoint REST API gateway HTTPS endpoint/PowerFlex Manager public IP for PowerFlex system. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on true - skipCertificateValidation Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. true true isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list. false false mdm mdm defines the MDM(s) that SDC should register with on start. This should be a list of MDM IP addresses or hostnames separated by comma. true - nasName nasName defines what NAS should be used for NFS volumes. NFS volumes are supported on arrays version 4.0.x false none Example: samples/secret.yaml\n- username: \"admin\" password: \"Password123\" systemID: \"2b11bb111111bb1b\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true isDefault: true mdm: \"10.0.0.3,10.0.0.4\" Example: samples/secret.yaml for PowerFlex storage system v4.0.x\n- username: \"admin\" password: \"Password123\" systemID: \"2b11bb111111bb1b\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true isDefault: true mdm: \"10.0.0.3,10.0.0.4\" nasName : \"nasServer\" NOTE: To use multiple arrays, copy and paste section above for each array. Make sure isDefault is set to true for only one array.\nIf replication feature is enabled, ensure the secret includes all the PowerFlex arrays involved in replication.\nAfter editing the file, run the below command to create a secret called vxflexos-config. This assumes vxflexos is release name, but it can be modified during install:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/secret.yaml Use the below command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/secret.yaml -o yaml --dry-run=client | kubectl replace -f - NOTE:\nThe user needs to validate the YAML syntax and array-related key/values while replacing the vxflexos-creds secret. If you want to create a new array or update the MDM values in the secret, you will need to reinstall the driver. If you change other details, such as login information, the secret will dynamically update – see dynamic-array-configuration for more details. Old json format of the array configuration file is still supported in this release. If you already have your configuration in json format, you may continue to maintain it or you may transfer this configuration to yamlformat and replace/update the secret. “insecure” parameter has been changed to “skipCertificateValidation” as insecure is deprecated and will be removed from use in config.yaml or secret.yaml in a future release. Users can continue to use any one of “insecure” or “skipCertificateValidation” for now. The driver would return an error if both parameters are used. Please note that log configuration parameters from v1.5 will no longer work in v2.0 and higher. Please refer to the Dynamic Logging Configuration section in Features for more information. If the user is using complex K8s version like “v1.21.3-mirantis-1”, use this kubeVersion check in helm/csi-unity/Chart.yaml file. kubeVersion: “\u003e= 1.21.0-0 \u003c 1.29.0-0” Default logging options are set during Helm install. To see possible configuration options, see the Dynamic Logging Configuration section in Features.\nIf using automated SDC deployment:\nCheck the SDC container image is the correct version for your version of PowerFlex. Download the default values.yaml file\ncd dell-csi-helm-installer \u0026\u0026 wget -O myvalues.yaml https://github.com/dell/helm-charts/raw/csi-vxflexos-2.9.2/charts/csi-vxflexos/values.yaml If you are using custom images, check the fields under images in my-vxflexos-settings.yaml to make sure that they are pointing to the correct image repository.\nLook over all the other fields myvalues.yaml and fill in/adjust any as needed. All the fields are described here:\nParameter Description Required Default version Set to verify the values file version matches driver version and used to pull the image as part of the image name. Yes 2.9.2 images List all the images used by the CSI driver and CSM. If you use a private repository, change the registries accordingly. Yes \"\" images.powerflexSdc Set to give the location of the SDC image used if automatic SDC deployment is being utilized. Yes dellemc/sdc:4.5 certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. No 0 logLevel CSI driver log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes “debug” logFormat CSI driver log format. Allowed values: “TEXT” or “JSON”. Yes “TEXT” kubeletConfigDir kubelet config directory path. Ensure that the secret.yaml file is present at this path. Yes /var/lib/kubelet defaultFsType Used to set the default FS type which will be used for mount volumes if FsType is not specified in the storage class. Allowed values: ext4, xfs. Yes ext4 fsGroupPolicy Defines which FS Group policy mode to be used. Supported modes areNone, File, and ReadWriteOnceWithFSType. No “ReadWriteOnceWithFSType” imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Allowed values: Always, IfNotPresent, Never. Yes IfNotPresent enablesnapshotcgdelete A boolean that, when enabled, will delete all snapshots in a consistency group everytime a snap in the group is deleted. Yes false enablelistvolumesnapshot A boolean that, when enabled, will allow list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap). It is recommend this be false unless instructed otherwise. Yes false allowRWOMultiPodAccess Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. Yes false enableQuota A boolean that, when enabled, will set quota limit for a newly provisioned NFS volume. No false externalAccess Defines additional entries for hostAccess of NFS volumes, single IP address and subnet are valid entries No \" \" controller This section allows the configuration of controller-specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - - volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. Yes “k8s” controllerCount Set to deploy multiple controller instances. If the controller count is greater than the number of available nodes, excess pods remain in a pending state. It should be greater than 0. You can increase the number of available nodes by configuring the “controller” section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2 snapshot.enabled A boolean that enable/disable volume snapshot feature. No true resizer.enabled A boolean that enable/disable volume expansion feature. No true nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. Yes \" \" tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install the controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. Yes \" \" healthMonitor This section configures the optional deployment of the external health monitor sidecar, for controller side volume health monitoring. - - enabled Enable/Disable deployment of external health monitor sidecar. No false interval Interval of monitoring volume health condition. Allowed values: Number followed by unit (s,m,h) No 60s node This section allows the configuration of node-specific parameters. - - healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition No false nodeSelector Defines what nodes would be selected for pods of node daemonset. Leave as blank to use all nodes. Yes \" \" tolerations Defines tolerations that would be applied to node daemonset. Leave as blank to install node driver only on worker nodes. Yes \" \" renameSDC This section allows the rename operation for SDC. - - enabled A boolean that enable/disable rename SDC feature. No false prefix Defines a string for the prefix of the SDC. No \" \" approveSDC.enabled A boolean that enable/disable SDC approval feature. No false storageCapacity Enable/Disable storage capacity tracking - - enabled A boolean that enables/disables storage capacity tracking feature. Yes true pollInterval Configure how often the driver checks for changed capacity No 5m monitor This section allows the configuration of the SDC monitoring pod. - - enabled Set to enable the usage of the monitoring pod. Yes false hostNetwork Set whether the monitor pod should run on the host network or not. Yes true hostPID Set whether the monitor pod should run in the host namespace or not. Yes true vgsnapshotter This section allows the configuration of the volume group snapshotter(vgsnapshotter) pod. - - enabled A boolean that enable/disable vg snapshotter feature. No false image Image for vg snapshotter. No \" \" podmon Podmon is an optional feature to enable application pods to be resilient to node failure. - - enabled A boolean that enables/disables podmon feature. No false authorization Authorization is an optional feature to apply credential shielding of the backend PowerFlex. - - enabled A boolean that enables/disables authorization feature. No false proxyHost Hostname of the csm-authorization server. No Empty skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization proxy server. No true Install the driver using csi-install.sh bash script by running cd dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values myvalues.yaml. You may modify the release name with the --release arg. If arg is not provided, release will be named vxflexos by default. Alternatively, to do a helm install solely with Helm charts (without shell scripts), refer to helm/README.md. NOTE:\nFor detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder.\nInstall script will validate MDM IP(s) in vxflexos-config secret and creates a new field consumed by the init container and sdc-monitor container\nThis install script also runs the verify.sh script. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes.\nIt is mandatory to run install script after changes to MDM configuration in vxflexos-config secret. Refer dynamic-array-configuration\nIf an extended Kubernetes version is being used (e.g. v1.21.3-mirantis-1) and is failing the version check in Helm even though it falls in the allowed range, then you must go into helm/csi-vxflexos/Chart.yaml and replace the standard kubeVersion check with the commented-out alternative. Please note that this will also allow the use of pre-release alpha and beta versions of Kubernetes, which is not supported.\n(Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.\nMount options are specified in storageclass yaml under mkfsFormatOption. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option. Certificate validation for PowerFlex Gateway REST API calls This topic provides details about setting up the certificate for the CSI Driver for Dell PowerFlex.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name vxflexos-certs-0 to vxflexos-certs-n based on the “.Values.certSecretCount” parameter present in the namespace vxflexos.\nThis secret contains the X509 certificates of the CA which signed PowerFlex gateway SSL certificate in PEM format.\nThe CSI driver exposes an install parameter in secret.yaml, skipCertificateValidation, which determines if the driver performs client-side verification of the gateway certificates.\nskipCertificateValidation parameter is set to true by default, and the driver does not verify the gateway certificates.\nIf skipCertificateValidation is set to false, then the secret vxflexos-certs-n must contain the CA certificate for the array gateway.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the gateway certificate is self-signed or if you are using an embedded gateway, then perform the following steps.\nTo fetch the certificate, run the following command.\nopenssl s_client -showcerts -connect \u003cGateway IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example:\nopenssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Run the following command to create the cert secret with index ‘0’:\nkubectl create secret generic vxflexos-certs-0 --from-file=cert-0=ca_cert_0.pem -n vxflexos Use the following command to replace the secret:\nkubectl create secret generic vxflexos-certs-0 -n vxflexos --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: vxflexos-certs-1, vxflexos-certs-2, etc)\nNotes:\n“vxflexos” is the namespace for Helm-based installation but namespace can be user-defined in operator-based installation. User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation. Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver. Updating vxflexos-certs-n secrets is a manual process, unlike vxflexos-config. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter. Storage Classes For CSI driver for PowerFlex version 1.4 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class: There are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\nEdit storageclass.yaml if you need ext4 filesystem, storageclass-xfs.yaml if you want xfs filesystem and storageclass-nfs.yaml if you need nfs filesystem Replace \u003cSTORAGE_POOL\u003e with the storage pool you have. Replace \u003cSYSTEM_ID\u003e with the system ID you have. Note there are two appearances in the file. Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. If using storageclass-nfs.yaml Replace \"nas-server\" with the NAS server’s name you have. Save the file and create it by using kubectl create -f storageclass.yaml / kubectl create -f storageclass-xfs.yaml/ kubectl create -f storageclass-nfs.yaml NOTE:\nAt least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the myvalues.yaml file leads to an update of the storage class(es): Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerFlex v1.5, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\n","categories":"","description":"Installing the CSI Driver for PowerFlex via Helm\n","excerpt":"Installing the CSI Driver for PowerFlex via Helm\n","ref":"/csm-docs/docs/csidriver/installation/helm/powerflex/","tags":"","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\nNavigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following: Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \"docker.io/centos:latest\" Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured. Results\nAn outline of this workflow is described below:\nThe 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below: kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol namespace: helmtest-vxflexos spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: vxflexos The volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use a storage class named vxflexos. This step yields a mounted ext4 file system. You can create the vxflexos and vxflexos-xfs storage classes by using the yamls located in samples/storageclass. If you compare pvol0.yaml and pvol1.yaml, you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos. NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.\nTest creating snapshots Test the workflow for snapshot creation.\nNOTE: Starting with version 2.0, CSI Driver for PowerFlex helm tests are designed to work exclusively with v1 snapshots.\nSteps\nStart the 2vols container and leave it running. Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly. Run sh snaptest.sh to start the test. This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in snap1.yaml and snap2.yaml. The following are the contents of snap1.yaml:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Results\nThe snaptest.sh script will create a snapshot using the definitions in the snap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0, then the created snapshot is named pvol0-snap1.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n helmtest-vxflexos.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell PowerFlex installation does not create this class. You will need to create instance of VolumeSnapshotClass from one of default samples in `samples/volumesnapshotclass’ directory.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\nRun sh snaprestoretest.sh to start the test. This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updated directory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\nHelm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly. Results\nAn outline of this workflow is described below:\nThe snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim, which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap1 kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi NOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in snap1.yaml.\nTest creating NFS volumes Steps\nNavigate to the test/helm directory, which contains the starttest.sh and the 1vol-nfs directories. This directory contains a simple Helm chart that will deploy a pod that uses one PowerFlex volumes for NFS filesystem type. NOTE:\nHelm tests are designed assuming users are using the storageclass name: vxflexos-nfs. If your storageclass names differ from these values, please update the templates in 1vol-nfs accordingly (located in test/helm/1vol-nfs/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 1vol-nfs to deploy the pod. You should see the following: Normal Scheduled default-scheduler, Successfully assigned helmtest-vxflexos/vxflextest-0 to worker-1-zwfjtd4eoblkg.domain Normal SuccessfulAttachVolume attachdetach-controller, AttachVolume.Attach succeeded for volume \"k8s-e279d47296\" Normal Pulled 13s kubelet, Successfully pulled image \"docker.io/centos:latest\" in 791.117427ms (791.125522ms including waiting) Normal Created 13s kubelet, Created container test Normal Started 13s kubelet, Started container test 10.x.x.x:/k8s-e279d47296 8388608 1582336 6806272 19% /data0 10.x.x.x:/k8s-e279d47296 on /data0 type nfs4 (rw,relatime,vers=4.2,rsize=262144,wsize=262144,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.x.x.x,local_lock=none,addr=10.x.x.x) To stop the test, run sh stoptest.sh 1vol-nfs. This script deletes the pods and the volumes depending on the retention setting you have configured. Results\nAn outline of this workflow is described below:\nThe 1vol-nfs helm chart contains one PersistentVolumeClaim definition in pvc0.yaml. It is referenced by the test.yaml which creates the pod. The contents of the pvc0.yaml file are described below: kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: vxflexos-nfs The volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos-nfs directs the system to use a storage class named vxflexos-nfs. This step yields a mounted nfs file system. You can create the vxflexos-nfs storage classes by using the yaml located in samples/storageclass. To see the volumes you created, run kubectl get persistentvolumeclaim -n helmtest-vxflexos and kubectl describe persistentvolumeclaim -n helmtest-vxflexos. NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.\nTest restoring NFS volume from snapshot Test the restore operation workflow to restore NFS volume from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\nRun sh snaprestoretest-nfs.sh to start the test. This script deploys the 1vol-nfs example, creates a snap of pvol0, and then updates the deployed helm chart from the updated directory 1vols+restore-nfs. This adds an additional volume that is created from the snapshot.\nNOTE:\nHelm tests are designed assuming users are using the storageclass name: vxflexos-nfs. If your storageclass names differ from these values, update the templates for 1vols+restore-nfs accordingly (located in test/helm/1vols+restore-nfs/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml accordingly. Results\nAn outline of this workflow is described below:\nThe snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 1vols+restore-nfs directory. The csi-vxflexos/test/helm/1vols+restore-nfs/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim, which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest-nfs.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos-nfs dataSource: name: pvol0-snap1 kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi NOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in snap1.yaml.\n","categories":"","description":"Tests to validate PowerFlex CSI Driver installation","excerpt":"Tests to validate PowerFlex CSI Driver installation","ref":"/csm-docs/docs/csidriver/installation/test/powerflex/","tags":"","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v2.9.2 New Features/Changes #947 - [FEATURE]: Support for Kubernetes 1.28 #1066 - [FEATURE]: Support for Openshift 4.14 #1067 - [FEATURE]: Support For PowerFlex 4.5 #851 - [FEATURE]: Helm Chart Enhancement - Container Images Configurable in values.yaml #905 - [FEATURE]: Add support for CSI Spec 1.6 #996 - [FEATURE]: Dell CSI to Dell CSM Operator Migration Process Fixed Issues #1011 - [BUG]: PowerFlex RWX volume no option to configure the nfs export host access ip address. #1014 - [BUG]: Missing error check for os.Stat call during volume publish #1020 - [BUG]: CSI-PowerFlex: SDC Rename fails when configuring multiple arrays in the secret #1030 - [BUG]: Comment out duplicate entries in the sample secret.yaml file #1050 - [BUG]: NFS Export gets deleted when one pod is deleted from the multiple pods consuming the same PowerFlex RWX NFS volume #1054 - [BUG]: The PowerFlex Dockerfile is incorrectly labeling the version as 2.7.0 for the 2.8.0 version. #1057 - [BUG]: CSI Driver - issue with creation volume from 1 of the worker nodes #1058 - [BUG]: CSI Health monitor for Node missing for CSM PowerFlex in Operator samples #1061 - [BUG]: Golint is not installing with go get command #1110 - [BUG]: Multi Controller defect - sidecars timeout #1103 - [BUG]: CSM Operator doesn’t apply fSGroupPolicy value to CSIDriver Object #1152 - [BUG]: CSI driver changes to facilitate SDC brownfield deployments) Known Issues Issue Workaround Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100 When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. sdc:3.6.0.6 is causing issues while installing the csi-powerflex driver on ubuntu,RHEL8.3 Workaround: Change the powerflexSdc to sdc:3.6 in values.yaml https://github.com/dell/csi-powerflex/blob/72b27acee7553006cc09df97f85405f58478d2e4/helm/csi-vxflexos/values.yaml#L13 sdc:3.6.1 is causing issues while installing the csi-powerflex driver on ubuntu. Workaround: Change the powerflexSdc to sdc:3.6 in values.yaml https://github.com/dell/csi-powerflex/blob/72b27acee7553006cc09df97f85405f58478d2e4/helm/csi-vxflexos/values.yaml#L13 A CSI ephemeral pod may not get created in OpenShift 4.13 and fail with the error \"error when creating pod: the pod uses an inline volume provided by CSIDriver csi-vxflexos.dellemc.com, and the namespace has a pod security enforcement level that is lower than privileged.\" This issue occurs because OpenShift 4.13 introduced the CSI Volume Admission plugin to restrict the use of a CSI driver capable of provisioning CSI ephemeral volumes during pod admission. Therefore, an additional label security.openshift.io/csi-ephemeral-volume-profile in csidriver.yaml file with the required security profile value should be provided. Follow OpenShift 4.13 documentation for CSI Ephemeral Volumes for more information. If the volume limit is exhausted and there are pending pods and PVCs due to exceed max volume count, the pending PVCs will be bound to PVs and the pending pods will be scheduled to nodes when the driver pods are restarted. It is advised not to have any pending pods or PVCs once the volume limit per node is exhausted on a CSI Driver. There is an open issue reported with kubenetes at https://github.com/kubernetes/kubernetes/issues/95911 with the same behavior. The PowerFlex Dockerfile is incorrectly labeling the version as 2.7.0 for the 2.8.0 version. Describe the driver pod using kubectl describe pod $podname -n vxflexos to ensure v2.8.0 is installed. Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for PowerFlex CSI driver","excerpt":"Release notes for PowerFlex CSI driver","ref":"/csm-docs/docs/csidriver/release/powerflex/","tags":"","title":"PowerFlex"},{"body":" Symptoms Prevention, Resolution or Workaround The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that need to pull an image of the driver. When you run the command kubectl describe pods vxflexos-controller-* –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command. The kubectl logs -n vxflexos vxflexos-controller-* driver logs show that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system. The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script. CreateVolume error System is not configured in the driver Powerflex name if used for systemID in StorageClass ensure same name is also used in array config systemID Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on a worker node, and ensure your container run time manager is properly configured to be utilized with SElinux. Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux. Installation of the driver on Kubernetes v1.25/v1.26/v1.27 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.23/v1.24/v1.25 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements The kubectl logs -n vxflexos vxflexos-controller-* driver logs show x509: certificate signed by unknown authority A self assigned certificate is used for PowerFlex array. See certificate validation for PowerFlex Gateway When you run the command kubectl apply -f snapclass-v1.yaml, you get the error error: unable to recognize \"snapclass-v1.yaml\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Check to make sure that the v1 snapshotter CRDs are installed, and not the v1beta1 CRDs, which are no longer supported. The controller pod is stuck and producing errors such as\" Failed to watch *v1.VolumeSnapshotContent: failed to list *v1.VolumeSnapshotContent: the server could not find the requested resource (get volumesnapshotcontents.snapshot.storage.k8s.io) Make sure that v1 snapshotter CRDs and v1 snapclass are installed, and not v1beta1, which is no longer supported. Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.21.0 \u003c= 1.28.0 which is incompatible with Kubernetes V1.21.11-mirantis-1 If you are using an extended Kubernetes version, see the helm Chart at helm/csi-vxflexos/Chart.yaml and use the alternate kubeVersion check that is provided in the comments. Note: this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. Volume metrics are missing Enable Volume Health Monitoring When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. CSI-PowerFlex volumes cannot mount; are being recognized as multipath devices CSI-PowerFlex does not support multipath; to fix: 1. Remove any multipath mapping involving a powerflex volume with multipath -f \u003cpowerflex volume\u003e 2. Blacklist CSI-PowerFlex volumes in multipath config file When attempting a driver upgrade, you see: spec.fsGroupPolicy: Invalid value: \"xxx\": field is immutable You cannot upgrade between drivers with different fsGroupPolicies. See upgrade documentation for more details When accessing ROX mode PVC in OpenShift where the worker nodes are non-root user, you see: Permission denied while accesing the PVC mount location from the pod. Set the securityContext for ROX mode PVC pod as below, as it defines privileges for the pods or containers.securityContext: runAsUser: 0 runAsGroup: 0 After installing version v2.6.0 of the driver using the default powerflexSdc image, sdc:3.6.0.6, the vxflexos-node pods are in an Init:CrashLoopBackOff state. This issue can happen on hosts that require the SDC to be installed manually. Automatic SDC is only supported on Red Hat CoreOS (RHCOS), RHEL 7.9, RHEL 8.4, RHEL 8.6. The SDC is already installed. Change the images.powerflexSdc value to an empty value in the values and re-install. After installing version v2.8.0 of the driver using the default powerflexSdc image, sdc:3.6.1, the vxflexos-node pods are in an Init:CrashLoopBackOff state. This issue can happen on hosts that require the SDC to be installed manually. Automatic SDC is only supported on Red Hat CoreOS (RHCOS), RHEL 7.9, RHEL 8.4, RHEL 8.6. The SDC is already installed. Change the images.powerflexSdc value to an empty value in the values and re-install. In version v2.6.0, the driver is crashing because the External Health Monitor sidecar crashes when a persistent volume is not found. This is a known issue reported at kubernetes-csi/external-health-monitor#100. In version v2.6.0, when a cluster node goes down, the block volumes attached to the node cannot be attached to another node. This is a known issue reported at kubernetes-csi/external-attacher#215. Workaround: 1. Force delete the pod running on the node that went down. 2. Delete the pod’s persistent volume attachment on the node that went down. Now the volume can be attached to the new node. A CSI ephemeral pod may not get created in OpenShift 4.13 and fail with the error \"error when creating pod: the pod uses an inline volume provided by CSIDriver csi-vxflexos.dellemc.com, and the namespace has a pod security enforcement level that is lower than privileged.\" This issue occurs because OpenShift 4.13 introduced the CSI Volume Admission plugin to restrict the use of a CSI driver capable of provisioning CSI ephemeral volumes during pod admission. Therefore, an additional label security.openshift.io/csi-ephemeral-volume-profile in csidriver.yaml file with the required security profile value should be provided. Follow OpenShift 4.13 documentation for CSI Ephemeral Volumes for more information. Standby controller pod is in crashloopbackoff state Scale down the replica count of the controller pod’s deployment to 1 using kubectl scale deployment \u003cdeployment_name\u003e --replicas=1 -n \u003cdriver_namespace\u003e ","categories":"","description":"Troubleshooting PowerFlex Driver","excerpt":"Troubleshooting PowerFlex Driver","ref":"/csm-docs/docs/csidriver/troubleshooting/powerflex/","tags":"","title":"PowerFlex"},{"body":"Installing CSI Driver for PowerFlex via Dell CSM Operator The CSI Driver for Dell PowerFlex can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using this command:\nkubectl get csm --all-namespaces Prerequisites If multipath is configured, ensure CSI-PowerFlex volumes are blacklisted by multipathd. See troubleshooting section for details. NOTE: This step can be skipped with OpenShift.\nSDC Deployment for Operator This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non-supported versions of the OS also do the manual SDC deployment steps given below. Refer to https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When the driver is created, MDM value for initContainers in driver CR is set by the operator from mdm attributes in the driver configuration file, config.yaml. An example of config.yaml is below in this document. Do not set MDM value for initContainers in the driver CR file manually. Optionally, enable sdc monitor by setting the enable flag for the sdc-monitor to true. Please note: If using sidecar, you will need to edit the value fields under the HOST_PID and MDM fields by filling the empty quotes with host PID and the MDM IPs. If not using sidecar, leave the enabled field set to false. Example CR: samples/storage_csm_powerflex_v290.yaml sideCars: # sdc-monitor is disabled by default, due to high CPU usage - name: sdc-monitor enabled: false image: dellemc/sdc:4.5 envs: - name: HOST_PID value: \"1\" - name: MDM value: \"10.xx.xx.xx,10.xx.xx.xx\" #provide the same MDM value from secret Manual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC using this procedure:\nSteps\nDownload the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide: For environments using RPM, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version. To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx1. Create namespace. Execute kubectl create namespace vxflexos to create the vxflexos namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘vxflexos’ NOTE: This step can be skipped with OpenShift CoreOS nodes.\nCreate Secret Create namespace: Execute kubectl create namespace vxflexos to create the vxflexos namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘vxflexos’\nPrepare the secret.yaml for driver configuration.\nExample: secret.yaml\n# Username for accessing PowerFlex system. # If authorization is enabled, username will be ignored. - username: \"admin\" # Password for accessing PowerFlex system. # If authorization is enabled, password will be ignored. password: \"password\" # System name/ID of PowerFlex system.\tsystemID: \"1a99aa999999aa9a\" # Previous names used in secret of PowerFlex system. allSystemNames: \"pflex-1,pflex-2\" # REST API gateway HTTPS endpoint for PowerFlex system. # If authorization is enabled, endpoint should be the HTTPS localhost endpoint that # the authorization sidecar will listen on endpoint: \"https://127.0.0.1\" # Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. # Allowed values: true or false # Default value: true skipCertificateValidation: true # indicates if this array is the default array # needed for backwards compatibility # only one array is allowed to have this set to true # Default value: false isDefault: true # defines the MDM(s) that SDC should register with on start. # Allowed values: a list of IP addresses or hostnames separated by comma. # Default value: none mdm: \"10.0.0.1,10.0.0.2\" # NFS is only supported on PowerFlex storage system 4.0.x # nasName: name of NAS server used for NFS volumes # nasName value must be specified in secret for performing NFS (file) operations. # Allowed Values: string # Default Value: \"none\" nasName: \"nas-server\" - username: \"admin\" password: \"Password123\" systemID: \"2b11bb111111bb1b\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true mdm: \"10.0.0.3,10.0.0.4\" If replication feature is enabled, ensure the secret includes all the PowerFlex arrays involved in replication.\nAfter editing the file, run this command to create a secret called vxflexos-config. If you are using a different namespace/secret name, just substitute those into the command.\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=secret.yaml Use this command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml -o yaml --dry-run=client | kubectl replace -f - Install Driver Follow all the prerequisites above\nCreate a CR (Custom Resource) for PowerFlex using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\nParameter Description Required Default dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, excess pods will become stay in a pending state. Defaults are 2 which allows for Controller high availability. Yes 2 storageCapacity.enabled Enable/Disable storage capacity tracking No true storageCapacity.pollInterval Configure how often the driver checks for changed capacity No 5m enableQuota a boolean that, when enabled, will set quota limit for a newly provisioned NFS volume No none maxVxflexosVolumesPerNode Specify default value for maximum number of volumes that controller can publish to the node.If value is zero CO SHALL decide how many volumes of this type can be published by the controller to the node Yes 0 Common parameters for node and controller X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false X_CSI_DEBUG To enable debug mode No true X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false Controller parameters X_CSI_POWERFLEX_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No empty X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition No false Node parameters X_CSI_RENAME_SDC_ENABLED Enable this to rename the SDC with the given prefix. The new name will be (“prefix” + “worker_node_hostname”) and it should not exceed 31 chars. Yes false X_CSI_APPROVE_SDC_ENABLED Enable this to to approve restricted SDC by GUID during setup Yes false X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Node plugin - volume condition No false Execute this command to create PowerFlex custom resource:\nkubectl create -f \u003cinput_sample_file.yaml\u003e This command will deploy the CSI-PowerFlex driver in the namespace specified in the input YAML file.\nVerify the CSI Driver installation\nNote :\nSnapshotter and resizer sidecars are installed by default. ","categories":"","description":"Installing Dell CSI Driver for PowerFlex via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for PowerFlex via Dell CSM Operator\n","ref":"/csm-docs/docs/deployment/csmoperator/drivers/powerflex/","tags":"","title":"PowerFlex"},{"body":"Configuring PowerFlex CSI Driver with CSM for Authorization Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow these steps to configure the CSI Drivers to work with the Authorization sidecar:\nApply the secret containing the tenant token data into the driver namespace. It’s assumed that the Kubernetes administrator has the token secret manifest, generated by your storage administrator via Generate a Token, saved in /tmp/token.yaml.\n#It is assumed that array type powerflex has the namepace “vxflexos”.\nkubectl apply -f /tmp/token.yaml -n vxflexos Edit these parameters in samples/secret/karavi-authorization-config.json file in the CSI PowerFlex driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\nParameter Description Required Default username Username for connecting to the backend storage array. This parameter is ignored. No - password Password for connecting to to the backend storage array. This parameter is ignored. No - intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes - endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400 systemID System ID of the backend storage array. Yes \" \" skipCertificateValidation A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml Create the karavi-authorization-config secret using this command:\nkubectl -n vxflexos create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f - Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n vxflexos create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f - Otherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n vxflexos create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f - Prepare the driver configuration secret, applicable to your driver installation method, to communicate with the CSM Authorization sidecar.\nHelm\nRefer to the Install the Driver section to edit the parameters in samples/config.yaml to configure the driver to communicate with the CSM Authorization sidecar.\nUpdate endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate skipCertificateValidation to true.\nThe username and password can be any value since they will be ignored.\nExample:\n- username: \"ignored\" password: \"ignored\" systemID: \"ID2\" endpoint: \"https://localhost:9400\" skipCertificateValidation: true isDefault: true mdm: \"10.0.0.3,10.0.0.4\" Operator\nRefer to the Create Secret section to prepare config.yaml to configure the driver to communicate with the CSM Authorization sidecar.\nUpdate endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate skipCertificateValidation to true.\nThe username and password can be any value since they will be ignored.\nExample:\n- username: \"ignored\" password: \"ignored\" systemID: \"ID2\" endpoint: \"https://localhost:9400\" skipCertificateValidation: true isDefault: true mdm: \"10.0.0.3,10.0.0.4\" Enable CSM Authorization in the driver installation applicable to your installation method.\nHelm\nRefer to the Install the Driver section to edit the parameters in myvalues.yaml to enable CSM Authorization.\nUpdate authorization.enabled to true.\nUpdate authorization.sidecarProxyImage to the image of the CSM Authorization sidecar. In most cases, you can leave the default value.\nUpdate authorization.proxyHost to the hostname of the CSM Authorization Proxy Server.\nUpdate authorization.skipCertificateValidation to true or false depending on if you want to disable or enable certificate validation of the CSM Authorization Proxy Server.\nExample:\nauthorization: enabled: true # sidecarProxyImage: the container image used for the csm-authorization-sidecar. # Default value: dellemc/csm-authorization-sidecar:v1.8.0 sidecarProxyImage: dellemc/csm-authorization-sidecar:v1.8.0 # proxyHost: hostname of the csm-authorization server # Default value: None proxyHost: csm-authorization.com # skipCertificateValidation: certificate validation of the csm-authorization server # Allowed Values: # \"true\" - TLS certificate verification will be skipped # \"false\" - TLS certificate will be verified # Default value: \"true\" skipCertificateValidation: true Operator\nRefer to the Install Driver section to edit the parameters in the Custom Resource to enable CSM Authorization.\nUnder modules, enable the module named authorization:\nUpdate the enabled field to true.\nUpdate the image to the image of the CSM Authorization sidecar. In most cases, you can leave the default value.\nUpdate the PROXY_HOST environment value to the hostname of the CSM Authorization Proxy Server.\nUpdate the SKIP_CERTIFICATE_VALIDATION environment value to true or false depending on if you want to disable or enable certificate validation of the CSM Authorization Proxy Server.\nExample:\nmodules: # Authorization: enable csm-authorization for RBAC - name: authorization # enable: Enable/Disable csm-authorization enabled: true configVersion: v1.8.0 components: - name: karavi-authorization-proxy image: dellemc/csm-authorization-sidecar:v1.8.0 envs: # proxyHost: hostname of the csm-authorization server - name: \"PROXY_HOST\" value: \"csm-authorization.com\" # skipCertificateValidation: Enable/Disable certificate validation of the csm-authorization server - name: \"SKIP_CERTIFICATE_VALIDATION\" value: \"true\" Install the Dell CSI PowerFlex driver following the appropriate documenation for your installation method.\n(Optional) Install dellctl to perform Kubernetes administrator commands for additional capabilities (e.g., list volumes). Please refer to the dellctl documentation page for the installation steps and command list.\n","categories":"","description":"Enabling CSM Authorization for PowerFlex CSI Driver\n","excerpt":"Enabling CSM Authorization for PowerFlex CSI Driver\n","ref":"/csm-docs/v1/authorization/configuration/powerflex/","tags":"","title":"PowerFlex"},{"body":"The CSI Driver for Dell PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\nCSI Driver for Dell PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\nCSI Driver for Dell PowerFlex Kubernetes Node Registrar, which handles the driver registration Prerequisites The following are requirements that must be met before installing the CSI Driver for Dell PowerFlex:\nInstall Kubernetes or OpenShift (see supported versions) Install Helm 3 Enable Zero Padding on PowerFlex Mount propagation is enabled on container runtime that is being used Install PowerFlex Storage Data Client If using Snapshot feature, satisfy all Volume Snapshot requirements A user must exist on the array with a role \u003e= FrontEndConfigure If enabling CSM for Authorization, please refer to the Authorization deployment steps first If multipath is configured, ensure CSI-PowerFlex volumes are blacklisted by multipathd. See troubleshooting section for details Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerFlex.\nSteps\nRun the command to install Helm 3.0.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Enable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. For more information to configure this setting, see Dell PowerFlex documentation.\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run the node portion of the CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment; for Red Hat CoreOS (RHCOS), RHEL 7.9, RHEL 8.6. On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions.\nNOTE: To install CSI driver for Powerflex with automated SDC deployment, you need below two packages on worker nodes.\nlibaio numactl-libs Optional: For a typical install, you will pull SDC kernel modules from the Dell FTP site, which is set up by default. Some users might want to mirror this repository to a local location. The PowerFlex KB article has instructions on how to do this.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\nDownload the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide: For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version. To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx Installation Wizard prerequisite, secret update: When the driver is installed using values generated by installation wizard, then the user needs to update the secret for driver by patching the MDM keys, as follows:\nSteps\necho -n '\u003cMDM_IPS\u003e' | base64 kubectl patch secret vxflexos-config -n vxflexos -p \"{\\\"data\\\": { \\\"MDM\\\": \\\"\u003cGENERATED_BASE64\u003e\\\"}}\" (Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\nInstall the Driver Steps\nRun git clone -b v2.8.0 https://github.com/dell/csi-powerflex.git to clone the git repository.\nA namespace for the driver is expected prior to running the command below. If one is not created already, you can run kubectl create namespace vxflexos to create a new one. Note that the namespace can be any user-defined name that follows the conventions for namespaces outlined by Kubernetes. In this example we assume that the namespace is ‘vxflexos’\nCollect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the scripts directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the values for these parameters as they must be entered into samples/secret.yaml.\nPrepare samples/secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\nParameter Description Required Default username Username for accessing PowerFlex system. If authorization is enabled, username will be ignored. true - password Password for accessing PowerFlex system. If authorization is enabled, password will be ignored. true - systemID PowerFlex system name or ID. true - allSystemNames List of previous names of powerflex array if used for PV create false - endpoint REST API gateway HTTPS endpoint/PowerFlex Manager public IP for PowerFlex system. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on true - skipCertificateValidation Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. true true isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list. false false mdm mdm defines the MDM(s) that SDC should register with on start. This should be a list of MDM IP addresses or hostnames separated by comma. true - nasName nasName defines what NAS should be used for NFS volumes. NFS volumes are supported on arrays version 4.0.x false none Example: samples/secret.yaml\n- username: \"admin\" password: \"Password123\" systemID: \"2b11bb111111bb1b\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true isDefault: true mdm: \"10.0.0.3,10.0.0.4\" Example: samples/secret.yaml for PowerFlex storage system v4.0.x\n- username: \"admin\" password: \"Password123\" systemID: \"2b11bb111111bb1b\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true isDefault: true mdm: \"10.0.0.3,10.0.0.4\" nasName : \"nasServer\" NOTE: To use multiple arrays, copy and paste section above for each array. Make sure isDefault is set to true for only one array.\nAfter editing the file, run the below command to create a secret called vxflexos-config. This assumes vxflexos is release name, but it can be modified during install:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/secret.yaml Use the below command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/secret.yaml -o yaml --dry-run=client | kubectl replace -f - NOTE:\nThe user needs to validate the YAML syntax and array-related key/values while replacing the vxflexos-creds secret. If you want to create a new array or update the MDM values in the secret, you will need to reinstall the driver. If you change other details, such as login information, the secret will dynamically update – see dynamic-array-configuration for more details. Old json format of the array configuration file is still supported in this release. If you already have your configuration in json format, you may continue to maintain it or you may transfer this configuration to yamlformat and replace/update the secret. “insecure” parameter has been changed to “skipCertificateValidation” as insecure is deprecated and will be removed from use in config.yaml or secret.yaml in a future release. Users can continue to use any one of “insecure” or “skipCertificateValidation” for now. The driver would return an error if both parameters are used. Please note that log configuration parameters from v1.5 will no longer work in v2.0 and higher. Please refer to the Dynamic Logging Configuration section in Features for more information. If the user is using complex K8s version like “v1.21.3-mirantis-1”, use this kubeVersion check in helm/csi-unity/Chart.yaml file. kubeVersion: “\u003e= 1.21.0-0 \u003c 1.29.0-0” Default logging options are set during Helm install. To see possible configuration options, see the Dynamic Logging Configuration section in Features.\nIf using automated SDC deployment:\nCheck the SDC container image is the correct version for your version of PowerFlex. Download the default values.yaml file\ncd dell-csi-helm-installer \u0026\u0026 wget -O myvalues.yaml https://github.com/dell/helm-charts/raw/csi-vxflexos-2.8.0/charts/csi-vxflexos/values.yaml Note: To connect to a PowerFlex 4.5 array, edit the powerflexSdc parameter in your values.yaml file to use dellemc/sdc:4.5:\npowerflexSdc: dellemc/sdc:4.5\nIf you are using a custom image, check the version and driverRepository fields in my-vxflexos-settings.yaml to make sure that they are pointing to the correct image repository and driver version. These two fields are spliced together to form the image name, as shown here: \u003cdriverRepository\u003e/csi-vxflexos:v\u003cversion\u003e\nLook over all the other fields myvalues.yaml and fill in/adjust any as needed. All the fields are described here:\nParameter Description Required Default version Set to verify the values file version matches driver version and used to pull the image as part of the image name. Yes 2.8.0 driverRepository Set to give the repository containing the driver image (used as part of the image name). Yes dellemc powerflexSdc Set to give the location of the SDC image used if automatic SDC deployment is being utilized. Yes dellemc/sdc:3.6.1 certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. No 0 logLevel CSI driver log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes “debug” logFormat CSI driver log format. Allowed values: “TEXT” or “JSON”. Yes “TEXT” kubeletConfigDir kubelet config directory path. Ensure that the secret.yaml file is present at this path. Yes /var/lib/kubelet defaultFsType Used to set the default FS type which will be used for mount volumes if FsType is not specified in the storage class. Allowed values: ext4, xfs. Yes ext4 fsGroupPolicy Defines which FS Group policy mode to be used. Supported modes areNone, File, and ReadWriteOnceWithFSType. No “ReadWriteOnceWithFSType” imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Allowed values: Always, IfNotPresent, Never. Yes IfNotPresent enablesnapshotcgdelete A boolean that, when enabled, will delete all snapshots in a consistency group everytime a snap in the group is deleted. Yes false enablelistvolumesnapshot A boolean that, when enabled, will allow list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap). It is recommend this be false unless instructed otherwise. Yes false allowRWOMultiPodAccess Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. Yes false enableQuota A boolean that, when enabled, will set quota limit for a newly provisioned NFS volume. No false controller This section allows the configuration of controller-specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - - volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. Yes “k8s” controllerCount Set to deploy multiple controller instances. If the controller count is greater than the number of available nodes, excess pods remain in a pending state. It should be greater than 0. You can increase the number of available nodes by configuring the “controller” section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2 snapshot.enabled A boolean that enable/disable volume snapshot feature. No true resizer.enabled A boolean that enable/disable volume expansion feature. No true nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. Yes \" \" tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install the controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. Yes \" \" healthMonitor This section configures the optional deployment of the external health monitor sidecar, for controller side volume health monitoring. - - enabled Enable/Disable deployment of external health monitor sidecar. No false interval Interval of monitoring volume health condition. Allowed values: Number followed by unit (s,m,h) No 60s node This section allows the configuration of node-specific parameters. - - healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition No false nodeSelector Defines what nodes would be selected for pods of node daemonset. Leave as blank to use all nodes. Yes \" \" tolerations Defines tolerations that would be applied to node daemonset. Leave as blank to install node driver only on worker nodes. Yes \" \" renameSDC This section allows the rename operation for SDC. - - enabled A boolean that enable/disable rename SDC feature. No false prefix Defines a string for the prefix of the SDC. No \" \" approveSDC.enabled A boolean that enable/disable SDC approval feature. No false storageCapacity Enable/Disable storage capacity tracking - - enabled A boolean that enables/disables storage capacity tracking feature. Yes true pollInterval Configure how often the driver checks for changed capacity No 5m monitor This section allows the configuration of the SDC monitoring pod. - - enabled Set to enable the usage of the monitoring pod. Yes false hostNetwork Set whether the monitor pod should run on the host network or not. Yes true hostPID Set whether the monitor pod should run in the host namespace or not. Yes true vgsnapshotter This section allows the configuration of the volume group snapshotter(vgsnapshotter) pod. - - enabled A boolean that enable/disable vg snapshotter feature. No false image Image for vg snapshotter. No \" \" podmon Podmon is an optional feature to enable application pods to be resilient to node failure. - - enabled A boolean that enables/disables podmon feature. No false image image for podmon. No \" \" authorization Authorization is an optional feature to apply credential shielding of the backend PowerFlex. - - enabled A boolean that enables/disables authorization feature. No false sidecarProxyImage Image for csm-authorization-sidecar. No \" \" proxyHost Hostname of the csm-authorization server. No Empty skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization proxy server. No true Install the driver using csi-install.sh bash script by running cd dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values myvalues.yaml. You may modify the release name with the --release arg. If arg is not provided, release will be named vxflexos by default. Alternatively, to do a helm install solely with Helm charts (without shell scripts), refer to helm/README.md. NOTE:\nFor detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder.\nInstall script will validate MDM IP(s) in vxflexos-config secret and creates a new field consumed by the init container and sdc-monitor container\nThis install script also runs the verify.sh script. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes.\nIt is mandatory to run install script after changes to MDM configuration in vxflexos-config secret. Refer dynamic-array-configuration\nIf an extended Kubernetes version is being used (e.g. v1.21.3-mirantis-1) and is failing the version check in Helm even though it falls in the allowed range, then you must go into helm/csi-vxflexos/Chart.yaml and replace the standard kubeVersion check with the commented-out alternative. Please note that this will also allow the use of pre-release alpha and beta versions of Kubernetes, which is not supported.\n(Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.\nMount options are specified in storageclass yaml under mkfsFormatOption. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option. Certificate validation for PowerFlex Gateway REST API calls This topic provides details about setting up the certificate for the CSI Driver for Dell PowerFlex.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name vxflexos-certs-0 to vxflexos-certs-n based on the “.Values.certSecretCount” parameter present in the namespace vxflexos.\nThis secret contains the X509 certificates of the CA which signed PowerFlex gateway SSL certificate in PEM format.\nThe CSI driver exposes an install parameter in secret.yaml, skipCertificateValidation, which determines if the driver performs client-side verification of the gateway certificates.\nskipCertificateValidation parameter is set to true by default, and the driver does not verify the gateway certificates.\nIf skipCertificateValidation is set to false, then the secret vxflexos-certs-n must contain the CA certificate for the array gateway.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the gateway certificate is self-signed or if you are using an embedded gateway, then perform the following steps.\nTo fetch the certificate, run the following command.\nopenssl s_client -showcerts -connect \u003cGateway IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example:\nopenssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Run the following command to create the cert secret with index ‘0’:\nkubectl create secret generic vxflexos-certs-0 --from-file=cert-0=ca_cert_0.pem -n vxflexos Use the following command to replace the secret:\nkubectl create secret generic vxflexos-certs-0 -n vxflexos --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: vxflexos-certs-1, vxflexos-certs-2, etc)\nNotes:\n“vxflexos” is the namespace for Helm-based installation but namespace can be user-defined in operator-based installation. User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation. Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver. Updating vxflexos-certs-n secrets is a manual process, unlike vxflexos-config. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter. Storage Classes For CSI driver for PowerFlex version 1.4 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class: There are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\nEdit storageclass.yaml if you need ext4 filesystem, storageclass-xfs.yaml if you want xfs filesystem and storageclass-nfs.yaml if you need nfs filesystem Replace \u003cSTORAGE_POOL\u003e with the storage pool you have. Replace \u003cSYSTEM_ID\u003e with the system ID you have. Note there are two appearances in the file. Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. If using storageclass-nfs.yaml Replace \"nas-server\" with the NAS server’s name you have. Save the file and create it by using kubectl create -f storageclass.yaml / kubectl create -f storageclass-xfs.yaml/ kubectl create -f storageclass-nfs.yaml NOTE:\nAt least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the myvalues.yaml file leads to an update of the storage class(es): Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerFlex v1.5, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\n","categories":"","description":"Installing the CSI Driver for PowerFlex via Helm\n","excerpt":"Installing the CSI Driver for PowerFlex via Helm\n","ref":"/csm-docs/v1/csidriver/installation/helm/powerflex/","tags":"","title":"PowerFlex"},{"body":" The Dell CSI Operator is no longer actively maintained or supported. Dell CSI Operator has been replaced with Dell CSM Operator. If you are currently using Dell CSI Operator, refer to the operator migration documentation to migrate from Dell CSI Operator to Dell CSM Operator. CSM 1.7.1 is applicable to helm based installations of PowerFlex driver.\nInstalling CSI Driver for PowerFlex via Operator The CSI Driver for Dell PowerFlex can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: If multipath is configured, ensure CSI-PowerFlex volumes are blacklisted by multipathd. See troubleshooting section for details SDC Deployment for Operator This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non-supported versions of the OS also do the manual SDC deployment steps given below. Refer to https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When the driver is created, MDM value for initContainers in driver CR is set by the operator from mdm attributes in the driver configuration file, secret.yaml. An example of secret.yaml is below in this document. Do not set MDM value for initContainers in the driver CR file manually. Note: To use an sdc-binary module from customer ftp site: Create a secret, sdc-repo-secret.yaml to contain the credentials for the private repo. To generate the base64 encoding of a credential: echo -n \u003ccredential\u003e| base64 -i secret sample to use: apiVersion: v1 kind: Secret metadata: name: sdc-repo-creds namespace: vxflexos type: Opaque data: # set username to the base64 encoded username, sdc default is username: \u003cusername in base64\u003e # set password to the base64 encoded password, sdc default is password: \u003cpassword in base64\u003e Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml. Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml. Please note the following: If using sidecar, you will need to edit the value fields under the HOST_PID and MDM fields by filling the empty quotes with host PID and the MDM IPs. If not using sidecar, please leave this commented out – otherwise, the empty fields will cause errors. Example CR: config/samples/vxflex_v270_ops_412.yaml sideCars: # Comment the following section if you don't want to run the monitoring sidecar - name: sdc-monitor envs: - name: HOST_PID value: \"1\" - name: MDM value: \"\" - name: external-health-monitor args: [\"--monitor-interval=60s\"] initContainers: - image: dellemc/sdc:3.6 imagePullPolicy: IfNotPresent name: sdc envs: - name: MDM value: \"10.x.x.x,10.x.x.x\" Note: Please comment the sdc-monitor sidecar section if you are not using it. Blank values for MDM will result in error. Do not comment the external-health-monitor argument.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\nDownload the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide: For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version. To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx Install Driver Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e command using the desired name to create the namespace.\nPrepare the secret.yaml for driver configuration.\nExample: secret.yaml\n# Username for accessing PowerFlex system.\t# Required: true - username: \"admin\" # Password for accessing PowerFlex system.\t# Required: true password: \"password\" # System name/ID of PowerFlex system.\t# Required: true systemID: \"ID1\" # REST API gateway HTTPS endpoint/PowerFlex Manager public IP for PowerFlex system. # Required: true endpoint: \"https://127.0.0.1\" # Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. # Allowed values: true or false # Required: true # Default value: true skipCertificateValidation: true # indicates if this array is the default array # needed for backwards compatibility # only one array is allowed to have this set to true # Required: false # Default value: false isDefault: true # defines the MDM(s) that SDC should register with on start. # Allowed values: a list of IP addresses or hostnames separated by comma. # Required: true # Default value: none mdm: \"10.0.0.1,10.0.0.2\" # Defines all system names used to create powerflex volumes # Required: false # Default value: none AllSystemNames: \"name1,name2\" - username: \"admin\" password: \"Password123\" systemID: \"ID2\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true mdm: \"10.0.0.3,10.0.0.4\" AllSystemNames: \"name1,name2\" After editing the file, run the following command to create a secret called vxflexos-config\nkubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=secret.yaml Use the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Note:\nSystem ID, MDM configuration, etc. now are taken directly from secret.yaml. MDM provided in the input_sample_file.yaml will be overidden with MDM values in secret.yaml. Please provide MDM values in input_sample_file.yaml so that it will be overidden by default value. Create a Custom Resource (CR) for PowerFlex using the sample files provided here.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\nParameter Description Required Default replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, excess pods will become stay in a pending state. Defaults are 2 which allows for Controller high availability. Yes 2 fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” Common parameters for node and controller X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false X_CSI_DEBUG To enable debug mode No true X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false Execute the kubectl create -f \u003cinput_sample_file.yaml\u003e command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.\nExample CR for PowerFlex Driver apiVersion: storage.dell.com/v1 kind: CSIVXFlexOS metadata: name: test-vxflexos namespace: test-vxflexos spec: driver: configVersion: v2.7.0 replicas: 1 dnsPolicy: ClusterFirstWithHostNet forceUpdate: false fsGroupPolicy: File common: image: \"dellemc/csi-vxflexos:v2.7.0\" imagePullPolicy: IfNotPresent envs: - name: X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT value: \"false\" - name: X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE value: \"false\" - name: X_CSI_DEBUG value: \"true\" - name: X_CSI_ALLOW_RWO_MULTI_POD_ACCESS value: \"false\" sideCars: # comment the following section if you don't want to run the monitoring sidecar - name: sdc-monitor envs: - name: HOST_PID value: \"1\" - name: MDM value: \"\" # Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" # X_CSI_MAX_VOLUMES_PER_NODE: Defines the maximum PowerFlex volumes that can be created per node # Allowed values: Any value greater than or equal to 0 # If value is 0 then the orchestrator decides how many volumes can be published by the controller to # the node # Default value: \"0\" - name: X_CSI_MAX_VOLUMES_PER_NODE value: \"0\" initContainers: - image: dellemc/sdc:3.6.1 imagePullPolicy: IfNotPresent name: sdc envs: - name: MDM value: \"10.xx.xx.xx,10.xx.xx.xx\" #provide MDM value --- apiVersion: v1 kind: ConfigMap metadata: name: vxflexos-config-params namespace: test-vxflexos data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"debug\" CSI_LOG_FORMAT: \"TEXT\" Pre-Requisite for installation with OLM Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM. #Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM\ngit clone https://github.com/dell/dell-csi-operator.git cd dell-csi-operator tar -czf config.tar.gz driverconfig/ kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u003coperator-namespace\u003e Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator.\nTo enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" ","categories":"","description":"Installing CSI Driver for PowerFlex via Operator\n","excerpt":"Installing CSI Driver for PowerFlex via Operator\n","ref":"/csm-docs/v1/csidriver/installation/operator/powerflex/","tags":"","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\nNavigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following: Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \"docker.io/centos:latest\" Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured. Results\nAn outline of this workflow is described below:\nThe 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below: kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol namespace: helmtest-vxflexos spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: vxflexos The volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use a storage class named vxflexos. This step yields a mounted ext4 file system. You can create the vxflexos and vxflexos-xfs storage classes by using the yamls located in samples/storageclass. If you compare pvol0.yaml and pvol1.yaml, you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos. NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.\nTest creating snapshots Test the workflow for snapshot creation.\nNOTE: Starting with version 2.0, CSI Driver for PowerFlex helm tests are designed to work exclusively with v1 snapshots.\nSteps\nStart the 2vols container and leave it running. Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly. Run sh snaptest.sh to start the test. This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in snap1.yaml and snap2.yaml. The following are the contents of snap1.yaml:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Results\nThe snaptest.sh script will create a snapshot using the definitions in the snap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0, then the created snapshot is named pvol0-snap1.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n helmtest-vxflexos.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell PowerFlex installation does not create this class. You will need to create instance of VolumeSnapshotClass from one of default samples in `samples/volumesnapshotclass’ directory.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\nRun sh snaprestoretest.sh to start the test. This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updated directory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\nHelm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly. Results\nAn outline of this workflow is described below:\nThe snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim, which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap1 kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi NOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in snap1.yaml.\nTest creating NFS volumes Steps\nNavigate to the test/helm directory, which contains the starttest.sh and the 1vol-nfs directories. This directory contains a simple Helm chart that will deploy a pod that uses one PowerFlex volumes for NFS filesystem type. NOTE:\nHelm tests are designed assuming users are using the storageclass name: vxflexos-nfs. If your storageclass names differ from these values, please update the templates in 1vol-nfs accordingly (located in test/helm/1vol-nfs/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 1vol-nfs to deploy the pod. You should see the following: Normal Scheduled default-scheduler, Successfully assigned helmtest-vxflexos/vxflextest-0 to worker-1-zwfjtd4eoblkg.domain Normal SuccessfulAttachVolume attachdetach-controller, AttachVolume.Attach succeeded for volume \"k8s-e279d47296\" Normal Pulled 13s kubelet, Successfully pulled image \"docker.io/centos:latest\" in 791.117427ms (791.125522ms including waiting) Normal Created 13s kubelet, Created container test Normal Started 13s kubelet, Started container test 10.x.x.x:/k8s-e279d47296 8388608 1582336 6806272 19% /data0 10.x.x.x:/k8s-e279d47296 on /data0 type nfs4 (rw,relatime,vers=4.2,rsize=262144,wsize=262144,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.x.x.x,local_lock=none,addr=10.x.x.x) To stop the test, run sh stoptest.sh 1vol-nfs. This script deletes the pods and the volumes depending on the retention setting you have configured. Results\nAn outline of this workflow is described below:\nThe 1vol-nfs helm chart contains one PersistentVolumeClaim definition in pvc0.yaml. It is referenced by the test.yaml which creates the pod. The contents of the pvc0.yaml file are described below: kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: vxflexos-nfs The volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos-nfs directs the system to use a storage class named vxflexos-nfs. This step yields a mounted nfs file system. You can create the vxflexos-nfs storage classes by using the yaml located in samples/storageclass. To see the volumes you created, run kubectl get persistentvolumeclaim -n helmtest-vxflexos and kubectl describe persistentvolumeclaim -n helmtest-vxflexos. NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.\nTest restoring NFS volume from snapshot Test the restore operation workflow to restore NFS volume from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\nRun sh snaprestoretest-nfs.sh to start the test. This script deploys the 1vol-nfs example, creates a snap of pvol0, and then updates the deployed helm chart from the updated directory 1vols+restore-nfs. This adds an additional volume that is created from the snapshot.\nNOTE:\nHelm tests are designed assuming users are using the storageclass name: vxflexos-nfs. If your storageclass names differ from these values, update the templates for 1vols+restore-nfs accordingly (located in test/helm/1vols+restore-nfs/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml accordingly. Results\nAn outline of this workflow is described below:\nThe snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 1vols+restore-nfs directory. The csi-vxflexos/test/helm/1vols+restore-nfs/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim, which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest-nfs.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos-nfs dataSource: name: pvol0-snap1 kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi NOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in snap1.yaml.\n","categories":"","description":"Tests to validate PowerFlex CSI Driver installation","excerpt":"Tests to validate PowerFlex CSI Driver installation","ref":"/csm-docs/v1/csidriver/installation/test/powerflex/","tags":"","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v2.8.0 New Features/Changes #724 - [FEATURE]: CSM support for Openshift 4.13 #763 - [FEATURE]: CSI-PowerFlex 4.0 NFS support #876 - [FEATURE]: CSI 1.5 spec support -StorageCapacityTracking #878 - [FEATURE]: CSI 1.5 spec support: Implement Volume Limits #885 - [FEATURE]: SDC 3.6.1 support Fixed Issues #916 - [BUG]: Remove references to deprecated io/ioutil package Known Issues Issue Workaround Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100 When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. sdc:3.6.0.6 is causing issues while installing the csi-powerflex driver on ubuntu,RHEL8.3 Workaround: Change the powerflexSdc to sdc:3.6 in values.yaml https://github.com/dell/csi-powerflex/blob/72b27acee7553006cc09df97f85405f58478d2e4/helm/csi-vxflexos/values.yaml#L13 sdc:3.6.1 is causing issues while installing the csi-powerflex driver on ubuntu. Workaround: Change the powerflexSdc to sdc:3.6 in values.yaml https://github.com/dell/csi-powerflex/blob/72b27acee7553006cc09df97f85405f58478d2e4/helm/csi-vxflexos/values.yaml#L13 A CSI ephemeral pod may not get created in OpenShift 4.13 and fail with the error \"error when creating pod: the pod uses an inline volume provided by CSIDriver csi-unity.dellemc.com, and the namespace has a pod security enforcement level that is lower than privileged.\" This issue occurs because OpenShift 4.13 introduced the CSI Volume Admission plugin to restrict the use of a CSI driver capable of provisioning CSI ephemeral volumes during pod admission. Therefore, an additional label security.openshift.io/csi-ephemeral-volume-profile in csidriver.yaml file with the required security profile value should be provided. Follow OpenShift 4.13 documentation for CSI Ephemeral Volumes for more information. If the volume limit is exhausted and there are pending pods and PVCs due to exceed max volume count, the pending PVCs will be bound to PVs and the pending pods will be scheduled to nodes when the driver pods are restarted. It is advised not to have any pending pods or PVCs once the volume limit per node is exhausted on a CSI Driver. There is an open issue reported with kubenetes at https://github.com/kubernetes/kubernetes/issues/95911 with the same behavior. The PowerFlex Dockerfile is incorrectly labeling the version as 2.7.0 for the 2.8.0 version. Describe the driver pod using kubectl describe pod $podname -n vxflexos to ensure v2.8.0 is installed. Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for PowerFlex CSI driver","excerpt":"Release notes for PowerFlex CSI driver","ref":"/csm-docs/v1/csidriver/release/powerflex/","tags":"","title":"PowerFlex"},{"body":" Symptoms Prevention, Resolution or Workaround The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that need to pull an image of the driver. When you run the command kubectl describe pods vxflexos-controller-* –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command. The kubectl logs -n vxflexos vxflexos-controller-* driver logs show that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system. The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script. CreateVolume error System is not configured in the driver Powerflex name if used for systemID in StorageClass ensure same name is also used in array config systemID Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on a worker node, and ensure your container run time manager is properly configured to be utilized with SElinux. Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux. Installation of the driver on Kubernetes v1.25/v1.26/v1.27 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.23/v1.24/v1.25 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements The kubectl logs -n vxflexos vxflexos-controller-* driver logs show x509: certificate signed by unknown authority A self assigned certificate is used for PowerFlex array. See certificate validation for PowerFlex Gateway When you run the command kubectl apply -f snapclass-v1.yaml, you get the error error: unable to recognize \"snapclass-v1.yaml\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Check to make sure that the v1 snapshotter CRDs are installed, and not the v1beta1 CRDs, which are no longer supported. The controller pod is stuck and producing errors such as\" Failed to watch *v1.VolumeSnapshotContent: failed to list *v1.VolumeSnapshotContent: the server could not find the requested resource (get volumesnapshotcontents.snapshot.storage.k8s.io) Make sure that v1 snapshotter CRDs and v1 snapclass are installed, and not v1beta1, which is no longer supported. Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.21.0 \u003c= 1.28.0 which is incompatible with Kubernetes V1.21.11-mirantis-1 If you are using an extended Kubernetes version, see the helm Chart at helm/csi-vxflexos/Chart.yaml and use the alternate kubeVersion check that is provided in the comments. Note: this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. Volume metrics are missing Enable Volume Health Monitoring When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. CSI-PowerFlex volumes cannot mount; are being recognized as multipath devices CSI-PowerFlex does not support multipath; to fix: 1. Remove any multipath mapping involving a powerflex volume with multipath -f \u003cpowerflex volume\u003e 2. Blacklist CSI-PowerFlex volumes in multipath config file When attempting a driver upgrade, you see: spec.fsGroupPolicy: Invalid value: \"xxx\": field is immutable You cannot upgrade between drivers with different fsGroupPolicies. See upgrade documentation for more details When accessing ROX mode PVC in OpenShift where the worker nodes are non-root user, you see: Permission denied while accesing the PVC mount location from the pod. Set the securityContext for ROX mode PVC pod as below, as it defines privileges for the pods or containers.securityContext: runAsUser: 0 runAsGroup: 0 After installing version v2.6.0 of the driver using the default powerflexSdc image, sdc:3.6.0.6, the vxflexos-node pods are in an Init:CrashLoopBackOff state. This issue can happen on hosts that require the SDC to be installed manually. Automatic SDC is only supported on Red Hat CoreOS (RHCOS), RHEL 7.9, RHEL 8.4, RHEL 8.6. The SDC is already installed. Change the images.powerflexSdc value to an empty value in the values and re-install. After installing version v2.8.0 of the driver using the default powerflexSdc image, sdc:3.6.1, the vxflexos-node pods are in an Init:CrashLoopBackOff state. This issue can happen on hosts that require the SDC to be installed manually. Automatic SDC is only supported on Red Hat CoreOS (RHCOS), RHEL 7.9, RHEL 8.4, RHEL 8.6. The SDC is already installed. Change the images.powerflexSdc value to an empty value in the values and re-install. In version v2.6.0, the driver is crashing because the External Health Monitor sidecar crashes when a persistent volume is not found. This is a known issue reported at kubernetes-csi/external-health-monitor#100. In version v2.6.0, when a cluster node goes down, the block volumes attached to the node cannot be attached to another node. This is a known issue reported at kubernetes-csi/external-attacher#215. Workaround: 1. Force delete the pod running on the node that went down. 2. Delete the pod’s persistent volume attachment on the node that went down. Now the volume can be attached to the new node. A CSI ephemeral pod may not get created in OpenShift 4.13 and fail with the error \"error when creating pod: the pod uses an inline volume provided by CSIDriver csi-vxflexos.dellemc.com, and the namespace has a pod security enforcement level that is lower than privileged.\" This issue occurs because OpenShift 4.13 introduced the CSI Volume Admission plugin to restrict the use of a CSI driver capable of provisioning CSI ephemeral volumes during pod admission. Therefore, an additional label security.openshift.io/csi-ephemeral-volume-profile in csidriver.yaml file with the required security profile value should be provided. Follow OpenShift 4.13 documentation for CSI Ephemeral Volumes for more information. Note: vxflexos-controller-* is the controller pod that acquires leader lease\n","categories":"","description":"Troubleshooting PowerFlex Driver","excerpt":"Troubleshooting PowerFlex Driver","ref":"/csm-docs/v1/csidriver/troubleshooting/powerflex/","tags":"","title":"PowerFlex"},{"body":"Installing CSI Driver for PowerFlex via Dell CSM Operator The CSI Driver for Dell PowerFlex can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using this command:\nkubectl get csm --all-namespaces Prerequisites If multipath is configured, ensure CSI-PowerFlex volumes are blacklisted by multipathd. See troubleshooting section for details. NOTE: This step can be skipped with OpenShift.\nSDC Deployment for Operator This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non-supported versions of the OS also do the manual SDC deployment steps given below. Refer to https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When the driver is created, MDM value for initContainers in driver CR is set by the operator from mdm attributes in the driver configuration file, config.yaml. An example of config.yaml is below in this document. Do not set MDM value for initContainers in the driver CR file manually. Optionally, enable sdc monitor by setting the enable flag for the sdc-monitor to true. Please note: If using sidecar, you will need to edit the value fields under the HOST_PID and MDM fields by filling the empty quotes with host PID and the MDM IPs. If not using sidecar, leave the enabled field set to false. Example CR: samples/storage_csm_powerflex_v280.yaml sideCars: # sdc-monitor is disabled by default, due to high CPU usage - name: sdc-monitor enabled: false image: dellemc/sdc:3.6.1 envs: - name: HOST_PID value: \"1\" - name: MDM value: \"10.xx.xx.xx,10.xx.xx.xx\" #provide the same MDM value from secret Note: To connect to a PowerFlex 4.5 array, edit the initContainers.image parameter in your samples file to use dellemc/sdc:4.5:\n- image: dellemc/sdc:4.5\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC using this procedure:\nSteps\nDownload the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide: For environments using RPM, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version. To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx1. Create namespace. Execute kubectl create namespace vxflexos to create the vxflexos namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘vxflexos’ NOTE: This step can be skipped with OpenShift CoreOS nodes.\nCreate Secret Create namespace: Execute kubectl create namespace vxflexos to create the vxflexos namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘vxflexos’\nPrepare the secret.yaml for driver configuration.\nExample: secret.yaml\n# Username for accessing PowerFlex system. # If authorization is enabled, username will be ignored. - username: \"admin\" # Password for accessing PowerFlex system. # If authorization is enabled, password will be ignored. password: \"password\" # System name/ID of PowerFlex system.\tsystemID: \"1a99aa999999aa9a\" # Previous names used in secret of PowerFlex system. allSystemNames: \"pflex-1,pflex-2\" # REST API gateway HTTPS endpoint for PowerFlex system. # If authorization is enabled, endpoint should be the HTTPS localhost endpoint that # the authorization sidecar will listen on endpoint: \"https://127.0.0.1\" # Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. # Allowed values: true or false # Default value: true skipCertificateValidation: true # indicates if this array is the default array # needed for backwards compatibility # only one array is allowed to have this set to true # Default value: false isDefault: true # defines the MDM(s) that SDC should register with on start. # Allowed values: a list of IP addresses or hostnames separated by comma. # Default value: none mdm: \"10.0.0.1,10.0.0.2\" # NFS is only supported on PowerFlex storage system 4.0.x # nasName: name of NAS server used for NFS volumes # nasName value must be specified in secret for performing NFS (file) operations. # Allowed Values: string # Default Value: \"none\" nasName: \"nas-server\" - username: \"admin\" password: \"Password123\" systemID: \"2b11bb111111bb1b\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true mdm: \"10.0.0.3,10.0.0.4\" After editing the file, run this command to create a secret called vxflexos-config. If you are using a different namespace/secret name, just substitute those into the command.\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=secret.yaml Use this command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml -o yaml --dry-run=client | kubectl replace -f - Install Driver Follow all the prerequisites above\nCreate a CR (Custom Resource) for PowerFlex using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\nParameter Description Required Default dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, excess pods will become stay in a pending state. Defaults are 2 which allows for Controller high availability. Yes 2 storageCapacity.enabled Enable/Disable storage capacity tracking No true storageCapacity.pollInterval Configure how often the driver checks for changed capacity No 5m enableQuota a boolean that, when enabled, will set quota limit for a newly provisioned NFS volume No none maxVxflexosVolumesPerNode Specify default value for maximum number of volumes that controller can publish to the node.If value is zero CO SHALL decide how many volumes of this type can be published by the controller to the node Yes 0 Common parameters for node and controller X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false X_CSI_DEBUG To enable debug mode No true X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false Node parameters X_CSI_RENAME_SDC_ENABLED Enable this to rename the SDC with the given prefix. The new name will be (“prefix” + “worker_node_hostname”) and it should not exceed 31 chars. Yes false X_CSI_APPROVE_SDC_ENABLED Enable this to to approve restricted SDC by GUID during setup Yes false Execute this command to create PowerFlex custom resource:\nkubectl create -f \u003cinput_sample_file.yaml\u003e This command will deploy the CSI-PowerFlex driver in the namespace specified in the input YAML file.\nVerify the CSI Driver installation\nNote :\nSnapshotter and resizer sidecars are installed by default. ","categories":"","description":"Installing Dell CSI Driver for PowerFlex via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for PowerFlex via Dell CSM Operator\n","ref":"/csm-docs/v1/deployment/csmoperator/drivers/powerflex/","tags":"","title":"PowerFlex"},{"body":"Configuring PowerFlex CSI Driver with CSM for Authorization Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow these steps to configure the CSI Drivers to work with the Authorization sidecar:\nApply the secret containing the tenant token data into the driver namespace. It’s assumed that the Kubernetes administrator has the token secret manifest, generated by your storage administrator via Generate a Token, saved in /tmp/token.yaml.\n#It is assumed that array type powerflex has the namepace “vxflexos”.\nkubectl apply -f /tmp/token.yaml -n vxflexos Edit these parameters in samples/secret/karavi-authorization-config.json file in the CSI PowerFlex driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\nParameter Description Required Default username Username for connecting to the backend storage array. This parameter is ignored. No - password Password for connecting to to the backend storage array. This parameter is ignored. No - intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes - endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400 systemID System ID of the backend storage array. Yes \" \" skipCertificateValidation A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml Create the karavi-authorization-config secret using this command:\nkubectl -n vxflexos create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f - Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n vxflexos create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f - Otherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n vxflexos create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f - Prepare the driver configuration secret, applicable to your driver installation method, to communicate with the CSM Authorization sidecar.\nHelm\nRefer to the Install the Driver section to edit the parameters in samples/config.yaml to configure the driver to communicate with the CSM Authorization sidecar.\nUpdate endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate skipCertificateValidation to true.\nThe username and password can be any value since they will be ignored.\nExample:\n- username: \"ignored\" password: \"ignored\" systemID: \"ID2\" endpoint: \"https://localhost:9400\" skipCertificateValidation: true isDefault: true mdm: \"10.0.0.3,10.0.0.4\" Operator\nRefer to the Create Secret section to prepare config.yaml to configure the driver to communicate with the CSM Authorization sidecar.\nUpdate endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate skipCertificateValidation to true.\nThe username and password can be any value since they will be ignored.\nExample:\n- username: \"ignored\" password: \"ignored\" systemID: \"ID2\" endpoint: \"https://localhost:9400\" skipCertificateValidation: true isDefault: true mdm: \"10.0.0.3,10.0.0.4\" Enable CSM Authorization in the driver installation applicable to your installation method.\nHelm\nRefer to the Install the Driver section to edit the parameters in myvalues.yaml to enable CSM Authorization.\nUpdate authorization.enabled to true.\nUpdate authorization.sidecarProxyImage to the image of the CSM Authorization sidecar. In most cases, you can leave the default value.\nUpdate authorization.proxyHost to the hostname of the CSM Authorization Proxy Server.\nUpdate authorization.skipCertificateValidation to true or false depending on if you want to disable or enable certificate validation of the CSM Authorization Proxy Server.\nExample:\nauthorization: enabled: true # sidecarProxyImage: the container image used for the csm-authorization-sidecar. # Default value: dellemc/csm-authorization-sidecar:v1.7.0 sidecarProxyImage: dellemc/csm-authorization-sidecar:v1.7.0 # proxyHost: hostname of the csm-authorization server # Default value: None proxyHost: csm-authorization.com # skipCertificateValidation: certificate validation of the csm-authorization server # Allowed Values: # \"true\" - TLS certificate verification will be skipped # \"false\" - TLS certificate will be verified # Default value: \"true\" skipCertificateValidation: true Operator\nRefer to the Install Driver section to edit the parameters in the Custom Resource to enable CSM Authorization.\nUnder modules, enable the module named authorization:\nUpdate the enabled field to true.\nUpdate the image to the image of the CSM Authorization sidecar. In most cases, you can leave the default value.\nUpdate the PROXY_HOST environment value to the hostname of the CSM Authorization Proxy Server.\nUpdate the SKIP_CERTIFICATE_VALIDATION environment value to true or false depending on if you want to disable or enable certificate validation of the CSM Authorization Proxy Server.\nExample:\nmodules: # Authorization: enable csm-authorization for RBAC - name: authorization # enable: Enable/Disable csm-authorization enabled: true configVersion: v1.7.0 components: - name: karavi-authorization-proxy image: dellemc/csm-authorization-sidecar:v1.7.0 envs: # proxyHost: hostname of the csm-authorization server - name: \"PROXY_HOST\" value: \"csm-authorization.com\" # skipCertificateValidation: Enable/Disable certificate validation of the csm-authorization server - name: \"SKIP_CERTIFICATE_VALIDATION\" value: \"true\" Install the Dell CSI PowerFlex driver following the appropriate documenation for your installation method.\n(Optional) Install dellctl to perform Kubernetes administrator commands for additional capabilities (e.g., list volumes). Please refer to the dellctl documentation page for the installation steps and command list.\n","categories":"","description":"Enabling CSM Authorization for PowerFlex CSI Driver\n","excerpt":"Enabling CSM Authorization for PowerFlex CSI Driver\n","ref":"/csm-docs/v2/authorization/configuration/powerflex/","tags":"","title":"PowerFlex"},{"body":"The CSI Driver for Dell PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\nCSI Driver for Dell PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\nCSI Driver for Dell PowerFlex Kubernetes Node Registrar, which handles the driver registration Prerequisites The following are requirements that must be met before installing the CSI Driver for Dell PowerFlex:\nInstall Kubernetes or OpenShift (see supported versions) Install Helm 3 Enable Zero Padding on PowerFlex Mount propagation is enabled on container runtime that is being used Install PowerFlex Storage Data Client If using Snapshot feature, satisfy all Volume Snapshot requirements A user must exist on the array with a role \u003e= FrontEndConfigure If enabling CSM for Authorization, please refer to the Authorization deployment steps first If multipath is configured, ensure CSI-PowerFlex volumes are blacklisted by multipathd. See troubleshooting section for details Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerFlex.\nSteps\nRun the command to install Helm 3.0.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Enable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. For more information to configure this setting, see Dell PowerFlex documentation.\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run the node portion of the CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment; for Red Hat CoreOS (RHCOS), RHEL 7.9, RHEL 8.4, RHEL 8.6. On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions.\nNOTE: To install CSI driver for Powerflex with automated SDC deployment, you need below two packages on worker nodes.\nlibaio numactl-libs Optional: For a typical install, you will pull SDC kernel modules from the Dell FTP site, which is set up by default. Some users might want to mirror this repository to a local location. The PowerFlex KB article has instructions on how to do this.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\nDownload the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide: For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version. To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx (Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\nInstall the Driver Steps\nRun git clone -b v2.7.1 https://github.com/dell/csi-powerflex.git to clone the git repository.\nA namespace for the driver is expected prior to running the command below. If one is not created already, you can run kubectl create namespace vxflexos to create a new one. Note that the namespace can be any user-defined name that follows the conventions for namespaces outlined by Kubernetes. In this example we assume that the namespace is ‘vxflexos’\nCollect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the scripts directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the values for these parameters as they must be entered into samples/secret.yaml.\nPrepare samples/secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\nParameter Description Required Default username Username for accessing PowerFlex system. If authorization is enabled, username will be ignored. true - password Password for accessing PowerFlex system. If authorization is enabled, password will be ignored. true - systemID PowerFlex system name or ID. true - allSystemNames List of previous names of powerflex array if used for PV create false - endpoint REST API gateway HTTPS endpoint/PowerFlex Manager public IP for PowerFlex system. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on true - skipCertificateValidation Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. true true isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list. false false mdm mdm defines the MDM(s) that SDC should register with on start. This should be a list of MDM IP addresses or hostnames separated by comma. true - Example: samples/secret.yaml\n- username: \"admin\" password: \"Password123\" systemID: \"2b11bb111111bb1b\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true isDefault: true mdm: \"10.0.0.3,10.0.0.4\" NOTE: To use multiple arrays, copy and paste section above for each array. Make sure isDefault is set to true for only one array.\nAfter editing the file, run the below command to create a secret called vxflexos-config. This assumes vxflexos is release name, but it can be modified during install:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/secret.yaml Use the below command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/secret.yaml -o yaml --dry-run=client | kubectl replace -f - NOTE:\nThe user needs to validate the YAML syntax and array-related key/values while replacing the vxflexos-creds secret. If you want to create a new array or update the MDM values in the secret, you will need to reinstall the driver. If you change other details, such as login information, the secret will dynamically update – see dynamic-array-configuration for more details. Old json format of the array configuration file is still supported in this release. If you already have your configuration in json format, you may continue to maintain it or you may transfer this configuration to yamlformat and replace/update the secret. “insecure” parameter has been changed to “skipCertificateValidation” as insecure is deprecated and will be removed from use in config.yaml or secret.yaml in a future release. Users can continue to use any one of “insecure” or “skipCertificateValidation” for now. The driver would return an error if both parameters are used. Please note that log configuration parameters from v1.5 will no longer work in v2.0 and higher. Please refer to the Dynamic Logging Configuration section in Features for more information. If the user is using complex K8s version like “v1.21.3-mirantis-1”, use this kubeVersion check in helm/csi-unity/Chart.yaml file. kubeVersion: “\u003e= 1.21.0-0 \u003c 1.28.0-0” Default logging options are set during Helm install. To see possible configuration options, see the Dynamic Logging Configuration section in Features.\nIf using automated SDC deployment:\nCheck the SDC container image is the correct version for your version of PowerFlex. Copy the default values.yaml file\ncd helm \u0026\u0026 cp csi-vxflexos/values.yaml myvalues.yaml If you are using a custom image, check the version and driverRepository fields in myvalues.yaml to make sure that they are pointing to the correct image repository and driver version. These two fields are spliced together to form the image name, as shown here: \u003cdriverRepository\u003e/csi-vxflexos:v\u003cversion\u003e\nLook over all the other fields myvalues.yaml and fill in/adjust any as needed. All the fields are described here:\nParameter Description Required Default version Set to verify the values file version matches driver version and used to pull the image as part of the image name. Yes 2.7.1 driverRepository Set to give the repository containing the driver image (used as part of the image name). Yes dellemc powerflexSdc Set to give the location of the SDC image used if automatic SDC deployment is being utilized. Yes dellemc/sdc:3.6.0.6 certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. No 0 logLevel CSI driver log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes “debug” logFormat CSI driver log format. Allowed values: “TEXT” or “JSON”. Yes “TEXT” kubeletConfigDir kubelet config directory path. Ensure that the secret.yaml file is present at this path. Yes /var/lib/kubelet defaultFsType Used to set the default FS type which will be used for mount volumes if FsType is not specified in the storage class. Allowed values: ext4, xfs. Yes ext4 fsGroupPolicy Defines which FS Group policy mode to be used. Supported modes areNone, File, and ReadWriteOnceWithFSType. No “ReadWriteOnceWithFSType” imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Allowed values: Always, IfNotPresent, Never. Yes IfNotPresent enablesnapshotcgdelete A boolean that, when enabled, will delete all snapshots in a consistency group everytime a snap in the group is deleted. Yes false enablelistvolumesnapshot A boolean that, when enabled, will allow list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap). It is recommend this be false unless instructed otherwise. Yes false allowRWOMultiPodAccess Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. Yes false controller This section allows the configuration of controller-specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - - volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. Yes “k8s” controllerCount Set to deploy multiple controller instances. If the controller count is greater than the number of available nodes, excess pods remain in a pending state. It should be greater than 0. You can increase the number of available nodes by configuring the “controller” section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2 snapshot.enabled A boolean that enable/disable volume snapshot feature. No true resizer.enabled A boolean that enable/disable volume expansion feature. No true nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. Yes \" \" tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install the controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. Yes \" \" healthMonitor This section configures the optional deployment of the external health monitor sidecar, for controller side volume health monitoring. - - enabled Enable/Disable deployment of external health monitor sidecar. No false interval Interval of monitoring volume health condition. Allowed values: Number followed by unit (s,m,h) No 60s node This section allows the configuration of node-specific parameters. - - healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition No false nodeSelector Defines what nodes would be selected for pods of node daemonset. Leave as blank to use all nodes. Yes \" \" tolerations Defines tolerations that would be applied to node daemonset. Leave as blank to install node driver only on worker nodes. Yes \" \" renameSDC This section allows the rename operation for SDC. - - enabled A boolean that enable/disable rename SDC feature. No false prefix Defines a string for the prefix of the SDC. No \" \" approveSDC.enabled A boolean that enable/disable SDC approval feature. No false monitor This section allows the configuration of the SDC monitoring pod. - - enabled Set to enable the usage of the monitoring pod. Yes false hostNetwork Set whether the monitor pod should run on the host network or not. Yes true hostPID Set whether the monitor pod should run in the host namespace or not. Yes true vgsnapshotter This section allows the configuration of the volume group snapshotter(vgsnapshotter) pod. - - enabled A boolean that enable/disable vg snapshotter feature. No false image Image for vg snapshotter. No \" \" podmon Podmon is an optional feature to enable application pods to be resilient to node failure. - - enabled A boolean that enables/disables podmon feature. No false image image for podmon. No \" \" authorization Authorization is an optional feature to apply credential shielding of the backend PowerFlex. - - enabled A boolean that enables/disables authorization feature. No false sidecarProxyImage Image for csm-authorization-sidecar. No \" \" proxyHost Hostname of the csm-authorization server. No Empty skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization proxy server. No true Install the driver using csi-install.sh bash script by running cd dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml. You may modify the release name with the --release arg. If arg is not provided, release will be named vxflexos by default. Alternatively, to do a helm install solely with Helm charts (without shell scripts), refer to helm/README.md. NOTE:\nFor detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder.\nInstall script will validate MDM IP(s) in vxflexos-config secret and creates a new field consumed by the init container and sdc-monitor container\nThis install script also runs the verify.sh script. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes.\nIt is mandatory to run install script after changes to MDM configuration in vxflexos-config secret. Refer dynamic-array-configuration\nIf an extended Kubernetes version is being used (e.g. v1.21.3-mirantis-1) and is failing the version check in Helm even though it falls in the allowed range, then you must go into helm/csi-vxflexos/Chart.yaml and replace the standard kubeVersion check with the commented-out alternative. Please note that this will also allow the use of pre-release alpha and beta versions of Kubernetes, which is not supported.\n(Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.\nMount options are specified in storageclass yaml under mkfsFormatOption. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option. Certificate validation for PowerFlex Gateway REST API calls This topic provides details about setting up the certificate for the CSI Driver for Dell PowerFlex.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name vxflexos-certs-0 to vxflexos-certs-n based on the “.Values.certSecretCount” parameter present in the namespace vxflexos.\nThis secret contains the X509 certificates of the CA which signed PowerFlex gateway SSL certificate in PEM format.\nThe CSI driver exposes an install parameter in secret.yaml, skipCertificateValidation, which determines if the driver performs client-side verification of the gateway certificates.\nskipCertificateValidation parameter is set to true by default, and the driver does not verify the gateway certificates.\nIf skipCertificateValidation is set to false, then the secret vxflexos-certs-n must contain the CA certificate for the array gateway.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the gateway certificate is self-signed or if you are using an embedded gateway, then perform the following steps.\nTo fetch the certificate, run the following command.\nopenssl s_client -showcerts -connect \u003cGateway IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example:\nopenssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Run the following command to create the cert secret with index ‘0’:\nkubectl create secret generic vxflexos-certs-0 --from-file=cert-0=ca_cert_0.pem -n vxflexos Use the following command to replace the secret:\nkubectl create secret generic vxflexos-certs-0 -n vxflexos --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: vxflexos-certs-1, vxflexos-certs-2, etc)\nNotes:\n“vxflexos” is the namespace for Helm-based installation but namespace can be user-defined in operator-based installation. User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation. Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver. Updating vxflexos-certs-n secrets is a manual process, unlike vxflexos-config. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter. Storage Classes For CSI driver for PowerFlex version 1.4 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class: There are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\nEdit storageclass.yaml if you need ext4 filesystem and storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have. Replace \u003cSYSTEM_ID\u003e with the system ID you have. Note there are two appearances in the file. Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f storageclass.yaml or kubectl create -f storageclass-xfs.yaml NOTE:\nAt least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es): Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerFlex v1.5, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\n","categories":"","description":"Installing the CSI Driver for PowerFlex via Helm\n","excerpt":"Installing the CSI Driver for PowerFlex via Helm\n","ref":"/csm-docs/v2/csidriver/installation/helm/powerflex/","tags":"","title":"PowerFlex"},{"body":" The Dell CSI Operator is no longer actively maintained or supported. Dell CSI Operator has been replaced with Dell CSM Operator. If you are currently using Dell CSI Operator, refer to the operator migration documentation to migrate from Dell CSI Operator to Dell CSM Operator.\nCSM 1.7.1 is applicable to helm based installations of PowerFlex driver.\nInstalling CSI Driver for PowerFlex via Operator The CSI Driver for Dell PowerFlex can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: If multipath is configured, ensure CSI-PowerFlex volumes are blacklisted by multipathd. See troubleshooting section for details SDC Deployment for Operator This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non-supported versions of the OS also do the manual SDC deployment steps given below. Refer to https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When the driver is created, MDM value for initContainers in driver CR is set by the operator from mdm attributes in the driver configuration file, secret.yaml. An example of secret.yaml is below in this document. Do not set MDM value for initContainers in the driver CR file manually. Note: To use an sdc-binary module from customer ftp site: Create a secret, sdc-repo-secret.yaml to contain the credentials for the private repo. To generate the base64 encoding of a credential: echo -n \u003ccredential\u003e| base64 -i secret sample to use: apiVersion: v1 kind: Secret metadata: name: sdc-repo-creds namespace: vxflexos type: Opaque data: # set username to the base64 encoded username, sdc default is username: \u003cusername in base64\u003e # set password to the base64 encoded password, sdc default is password: \u003cpassword in base64\u003e Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml. Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml. Please note the following: If using sidecar, you will need to edit the value fields under the HOST_PID and MDM fields by filling the empty quotes with host PID and the MDM IPs. If not using sidecar, please leave this commented out – otherwise, the empty fields will cause errors. Example CR: config/samples/vxflex_v270_ops_412.yaml sideCars: # Comment the following section if you don't want to run the monitoring sidecar - name: sdc-monitor envs: - name: HOST_PID value: \"1\" - name: MDM value: \"\" - name: external-health-monitor args: [\"--monitor-interval=60s\"] initContainers: - image: dellemc/sdc:3.6 imagePullPolicy: IfNotPresent name: sdc envs: - name: MDM value: \"10.x.x.x,10.x.x.x\" Note: Please comment the sdc-monitor sidecar section if you are not using it. Blank values for MDM will result in error. Do not comment the external-health-monitor argument.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\nDownload the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide: For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version. To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx Install Driver Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e command using the desired name to create the namespace.\nPrepare the secret.yaml for driver configuration.\nExample: secret.yaml\n# Username for accessing PowerFlex system.\t# Required: true - username: \"admin\" # Password for accessing PowerFlex system.\t# Required: true password: \"password\" # System name/ID of PowerFlex system.\t# Required: true systemID: \"ID1\" # REST API gateway HTTPS endpoint/PowerFlex Manager public IP for PowerFlex system. # Required: true endpoint: \"https://127.0.0.1\" # Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. # Allowed values: true or false # Required: true # Default value: true skipCertificateValidation: true # indicates if this array is the default array # needed for backwards compatibility # only one array is allowed to have this set to true # Required: false # Default value: false isDefault: true # defines the MDM(s) that SDC should register with on start. # Allowed values: a list of IP addresses or hostnames separated by comma. # Required: true # Default value: none mdm: \"10.0.0.1,10.0.0.2\" # Defines all system names used to create powerflex volumes # Required: false # Default value: none AllSystemNames: \"name1,name2\" - username: \"admin\" password: \"Password123\" systemID: \"ID2\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true mdm: \"10.0.0.3,10.0.0.4\" AllSystemNames: \"name1,name2\" After editing the file, run the following command to create a secret called vxflexos-config\nkubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=secret.yaml Use the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Note:\nSystem ID, MDM configuration, etc. now are taken directly from secret.yaml. MDM provided in the input_sample_file.yaml will be overidden with MDM values in secret.yaml. Please provide MDM values in input_sample_file.yaml so that it will be overidden by default value. Create a Custom Resource (CR) for PowerFlex using the sample files provided here.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\nParameter Description Required Default replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, excess pods will become stay in a pending state. Defaults are 2 which allows for Controller high availability. Yes 2 fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” Common parameters for node and controller X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false X_CSI_DEBUG To enable debug mode No true X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false Execute the kubectl create -f \u003cinput_sample_file.yaml\u003e command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.\nExample CR for PowerFlex Driver apiVersion: storage.dell.com/v1 kind: CSIVXFlexOS metadata: name: test-vxflexos namespace: test-vxflexos spec: driver: configVersion: v2.6.0 replicas: 1 dnsPolicy: ClusterFirstWithHostNet forceUpdate: false fsGroupPolicy: File common: image: \"dellemc/csi-vxflexos:v2.7.0\" imagePullPolicy: IfNotPresent envs: - name: X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT value: \"false\" - name: X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE value: \"false\" - name: X_CSI_DEBUG value: \"true\" - name: X_CSI_ALLOW_RWO_MULTI_POD_ACCESS value: \"false\" sideCars: # comment the following section if you don't want to run the monitoring sidecar - name: sdc-monitor envs: - name: HOST_PID value: \"1\" - name: MDM value: \"\" # Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" initContainers: - image: dellemc/sdc:3.6.0.6 imagePullPolicy: IfNotPresent name: sdc envs: - name: MDM value: \"10.xx.xx.xx,10.xx.xx.xx\" #provide MDM value --- apiVersion: v1 kind: ConfigMap metadata: name: vxflexos-config-params namespace: test-vxflexos data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"debug\" CSI_LOG_FORMAT: \"TEXT\" Pre-Requisite for installation with OLM Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM. #Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM\ngit clone https://github.com/dell/dell-csi-operator.git cd dell-csi-operator tar -czf config.tar.gz driverconfig/ kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u003coperator-namespace\u003e Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator.\nTo enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" ","categories":"","description":"Installing CSI Driver for PowerFlex via Operator\n","excerpt":"Installing CSI Driver for PowerFlex via Operator\n","ref":"/csm-docs/v2/csidriver/installation/operator/powerflex/","tags":"","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\nNavigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following: Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \"docker.io/centos:latest\" Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured. Results\nAn outline of this workflow is described below:\nThe 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below: kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol namespace: helmtest-vxflexos spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: vxflexos The volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use a storage class named vxflexos. This step yields a mounted ext4 file system. You can create the vxflexos and vxflexos-xfs storage classes by using the yamls located in samples/storageclass. If you compare pvol0.yaml and pvol1.yaml, you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos. NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.\nTest creating snapshots Test the workflow for snapshot creation.\nNOTE: Starting with version 2.0, CSI Driver for PowerFlex helm tests are designed to work exclusively with v1 snapshots.\nSteps\nStart the 2vols container and leave it running. Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly. Run sh snaptest.sh to start the test. This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in snap1.yaml and snap2.yaml. The following are the contents of snap1.yaml:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Results\nThe snaptest.sh script will create a snapshot using the definitions in the snap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0, then the created snapshot is named pvol0-snap1.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n helmtest-vxflexos.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell PowerFlex installation does not create this class. You will need to create instance of VolumeSnapshotClass from one of default samples in `samples/volumesnapshotclass’ directory.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\nRun sh snaprestoretest.sh to start the test. This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updated directory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\nHelm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly. Results\nAn outline of this workflow is described below:\nThe snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim, which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap1 kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi NOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in snap1.yaml.\n","categories":"","description":"Tests to validate PowerFlex CSI Driver installation","excerpt":"Tests to validate PowerFlex CSI Driver installation","ref":"/csm-docs/v2/csidriver/installation/test/powerflex/","tags":"","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v2.7.1 New Features/Changes K8 1.27 support added. OCP 4.12 support added CSM Operator: Support install of Resiliency module Fixed Issues Fix the offline helm installation failure Known Issues Issue Workaround Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100 When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. sdc:3.6.0.6 is causing issues while installing the csi-powerflex driver on ubuntu,RHEL8.3 Workaround: Change the powerflexSdc to sdc:3.6 in values.yaml https://github.com/dell/csi-powerflex/blob/72b27acee7553006cc09df97f85405f58478d2e4/helm/csi-vxflexos/values.yaml#L13 Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.\nCSI-PowerFlex v2.7.1 is applicable only for helm based installations.\n","categories":"","description":"Release notes for PowerFlex CSI driver","excerpt":"Release notes for PowerFlex CSI driver","ref":"/csm-docs/v2/csidriver/release/powerflex/","tags":"","title":"PowerFlex"},{"body":" Symptoms Prevention, Resolution or Workaround The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that need to pull an image of the driver. When you run the command kubectl describe pods vxflexos-controller-* –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command. The kubectl logs -n vxflexos vxflexos-controller-* driver logs show that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system. The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script. CreateVolume error System is not configured in the driver Powerflex name if used for systemID in StorageClass ensure same name is also used in array config systemID Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on a worker node, and ensure your container run time manager is properly configured to be utilized with SElinux. Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux. Installation of the driver on Kubernetes v1.25/v1.26/v1.27 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.23/v1.24/v1.25 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements The kubectl logs -n vxflexos vxflexos-controller-* driver logs show x509: certificate signed by unknown authority A self assigned certificate is used for PowerFlex array. See certificate validation for PowerFlex Gateway When you run the command kubectl apply -f snapclass-v1.yaml, you get the error error: unable to recognize \"snapclass-v1.yaml\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Check to make sure that the v1 snapshotter CRDs are installed, and not the v1beta1 CRDs, which are no longer supported. The controller pod is stuck and producing errors such as\" Failed to watch *v1.VolumeSnapshotContent: failed to list *v1.VolumeSnapshotContent: the server could not find the requested resource (get volumesnapshotcontents.snapshot.storage.k8s.io) Make sure that v1 snapshotter CRDs and v1 snapclass are installed, and not v1beta1, which is no longer supported. Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.21.0 \u003c= 1.27.0 which is incompatible with Kubernetes V1.21.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart at helm/csi-vxflexos/Chart.yaml and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. Volume metrics are missing Enable Volume Health Monitoring When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. CSI-PowerFlex volumes cannot mount; are being recognized as multipath devices CSI-PowerFlex does not support multipath; to fix: 1. Remove any multipath mapping involving a powerflex volume with multipath -f \u003cpowerflex volume\u003e 2. Blacklist CSI-PowerFlex volumes in multipath config file When attempting a driver upgrade, you see: spec.fsGroupPolicy: Invalid value: \"xxx\": field is immutable You cannot upgrade between drivers with different fsGroupPolicies. See upgrade documentation for more details When accessing ROX mode PVC in OpenShift where the worker nodes are non-root user, you see: Permission denied while accesing the PVC mount location from the pod. Set the securityContext for ROX mode PVC pod as below, as it defines privileges for the pods or containers.securityContext: runAsUser: 0 runAsGroup: 0 After installing version v2.6.0 of the driver using the default powerflexSdc image, sdc:3.6.0.6, the vxflexos-node pods are in an Init:CrashLoopBackOff state. This issue can happen on hosts that require the SDC to be installed manually. Automatic SDC is only supported on Red Hat CoreOS (RHCOS), RHEL 7.9, RHEL 8.4, RHEL 8.6. The SDC is already installed. Change the images.powerflexSdc value to an empty value in the values and re-install. In version v2.6.0, the driver is crashing because the External Health Monitor sidecar crashes when a persistent volume is not found. This is a known issue reported at kubernetes-csi/external-health-monitor#100. In version v2.6.0, when a cluster node goes down, the block volumes attached to the node cannot be attached to another node. This is a known issue reported at kubernetes-csi/external-attacher#215. Workaround: 1. Force delete the pod running on the node that went down. 2. Delete the pod’s persistent volume attachment on the node that went down. Now the volume can be attached to the new node. Note: vxflexos-controller-* is the controller pod that acquires leader lease\n","categories":"","description":"Troubleshooting PowerFlex Driver","excerpt":"Troubleshooting PowerFlex Driver","ref":"/csm-docs/v2/csidriver/troubleshooting/powerflex/","tags":"","title":"PowerFlex"},{"body":" CSM 1.7.1 is applicable to helm based installations of PowerFlex driver.\nInstalling CSI Driver for PowerFlex via Dell CSM Operator The CSI Driver for Dell PowerFlex can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using this command:\nkubectl get csm --all-namespaces Prerequisites If multipath is configured, ensure CSI-PowerFlex volumes are blacklisted by multipathd. See troubleshooting section for details. NOTE: This step can be skipped with OpenShift.\nSDC Deployment for Operator This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non-supported versions of the OS also do the manual SDC deployment steps given below. Refer to https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When the driver is created, MDM value for initContainers in driver CR is set by the operator from mdm attributes in the driver configuration file, config.yaml. An example of config.yaml is below in this document. Do not set MDM value for initContainers in the driver CR file manually. Optionally, enable sdc monitor by setting the enable flag for the sdc-monitor to true. Please note: If using sidecar, you will need to edit the value fields under the HOST_PID and MDM fields by filling the empty quotes with host PID and the MDM IPs. If not using sidecar, leave the enabled field set to false. Example CR: samples/storage_csm_powerflex_v290.yaml sideCars: # sdc-monitor is disabled by default, due to high CPU usage - name: sdc-monitor enabled: false image: dellemc/sdc:3.6.0.6 envs: - name: HOST_PID value: \"1\" - name: MDM value: \"10.xx.xx.xx,10.xx.xx.xx\" #provide the same MDM value from secret Manual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC using this procedure:\nSteps\nDownload the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide: For environments using RPM, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version. To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx1. Create namespace. Execute kubectl create namespace vxflexos to create the vxflexos namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘vxflexos’ NOTE: This step can be skipped with OpenShift CoreOS nodes.\nCreate Secret Create namespace: Execute kubectl create namespace vxflexos to create the vxflexos namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘vxflexos’\nPrepare the secret.yaml for driver configuration.\nExample: secret.yaml\n# Username for accessing PowerFlex system. # If authorization is enabled, username will be ignored. - username: \"admin\" # Password for accessing PowerFlex system. # If authorization is enabled, password will be ignored. password: \"password\" # System name/ID of PowerFlex system.\t# Required: true systemID: \"1a99aa999999aa9a\" # Required: false # Previous names used in secret of PowerFlex system. Only needed if PowerFlex System Name has been changed by user # and old resources are still based on the old name. allSystemNames: \"pflex-1,pflex-2\" # REST API gateway HTTPS endpoint for PowerFlex system. # If authorization is enabled, endpoint should be the HTTPS localhost endpoint that # the authorization sidecar will listen on endpoint: \"https://127.0.0.1\" # Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. # Allowed values: true or false # Default value: true skipCertificateValidation: true # indicates if this array is the default array # needed for backwards compatibility # only one array is allowed to have this set to true # Default value: false isDefault: true # defines the MDM(s) that SDC should register with on start. # Allowed values: a list of IP addresses or hostnames separated by comma. # Default value: none mdm: \"10.0.0.1,10.0.0.2\" # Defines all system names used to create powerflex volumes # Required: false # Default value: none AllSystemNames: \"name1,name2\" - username: \"admin\" password: \"Password123\" systemID: \"2b11bb111111bb1b\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true mdm: \"10.0.0.3,10.0.0.4\" AllSystemNames: \"name1,name2\" After editing the file, run this command to create a secret called test-vxflexos-config. If you are using a different namespace/secret name, just substitute those into the command.\nkubectl create secret generic test-vxflexos-config -n test-vxflexos --from-file=config=config.yaml Use this command to replace or update the secret:\nkubectl create secret generic test-vxflexos-config -n test-vxflexos --from-file=config=config.yaml -o yaml --dry-run=client | kubectl replace -f - Install Driver Follow all the prerequisites above\nCreate a CR (Custom Resource) for PowerFlex using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\nParameter Description Required Default dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, excess pods will become stay in a pending state. Defaults are 2 which allows for Controller high availability. Yes 2 Common parameters for node and controller X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false X_CSI_DEBUG To enable debug mode No true X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false Execute this command to create PowerFlex custom resource:\nkubectl create -f \u003cinput_sample_file.yaml\u003e This command will deploy the CSI-PowerFlex driver in the namespace specified in the input YAML file.\nVerify the CSI Driver installation\nNote :\n“Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. ","categories":"","description":"Installing Dell CSI Driver for PowerFlex via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for PowerFlex via Dell CSM Operator\n","ref":"/csm-docs/v2/deployment/csmoperator/drivers/powerflex/","tags":"","title":"PowerFlex"},{"body":"Configuring PowerFlex CSI Driver with CSM for Authorization Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow these steps to configure the CSI Drivers to work with the Authorization sidecar:\nApply the secret containing the token data into the driver namespace. It’s assumed that the Kubernetes administrator has the token secret manifest saved in /tmp/token.yaml.\n# It is assumed that array type powerflex has the namepace \"vxflexos\". kubectl apply -f /tmp/token.yaml -n vxflexos Edit these parameters in samples/secret/karavi-authorization-config.json file in the CSI PowerFlex driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\nParameter Description Required Default username Username for connecting to the backend storage array. This parameter is ignored. No - password Password for connecting to to the backend storage array. This parameter is ignored. No - intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes - endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400 systemID System ID of the backend storage array. Yes \" \" skipCertificateValidation A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml Create the karavi-authorization-config secret using this command:\nkubectl -n vxflexos create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f -\nNote:\nCreate the driver secret as you would normally except update/add the connection information for communicating with the sidecar instead of the backend storage array and scrub the username and password. Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n vxflexos create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f -\nOtherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n vxflexos create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f -\nPlease refer to step 4 in the installation steps for PowerFlex to edit the parameters in samples/config.yaml file to communicate with the sidecar.\nUpdate endpoint to match the endpoint in samples/secret/karavi-authorization-config.json\nCreate the vxflexos-config secret using this command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml -o yaml --dry-run=client | kubectl apply -f -\nPlease refer to step 9 in the installation steps for PowerFlex to edit the parameters in myvalues.yaml file to communicate with the sidecar.\nEnable CSM for Authorization and provide the proxyHost address\nInstall the CSI PowerFlex driver\n(Optional) Install dellctl to perform Kubernetes administrator commands for additional capabilities (e.g., list volumes). Please refer to the dellctl documentation page for the installation steps and command list.\n","categories":"","description":"Enabling CSM Authorization for PowerFlex CSI Driver\n","excerpt":"Enabling CSM Authorization for PowerFlex CSI Driver\n","ref":"/csm-docs/v3/authorization/configuration/powerflex/","tags":"","title":"PowerFlex"},{"body":"The CSI Driver for Dell PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\nCSI Driver for Dell PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\nCSI Driver for Dell PowerFlex Kubernetes Node Registrar, which handles the driver registration Prerequisites The following are requirements that must be met before installing the CSI Driver for Dell PowerFlex:\nInstall Kubernetes or OpenShift (see supported versions) Install Helm 3 Enable Zero Padding on PowerFlex Mount propagation is enabled on container runtime that is being used Install PowerFlex Storage Data Client If using Snapshot feature, satisfy all Volume Snapshot requirements A user must exist on the array with a role \u003e= FrontEndConfigure If enabling CSM for Authorization, please refer to the Authorization deployment steps first If multipath is configured, ensure CSI-PowerFlex volumes are blacklisted by multipathd. See troubleshooting section for details Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nEnable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. For more information to configure this setting, see Dell PowerFlex documentation.\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run the node portion of the CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment; for Red Hat CoreOS (RHCOS), RHEL 7.9, RHEL 8.4, RHEL 8.6. On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions.\nNOTE: To install CSI driver for Powerflex with automated SDC deployment, you need below two packages on worker nodes.\nlibaio numactl-libs Optional: For a typical install, you will pull SDC kernel modules from the Dell FTP site, which is set up by default. Some users might want to mirror this repository to a local location. The PowerFlex KB article has instructions on how to do this.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\nDownload the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide: For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version. To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx (Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\nInstall the Driver Steps\nRun git clone -b v2.6.0 https://github.com/dell/csi-powerflex.git to clone the git repository.\nA namespace for the driver is expected prior to running the command below. If one is not created already, you can run kubectl create namespace vxflexos to create a new one. Note that the namespace can be any user-defined name that follows the conventions for namespaces outlined by Kubernetes. In this example we assume that the namespace is ‘vxflexos’\nCollect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the scripts directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the values for these parameters as they must be entered into samples/secret.yaml.\nPrepare samples/secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\nParameter Description Required Default username Username for accessing PowerFlex system. If authorization is enabled, username will be ignored. true - password Password for accessing PowerFlex system. If authorization is enabled, password will be ignored. true - systemID PowerFlex system name or ID. true - allSystemNames List of previous names of powerflex array if used for PV create false - endpoint REST API gateway HTTPS endpoint/PowerFlex Manager public IP for PowerFlex system. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on true - skipCertificateValidation Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. true true isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list. false false mdm mdm defines the MDM(s) that SDC should register with on start. This should be a list of MDM IP addresses or hostnames separated by comma. true - Example: samples/secret.yaml\n- username: \"admin\" password: \"Password123\" systemID: \"2b11bb111111bb1b\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true isDefault: true mdm: \"10.0.0.3,10.0.0.4\" NOTE: To use multiple arrays, copy and paste section above for each array. Make sure isDefault is set to true for only one array.\nAfter editing the file, run the below command to create a secret called vxflexos-config. This assumes vxflexos is release name, but it can be modified during install:\n`kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/secret.yaml` Use the below command to replace or update the secret:\n`kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/secret.yaml -o yaml --dry-run=client | kubectl replace -f -` NOTE:\nThe user needs to validate the YAML syntax and array-related key/values while replacing the vxflexos-creds secret. If you want to create a new array or update the MDM values in the secret, you will need to reinstall the driver. If you change other details, such as login information, the secret will dynamically update – see dynamic-array-configuration for more details. Old json format of the array configuration file is still supported in this release. If you already have your configuration in json format, you may continue to maintain it or you may transfer this configuration to yamlformat and replace/update the secret. “insecure” parameter has been changed to “skipCertificateValidation” as insecure is deprecated and will be removed from use in config.yaml or secret.yaml in a future release. Users can continue to use any one of “insecure” or “skipCertificateValidation” for now. The driver would return an error if both parameters are used. Please note that log configuration parameters from v1.5 will no longer work in v2.0 and higher. Please refer to the Dynamic Logging Configuration section in Features for more information. If the user is using complex K8s version like “v1.21.3-mirantis-1”, use this kubeVersion check in helm/csi-unity/Chart.yaml file. kubeVersion: “\u003e= 1.21.0-0 \u003c 1.27.0-0” Default logging options are set during Helm install. To see possible configuration options, see the Dynamic Logging Configuration section in Features.\nIf using automated SDC deployment:\nCheck the SDC container image is the correct version for your version of PowerFlex. Copy the default values.yaml file cd helm \u0026\u0026 cp csi-vxflexos/values.yaml myvalues.yaml\nIf you are using a custom image, check the version and driverRepository fields in myvalues.yaml to make sure that they are pointing to the correct image repository and driver version. These two fields are spliced together to form the image name, as shown here: \u003cdriverRepository\u003e/csi-vxflexos:v\u003cversion\u003e\nLook over all the other fields myvalues.yaml and fill in/adjust any as needed. All the fields are described here:\nParameter Description Required Default version Set to verify the values file version matches driver version and used to pull the image as part of the image name. Yes 2.6.0 driverRepository Set to give the repository containing the driver image (used as part of the image name). Yes dellemc powerflexSdc Set to give the location of the SDC image used if automatic SDC deployment is being utilized. Yes dellemc/sdc:3.6.1 certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. No 0 logLevel CSI driver log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes “debug” logFormat CSI driver log format. Allowed values: “TEXT” or “JSON”. Yes “TEXT” kubeletConfigDir kubelet config directory path. Ensure that the config.yaml file is present at this path. Yes /var/lib/kubelet defaultFsType Used to set the default FS type which will be used for mount volumes if FsType is not specified in the storage class. Allowed values: ext4, xfs. Yes ext4 fsGroupPolicy Defines which FS Group policy mode to be used. Supported modes areNone, File, and ReadWriteOnceWithFSType. No “ReadWriteOnceWithFSType” imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Allowed values: Always, IfNotPresent, Never. Yes IfNotPresent enablesnapshotcgdelete A boolean that, when enabled, will delete all snapshots in a consistency group everytime a snap in the group is deleted. Yes false enablelistvolumesnapshot A boolean that, when enabled, will allow list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap). It is recommend this be false unless instructed otherwise. Yes false allowRWOMultiPodAccess Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. Yes false controller This section allows the configuration of controller-specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - - volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. Yes “k8s” controllerCount Set to deploy multiple controller instances. If the controller count is greater than the number of available nodes, excess pods remain in a pending state. It should be greater than 0. You can increase the number of available nodes by configuring the “controller” section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2 snapshot.enabled A boolean that enable/disable volume snapshot feature. No true resizer.enabled A boolean that enable/disable volume expansion feature. No true nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. Yes \" \" tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install the controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. Yes \" \" healthMonitor This section configures the optional deployment of the external health monitor sidecar, for controller side volume health monitoring. - - enabled Enable/Disable deployment of external health monitor sidecar. No false interval Interval of monitoring volume health condition. Allowed values: Number followed by unit (s,m,h) No 60s node This section allows the configuration of node-specific parameters. - - healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition No false nodeSelector Defines what nodes would be selected for pods of node daemonset. Leave as blank to use all nodes. Yes \" \" tolerations Defines tolerations that would be applied to node daemonset. Leave as blank to install node driver only on worker nodes. Yes \" \" renameSDC This section allows the rename operation for SDC. - - enabled A boolean that enable/disable rename SDC feature. No false prefix Defines a string for the prefix of the SDC. No \" \" approveSDC.enabled A boolean that enable/disable SDC approval feature. No false monitor This section allows the configuration of the SDC monitoring pod. - - enabled Set to enable the usage of the monitoring pod. Yes false hostNetwork Set whether the monitor pod should run on the host network or not. Yes true hostPID Set whether the monitor pod should run in the host namespace or not. Yes true vgsnapshotter This section allows the configuration of the volume group snapshotter(vgsnapshotter) pod. - - enabled A boolean that enable/disable vg snapshotter feature. No false image Image for vg snapshotter. No \" \" podmon Podmon is an optional feature to enable application pods to be resilient to node failure. - - enabled A boolean that enables/disables podmon feature. No false image image for podmon. No \" \" authorization Authorization is an optional feature to apply credential shielding of the backend PowerFlex. - - enabled A boolean that enables/disables authorization feature. No false sidecarProxyImage Image for csm-authorization-sidecar. No \" \" proxyHost Hostname of the csm-authorization server. No Empty skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. No true Install the driver using csi-install.sh bash script by running cd dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml. You may modify the release name with the --release arg. If arg is not provided, release will be named vxflexos by default. Alternatively, to do a helm install solely with Helm charts (without shell scripts), refer to helm/README.md. NOTE:\nFor detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder.\nInstall script will validate MDM IP(s) in vxflexos-config secret and creates a new field consumed by the init container and sdc-monitor container\nThis install script also runs the verify.sh script. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes.\nIt is mandatory to run install script after changes to MDM configuration in vxflexos-config secret. Refer dynamic-array-configuration\nIf an extended Kubernetes version is being used (e.g. v1.21.3-mirantis-1) and is failing the version check in Helm even though it falls in the allowed range, then you must go into helm/csi-vxflexos/Chart.yaml and replace the standard kubeVersion check with the commented-out alternative. Please note that this will also allow the use of pre-release alpha and beta versions of Kubernetes, which is not supported.\n(Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.\nMount options are specified in storageclass yaml under mkfsFormatOption. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option. Certificate validation for PowerFlex Gateway REST API calls This topic provides details about setting up the certificate for the CSI Driver for Dell PowerFlex.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name vxflexos-certs-0 to vxflexos-certs-n based on the “.Values.certSecretCount” parameter present in the namespace vxflexos.\nThis secret contains the X509 certificates of the CA which signed PowerFlex gateway SSL certificate in PEM format.\nThe CSI driver exposes an install parameter in config.yaml, skipCertificateValidation, which determines if the driver performs client-side verification of the gateway certificates.\nskipCertificateValidation parameter is set to true by default, and the driver does not verify the gateway certificates.\nIf skipCertificateValidation is set to false, then the secret vxflexos-certs-n must contain the CA certificate for the array gateway.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the gateway certificate is self-signed or if you are using an embedded gateway, then perform the following steps.\nTo fetch the certificate, run the following command.\n`openssl s_client -showcerts -connect \u003cGateway IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem` Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem\nRun the following command to create the cert secret with index ‘0’:\n`kubectl create secret generic vxflexos-certs-0 --from-file=cert-0=ca_cert_0.pem -n vxflexos` Use the following command to replace the secret:\n`kubectl create secret generic vxflexos-certs-0 -n vxflexos --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -` Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: vxflexos-certs-1, vxflexos-certs-2, etc)\nNotes:\n“vxflexos” is the namespace for Helm-based installation but namespace can be user-defined in operator-based installation. User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation. Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver. Updating vxflexos-certs-n secrets is a manual process, unlike vxflexos-config. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter. Storage Classes For CSI driver for PowerFlex version 1.4 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class: There are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\nEdit storageclass.yaml if you need ext4 filesystem and storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have. Replace \u003cSYSTEM_ID\u003e with the system ID you have. Note there are two appearances in the file. Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f storageclass.yaml or kubectl create -f storageclass-xfs.yaml NOTE:\nAt least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es): Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerFlex v1.5, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\n","categories":"","description":"Installing the CSI Driver for PowerFlex via Helm\n","excerpt":"Installing the CSI Driver for PowerFlex via Helm\n","ref":"/csm-docs/v3/csidriver/installation/helm/powerflex/","tags":"","title":"PowerFlex"},{"body":"Installing CSI Driver for PowerFlex via Operator The CSI Driver for Dell PowerFlex can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: If multipath is configured, ensure CSI-PowerFlex volumes are blacklisted by multipathd. See troubleshooting section for details SDC Deployment for Operator This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non-supported versions of the OS also do the manual SDC deployment steps given below. Refer to https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When the driver is created, MDM value for initContainers in driver CR is set by the operator from mdm attributes in the driver configuration file, config.yaml. An example of config.yaml is below in this document. Do not set MDM value for initContainers in the driver CR file manually. Note: To use an sdc-binary module from customer ftp site: Create a secret, sdc-repo-secret.yaml to contain the credentials for the private repo. To generate the base64 encoding of a credential: echo -n \u003ccredential\u003e| base64 -i secret sample to use:\napiVersion: v1 kind: Secret metadata: name: sdc-repo-creds namespace: vxflexos type: Opaque data: # set username to the base64 encoded username, sdc default is username: \u003cusername in base64\u003e # set password to the base64 encoded password, sdc default is password: \u003cpassword in base64\u003e Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml. Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml. Please note the following: If using sidecar, you will need to edit the value fields under the HOST_PID and MDM fields by filling the empty quotes with host PID and the MDM IPs. If not using sidecar, please leave this commented out – otherwise, the empty fields will cause errors. Example CR: config/samples/vxflex_v260_ops_411.yaml sideCars: # Comment the following section if you don't want to run the monitoring sidecar - name: sdc-monitor envs: - name: HOST_PID value: \"1\" - name: MDM value: \"\" - name: external-health-monitor args: [\"--monitor-interval=60s\"] initContainers: - image: dellemc/sdc:3.6 imagePullPolicy: IfNotPresent name: sdc envs: - name: MDM value: \"10.x.x.x,10.x.x.x\" Note: Please comment the sdc-monitor sidecar section if you are not using it. Blank values for MDM will result in error. Do not comment the external-health-monitor argument.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\nDownload the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide: For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version. To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx Install Driver Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e command using the desired name to create the namespace.\nPrepare the config.yaml for driver configuration.\nExample: config.yaml\n# Username for accessing PowerFlex system.\t# Required: true - username: \"admin\" # Password for accessing PowerFlex system.\t# Required: true password: \"password\" # System name/ID of PowerFlex system.\t# Required: true systemID: \"ID1\" # REST API gateway HTTPS endpoint/PowerFlex Manager public IP for PowerFlex system. # Required: true endpoint: \"https://127.0.0.1\" # Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. # Allowed values: true or false # Required: true # Default value: true skipCertificateValidation: true # indicates if this array is the default array # needed for backwards compatibility # only one array is allowed to have this set to true # Required: false # Default value: false isDefault: true # defines the MDM(s) that SDC should register with on start. # Allowed values: a list of IP addresses or hostnames separated by comma. # Required: true # Default value: none mdm: \"10.0.0.1,10.0.0.2\" # Defines all system names used to create powerflex volumes # Required: false # Default value: none AllSystemNames: \"name1,name2\" - username: \"admin\" password: \"Password123\" systemID: \"ID2\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true mdm: \"10.0.0.3,10.0.0.4\" AllSystemNames: \"name1,name2\" After editing the file, run the following command to create a secret called vxflexos-config kubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.yaml -o yaml --dry-run=client | kubectl replace -f -\nNote:\nSystem ID, MDM configuration, etc. now are taken directly from config.yaml. MDM provided in the input_sample_file.yaml will be overidden with MDM values in config.yaml. Please provide MDM values in input_sample_file.yaml so that it will be overidden by default value. Create a Custom Resource (CR) for PowerFlex using the sample files provided here.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\nParameter Description Required Default replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, excess pods will become stay in a pending state. Defaults are 2 which allows for Controller high availability. Yes 2 fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” Common parameters for node and controller X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false X_CSI_DEBUG To enable debug mode No true X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false Execute the kubectl create -f \u003cinput_sample_file.yaml\u003e command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.\nExample CR for PowerFlex Driver apiVersion: storage.dell.com/v1 kind: CSIVXFlexOS metadata: name: test-vxflexos namespace: test-vxflexos spec: driver: configVersion: v2.6.0 replicas: 1 dnsPolicy: ClusterFirstWithHostNet forceUpdate: false fsGroupPolicy: File common: image: \"dellemc/csi-vxflexos:v2.6.0\" imagePullPolicy: IfNotPresent envs: - name: X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT value: \"false\" - name: X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE value: \"false\" - name: X_CSI_DEBUG value: \"true\" - name: X_CSI_ALLOW_RWO_MULTI_POD_ACCESS value: \"false\" sideCars: # comment the following section if you don't want to run the monitoring sidecar - name: sdc-monitor envs: - name: HOST_PID value: \"1\" - name: MDM value: \"\" # Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" initContainers: - image: dellemc/sdc:3.6 imagePullPolicy: IfNotPresent name: sdc envs: - name: MDM value: \"10.xx.xx.xx,10.xx.xx.xx\" #provide MDM value --- apiVersion: v1 kind: ConfigMap metadata: name: vxflexos-config-params namespace: test-vxflexos data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"debug\" CSI_LOG_FORMAT: \"TEXT\" ``` ### Pre-Requisite for installation with OLM Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM. ```yaml 1. git clone https://github.com/dell/dell-csi-operator.git 2. cd dell-csi-operator 3. tar -czf config.tar.gz driverconfig/ # Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM 4. kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u003coperator-namespace\u003e Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator.\nTo enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" ","categories":"","description":"Installing CSI Driver for PowerFlex via Operator\n","excerpt":"Installing CSI Driver for PowerFlex via Operator\n","ref":"/csm-docs/v3/csidriver/installation/operator/powerflex/","tags":"","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\nNavigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following: Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \"docker.io/centos:latest\" Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured. Results\nAn outline of this workflow is described below:\nThe 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below: kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol namespace: helmtest-vxflexos spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: vxflexos The volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use a storage class named vxflexos. This step yields a mounted ext4 file system. You can create the vxflexos and vxflexos-xfs storage classes by using the yamls located in samples/storageclass. If you compare pvol0.yaml and pvol1.yaml, you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos. NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.\nTest creating snapshots Test the workflow for snapshot creation.\nNOTE: Starting with version 2.0, CSI Driver for PowerFlex helm tests are designed to work exclusively with v1 snapshots.\nSteps\nStart the 2vols container and leave it running. Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly. Run sh snaptest.sh to start the test. This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in snap1.yaml and snap2.yaml. The following are the contents of snap1.yaml:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Results\nThe snaptest.sh script will create a snapshot using the definitions in the snap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0, then the created snapshot is named pvol0-snap1.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n helmtest-vxflexos.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell PowerFlex installation does not create this class. You will need to create instance of VolumeSnapshotClass from one of default samples in `samples/volumesnapshotclass’ directory.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\nRun sh snaprestoretest.sh to start the test. This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updated directory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\nHelm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly. Results\nAn outline of this workflow is described below:\nThe snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim, which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap1 kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi NOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in snap1.yaml.\n","categories":"","description":"Tests to validate PowerFlex CSI Driver installation","excerpt":"Tests to validate PowerFlex CSI Driver installation","ref":"/csm-docs/v3/csidriver/installation/test/powerflex/","tags":"","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v2.6.0 New Features/Changes PowerFlex pre-approved GUIDs support added. Rename SDC support added. K8 1.26 support added. RKE 1.4.1 support added. MKE 3.6.0 support added. Fixed Issues Known Issues Issue Workaround Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100 When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. sdc:3.6.0.6 is causing issues while installing the csi-powerflex driver on ubuntu,RHEL8.3 Workaround: Change the powerflexSdc to sdc:3.6 in values.yaml https://github.com/dell/csi-powerflex/blob/72b27acee7553006cc09df97f85405f58478d2e4/helm/csi-vxflexos/values.yaml#L13 CSI-Powerflex driver installation is failing with the offline helm installer. This is a known issue and has been reported at https://github.com/dell/csm/issues/868. Workaround: Remove the ‘v’ from the following lines https://github.com/dell/csi-powerflex/blob/v2.6.0/dell-csi-helm-installer/csi-offline-bundle.sh#LL94C1-L95C92, Now there will not be any issue in CSI-Powerflex driver offline helm installtion. Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for PowerFlex CSI driver","excerpt":"Release notes for PowerFlex CSI driver","ref":"/csm-docs/v3/csidriver/release/powerflex/","tags":"","title":"PowerFlex"},{"body":" Symptoms Prevention, Resolution or Workaround The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that need to pull an image of the driver. When you run the command kubectl describe pods vxflexos-controller-* –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command. The kubectl logs -n vxflexos vxflexos-controller-* driver logs show that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system. The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script. CreateVolume error System is not configured in the driver Powerflex name if used for systemID in StorageClass ensure same name is also used in array config systemID Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on a worker node, and ensure your container run time manager is properly configured to be utilized with SElinux. Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux. Installation of the driver on Kubernetes v1.24/v1.25/v1.26 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.23/v1.24/v1.25 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements The kubectl logs -n vxflexos vxflexos-controller-* driver logs show x509: certificate signed by unknown authority A self assigned certificate is used for PowerFlex array. See certificate validation for PowerFlex Gateway When you run the command kubectl apply -f snapclass-v1.yaml, you get the error error: unable to recognize \"snapclass-v1.yaml\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Check to make sure that the v1 snapshotter CRDs are installed, and not the v1beta1 CRDs, which are no longer supported. The controller pod is stuck and producing errors such as\" Failed to watch *v1.VolumeSnapshotContent: failed to list *v1.VolumeSnapshotContent: the server could not find the requested resource (get volumesnapshotcontents.snapshot.storage.k8s.io) Make sure that v1 snapshotter CRDs and v1 snapclass are installed, and not v1beta1, which is no longer supported. Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.21.0 \u003c= 1.26.0 which is incompatible with Kubernetes V1.21.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart at helm/csi-vxflexos/Chart.yaml and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. Volume metrics are missing Enable Volume Health Monitoring When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. CSI-PowerFlex volumes cannot mount; are being recognized as multipath devices CSI-PowerFlex does not support multipath; to fix: 1. Remove any multipath mapping involving a powerflex volume with multipath -f \u003cpowerflex volume\u003e 2. Blacklist CSI-PowerFlex volumes in multipath config file When attempting a driver upgrade, you see: spec.fsGroupPolicy: Invalid value: \"xxx\": field is immutable You cannot upgrade between drivers with different fsGroupPolicies. See upgrade documentation for more details Note: vxflexos-controller-* is the controller pod that acquires leader lease\n","categories":"","description":"Troubleshooting PowerFlex Driver","excerpt":"Troubleshooting PowerFlex Driver","ref":"/csm-docs/v3/csidriver/troubleshooting/powerflex/","tags":"","title":"PowerFlex"},{"body":"Installing CSI Driver for PowerFlex via Dell CSM Operator The CSI Driver for Dell PowerFlex can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nNote: MKE (Mirantis Kubernetes Engine) does not support the installation of CSI-PowerFlex via Operator.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using this command: kubectl get csm --all-namespaces\nPrerequisites If multipath is configured, ensure CSI-PowerFlex volumes are blacklisted by multipathd. See troubleshooting section for details SDC Deployment for Operator This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non-supported versions of the OS also do the manual SDC deployment steps given below. Refer to https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When the driver is created, MDM value for initContainers in driver CR is set by the operator from mdm attributes in the driver configuration file, secret.yaml. An example of secret.yaml is provided in this document below. Do not set MDM value for initContainers in the driver CR file manually. Optionally, enable sdc monitor by setting the enable flag for the sdc-monitor to true. Please note: If using sidecar, you will need to edit the value fields under the HOST_PID and MDM fields by filling the empty quotes with host PID and the MDM IPs. If not using sidecar, leave the enabled field set to false. Example CR: samples/storage_csm_powerflex_v270.yaml sideCars: # sdc-monitor is disabled by default, due to high CPU usage - name: sdc-monitor enabled: false image: dellemc/sdc:3.6.1 envs: - name: HOST_PID value: \"1\" - name: MDM value: \"10.xx.xx.xx,10.xx.xx.xx\" #provide the same MDM value from secret Manual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC using this procedure:\nSteps\nDownload the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide: For environments using RPM, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version. To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx1. Create namespace. Execute kubectl create namespace vxflexos to create the vxflexos namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘vxflexos’ Create Secret Create namespace: Execute kubectl create namespace vxflexos to create the vxflexos namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘vxflexos’\nPrepare the secret.yaml for driver configuration.\nExample: secret.yaml\n# Username for accessing PowerFlex system. # If authorization is enabled, username will be ignored. - username: \"admin\" # Password for accessing PowerFlex system. # If authorization is enabled, password will be ignored. password: \"password\" # System name/ID of PowerFlex system.\t# Required: true systemID: \"1a99aa999999aa9a\" # Required: false # Previous names used in secret of PowerFlex system. Only needed if PowerFlex System Name has been changed by user # and old resources are still based on the old name. allSystemNames: \"pflex-1,pflex-2\" # REST API gateway HTTPS endpoint for PowerFlex system. # If authorization is enabled, endpoint should be the HTTPS localhost endpoint that # the authorization sidecar will listen on endpoint: \"https://127.0.0.1\" # Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. # Allowed values: true or false # Default value: true skipCertificateValidation: true # indicates if this array is the default array # needed for backwards compatibility # only one array is allowed to have this set to true # Default value: false isDefault: true # defines the MDM(s) that SDC should register with on start. # Allowed values: a list of IP addresses or hostnames separated by comma. # Default value: none mdm: \"10.0.0.1,10.0.0.2\" # Defines all system names used to create powerflex volumes # Required: false # Default value: none AllSystemNames: \"name1,name2\" - username: \"admin\" password: \"Password123\" systemID: \"2b11bb111111bb1b\" endpoint: \"https://127.0.0.2\" skipCertificateValidation: true mdm: \"10.0.0.3,10.0.0.4\" AllSystemNames: \"name1,name2\" After editing the file, run this command to create a secret called vxflexos-config. kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=secret.yaml\nUse this command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f -\nInstall Driver Follow all the prerequisites above\nCreate a CR (Custom Resource) for PowerFlex using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\nParameter Description Required Default dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, excess pods will become stay in a pending state. Defaults are 2 which allows for Controller high availability. Yes 2 Common parameters for node and controller X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false X_CSI_DEBUG To enable debug mode No true X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false Execute this command to create PowerFlex custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerFlex driver in the namespace specified in the input YAML file.\nVerify the CSI Driver installation\nNote :\n“Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. ","categories":"","description":"Installing Dell CSI Driver for PowerFlex via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for PowerFlex via Dell CSM Operator\n","ref":"/csm-docs/v3/deployment/csmoperator/drivers/powerflex/","tags":"","title":"PowerFlex"},{"body":"Configuring PowerMax CSI Driver with CSM for Authorization Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow these steps to configure the CSI Drivers to work with the Authorization sidecar:\nApply the secret containing the tenant token data into the driver namespace. It’s assumed that the Kubernetes administrator has the token secret manifest, generated by your storage administrator via Generate a Token, saved in /tmp/token.yaml.\n#It is assumed that array type powermax has the namespace “powermax”.\nkubectl apply -f /tmp/token.yaml -n powermax Edit these parameters in samples/secret/karavi-authorization-config.json file in CSI PowerMax driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\nParameter Description Required Default username Username for connecting to the backend storage array. This parameter is ignored. No - password Password for connecting to to the backend storage array. This parameter is ignored. No - intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes - endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400 systemID System ID of the backend storage array. Yes \" \" skipCertificateValidation A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml Create the karavi-authorization-config secret using this command:\nkubectl -n powermax create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f - Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n powermax create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f - Otherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n powermax create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f - Enable CSM Authorization in the driver installation applicable to your installation method.\nHelm\nIn Install the Driver where you edit samples/secret/secret.yaml with the credentials of the PowerMax, you can leave these with the default values as they will be ignored.\nRefer to the Install the Driver section to edit the parameters in my-powermax-settings.yaml file to configure the driver to communicate with the CSM Authorization sidecar.\nUpdate global.storageArrays.endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate global.managementServers.endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate authorization.enabled to true.\nUpdate images.authorization to the image of the CSM Authorization sidecar. In most cases, you can leave the default value.\nUpdate authorization.proxyHost to the hostname of the CSM Authorization Proxy Server.\nUpdate authorization.skipCertificateValidation to true or false depending on if you want to disable or enable certificate validation of the CSM Authorization Proxy Server.\nExample:\nglobal: storageArrays: - storageArrayId: \"123456789\" endpoint: https://localhost:9400 managementServers: - endpoint: https://localhost:9400 authorization: enabled: true # sidecarProxyImage: the container image used for the csm-authorization-sidecar. # Default value: dellemc/csm-authorization-sidecar:v1.9.0 sidecarProxyImage: dellemc/csm-authorization-sidecar:v1.9.0 # proxyHost: hostname of the csm-authorization server # Default value: None proxyHost: csm-authorization.com # skipCertificateValidation: certificate validation of the csm-authorization server # Allowed Values: # \"true\" - TLS certificate verification will be skipped # \"false\" - TLS certificate will be verified # Default value: \"true\" skipCertificateValidation: true Operator\nRefer to the Install Driver section to edit the parameters in the Custom Resource to enable CSM Authorization.\nUnder modules, enable the module named authorization:\nUpdate the enabled field to true.\nUpdate the image to the image of the CSM Authorization sidecar. In most cases, you can leave the default value.\nUpdate the PROXY_HOST environment value to the hostname of the CSM Authorization Proxy Server.\nUpdate the SKIP_CERTIFICATE_VALIDATION environment value to true or false depending on if you want to disable or enable certificate validation of the CSM Authorization Proxy Server.\nExample:\nmodules: # Authorization: enable csm-authorization for RBAC - name: authorization # enable: Enable/Disable csm-authorization enabled: true configVersion: v1.9.0 components: - name: karavi-authorization-proxy image: dellemc/csm-authorization-sidecar:v1.9.0 envs: # proxyHost: hostname of the csm-authorization server - name: \"PROXY_HOST\" value: \"csm-authorization.com\" # skipCertificateValidation: Enable/Disable certificate validation of the csm-authorization server - name: \"SKIP_CERTIFICATE_VALIDATION\" value: \"true\" Install the Dell CSI PowerMax driver following the appropriate documenation for your installation method.\n(Optional) Install dellctl to perform Kubernetes administrator commands for additional capabilities (e.g., list volumes). Please refer to the dellctl documentation page for the installation steps and command list.\n","categories":"","description":"Enabling CSM Authorization for PowerMax CSI Driver\n","excerpt":"Enabling CSM Authorization for PowerMax CSI Driver\n","ref":"/csm-docs/docs/authorization/configuration/powermax/","tags":"","title":"PowerMax"},{"body":"CSI Driver for Dell PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, see the script documentation.\nPrerequisites The following requirements must be met before installing CSI Driver for Dell PowerMax:\nInstall Kubernetes or OpenShift (see supported versions) Install Helm 3 Fibre Channel requirements iSCSI requirements NFS requirements Auto RDM for vSphere over FC requirements Certificate validation for Unisphere REST API calls Mount propagation is enabled on container runtime that is being used Linux multipathing requirements If using Snapshot feature, satisfy all Volume Snapshot requirements If enabling CSM for Authorization, please refer to the Authorization deployment steps first If using Powerpath , install the PowerPath for Linux requirements Prerequisite for CSI Reverse Proxy CSI PowerMax Reverse Proxy is an HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be an X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nStarting from v2.7.0 , these secrets will be created automatically using the following tls.key and tls.cert contents provided in my-powermax-settings.yaml file. For this , we need to install cert-manager using below command which manages the certs and secrets .\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml Here is an example showing how to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 Install Helm 3 Install Helm 3 on the master node before you install CSI Driver for Dell PowerMax.\nSteps\nRun the command to install Helm 3.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Fibre Channel Requirements CSI Driver for Dell PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install CSI Driver:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs. iSCSI Requirements The CSI Driver for Dell PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\nAll Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell PowerMax array. All the port group names supplied to the driver must exist on each Dell PowerMax with the same name. For more information about configuring iSCSI, see Dell Host Connectivity guide.\nNFS requirements CSI Driver for Dell PowerMax supports NFS communication. Ensure that the following requirements are met before you install CSI Driver:\nConfigure the NFS network. Please refer here for more details. PowerMax Embedded Management guest to access Unisphere for PowerMax. Create the NAS server. Please refer here for more details. Auto RDM for vSphere over FC requirements The CSI Driver for Dell PowerMax supports auto RDM for vSphere over FC. These requirements are applicable for the clusters deployed on ESX/ESXi using virtualized environement.\nSet up the environment as follows:\nRequires VMware vCenter management software to manage all ESX/ESXis where the cluster is hosted.\nAdd all FC array ports zoned to the ESX/ESXis to a port group where the cluster is hosted .\nAdd initiators from all ESX/ESXis to a host(initiator group) where the cluster is hosted.\nEdit samples/secret/vcenter-secret.yaml file, to point to the correct namespace, and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with vCenter privileges.\nCreate the secret by running the below command,\nkubectl create -f samples/secret/vcenter-secret.yaml Certificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\nTo fetch the certificate, run\nopenssl s_client -showcerts -connect [Unisphere IP]:8443 \u003c/dev/null 2\u003e /dev/null | openssl x509 -outform PEM \u003e ca_cert.pem NOTE: The IP address varies for each user.\nTo create the secret, run\nkubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax Ports in the port group There are no restrictions to how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell PowerMax to ensure that you have multiple paths to your data volumes.\nLinux multipathing requirements CSI Driver for Dell PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\nAll the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file. As a best practice, use the following options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 multipathd MachineConfig If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\nuser_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0 Use the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: workers-multipath-conf-default labels: machineconfiguration.openshift.io/role: worker spec: config: ignition: version: 3.2.0 storage: files: - contents: source: data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0K verification: {} filesystem: root mode: 400 path: /etc/multipath.conf After deploying thisMachineConfig object, CoreOS will start multipath service automatically. Alternatively, you can check the status of the multipath service by entering the following command in each worker nodes. sudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally, you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer to official documentation of the multipath configuration.\nPowerPath for Linux requirements CSI Driver for Dell PowerMax supports PowerPath for Linux. Configure Linux PowerPath before installing the CSI Driver.\nSet up the PowerPath for Linux as follows:\nAll the nodes must have the PowerPath package installed . Download the PowerPath archive for the environment from Dell Online Support. Untar the PowerPath archive, Copy the RPM package into a temporary folder and Install PowerPath using rpm -ivh DellEMCPower.LINUX-\u003cversion\u003e-\u003cbuild\u003e.\u003cplatform\u003e.x86_64.rpm Start the PowerPath service using systemctl start PowerPath Note: Do not install Dell PowerPath if multi-path software is already installed, as they cannot co-exist with native multi-path software.\n(Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\n(Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in my-powermax-settings.yaml\nreplication: enabled: true Replication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in the csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\nRun git clone -b v2.9.1 https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the samples/secret/secret.yaml file,to point to the correct namespace, and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with PowerMax privileges. Create the secret by running kubectl create -f samples/secret/secret.yaml Download the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 wget -O my-powermax-settings.yaml https://github.com/dell/helm-charts/raw/csi-powermax-2.9.1/charts/csi-powermax/values.yaml Ensure the unisphere have 10.0 REST endpoint support by clicking on Unisphere -\u003e Help (?) -\u003e About in Unisphere for PowerMax GUI. Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml Parameter Description Required Default global This section refers to configuration options for both CSI PowerMax Driver and Reverse Proxy - - defaultCredentialsSecret This secret name refers to:\n1 The proxy credentials if the driver is installed with proxy in StandAlone mode.\n2. The default Unisphere credentials if credentialsSecret is not specified for a management server. Yes powermax-creds storageArrays This section refers to the list of arrays managed by the driver and Reverse Proxy in StandAlone mode. - - storageArrayId This refers to PowerMax Symmetrix ID. Yes 000000000001 endpoint This refers to the URL of the Unisphere server managing storageArrayId. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes if Reverse Proxy mode is StandAlone https://primary-1.unisphe.re:8443 backupEndpoint This refers to the URL of the backup Unisphere server managing storageArrayId, if Reverse Proxy is installed in StandAlone mode. If authorization is enabled, backupEndpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes https://backup-1.unisphe.re:8443 managementServers This section refers to the list of configurations for Unisphere servers managing powermax arrays. - - endpoint This refers to the URL of the Unisphere server. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes https://primary-1.unisphe.re:8443 credentialsSecret This refers to the user credentials for endpoint Yes primary-1-secret skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. No “True” certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty limits This refers to various limits for Reverse Proxy No - maxActiveRead This refers to the maximum concurrent READ request handled by the reverse proxy. No 5 maxActiveWrite This refers to the maximum concurrent WRITE request handled by the reverse proxy. No 4 maxOutStandingRead This refers to maximum queued READ request when reverse proxy receives more than maxActiveRead requests. No 50 maxOutStandingWrite This refers to maximum queued WRITE request when reverse proxy receives more than maxActiveWrite requests. No 50 kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes “ABC” logLevel CSI driver log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes “debug” logFormat CSI driver log format. Allowed values: “TEXT” or “JSON”. Yes “TEXT” kubeletConfigDir kubelet config directory path. Ensure that the config.yaml file is present at this path. Yes /var/lib/kubelet defaultFsType Used to set the default FS type for external provisioner Yes ext4 portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only “PortGroup1, PortGroup2, PortGroup3” skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No “True” transportProtocol Set the preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty nodeNameTemplate Used to specify a template that will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, leave this value empty. No Empty modifyHostName Change any existing host names. When nodenametemplate is set, it changes the name to the specified format else it uses driver default host name format. No false powerMaxDebug Enables low level and http traffic logging between the CSI driver and Unisphere. Don’t enable this unless asked to do so by the support team. No false enableCHAP Determine if the driver is going to configure SCSI node databases on the nodes with the CHAP credentials. If enabled, the CHAP secret must be provided in the credentials secret and set to the key “chapsecret” No false fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” version Current version of the driver. Don’t modify this value as this value will be used by the install script. Yes v2.9.1 images List all the images used by the CSI driver and CSM. If you use a private repository, change the registries accordingly. Yes \"\" maxPowerMaxVolumesPerNode Specifies the maximum number of volume that can be created on a node. Yes 0 controller Allows configuration of the controller-specific parameters. - - controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2 volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s” snapshot.enabled Enable/Disable volume snapshot feature Yes true snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot” resizer.enabled Enable/Disable volume expansion feature Yes true healthMonitor.enabled Allows to enable/disable volume health monitor No false healthMonitor.interval Interval of monitoring volume health condition No 60s nodeSelector Define node selection constraints for pods of controller deployment No tolerations Define tolerations for the controller deployment, if required No node Allows configuration of the node-specific parameters. - - tolerations Add tolerations as per requirement No - nodeSelector Add node selectors as per requirement No - healthMonitor.enabled Allows to enable/disable volume health monitor No false topologyControl.enabled Allows to enable/disable topology control to filter topology keys No false csireverseproxy This section refers to the configuration options for CSI PowerMax Reverse Proxy - - tlsSecret This refers to the TLS secret of the Reverse Proxy Server. Yes csirevproxy-tls-secret deployAsSidecar If set to true, the Reverse Proxy is installed as a sidecar to the driver’s controller pod otherwise it is installed as a separate deployment. Yes “True” port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation Yes 2222 certManager Auto-create TLS certificate for csi-reverseproxy - - selfSignedCert Set selfSignedCert to use a self-signed certificate No true certificateFile certificateFile has tls.key content in encoded format No tls.crt.encoded64 privateKeyFile privateKeyFile has tls.key content in encoded format No tls.key.encoded64 authorization Authorization is an optional feature to apply credential shielding of the backend PowerMax. - - enabled A boolean that enables/disables authorization feature. No false proxyHost Hostname of the csm-authorization server. No Empty skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization proxy server. No true migration Migration is an optional feature to enable migration between storage classes - - enabled A boolean that enables/disables migration feature. No false image Image for dell-csi-migrator sidecar. No \" \" nodeRescanSidecarImage Image for node rescan sidecar which rescans nodes for identifying new paths. No \" \" migrationPrefix enables migration sidecar to read required information from the storage class fields No migration.storage.dell.com replication Replication is an optional feature to enable replication \u0026 disaster recovery capabilities of PowerMax to Kubernetes clusters. - - enabled A boolean that enables/disables replication feature. No false replicationContextPrefix enables side cars to read required information from the volume context No powermax replicationPrefix Determine if replication is enabled No replication.storage.dell.com storageCapacity It is an optional feature that enable storagecapacity \u0026 helps the scheduler to check whether the requested capacity is available on the PowerMax array and allocate it to the nodes. - - enabled A boolean that enables/disables storagecapacity feature. - true pollInterval It configure how often external-provisioner polls the driver to detect changed capacity - 5m vSphere This section refers to the configuration options for VMware virtualized environment support via RDM - - enabled A boolean that enables/disables VMware virtualized environment support. No false fcPortGroup Existing portGroup that driver will use for vSphere. Yes \"\" fcHostGroup Existing host(initiator group)/hostgroup(cascaded initiator group) that driver will use for vSphere. Yes \"\" vCenterHost URL/endpoint of the vCenter where all the ESX are present Yes \"\" vCenterCredSecret Secret name for the vCenter credentials. Yes \"\" Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml Or you can also install the driver using standalone helm chart using the command helm install --values my-powermax-settings.yaml --namespace powermax powermax ./csi-powermax Note:\nFor detailed instructions on how to run the install scripts, see the readme document in the dell-csi-helm-installer folder. There are a set of samples provided here to help you configure the driver with reverse proxy This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option In order to enable authorization, there should be an authorization proxy server already installed. PowerMax Array username must have role as StorageAdmin to be able to perform CRUD operations. If the user is using complex K8s version like “v1.24.3-mirantis-1”, use this kubeVersion check in helm Chart file. kubeVersion: “\u003e= 1.24.0-0 \u003c 1.29.0-0”. User should provide all boolean values with double-quotes. This applies only for values.yaml. Example: “true”/“false”. controllerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails. Endpoint should not have any special character at the end apart from port number. Storage Classes A wide set of annotated storage class manifests has been provided in the samples/storageclass folder. Please use these samples to create new storage classes to provision storage.\nVolume Snapshot Class Starting with CSI PowerMax v1.7.0, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nSample values file The following sections have useful snippets from values.yaml file which provides more information on how to configure the CSI PowerMax driver along with CSI PowerMax ReverseProxy in StandAlone mode.\nCSI PowerMax driver with Proxy in StandAlone mode This is the most advanced configuration which provides you with the capability to connect to Multiple Unisphere servers. You can specify primary and backup Unisphere servers for each storage array. If you have different credentials for your Unisphere servers, you can also specify different credential secrets.\nglobal: defaultCredentialsSecret: powermax-creds storageArrays: - storageArrayId: \"000000000001\" endpoint: https://primary-1.unisphe.re:8443 backupEndpoint: https://backup-1.unisphe.re:8443 - storageArrayId: \"000000000002\" endpoint: https://primary-2.unisphe.re:8443 backupEndpoint: https://backup-2.unisphe.re:8443 managementServers: - endpoint: https://primary-1.unisphe.re:8443 credentialsSecret: primary-1-secret skipCertificateValidation: false certSecret: primary-cert limits: maxActiveRead: 5 maxActiveWrite: 4 maxOutStandingRead: 50 maxOutStandingWrite: 50 - endpoint: https://backup-1.unisphe.re:8443 credentialsSecret: backup-1-secret skipCertificateValidation: true - endpoint: https://primary-2.unisphe.re:8443 credentialsSecret: primary-2-secret skipCertificateValidation: true - endpoint: https://backup-2.unisphe.re:8443 credentialsSecret: backup-2-secret skipCertificateValidation: true # \"csireverseproxy\" refers to the subchart csireverseproxy csireverseproxy: tlsSecret: csirevproxy-tls-secret deployAsSidecar: true port: 2222 mode: StandAlone Note: If the credential secret is missing from any management server details, the installer will try to use the defaultCredentialsSecret\n","categories":"","description":"Installing CSI Driver for PowerMax via Helm\n","excerpt":"Installing CSI Driver for PowerMax via Helm\n","ref":"/csm-docs/docs/csidriver/installation/helm/powermax/","tags":"","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use CSI Driver for Dell PowerMax. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with a different number of volumes in a given namespace using the storageclass provided. To test the installation of the CSI driver, perform these tests:\nVolume clone test Volume test Snapshot test Volume test Use this procedure to perform a volume test.\nCreate a namespace with the name test.\nRun the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\nRun the starttest.sh script and provide it with a test name. The following sample command can be used to run the 2vols test:\n./starttest.sh -t 2vols -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e This script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\nRun the /stoptest.sh -t 2vols -n \u003ctest_namespace\u003e script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\nNOTE: Helm tests have been designed assuming that users have created storageclass names like storageclass-name and storageclass-name-xfs. You can use kubectl get sc to check for the storageclass names.\nVolume clone test Use this procedure to perform a volume clone test.\nCreate a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: volumeclonetest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e This script does the following:\nInstalls a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum, and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test. Snapshot test Use this procedure to perform a snapshot test.\nCreate a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e This script does the following:\nInstalls a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot of that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test. NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass. You must update the snapshot class name in the file snap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\nVolume Expansion test Use this procedure to perform a volume expansion test.\nCreate a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e This script does the following:\nInstalls a helm chart that creates a Pod with a container, creates one PVC, and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC, and then recalculates the checksum Cleans up all the resources that were created as part of the test Note: This is not applicable for replicated volumes.\nSetting Application Prefix Application prefix is the name of the application that can be used to group the PowerMax volumes. We can use it while naming storage group. To set the application prefix for PowerMax, please refer to the sample storage class https://github.com/dell/csi-powermax/blob/main/samples/storageclass/powermax.yaml.\n# Name of application to be used to group volumes # This is used in naming storage group # Optional: true, Default value: None # Examples: APP, app, sanity, tests ApplicationPrefix: \u003capplication prefix\u003e Note: Supported length of storage group for PowerMax is 64 characters. Storage group name is of the format “csi-clusterprefix-application prefix-SLO name-SRP name-SG”. Based on the other inputs like clusterprefix,SLO name and SRP name maximum length of the ApplicationPrefix can vary.\nConsuming existing volumes with static provisioning Use this procedure to consume existing volumes with static provisioning.\nOpen your Unisphere for PowerMax, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, storage class is assumed as ‘powermax’, cluster prefix as ‘ABC’ and volume’s internal name as ‘00001’, array ID as ‘000000000001’, volume ID as ‘1abc23456’. The volume-handle should be in the format of csi-clusterPrefix-volumeNamePrefix-id-arrayID-volumeID. apiVersion: v1 kind: PersistentVolume metadata: name: pvol namespace: test spec: accessModes: - ReadWriteOnce capacity: storage: 8Gi csi: driver: csi-powermax.dellemc.com volumeHandle: csi-ABC-pmax-1abc23456-000000000001-00001 persistentVolumeReclaimPolicy: Retain storageClassName: powermax volumeMode: Filesystem Create PersistentVolumeClaim to use this PersistentVolume. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvc namespace: test spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: powermax volumeMode: Filesystem volumeName: pvol Then use this PVC as a volume in a pod. apiVersion: v1 kind: ServiceAccount metadata: name: powermaxtest namespace: test --- kind: StatefulSet apiVersion: apps/v1 metadata: name: powermaxtest namespace: test spec: selector: matchLabels: app: powermaxtest serviceName: staticprovisioning template: metadata: labels: app: powermaxtest spec: serviceAccount: powermaxtest containers: - name: test image: docker.io/centos:latest command: [ \"/bin/sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data\" name: pvc volumes: - name: pvc persistentVolumeClaim: claimName: pvc After the pod becomes Ready and Running, you can start to use this pod and volume. Note: CSI driver for PowerMax will create the necessary objects like Storage group, HostID and Masking View. They must not be created manually.\nSetting QoS parameters for throttling performance and bandwidth Use this procedure to set QoS parameters for throttling performance and bandwidth\nCreate storage class with the following parameters set. # Following params are for HostLimits, set them only if you want to set IOLimits # HostLimitName uniquely identifies given set of limits on a storage class # This is used in naming storage group, max of 3 letter # Optional: true # Example: \"HL1\", \"HL2\" #HostLimitName: \"HL1\" # The MBs per Second Host IO limit for the storage class # Optional: true, Default: \"\" # Examples: 100, 200, NOLIMIT #HostIOLimitMBSec: \"\" # The IOs per Second Host IO limit for the storage class # Optional: true, Default: \"\" # Examples: 100, 200, NOLIMIT #HostIOLimitIOSec: \"\" # distribution of the Host IO limits for the storage class # Optional: true, Default: \"\" # Allowed values: Never\",\"Always\" or \"OnFailure\" only #DynamicDistribution: \"\" Use the above storage class to create the PVC and provision the volume to the pod.\nOnce the pod becones Ready and Running, you will see the QoS parameters applied for throttling performance and bandwidth.\n","categories":"","description":"Tests to validate PowerMax CSI Driver installation","excerpt":"Tests to validate PowerMax CSI Driver installation","ref":"/csm-docs/docs/csidriver/installation/test/powermax/","tags":"","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v2.9.1 Note: Auto SRDF group creation is currently not supported in PowerMaxOS 10.1 (6079) Arrays.\nNote: Starting from CSI v2.4.0, Only Unisphere 10.0 REST endpoints are supported. It is mandatory that Unisphere should be updated to 10.0. Please find the instructions here.\nNote: File Replication for PowerMax is currently not supported\nNew Features/Changes #947 - [FEATURE]: Support for Kubernetes 1.28 #1066 - [FEATURE]: Support for Openshift 4.14 #851 - [FEATURE]: Helm Chart Enhancement - Container Images Configurable in values.yaml #905 - [FEATURE]: Add support for CSI Spec 1.6 #991 - [FEATURE]:Remove linked proxy mode for PowerMax #996 - [FEATURE]: Dell CSI to Dell CSM Operator Migration Process #1062 - [FEATURE]: CSM PowerMax: Support PowerMax v10.1 Fixed Issues #983 - [BUG]: storageCapacity can be set in unsupported CSI Powermax with CSM Operator #1014 - [BUG]: Missing error check for os.Stat call during volume publish #1037 - [BUG]: Document instructions update: Either Multi-Path or the Power-Path software should be enabled for PowerMax #1051 - [BUG]: make docker command is failing with error #1053 - [BUG]: make gosec is erroring out - Repos PowerMax,PowerStore,PowerScale (gosec is installed) #1056 - [BUG]: Missing runtime dependencies reference in PowerMax README file. #1061 - [BUG]: Golint is not installing with go get command #1110 - [BUG]: Multi Controller defect - sidecars timeout #1103 - [BUG]: CSM Operator doesn’t apply fSGroupPolicy value to CSIDriver Object Known Issues Issue Workaround Unable to update Host: A problem occurred modifying the host resource This issue occurs when the nodes do not have unique hostnames or when an IP address/FQDN with same sub-domains are used as hostnames. The workaround is to use unique hostnames or FQDN with unique sub-domains When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node If the volume limit is exhausted and there are pending pods and PVCs due to exceed max volume count, the pending PVCs will be bound to PVs and the pending pods will be scheduled to nodes when the driver pods are restarted. It is advised not to have any pending pods or PVCs once the volume limit per node is exhausted on a CSI Driver. There is an open issue reported with kubenetes at https://github.com/kubernetes/kubernetes/issues/95911 with the same behavior. Automatic SRDF group creation is failing with “Unable to get Remote Port on SAN for Auto SRDF” for PowerMaxOS 10.1 arrays Create the SRDF Group and add it to the storage class Node stage is failing with error “wwn for FC device not found” This is an intermittent issue, rebooting the node will resolve this issue Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for PowerMax CSI driver","excerpt":"Release notes for PowerMax CSI driver","ref":"/csm-docs/docs/csidriver/release/powermax/","tags":"","title":"PowerMax"},{"body":" Symptoms Prevention, Resolution or Workaround kubectl describe pod powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e indicates that the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or log in to the docker registry kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver cannot authenticate Check your secret’s username and password kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver failed to connect to the U4P because it could not verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.23.0 \u003c 1.27.0 which is incompatible with Kubernetes V1.23.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which are not supported. When a node goes down, the block volumes attached to the node cannot be attached to another node 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. When attempting a driver upgrade, you see: spec.fsGroupPolicy: Invalid value: \"xxx\": field is immutable You cannot upgrade between drivers with different fsGroupPolicies. See upgrade documentation for more details Ater the migration group is in “migrated” state but unable to move to “commit ready” state because the new paths are not being discovered on the cluster nodes. Run the following commands manually on the cluster nodes rescan-scsi-bus.sh -i rescan-scsi-bus.sh -a Failed to fetch details for array: 000000000000. [Unauthorized]\" Please make sure that correct encrypted username and password in secret files are used, also ensure whether the RBAC is enabled for the user Error looking up volume for idempotence check: Not Found or Get Volume step fails for: (000000000000) symID with error (Invalid Response from API) Make sure that Unisphere endpoint doesn’t end with front slash FailedPrecondition desc = no topology keys could be generate Make sure that FC or iSCSI connectivity to the arrays are proper CreateHost failed with error initiator is already part of different host. Update modifyHostName to true in values.yaml Or Remove the initiator from existing host kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs says connection refused and the reverseproxy logs says “Failed to setup server.(secrets \"secret-name\" not found)” Make sure the given secret exist on the cluster nodestage is failing with error Error invalid IQN Target iqn.EMC.0648.SE1F 1. Update initiator name to full default name , ex: iqn.1993-08.org.debian:01:e9afae962192 2.Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed and it should be full default name. Volume mount is failing on few OS(ex:VMware Virtual Platform) during node publish with error wrong fs type, bad option, bad superblock 1. Check the multipath configuration(if enabled) 2. Edit Vm Advanced settings-\u003ehardware and add the param disk.enableUUID=true and reboot the node Standby controller pod is in crashloopbackoff state Scale down the replica count of the controller pod’s deployment to 1 using kubectl scale deployment \u003cdeployment_name\u003e --replicas=1 -n \u003cdriver_namespace\u003e ","categories":"","description":"Troubleshooting PowerMax Driver","excerpt":"Troubleshooting PowerMax Driver","ref":"/csm-docs/docs/csidriver/troubleshooting/powermax/","tags":"","title":"PowerMax"},{"body":"The CSI Driver for Dell PowerMax can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nPrerequisites The CSI Driver for Dell PowerMax can create PVC with different storage protocols access :\ndirect Fiber Channel direct iSCSI NFS Fiber Channel via VMware Raw Device Mapping In most cases, you will use one protocol only; therefore you should comply with the according prerequisites and not the others. Listing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using this command:\nkubectl get csm --all-namespaces Fibre Channel Requirements CSI Driver for Dell PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install CSI Driver:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs. iSCSI Requirements The CSI Driver for Dell PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\nAll Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell PowerMax array. All the port group names supplied to the driver must exist on each Dell PowerMax with the same name. For more information about configuring iSCSI, see Dell Host Connectivity guide.\niscsi-daemon MachineConfig To configure iSCSI in Red Hat OpenShift clusters, you can create a MachineConfig object using the console or oc to ensure that the iSCSI daemon starts on all the Red Hat CoreOS nodes. Here is an example of a MachineConfig object:\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: 99-iscsid labels: machineconfiguration.openshift.io/role: worker spec: config: ignition: version: 3.2.0 systemd: units: - name: \"iscsid.service\" enabled: true Once the MachineConfig object has been deployed, CoreOS will ensure that iscsid.service starts automatically.\nAlternatively, you can check the status of the iSCSI service by entering the following command on each worker node in the cluster:\nsudo systemctl status iscsid The service should be up and running (i.e. should be active state).\nIf the iscsid.service is not running, then perform the following steps on each worker node in the cluster\nLogin to worker nodes and check if the file /etc/iscsi/initiatorname.iscsi has been created properly If the file doesn’t exist or it doesn’t contain a valid ISCSI IQN, then make sure it exists with valid entries Ensure that iscsid service is running - Enable sudo systemctl enable iscsid \u0026 restart sudo systemctl restart iscsid iscsid if necessary. Note: If your worker nodes are running Red Hat CoreOS, make sure that automatic ISCSI login at boot is configured. Please contact RedHat for more details. NFS requirements CSI Driver for Dell PowerMax supports NFS communication. Ensure that the following requirements are met before you install CSI Driver:\nConfigure the NFS network. Please refer here for more details. PowerMax Embedded Management guest to access Unisphere for PowerMax. Create the NAS server. Please refer here for more details. Linux multipathing requirements CSI Driver for Dell PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\nAll the nodes must have the Device Mapper Multipathing package installed. NOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file. As a best practice, use these options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 multipathd MachineConfig If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\nuser_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0 Use the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: workers-multipath-conf-default labels: machineconfiguration.openshift.io/role: worker spec: config: ignition: version: 3.2.0 storage: files: - contents: source: data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0K verification: {} filesystem: root mode: 400 path: /etc/multipath.conf After deploying thisMachineConfig object, CoreOS will start multipath service automatically. Alternatively, you can check the status of the multipath service by entering the following command in each worker nodes. sudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally, you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer to official documentation of the multipath configuration.\nPowerPath for Linux requirements CSI Driver for Dell PowerMax supports PowerPath for Linux. Configure Linux PowerPath before installing the CSI Driver.\nFollow this procedure to set up PowerPath for Linux:\nAll the nodes must have the PowerPath package installed . Download the PowerPath archive for the environment from Dell Online Support. Untar the PowerPath archive, Copy the RPM package into a temporary folder and Install PowerPath using rpm -ivh DellEMCPower.LINUX-\u003cversion\u003e-\u003cbuild\u003e.\u003cplatform\u003e.x86_64.rpm Start the PowerPath service using systemctl start PowerPath Note: Do not install Dell PowerPath if multi-path software is already installed, as they cannot co-exist with native multi-path software.\nAuto RDM for vSphere over FC requirements The CSI Driver for Dell PowerMax supports auto RDM for vSphere over FC. These requirements are applicable for the clusters deployed on ESX/ESXi using virtualized environement.\nSet up the environment as follows:\nRequires VMware vCenter management software to manage all ESX/ESXis where the cluster is hosted.\nAdd all FC array ports zoned to the ESX/ESXis to a port group where the cluster is hosted .\nAdd initiators from all ESX/ESXis to a host(initiator group)/host group(cascaded initiator group) where the cluster is hosted.\nCreate a secret which contains vCenter privileges. Follow the steps here to create the same.\nCSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is a component that will be installed with the CSI PowerMax driver. For more details on this feature, see the related documentation.\nPre-requisites Create a TLS secret that holds an SSL certificate and a private key. This is required by the reverse proxy server. Use a tool such as openssl to generate this secret using the example below:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt --key=tls.key Installation (Optional) Create secret for client-side TLS verification Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. See the detailed documentation on how to create this secret here.\nInstall Driver Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\nCreate PowerMax credentials: Create a file called powermax-creds.yaml with the following content:\napiVersion: v1 kind: Secret metadata: name: powermax-creds # Replace driver-namespace with the namespace where driver is being deployed namespace: \u003cdriver-namespace\u003e type: Opaque data: # set username to the base64 encoded username username: \u003cbase64 username\u003e # set password to the base64 encoded password password: \u003cbase64 password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards) # chapsecret: \u003cbase64 CHAP secret\u003e Replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run the kubectl create -f powermax-creds.yaml command to create the secret.\nCreate a configmap using sample here. Fill in the appropriate values for driver configuration. Example: config.yaml\nmode: StandAlone # Mode for the reverseproxy, should not be changed port: 2222 # Port on which reverseproxy will listen logLevel: debug logFormat: text standAloneConfig: storageArrays: - storageArrayId: \"000000000001\" # arrayID primaryURL: https://primary-1.unisphe.re:8443 # primary unisphere for arrayID backupURL: https://backup-1.unisphe.re:8443 # backup unisphere for arrayID proxyCredentialSecrets: - proxy-secret-11 # credential secret for primary unisphere, e.g., powermax-creds - proxy-secret-12 # credential secret for backup unisphere, e.g., powermax-creds - storageArrayId: \"000000000002\" primaryURL: https://primary-2.unisphe.re:8443 backupURL: https://backup-2.unisphe.re:8443 proxyCredentialSecrets: - proxy-secret-21 - proxy-secret-22 managementServers: - url: https://primary-1.unisphe.re:8443 # primary unisphere endpoint arrayCredentialSecret: primary-1-secret # primary credential secret e.g., powermax-creds skipCertificateValidation: true - url: https://backup-1.unisphe.re:8443 # backup unisphere endpoint arrayCredentialSecret: backup-1-secret # backup credential secret e.g., powermax-creds skipCertificateValidation: false # value false, to verify unisphere certificate and provide certSecret certSecret: primary-certs # unisphere verification certificate - url: https://primary-2.unisphe.re:8443 arrayCredentialSecret: primary-2-secret skipCertificateValidation: true - url: https://backup-2.unisphe.re:8443 arrayCredentialSecret: backup-2-secret skipCertificateValidation: false certSecret: primary-certs After editing the file, run this command to create a secret called powermax-reverseproxy-config. If you are using a different namespace/secret name, just substitute those into the command.\nkubectl create configmap powermax-reverseproxy-config --from-file config.yaml -n powermax Create a CR (Custom Resource) for PowerMax using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:\nParameter Description Required Default dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet replicas Controls the number of controller Pods you deploy. If controller Pods are greater than the number of available nodes, excess Pods will become stuck in pending. The default is 2 which allows for Controller high availability. Yes 2 fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” Common parameters for node and controller X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended to all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443 X_CSI_TRANSPORT_PROTOCOL Choose which transport protocol to use (ISCSI, FC, auto or None) Yes auto X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No - X_CSI_MANAGED_ARRAYS List of comma-separated array ID(s) which will be managed by the driver Yes - X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Yes csipowermax-reverseproxy X_CSI_IG_MODIFY_HOSTNAME Change any existing host names. When nodenametemplate is set, it changes the name to the specified format else it uses driver default host name format. No false X_CSI_IG_NODENAME_TEMPLATE Provide a template for the CSI driver to use while creating the Host/IG on the array for the nodes in the cluster. It is of the format a-b-c-%foo%-xyz where foo will be replaced by host name of each node in the cluster. No - X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature see the related documentation No - X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller and Node plugin. Provides details of volume status, usage and volume condition. As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar No false X_CSI_VSPHERE_ENABLED Enable VMware virtualized environment support via RDM No false X_CSI_VSPHERE_PORTGROUP Existing portGroup that driver will use for vSphere Yes \"\" X_CSI_VSPHERE_HOSTNAME Existing host(initiator group)/host group(cascaded initiator group) that driver will use for vSphere Yes \"\" X_CSI_VCenter_HOST URL/endpoint of the vCenter where all the ESX are present Yes \"\" Node parameters X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature see the related documentation No false X_CSI_TOPOLOGY_CONTROL_ENABLED Enable/Disabe topology control. It filters out arrays, associated transport protocol available to each node and creates topology keys based on any such user input. No false CSI Reverseproxy Module X_CSI_REVPROXY_TLS_SECRET Name of TLS secret defined in config map Yes “csirevproxy-tls-secret” X_CSI_REVPROXY_PORT Port number where reverseproxy will listen as defined in config map Yes “2222” X_CSI_CONFIG_MAP_NAME Name of config map as created for CSI PowerMax Yes “powermax-reverseproxy-config” Execute the following command to create the PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.\nThe mandatory module CSI PowerMax Reverseproxy will be installed automatically with the same command.\nOther features to enable Dynamic Logging Configuration This feature is introduced in CSI Driver for powermax version 2.0.0.\nAs part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Volume Health Monitoring This feature is introduced in CSI Driver for PowerMax version 2.2.0.\nVolume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via CSM operator.\nTo enable this feature, set X_CSI_HEALTH_MONITOR_ENABLED to true in the driver manifest under controller and node section. Also, install the external-health-monitor from sideCars section for controller plugin. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" Support for custom topology keys This feature is introduced in CSI Driver for PowerMax version 2.3.0.\nSupport for custom topology keys is optional and by default this feature is disabled for drivers when installed via CSM operator.\nX_CSI_TOPOLOGY_CONTROL_ENABLED provides a way to filter topology keys on a node based on array and transport protocol. If enabled, user can create custom topology keys by editing node-topology-config configmap.\nTo enable this feature, set X_CSI_TOPOLOGY_CONTROL_ENABLED to true in the driver manifest under node section.\n# X_CSI_TOPOLOGY_CONTROL_ENABLED provides a way to filter topology keys on a node based on array and transport protocol # if enabled, user can create custom topology keys by editing node-topology-config configmap. # Allowed values: # true: enable the filtration based on config map # false: disable the filtration based on config map # Default value: false - name: X_CSI_TOPOLOGY_CONTROL_ENABLED value: \"false\" Edit the sample config map “node-topology-config” as described here with appropriate values: Example:\nkind: ConfigMap metadata: name: node-topology-config namespace: powermax data: topologyConfig.yaml: | allowedConnections: - nodeName: \"node1\" rules: - \"000000000001:FC\" - \"000000000002:FC\" - nodeName: \"*\" rules: - \"000000000002:FC\" deniedConnections: - nodeName: \"node2\" rules: - \"000000000002:*\" - nodeName: \"node3\" rules: - \"*:*\" Parameter Description allowedConnections List of node, array and protocol info for user allowed configuration allowedConnections.nodeName Name of the node on which user wants to apply given rules allowedConnections.rules List of StorageArrayID:TransportProtocol pair deniedConnections List of node, array and protocol info for user denied configuration deniedConnections.nodeName Name of the node on which user wants to apply given rules deniedConnections.rules List of StorageArrayID:TransportProtocol pair Run following command to create the configmap\nkubectl create -f topologyConfig.yaml Note: Name of the configmap should always be node-topology-config.\nSupport for auto RDM for vSphere over FC This feature is introduced in CSI Driver for PowerMax version 2.5.0.\nSupport for auto RDM for vSphere over FC feature is optional and by default this feature is disabled for drivers when installed via CSM operator.\nTo enable this feature, set X_CSI_VSPHERE_ENABLED to true in the driver manifest under controller and node section.\n# VMware/vSphere virtualization support # set X_CSI_VSPHERE_ENABLED to true, if you to enable VMware virtualized environment support via RDM # Allowed values: # \"true\" - vSphere volumes are enabled # \"false\" - vSphere volumes are disabled # Default value: \"false\" - name: \"X_CSI_VSPHERE_ENABLED\" value: \"false\" # X_CSI_VSPHERE_PORTGROUP: An existing portGroup that driver will use for vSphere # recommended format: csi-x-VC-PG, x can be anything of user choice # Allowed value: valid existing port group on the array # Default value: \"\" \u003cempty\u003e - name: \"X_CSI_VSPHERE_PORTGROUP\" value: \"\" # X_CSI_VSPHERE_HOSTNAME: An existing host(initiator group)/ host group(cascaded intiator group) that driver will use for vSphere # this host/host group should contain initiators from all the ESXs/ESXi host where the cluster is deployed # recommended format: csi-x-VC-HN, x can be anything of user choice # Allowed value: valid existing host(initiator group)/ host group(cascaded intiator group) on the array # Default value: \"\" \u003cempty\u003e - name: \"X_CSI_VSPHERE_HOSTNAME\" value: \"\" Edit the Secret file vcenter-creds here with required values. Example:\napiVersion: v1 kind: Secret metadata: name: vcenter-creds # Set driver namespace namespace: powermax type: Opaque data: # set username to the base64 encoded username username: YWRtaW4= # set password to the base64 encoded password password: YWRtaW4= These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with vCenter privileges. 3. 4. Run following command to create the configmap\nkubectl create -f vcenter-secret.yaml Note: Name of the secret should always be vcenter-creds.\n","categories":"","description":"Installing Dell CSI Driver for PowerMax via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for PowerMax via Dell CSM Operator\n","ref":"/csm-docs/docs/deployment/csmoperator/drivers/powermax/","tags":"","title":"PowerMax"},{"body":"Configuring PowerMax CSI Driver with CSM for Authorization Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow these steps to configure the CSI Drivers to work with the Authorization sidecar:\nApply the secret containing the tenant token data into the driver namespace. It’s assumed that the Kubernetes administrator has the token secret manifest, generated by your storage administrator via Generate a Token, saved in /tmp/token.yaml.\n#It is assumed that array type powermax has the namespace “powermax”.\nkubectl apply -f /tmp/token.yaml -n powermax Edit these parameters in samples/secret/karavi-authorization-config.json file in CSI PowerMax driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\nParameter Description Required Default username Username for connecting to the backend storage array. This parameter is ignored. No - password Password for connecting to to the backend storage array. This parameter is ignored. No - intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes - endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400 systemID System ID of the backend storage array. Yes \" \" skipCertificateValidation A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml Create the karavi-authorization-config secret using this command:\nkubectl -n powermax create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f - Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n powermax create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f - Otherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n powermax create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f - Enable CSM Authorization in the driver installation applicable to your installation method.\nHelm\nIn Install the Driver where you edit samples/secret/secret.yaml with the credentials of the PowerMax, you can leave these with the default values as they will be ignored.\nRefer to the Install the Driver section to edit the parameters in my-powermax-settings.yaml file to configure the driver to communicate with the CSM Authorization sidecar.\nUpdate global.storageArrays.endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate global.managementServers.endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate authorization.enabled to true.\nUpdate authorization.sidecarProxyImage to the image of the CSM Authorization sidecar. In most cases, you can leave the default value.\nUpdate authorization.proxyHost to the hostname of the CSM Authorization Proxy Server.\nUpdate authorization.skipCertificateValidation to true or false depending on if you want to disable or enable certificate validation of the CSM Authorization Proxy Server.\nExample:\nglobal: storageArrays: - storageArrayId: \"123456789\" endpoint: https://localhost:9400 managementServers: - endpoint: https://localhost:9400 authorization: enabled: true # sidecarProxyImage: the container image used for the csm-authorization-sidecar. # Default value: dellemc/csm-authorization-sidecar:v1.8.0 sidecarProxyImage: dellemc/csm-authorization-sidecar:v1.8.0 # proxyHost: hostname of the csm-authorization server # Default value: None proxyHost: csm-authorization.com # skipCertificateValidation: certificate validation of the csm-authorization server # Allowed Values: # \"true\" - TLS certificate verification will be skipped # \"false\" - TLS certificate will be verified # Default value: \"true\" skipCertificateValidation: true Install the Dell CSI PowerMax driver following the appropriate documenation for your installation method.\n(Optional) Install dellctl to perform Kubernetes administrator commands for additional capabilities (e.g., list volumes). Please refer to the dellctl documentation page for the installation steps and command list.\n","categories":"","description":"Enabling CSM Authorization for PowerMax CSI Driver\n","excerpt":"Enabling CSM Authorization for PowerMax CSI Driver\n","ref":"/csm-docs/v1/authorization/configuration/powermax/","tags":"","title":"PowerMax"},{"body":" Linked Proxy mode for CSI reverse proxy is no longer actively maintained or supported. It will be deprecated in CSM 1.9 (Driver Version 2.9.0). It is highly recommended that you use stand alone mode going forward. CSI Driver for Dell PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, see the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\nCSI Driver for Dell PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support- CSI PowerMax ReverseProxy, which maximizes CSI driver and Unisphere performance Kubernetes External Resizer, which resizes the volume (optional) Kubernetes External health monitor, which provides volume health status (optional) Dell CSI Replicator, which provides Replication capability. (optional) Dell CSI Migrator, which provides migrating capability within and across arrays (optional) Node rescanner, which rescans the node for new data paths after migration The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\nCSI Driver for Dell PowerMax Kubernetes Node Registrar, which handles the driver registration Prerequisites The following requirements must be met before installing CSI Driver for Dell PowerMax:\nInstall Kubernetes or OpenShift (see supported versions) Install Helm 3 Fibre Channel requirements iSCSI requirements NFS requirements Auto RDM for vSphere over FC requirements Certificate validation for Unisphere REST API calls Mount propagation is enabled on container runtime that is being used Linux multipathing requirements If using Snapshot feature, satisfy all Volume Snapshot requirements If enabling CSM for Authorization, please refer to the Authorization deployment steps first If using Powerpath , install the PowerPath for Linux requirements Prerequisite for CSI Reverse Proxy CSI PowerMax Reverse Proxy is an HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be an X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nStarting from v2.7.0 , these secrets will be created automatically using the following tls.key and tls.cert contents provided in my-powermax-settings.yaml file. For this , we need to install cert-manager using below command which manages the certs and secrets .\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml Here is an example showing how to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 Install Helm 3 Install Helm 3 on the master node before you install CSI Driver for Dell PowerMax.\nSteps\nRun the command to install Helm 3.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Fibre Channel Requirements CSI Driver for Dell PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install CSI Driver:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs. iSCSI Requirements The CSI Driver for Dell PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\nAll Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell PowerMax array. All the port group names supplied to the driver must exist on each Dell PowerMax with the same name. For more information about configuring iSCSI, see Dell Host Connectivity guide.\nNFS requirements CSI Driver for Dell PowerMax supports NFS communication. Ensure that the following requirements are met before you install CSI Driver:\nConfigure the NFS network. Please refer here for more details. PowerMax Embedded Management guest to access Unisphere for PowerMax. Create the NAS server. Please refer here for more details. Auto RDM for vSphere over FC requirements The CSI Driver for Dell PowerMax supports auto RDM for vSphere over FC. These requirements are applicable for the clusters deployed on ESX/ESXi using virtualized environement.\nSet up the environment as follows:\nRequires VMware vCenter management software to manage all ESX/ESXis where the cluster is hosted.\nAdd all FC array ports zoned to the ESX/ESXis to a port group where the cluster is hosted .\nAdd initiators from all ESX/ESXis to a host(initiator group) where the cluster is hosted.\nEdit samples/secret/vcenter-secret.yaml file, to point to the correct namespace, and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with vCenter privileges.\nCreate the secret by running the below command,\nkubectl create -f samples/secret/vcenter-secret.yaml Certificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\nTo fetch the certificate, run\nopenssl s_client -showcerts -connect [Unisphere IP]:8443 \u003c/dev/null 2\u003e /dev/null | openssl x509 -outform PEM \u003e ca_cert.pem NOTE: The IP address varies for each user.\nTo create the secret, run\nkubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax Ports in the port group There are no restrictions to how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell PowerMax to ensure that you have multiple paths to your data volumes.\nLinux multipathing requirements CSI Driver for Dell PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\nAll the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file. As a best practice, use the following options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 PowerPath for Linux requirements CSI Driver for Dell PowerMax supports PowerPath for Linux. Configure Linux PowerPath before installing the CSI Driver.\nSet up the PowerPath for Linux as follows:\nAll the nodes must have the PowerPath package installed . Download the PowerPath archive for the environment from Dell Online Support. Untar the PowerPath archive, Copy the RPM package into a temporary folder and Install PowerPath using rpm -ivh DellEMCPower.LINUX-\u003cversion\u003e-\u003cbuild\u003e.\u003cplatform\u003e.x86_64.rpm Start the PowerPath service using systemctl start PowerPath Note: Do not install Dell PowerPath if multi-path software is already installed, as they cannot co-exist with native multi-path software.\n(Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\n(Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in my-powermax-settings.yaml\nreplication: enabled: true Replication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in the csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\nRun git clone -b v2.8.0 https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the samples/secret/secret.yaml file,to point to the correct namespace, and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with PowerMax privileges. Create the secret by running kubectl create -f samples/secret/secret.yaml Download the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 wget -O my-powermax-settings.yaml https://github.com/dell/helm-charts/raw/csi-powermax-2.8.0/charts/csi-powermax/values.yaml Ensure the unisphere have 10.0 REST endpoint support by clicking on Unisphere -\u003e Help (?) -\u003e About in Unisphere for PowerMax GUI. Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml Parameter Description Required Default global This section refers to configuration options for both CSI PowerMax Driver and Reverse Proxy - - defaultCredentialsSecret This secret name refers to:\n1. The Unisphere credentials if the driver is installed without proxy or with proxy in Linked mode.\n2. The proxy credentials if the driver is installed with proxy in StandAlone mode.\n3. The default Unisphere credentials if credentialsSecret is not specified for a management server. Yes powermax-creds storageArrays This section refers to the list of arrays managed by the driver and Reverse Proxy in StandAlone mode. - - storageArrayId This refers to PowerMax Symmetrix ID. Yes 000000000001 endpoint This refers to the URL of the Unisphere server managing storageArrayId. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes if Reverse Proxy mode is StandAlone https://primary-1.unisphe.re:8443 backupEndpoint This refers to the URL of the backup Unisphere server managing storageArrayId, if Reverse Proxy is installed in StandAlone mode. If authorization is enabled, backupEndpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes https://backup-1.unisphe.re:8443 managementServers This section refers to the list of configurations for Unisphere servers managing powermax arrays. - - endpoint This refers to the URL of the Unisphere server. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes https://primary-1.unisphe.re:8443 credentialsSecret This refers to the user credentials for endpoint Yes primary-1-secret skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. No “True” certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty limits This refers to various limits for Reverse Proxy No - maxActiveRead This refers to the maximum concurrent READ request handled by the reverse proxy. No 5 maxActiveWrite This refers to the maximum concurrent WRITE request handled by the reverse proxy. No 4 maxOutStandingRead This refers to maximum queued READ request when reverse proxy receives more than maxActiveRead requests. No 50 maxOutStandingWrite This refers to maximum queued WRITE request when reverse proxy receives more than maxActiveWrite requests. No 50 kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes “ABC” logLevel CSI driver log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes “debug” logFormat CSI driver log format. Allowed values: “TEXT” or “JSON”. Yes “TEXT” kubeletConfigDir kubelet config directory path. Ensure that the config.yaml file is present at this path. Yes /var/lib/kubelet defaultFsType Used to set the default FS type for external provisioner Yes ext4 portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only “PortGroup1, PortGroup2, PortGroup3” skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No “True” transportProtocol Set the preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty nodeNameTemplate Used to specify a template that will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, leave this value empty. No Empty modifyHostName Change any existing host names. When nodenametemplate is set, it changes the name to the specified format else it uses driver default host name format. No false powerMaxDebug Enables low level and http traffic logging between the CSI driver and Unisphere. Don’t enable this unless asked to do so by the support team. No false enableCHAP Determine if the driver is going to configure SCSI node databases on the nodes with the CHAP credentials. If enabled, the CHAP secret must be provided in the credentials secret and set to the key “chapsecret” No false fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” version Current version of the driver. Don’t modify this value as this value will be used by the install script. Yes v2.3.0 images Defines the container images used by the driver. - - driverRepository Defines the registry of the container image used for the driver. Yes dellemc maxPowerMaxVolumesPerNode Specifies the maximum number of volume that can be created on a node. Yes 0 controller Allows configuration of the controller-specific parameters. - - controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2 volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s” snapshot.enabled Enable/Disable volume snapshot feature Yes true snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot” resizer.enabled Enable/Disable volume expansion feature Yes true healthMonitor.enabled Allows to enable/disable volume health monitor No false healthMonitor.interval Interval of monitoring volume health condition No 60s nodeSelector Define node selection constraints for pods of controller deployment No tolerations Define tolerations for the controller deployment, if required No node Allows configuration of the node-specific parameters. - - tolerations Add tolerations as per requirement No - nodeSelector Add node selectors as per requirement No - healthMonitor.enabled Allows to enable/disable volume health monitor No false topologyControl.enabled Allows to enable/disable topology control to filter topology keys No false csireverseproxy This section refers to the configuration options for CSI PowerMax Reverse Proxy - - image This refers to the image of the CSI PowerMax Reverse Proxy container. Yes dellemc/csipowermax-reverseproxy:v2.4.0 tlsSecret This refers to the TLS secret of the Reverse Proxy Server. Yes csirevproxy-tls-secret deployAsSidecar If set to true, the Reverse Proxy is installed as a sidecar to the driver’s controller pod otherwise it is installed as a separate deployment. Yes “True” port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation Yes 2222 mode This refers to the installation mode of Reverse Proxy. It can be set to:\n1. Linked: In this mode, the Reverse Proxy communicates with a primary or a backup Unisphere managing the same set of arrays.\n2. StandAlone: In this mode, the Reverse Proxy communicates with multiple arrays managed by different Unispheres. Yes “StandAlone” certManager Auto-create TLS certificate for csi-reverseproxy - - selfSignedCert Set selfSignedCert to use a self-signed certificate No true certificateFile certificateFile has tls.key content in encoded format No tls.crt.encoded64 privateKeyFile privateKeyFile has tls.key content in encoded format No tls.key.encoded64 authorization Authorization is an optional feature to apply credential shielding of the backend PowerMax. - - enabled A boolean that enables/disables authorization feature. No false sidecarProxyImage Image for csm-authorization-sidecar. No \" \" proxyHost Hostname of the csm-authorization server. No Empty skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization proxy server. No true migration Migration is an optional feature to enable migration between storage classes - - enabled A boolean that enables/disables migration feature. No false image Image for dell-csi-migrator sidecar. No \" \" nodeRescanSidecarImage Image for node rescan sidecar which rescans nodes for identifying new paths. No \" \" migrationPrefix enables migration sidecar to read required information from the storage class fields No migration.storage.dell.com replication Replication is an optional feature to enable replication \u0026 disaster recovery capabilities of PowerMax to Kubernetes clusters. - - enabled A boolean that enables/disables replication feature. No false image Image for dell-csi-replicator sidecar. No \" \" replicationContextPrefix enables side cars to read required information from the volume context No powermax replicationPrefix Determine if replication is enabled No replication.storage.dell.com storageCapacity It is an optional feature that enable storagecapacity \u0026 helps the scheduler to check whether the requested capacity is available on the PowerMax array and allocate it to the nodes. - - enabled A boolean that enables/disables storagecapacity feature. - true pollInterval It configure how often external-provisioner polls the driver to detect changed capacity - 5m vSphere This section refers to the configuration options for VMware virtualized environment support via RDM - - enabled A boolean that enables/disables VMware virtualized environment support. No false fcPortGroup Existing portGroup that driver will use for vSphere. Yes \"\" fcHostGroup Existing host(initiator group)/hostgroup(cascaded initiator group) that driver will use for vSphere. Yes \"\" vCenterHost URL/endpoint of the vCenter where all the ESX are present Yes \"\" vCenterCredSecret Secret name for the vCenter credentials. Yes \"\" Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml Or you can also install the driver using standalone helm chart using the command helm install --values my-powermax-settings.yaml --namespace powermax powermax ./csi-powermax Note:\nFor detailed instructions on how to run the install scripts, see the readme document in the dell-csi-helm-installer folder. There are a set of samples provided here to help you configure the driver with reverse proxy This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option In order to enable authorization, there should be an authorization proxy server already installed. PowerMax Array username must have role as StorageAdmin to be able to perform CRUD operations. If the user is using complex K8s version like “v1.23.3-mirantis-1”, use this kubeVersion check in helm Chart file. kubeVersion: “\u003e= 1.23.0-0 \u003c 1.27.0-0”. User should provide all boolean values with double-quotes. This applies only for values.yaml. Example: “true”/“false”. controllerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails. Endpoint should not have any special character at the end apart from port number. Storage Classes A wide set of annotated storage class manifests has been provided in the samples/storageclass folder. Please use these samples to create new storage classes to provision storage.\nVolume Snapshot Class Starting with CSI PowerMax v1.7.0, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nSample values file The following sections have useful snippets from values.yaml file which provides more information on how to configure the CSI PowerMax driver along with CSI PowerMax ReverseProxy in various modes\nCSI PowerMax driver with Proxy in Linked mode In this mode, the CSI PowerMax ReverseProxy acts as a passthrough for the RESTAPI calls and only provides limited functionality such as rate limiting, backup Unisphere server. The CSI PowerMax driver is still responsible for the authentication with the Unisphere server.\nThe first endpoint in the list of management servers is the primary Unisphere server and if you provide a second endpoint, then it will be considered as the backup Unisphere’s endpoint.\nglobal: defaultCredentialsSecret: powermax-creds storageArrays: - storageArrayId: \"000000000001\" - storageArrayId: \"000000000002\" managementServers: - endpoint: https://primary-unisphere:8443 skipCertificateValidation: false certSecret: primary-cert limits: maxActiveRead: 5 maxActiveWrite: 4 maxOutStandingRead: 50 maxOutStandingWrite: 50 - endpoint: https://backup-unisphere:8443 # \"csireverseproxy\" refers to the subchart csireverseproxy csireverseproxy: # Set enabled to true if you want to use proxy image: dellemc/csipowermax-reverseproxy:v2.4.0 tlsSecret: csirevproxy-tls-secret deployAsSidecar: true port: 2222 mode: Linked Note: Since the driver is still responsible for authentication when used with Proxy in Linked mode, the credentials for both primary and backup Unisphere need to be the same.\nCSI PowerMax driver with Proxy in StandAlone mode This is the most advanced configuration which provides you with the capability to connect to Multiple Unisphere servers. You can specify primary and backup Unisphere servers for each storage array. If you have different credentials for your Unisphere servers, you can also specify different credential secrets.\nglobal: defaultCredentialsSecret: powermax-creds storageArrays: - storageArrayId: \"000000000001\" endpoint: https://primary-1.unisphe.re:8443 backupEndpoint: https://backup-1.unisphe.re:8443 - storageArrayId: \"000000000002\" endpoint: https://primary-2.unisphe.re:8443 backupEndpoint: https://backup-2.unisphe.re:8443 managementServers: - endpoint: https://primary-1.unisphe.re:8443 credentialsSecret: primary-1-secret skipCertificateValidation: false certSecret: primary-cert limits: maxActiveRead: 5 maxActiveWrite: 4 maxOutStandingRead: 50 maxOutStandingWrite: 50 - endpoint: https://backup-1.unisphe.re:8443 credentialsSecret: backup-1-secret skipCertificateValidation: true - endpoint: https://primary-2.unisphe.re:8443 credentialsSecret: primary-2-secret skipCertificateValidation: true - endpoint: https://backup-2.unisphe.re:8443 credentialsSecret: backup-2-secret skipCertificateValidation: true # \"csireverseproxy\" refers to the subchart csireverseproxy csireverseproxy: image: dellemc/csipowermax-reverseproxy:v2.4.0 tlsSecret: csirevproxy-tls-secret deployAsSidecar: true port: 2222 mode: StandAlone Note: If the credential secret is missing from any management server details, the installer will try to use the defaultCredentialsSecret\n","categories":"","description":"Installing CSI Driver for PowerMax via Helm\n","excerpt":"Installing CSI Driver for PowerMax via Helm\n","ref":"/csm-docs/v1/csidriver/installation/helm/powermax/","tags":"","title":"PowerMax"},{"body":" The Dell CSI Operator is no longer actively maintained or supported. Dell CSI Operator has been replaced with Dell CSM Operator. If you are currently using Dell CSI Operator, refer to the operator migration documentation to migrate from Dell CSI Operator to Dell CSM Operator.\nLinked Proxy mode for CSI reverse proxy is no longer actively maintained or supported. It will be deprecated in CSM 1.9. It is highly recommended that you use stand alone mode going forward. Installing CSI Driver for PowerMax via Operator CSI Driver for Dell PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Fibre Channel Requirements CSI Driver for Dell PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install CSI Driver:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs. iSCSI Requirements The CSI Driver for Dell PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\nAll Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell PowerMax array. All the port group names supplied to the driver must exist on each Dell PowerMax with the same name. For more information about configuring iSCSI, see Dell Host Connectivity guide.\nAuto RDM for vSphere over FC requirements The CSI Driver for Dell PowerMax supports auto RDM for vSphere over FC. These requirements are applicable for the clusters deployed on ESX/ESXi using virtualized environement.\nSet up the environment as follows:\nRequires VMware vCenter management software to manage all ESX/ESXis where the cluster is hosted.\nAdd all FC array ports zoned to the ESX/ESXis to a port group where the cluster is hosted .\nAdd initiators from all ESX/ESXis to a host(initiator group)/host group(cascaded initiator group) where the cluster is hosted.\nCreate a secret which contains vCenter privileges. Follow the steps here to create the same.\nNote: Hostgroups support with vSphere environment will be only available on csm-operator.\nLinux multipathing requirements CSI Driver for Dell PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\nAll the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file. As a best practice, use these options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 PowerPath for Linux requirements CSI Driver for Dell PowerMax supports PowerPath for Linux. Configure Linux PowerPath before installing the CSI Driver.\nFollow this procedure to set up PowerPath for Linux:\nAll the nodes must have the PowerPath package installed . Download the PowerPath archive for the environment from Dell Online Support. Untar the PowerPath archive, Copy the RPM package into a temporary folder and Install PowerPath using rpm -ivh DellEMCPower.LINUX-\u003cversion\u003e-\u003cbuild\u003e.\u003cplatform\u003e.x86_64.rpm Start the PowerPath service using systemctl start PowerPath Note: Do not install Dell PowerPath if multi-path software is already installed, as they cannot co-exist with native multi-path software.\nCreate secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. See the detailed documentation on how to create this secret here.\nInstall Driver Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\nCreate PowerMax credentials: Create a file called powermax-creds.yaml with the following content:\napiVersion: v1 kind: Secret metadata: name: powermax-creds # Replace driver-namespace with the namespace where driver is being deployed namespace: \u003cdriver-namespace\u003e type: Opaque data: # set username to the base64 encoded username username: \u003cbase64 username\u003e # set password to the base64 encoded password password: \u003cbase64 password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards) # chapsecret: \u003cbase64 CHAP secret\u003e Replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run the kubectl create -f powermax-creds.yaml command to create the secret.\nCreate a Custom Resource (CR) for PowerMax using the sample files provided here.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:\nParameter Description Required Default replicas Controls the number of controller Pods you deploy. If controller Pods are greater than the number of available nodes, excess Pods will become stuck in pending. The default is 2 which allows for Controller high availability. Yes 2 fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity Helps the scheduler to schedule the pod on a node satisfying the topology constraints, only if the requested capacity is available on the storage array - true Common parameters for node and controller X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended to all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443 X_CSI_TRANSPORT_PROTOCOL Choose which transport protocol to use (ISCSI, FC, auto or None) Yes auto X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No - X_CSI_MANAGED_ARRAYS List of comma-separated array ID(s) which will be managed by the driver Yes - X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Yes powermax-reverseproxy X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4 X_CSI_IG_MODIFY_HOSTNAME Change any existing host names. When nodenametemplate is set, it changes the name to the specified format else it uses driver default host name format. No false X_CSI_IG_NODENAME_TEMPLATE Provide a template for the CSI driver to use while creating the Host/IG on the array for the nodes in the cluster. It is of the format a-b-c-%foo%-xyz where foo will be replaced by host name of each node in the cluster. No - X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature see the related documentation No - X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller and Node plugin. Provides details of volume status, usage and volume condition. As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar No false X_CSI_VSPHERE_ENABLED Enable VMware virtualized environment support via RDM No false X_CSI_VSPHERE_PORTGROUP Existing portGroup that driver will use for vSphere Yes \"\" X_CSI_VSPHERE_HOSTNAME Existing host(initiator group)/host group(cascaded initiator group) that driver will use for vSphere Yes \"\" X_CSI_VCenter_HOST URL/endpoint of the vCenter where all the ESX are present Yes \"\" Node parameters X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature see the related documentation No false X_CSI_TOPOLOGY_CONTROL_ENABLED Enable/Disabe topology control. It filters out arrays, associated transport protocol available to each node and creates topology keys based on any such user input. No false X_CSI_MAX_VOLUMES_PER_NODE Enable volume limits. It specifies the maximum number of volumes that can be created on a node. Yes 0 Execute the following command to create the PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.\nNote - If CSI driver is getting installed using OCP UI , create these two configmaps manually using the command oc create -f \u003cconfigfilename\u003e\nConfigmap name powermax-config-params apiVersion: v1 kind: ConfigMap metadata: name: powermax-config-params namespace: test-powermax data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"debug\" CSI_LOG_FORMAT: \"JSON\" Configmap name node-topology-config apiVersion: v1 kind: ConfigMap metadata: name: node-topology-config namespace: test-powermax data: topologyConfig.yaml: | allowedConnections: - nodeName: \"node1\" rules: - \"000000000001:FC\" - \"000000000002:FC\" - nodeName: \"*\" rules: - \"000000000002:FC\" deniedConnections: - nodeName: \"node2\" rules: - \"000000000002:*\" - nodeName: \"node3\" rules: - \"*:*\" CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is component that will be installed along with the CSI PowerMax driver. For more details on this feature see the related documentation.\nDeployment and ClusterIP service will be created by dell-csi-operator.\nPre-requisites Create a TLS secret that holds an SSL certificate and a private key which is required by the reverse proxy server. Use a tool such as openssl to generate this secret using the example below:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs config : This section contains the details of the Reverse Proxy configuration mode : This value is set to Linked by default. Do not change this value linkConfig : This section contains the configuration of the Linked mode primary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if the primary Unisphere is unreachable url : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false standAloneConfig : This section contains the configuration of the StandAlone mode. Refer to the sample below for the detailed config Note: Only one of the Linked or StandAlone configurations needs to be supplied. The appropriate mode needs to be set in the spec as well.\nHere is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion: storage.dell.com/v1 kind: CSIPowerMaxRevProxy metadata: name: powermax-reverseproxy # \u003c- Name of the CSIPowerMaxRevProxy object namespace: test-powermax # \u003c- Set the namespace to where you will install the CSI PowerMax driver spec: # Image for CSI PowerMax ReverseProxy image: dellemc/csipowermax-reverseproxy:v2.3.0 # \u003c- CSI PowerMax Reverse Proxy image imagePullPolicy: Always # TLS secret which contains SSL certificate and private key for the Reverse Proxy server tlsSecret: csirevproxy-tls-secret config: mode: Linked linkConfig: primary: url: https://0.0.0.0:8443 #Unisphere URL skipCertificateValidation: true # This setting determines if client side Unisphere certificate validation is to be skipped certSecret: \"\" # Provide this value if skipCertificateValidation is set to false backup: # This is an optional field and lets you configure a backup unisphere which can be used by proxy server url: https://0.0.0.0:8443 #Unisphere URL skipCertificateValidation: true standAloneConfig: # Set mode to \"StandAlone\" in order to use this config storageArrays: - storageArrayId: \"000000000001\" # Unisphere server managing the PowerMax array primaryURL: https://unisphere-1-addr:8443 # proxyCredentialSecrets are used by the clients of the proxy to connect to it # If using proxy in the stand alone mode, then the driver must be provided the same secret. # The format of the proxy credential secret are exactly the same as the unisphere credential secret # For using the proxy with the driver, use the same proxy credential secrets for # all the managed storage arrays proxyCredentialSecrets: - proxy-creds - storageArrayId: \"000000000002\" primaryURL: https://unisphere-2-addr:8443 # An optional backup Unisphere server managing the same array # This can be used by the proxy to fall back to in case the primary # Unisphere is inaccessible temporarily backupURL: unisphere-3-addr:8443 proxyCredentialSecrets: - proxy-creds managementServers: - url: https://unisphere-1-addr:8443 # Secret containing the credentials of the Unisphere server arrayCredentialSecret: unsiphere-1-creds skipCertificateValidation: true - url: https://unisphere-2-addr:8443 arrayCredentialSecret: unsiphere-2-creds skipCertificateValidation: true - url: https://unisphere-3-addr:8443 arrayCredentialSecret: unsiphere-3-creds skipCertificateValidation: true Installation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service:\nkubectl create -f powermax_reverseproxy.yaml You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u003cnamespace\u003e kubectl get svc -n \u003cnamespace\u003e There is a new sample file - powermax_revproxy_standalone_with_driver.yaml in the samples folder which enables installation of CSI PowerMax ReverseProxy in StandAlone mode along with the CSI PowerMax driver. This mode enables the CSI PowerMax driver to connect to multiple Unisphere servers for managing multiple PowerMax arrays. Please follow the same steps described above to install ReverseProxy with this new sample file.\nDynamic Logging Configuration This feature is introduced in CSI Driver for powermax version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Sample CRD file for powermax You can find the sample CRD file here\nNote:\nKubelet config dir path is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. Volume Health Monitoring This feature is introduced in CSI Driver for PowerMax version 2.2.0.\nOperator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator.\nTo enable this feature, set X_CSI_HEALTH_MONITOR_ENABLED to true in the driver manifest under controller and node section. Also, install the external-health-monitor from sideCars section for controller plugin. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" Support for custom topology keys This feature is introduced in CSI Driver for PowerMax version 2.3.0.\nOperator based installation Support for custom topology keys is optional and by default this feature is disabled for drivers when installed via operator.\nX_CSI_TOPOLOGY_CONTROL_ENABLED provides a way to filter topology keys on a node based on array and transport protocol. If enabled, user can create custom topology keys by editing node-topology-config configmap.\nTo enable this feature, set X_CSI_TOPOLOGY_CONTROL_ENABLED to true in the driver manifest under node section. # X_CSI_TOPOLOGY_CONTROL_ENABLED provides a way to filter topology keys on a node based on array and transport protocol # if enabled, user can create custom topology keys by editing node-topology-config configmap. # Allowed values: # true: enable the filtration based on config map # false: disable the filtration based on config map # Default value: false - name: X_CSI_TOPOLOGY_CONTROL_ENABLED value: \"false\" Edit the sample config map “node-topology-config” present in sample CRD with appropriate values: Parameter Description allowedConnections List of node, array and protocol info for user allowed configuration allowedConnections.nodeName Name of the node on which user wants to apply given rules allowedConnections.rules List of StorageArrayID:TransportProtocol pair deniedConnections List of node, array and protocol info for user denied configuration deniedConnections.nodeName Name of the node on which user wants to apply given rules deniedConnections.rules List of StorageArrayID:TransportProtocol pair Note: Name of the configmap should always be node-topology-config.\nSupport for auto RDM for vSphere over FC This feature is introduced in CSI Driver for PowerMax version 2.5.0.\nOperator based installation Support for auto RDM for vSphere over FC feature is optional and by default this feature is disabled for drivers when installed via operator.\nTo enable this feature, set X_CSI_VSPHERE_ENABLED to true in the driver manifest under controller and node section.\n# VMware/vSphere virtualization support # set X_CSI_VSPHERE_ENABLED to true, if you to enable VMware virtualized environment support via RDM # Allowed values: # \"true\" - vSphere volumes are enabled # \"false\" - vSphere volumes are disabled # Default value: \"false\" - name: \"X_CSI_VSPHERE_ENABLED\" value: \"false\" # X_CSI_VSPHERE_PORTGROUP: An existing portGroup that driver will use for vSphere # recommended format: csi-x-VC-PG, x can be anything of user choice # Allowed value: valid existing port group on the array # Default value: \"\" \u003cempty\u003e - name: \"X_CSI_VSPHERE_PORTGROUP\" value: \"\" # X_CSI_VSPHERE_HOSTNAME: An existing host(initiator group)/ host group(cascaded intiator group) that driver will use for vSphere # this host/host group should contain initiators from all the ESXs/ESXi host where the cluster is deployed # recommended format: csi-x-VC-HN, x can be anything of user choice # Allowed value: valid existing host(initiator group)/ host group(cascaded intiator group) on the array # Default value: \"\" \u003cempty\u003e - name: \"X_CSI_VSPHERE_HOSTNAME\" value: \"\" Edit the section in the driver manifest having the sample for the following Secret with required values.\napiVersion: v1 kind: Secret metadata: name: vcenter-creds # Set driver namespace namespace: test-powermax type: Opaque data: # set username to the base64 encoded username username: YWRtaW4= # set password to the base64 encoded password password: YWRtaW4= These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with vCenter privileges.\n","categories":"","description":"Installing CSI Driver for PowerMax via Operator\n","excerpt":"Installing CSI Driver for PowerMax via Operator\n","ref":"/csm-docs/v1/csidriver/installation/operator/powermax/","tags":"","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use CSI Driver for Dell PowerMax. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with a different number of volumes in a given namespace using the storageclass provided. To test the installation of the CSI driver, perform these tests:\nVolume clone test Volume test Snapshot test Volume test Use this procedure to perform a volume test.\nCreate a namespace with the name test.\nRun the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\nRun the starttest.sh script and provide it with a test name. The following sample command can be used to run the 2vols test:\n./starttest.sh -t 2vols -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e This script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\nRun the /stoptest.sh -t 2vols -n \u003ctest_namespace\u003e script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\nNOTE: Helm tests have been designed assuming that users have created storageclass names like storageclass-name and storageclass-name-xfs. You can use kubectl get sc to check for the storageclass names.\nVolume clone test Use this procedure to perform a volume clone test.\nCreate a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: volumeclonetest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e This script does the following:\nInstalls a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum, and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test. Snapshot test Use this procedure to perform a snapshot test.\nCreate a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e This script does the following:\nInstalls a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot of that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test. NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass. You must update the snapshot class name in the file snap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\nVolume Expansion test Use this procedure to perform a volume expansion test.\nCreate a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e This script does the following:\nInstalls a helm chart that creates a Pod with a container, creates one PVC, and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC, and then recalculates the checksum Cleans up all the resources that were created as part of the test Note: This is not applicable for replicated volumes.\nSetting Application Prefix Application prefix is the name of the application that can be used to group the PowerMax volumes. We can use it while naming storage group. To set the application prefix for PowerMax, please refer to the sample storage class https://github.com/dell/csi-powermax/blob/main/samples/storageclass/powermax.yaml.\n# Name of application to be used to group volumes # This is used in naming storage group # Optional: true, Default value: None # Examples: APP, app, sanity, tests ApplicationPrefix: \u003capplication prefix\u003e Note: Supported length of storage group for PowerMax is 64 characters. Storage group name is of the format “csi-clusterprefix-application prefix-SLO name-SRP name-SG”. Based on the other inputs like clusterprefix,SLO name and SRP name maximum length of the ApplicationPrefix can vary.\nConsuming existing volumes with static provisioning Use this procedure to consume existing volumes with static provisioning.\nOpen your Unisphere for PowerMax, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, storage class is assumed as ‘powermax’, cluster prefix as ‘ABC’ and volume’s internal name as ‘00001’, array ID as ‘000000000001’, volume ID as ‘1abc23456’. The volume-handle should be in the format of csi-clusterPrefix-volumeNamePrefix-id-arrayID-volumeID. apiVersion: v1 kind: PersistentVolume metadata: name: pvol namespace: test spec: accessModes: - ReadWriteOnce capacity: storage: 8Gi csi: driver: csi-powermax.dellemc.com volumeHandle: csi-ABC-pmax-1abc23456-000000000001-00001 persistentVolumeReclaimPolicy: Retain storageClassName: powermax volumeMode: Filesystem Create PersistentVolumeClaim to use this PersistentVolume. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvc namespace: test spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: powermax volumeMode: Filesystem volumeName: pvol Then use this PVC as a volume in a pod. apiVersion: v1 kind: ServiceAccount metadata: name: powermaxtest namespace: test --- kind: StatefulSet apiVersion: apps/v1 metadata: name: powermaxtest namespace: test spec: selector: matchLabels: app: powermaxtest serviceName: staticprovisioning template: metadata: labels: app: powermaxtest spec: serviceAccount: powermaxtest containers: - name: test image: docker.io/centos:latest command: [ \"/bin/sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data\" name: pvc volumes: - name: pvc persistentVolumeClaim: claimName: pvc After the pod becomes Ready and Running, you can start to use this pod and volume. Note: CSI driver for PowerMax will create the necessary objects like Storage group, HostID and Masking View. They must not be created manually.\nSetting QoS parameters for throttling performance and bandwidth Use this procedure to set QoS parameters for throttling performance and bandwidth\nCreate storage class with the following parameters set. # Following params are for HostLimits, set them only if you want to set IOLimits # HostLimitName uniquely identifies given set of limits on a storage class # This is used in naming storage group, max of 3 letter # Optional: true # Example: \"HL1\", \"HL2\" #HostLimitName: \"HL1\" # The MBs per Second Host IO limit for the storage class # Optional: true, Default: \"\" # Examples: 100, 200, NOLIMIT #HostIOLimitMBSec: \"\" # The IOs per Second Host IO limit for the storage class # Optional: true, Default: \"\" # Examples: 100, 200, NOLIMIT #HostIOLimitIOSec: \"\" # distribution of the Host IO limits for the storage class # Optional: true, Default: \"\" # Allowed values: Never\",\"Always\" or \"OnFailure\" only #DynamicDistribution: \"\" Use the above storage class to create the PVC and provision the volume to the pod.\nOnce the pod becones Ready and Running, you will see the QoS parameters applied for throttling performance and bandwidth.\n","categories":"","description":"Tests to validate PowerMax CSI Driver installation","excerpt":"Tests to validate PowerMax CSI Driver installation","ref":"/csm-docs/v1/csidriver/installation/test/powermax/","tags":"","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v2.8.0 Linked Proxy mode for CSI reverse proxy is no longer actively maintained or supported. It will be deprecated in CSM 1.9. It is highly recommended that you use stand alone mode going forward. Note: Starting from CSI v2.4.0, Only Unisphere 10.0 REST endpoints are supported. It is mandatory that Unisphere should be updated to 10.0. Please find the instructions here.\nNote: File Replication for PowerMax is currently not supported\nNew Features/Changes #724 - [FEATURE]: CSM support for Openshift 4.13 #861 - [FEATURE]: CSM for PowerMax file support #876 - [FEATURE]: CSI 1.5 spec support -StorageCapacityTracking #877 - [FEATURE]: Make standalone helm chart available from helm repository : https://dell.github.io/dell/helm-charts #878 - [FEATURE]: CSI 1.5 spec support: Implement Volume Limits #922 - [FEATURE]: Use ubi9 micro as base image #937 - [FEATURE]: Google Anthos 1.15 support for PowerMax Fixed Issues #916 - [BUG]: Remove references to deprecated io/ioutil package Known Issues Issue Workaround Unable to update Host: A problem occurred modifying the host resource This issue occurs when the nodes do not have unique hostnames or when an IP address/FQDN with same sub-domains are used as hostnames. The workaround is to use unique hostnames or FQDN with unique sub-domains When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node If the volume limit is exhausted and there are pending pods and PVCs due to exceed max volume count, the pending PVCs will be bound to PVs and the pending pods will be scheduled to nodes when the driver pods are restarted. It is advised not to have any pending pods or PVCs once the volume limit per node is exhausted on a CSI Driver. There is an open issue reported with kubenetes at https://github.com/kubernetes/kubernetes/issues/95911 with the same behavior. Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for PowerMax CSI driver","excerpt":"Release notes for PowerMax CSI driver","ref":"/csm-docs/v1/csidriver/release/powermax/","tags":"","title":"PowerMax"},{"body":" Symptoms Prevention, Resolution or Workaround kubectl describe pod powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e indicates that the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or log in to the docker registry kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver cannot authenticate Check your secret’s username and password kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver failed to connect to the U4P because it could not verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.23.0 \u003c 1.27.0 which is incompatible with Kubernetes V1.23.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which are not supported. When a node goes down, the block volumes attached to the node cannot be attached to another node 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. When attempting a driver upgrade, you see: spec.fsGroupPolicy: Invalid value: \"xxx\": field is immutable You cannot upgrade between drivers with different fsGroupPolicies. See upgrade documentation for more details Ater the migration group is in “migrated” state but unable to move to “commit ready” state because the new paths are not being discovered on the cluster nodes. Run the following commands manually on the cluster nodes rescan-scsi-bus.sh -i rescan-scsi-bus.sh -a Failed to fetch details for array: 000000000000. [Unauthorized]\" Please make sure that correct encrypted username and password in secret files are used, also ensure whether the RBAC is enabled for the user Error looking up volume for idempotence check: Not Found or Get Volume step fails for: (000000000000) symID with error (Invalid Response from API) Make sure that Unisphere endpoint doesn’t end with front slash FailedPrecondition desc = no topology keys could be generate Make sure that FC or iSCSI connectivity to the arrays are proper CreateHost failed with error initiator is already part of different host. Update modifyHostName to true in values.yaml Or Remove the initiator from existing host kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs says connection refused and the reverseproxy logs says “Failed to setup server.(secrets \"secret-name\" not found)” Make sure the given secret exist on the cluster nodestage is failing with error Error invalid IQN Target iqn.EMC.0648.SE1F 1. Update initiator name to full default name , ex: iqn.1993-08.org.debian:01:e9afae962192 2.Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed and it should be full default name. Volume mount is failing on few OS(ex:VMware Virtual Platform) during node publish with error wrong fs type, bad option, bad superblock 1. Check the multipath configuration(if enabled) 2. Edit Vm Advanced settings-\u003ehardware and add the param disk.enableUUID=true and reboot the node ","categories":"","description":"Troubleshooting PowerMax Driver","excerpt":"Troubleshooting PowerMax Driver","ref":"/csm-docs/v1/csidriver/troubleshooting/powermax/","tags":"","title":"PowerMax"},{"body":"The CSI Driver for Dell PowerMax can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nPrerequisites The CSI Driver for Dell PowerMax can create PVC with different storage protocols access :\ndirect Fiber Channel direct iSCSI NFS Fiber Channel via VMware Raw Device Mapping In most cases, you will use one protocol only; therefore you should comply with the according prerequisites and not the others. Listing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using this command:\nkubectl get csm --all-namespaces Fibre Channel Requirements CSI Driver for Dell PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install CSI Driver:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs. iSCSI Requirements The CSI Driver for Dell PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\nAll Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell PowerMax array. All the port group names supplied to the driver must exist on each Dell PowerMax with the same name. For more information about configuring iSCSI, see Dell Host Connectivity guide.\niscsi-daemon MachineConfig To configure iSCSI in Red Hat OpenShift clusters, you can create a MachineConfig object using the console or oc to ensure that the iSCSI daemon starts on all the Red Hat CoreOS nodes. Here is an example of a MachineConfig object:\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: 99-iscsid labels: machineconfiguration.openshift.io/role: worker spec: config: ignition: version: 3.2.0 systemd: units: - name: \"iscsid.service\" enabled: true Once the MachineConfig object has been deployed, CoreOS will ensure that iscsid.service starts automatically.\nAlternatively, you can check the status of the iSCSI service by entering the following command on each worker node in the cluster:\nsudo systemctl status iscsid The service should be up and running (i.e. should be active state).\nIf the iscsid.service is not running, then perform the following steps on each worker node in the cluster\nLogin to worker nodes and check if the file /etc/iscsi/initiatorname.iscsi has been created properly If the file doesn’t exist or it doesn’t contain a valid ISCSI IQN, then make sure it exists with valid entries Ensure that iscsid service is running - Enable sudo systemctl enable iscsid \u0026 restart sudo systemctl restart iscsid iscsid if necessary. Note: If your worker nodes are running Red Hat CoreOS, make sure that automatic ISCSI login at boot is configured. Please contact RedHat for more details. NFS requirements CSI Driver for Dell PowerMax supports NFS communication. Ensure that the following requirements are met before you install CSI Driver:\nConfigure the NFS network. Please refer here for more details. PowerMax Embedded Management guest to access Unisphere for PowerMax. Create the NAS server. Please refer here for more details. Linux multipathing requirements CSI Driver for Dell PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\nAll the nodes must have the Device Mapper Multipathing package installed. NOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file. As a best practice, use these options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 multipathd MachineConfig If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\nuser_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0 Use the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: workers-multipath-conf-default labels: machineconfiguration.openshift.io/role: worker spec: config: ignition: version: 3.2.0 storage: files: - contents: source: data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0K verification: {} filesystem: root mode: 400 path: /etc/multipath.conf After deploying thisMachineConfig object, CoreOS will start multipath service automatically. Alternatively, you can check the status of the multipath service by entering the following command in each worker nodes. sudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally, you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer to official documentation of the multipath configuration.\nPowerPath for Linux requirements CSI Driver for Dell PowerMax supports PowerPath for Linux. Configure Linux PowerPath before installing the CSI Driver.\nFollow this procedure to set up PowerPath for Linux:\nAll the nodes must have the PowerPath package installed . Download the PowerPath archive for the environment from Dell Online Support. Untar the PowerPath archive, Copy the RPM package into a temporary folder and Install PowerPath using rpm -ivh DellEMCPower.LINUX-\u003cversion\u003e-\u003cbuild\u003e.\u003cplatform\u003e.x86_64.rpm Start the PowerPath service using systemctl start PowerPath Note: Do not install Dell PowerPath if multi-path software is already installed, as they cannot co-exist with native multi-path software.\nAuto RDM for vSphere over FC requirements The CSI Driver for Dell PowerMax supports auto RDM for vSphere over FC. These requirements are applicable for the clusters deployed on ESX/ESXi using virtualized environement.\nSet up the environment as follows:\nRequires VMware vCenter management software to manage all ESX/ESXis where the cluster is hosted.\nAdd all FC array ports zoned to the ESX/ESXis to a port group where the cluster is hosted .\nAdd initiators from all ESX/ESXis to a host(initiator group)/host group(cascaded initiator group) where the cluster is hosted.\nCreate a secret which contains vCenter privileges. Follow the steps here to create the same.\nInstallation (Optional) Create secret for client-side TLS verification Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. See the detailed documentation on how to create this secret here.\nInstall Driver Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\nCreate PowerMax credentials: Create a file called powermax-creds.yaml with the following content:\napiVersion: v1 kind: Secret metadata: name: powermax-creds # Replace driver-namespace with the namespace where driver is being deployed namespace: \u003cdriver-namespace\u003e type: Opaque data: # set username to the base64 encoded username username: \u003cbase64 username\u003e # set password to the base64 encoded password password: \u003cbase64 password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards) # chapsecret: \u003cbase64 CHAP secret\u003e Replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run the kubectl create -f powermax-creds.yaml command to create the secret.\nCreate a configmap using sample here. Fill in the appropriate values for driver configuration. Example: config.yaml\nmode: StandAlone # Mode for the reverseproxy, should not be changed port: 2222 # Port on which reverseproxy will listen logLevel: debug logFormat: text standAloneConfig: storageArrays: - storageArrayId: \"000000000001\" # arrayID primaryURL: https://primary-1.unisphe.re:8443 # primary unisphere for arrayID backupURL: https://backup-1.unisphe.re:8443 # backup unisphere for arrayID proxyCredentialSecrets: - proxy-secret-11 # credential secret for primary unisphere, e.g., powermax-creds - proxy-secret-12 # credential secret for backup unisphere, e.g., powermax-creds - storageArrayId: \"000000000002\" primaryURL: https://primary-2.unisphe.re:8443 backupURL: https://backup-2.unisphe.re:8443 proxyCredentialSecrets: - proxy-secret-21 - proxy-secret-22 managementServers: - url: https://primary-1.unisphe.re:8443 # primary unisphere endpoint arrayCredentialSecret: primary-1-secret # primary credential secret e.g., powermax-creds skipCertificateValidation: true - url: https://backup-1.unisphe.re:8443 # backup unisphere endpoint arrayCredentialSecret: backup-1-secret # backup credential secret e.g., powermax-creds skipCertificateValidation: false # value false, to verify unisphere certificate and provide certSecret certSecret: primary-certs # unisphere verification certificate - url: https://primary-2.unisphe.re:8443 arrayCredentialSecret: primary-2-secret skipCertificateValidation: true - url: https://backup-2.unisphe.re:8443 arrayCredentialSecret: backup-2-secret skipCertificateValidation: false certSecret: primary-certs After editing the file, run this command to create a secret called powermax-reverseproxy-config. If you are using a different namespace/secret name, just substitute those into the command.\nkubectl create configmap powermax-reverseproxy-config --from-file config.yaml -n powermax Create a CR (Custom Resource) for PowerMax using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:\nParameter Description Required Default dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet replicas Controls the number of controller Pods you deploy. If controller Pods are greater than the number of available nodes, excess Pods will become stuck in pending. The default is 2 which allows for Controller high availability. Yes 2 fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” Common parameters for node and controller X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended to all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443 X_CSI_TRANSPORT_PROTOCOL Choose which transport protocol to use (ISCSI, FC, auto or None) Yes auto X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No - X_CSI_MANAGED_ARRAYS List of comma-separated array ID(s) which will be managed by the driver Yes - X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Yes csipowermax-reverseproxy X_CSI_IG_MODIFY_HOSTNAME Change any existing host names. When nodenametemplate is set, it changes the name to the specified format else it uses driver default host name format. No false X_CSI_IG_NODENAME_TEMPLATE Provide a template for the CSI driver to use while creating the Host/IG on the array for the nodes in the cluster. It is of the format a-b-c-%foo%-xyz where foo will be replaced by host name of each node in the cluster. No - X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature see the related documentation No - X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller and Node plugin. Provides details of volume status, usage and volume condition. As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar No false X_CSI_VSPHERE_ENABLED Enable VMware virtualized environment support via RDM No false X_CSI_VSPHERE_PORTGROUP Existing portGroup that driver will use for vSphere Yes \"\" X_CSI_VSPHERE_HOSTNAME Existing host(initiator group)/host group(cascaded initiator group) that driver will use for vSphere Yes \"\" X_CSI_VCenter_HOST URL/endpoint of the vCenter where all the ESX are present Yes \"\" Node parameters X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature see the related documentation No false X_CSI_TOPOLOGY_CONTROL_ENABLED Enable/Disabe topology control. It filters out arrays, associated transport protocol available to each node and creates topology keys based on any such user input. No false CSI Reverseproxy Module X_CSI_REVPROXY_TLS_SECRET Name of TLS secret defined in config map Yes “csirevproxy-tls-secret” X_CSI_REVPROXY_PORT Port number where reverseproxy will listen as defined in config map Yes “2222” X_CSI_CONFIG_MAP_NAME Name of config map as created for CSI PowerMax Yes “powermax-reverseproxy-config” Execute the following command to create the PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.\nThe mandatory module CSI PowerMax Reverseproxy will be installed automatically with the same command.\nOther features to enable Dynamic Logging Configuration This feature is introduced in CSI Driver for powermax version 2.0.0.\nAs part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Volume Health Monitoring This feature is introduced in CSI Driver for PowerMax version 2.2.0.\nVolume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via CSM operator.\nTo enable this feature, set X_CSI_HEALTH_MONITOR_ENABLED to true in the driver manifest under controller and node section. Also, install the external-health-monitor from sideCars section for controller plugin. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" Support for custom topology keys This feature is introduced in CSI Driver for PowerMax version 2.3.0.\nSupport for custom topology keys is optional and by default this feature is disabled for drivers when installed via CSM operator.\nX_CSI_TOPOLOGY_CONTROL_ENABLED provides a way to filter topology keys on a node based on array and transport protocol. If enabled, user can create custom topology keys by editing node-topology-config configmap.\nTo enable this feature, set X_CSI_TOPOLOGY_CONTROL_ENABLED to true in the driver manifest under node section.\n# X_CSI_TOPOLOGY_CONTROL_ENABLED provides a way to filter topology keys on a node based on array and transport protocol # if enabled, user can create custom topology keys by editing node-topology-config configmap. # Allowed values: # true: enable the filtration based on config map # false: disable the filtration based on config map # Default value: false - name: X_CSI_TOPOLOGY_CONTROL_ENABLED value: \"false\" Edit the sample config map “node-topology-config” as described here with appropriate values: Example:\nkind: ConfigMap metadata: name: node-topology-config namespace: powermax data: topologyConfig.yaml: | allowedConnections: - nodeName: \"node1\" rules: - \"000000000001:FC\" - \"000000000002:FC\" - nodeName: \"*\" rules: - \"000000000002:FC\" deniedConnections: - nodeName: \"node2\" rules: - \"000000000002:*\" - nodeName: \"node3\" rules: - \"*:*\" Parameter Description allowedConnections List of node, array and protocol info for user allowed configuration allowedConnections.nodeName Name of the node on which user wants to apply given rules allowedConnections.rules List of StorageArrayID:TransportProtocol pair deniedConnections List of node, array and protocol info for user denied configuration deniedConnections.nodeName Name of the node on which user wants to apply given rules deniedConnections.rules List of StorageArrayID:TransportProtocol pair Run following command to create the configmap\nkubectl create -f topologyConfig.yaml Note: Name of the configmap should always be node-topology-config.\nSupport for auto RDM for vSphere over FC This feature is introduced in CSI Driver for PowerMax version 2.5.0.\nSupport for auto RDM for vSphere over FC feature is optional and by default this feature is disabled for drivers when installed via CSM operator.\nTo enable this feature, set X_CSI_VSPHERE_ENABLED to true in the driver manifest under controller and node section.\n# VMware/vSphere virtualization support # set X_CSI_VSPHERE_ENABLED to true, if you to enable VMware virtualized environment support via RDM # Allowed values: # \"true\" - vSphere volumes are enabled # \"false\" - vSphere volumes are disabled # Default value: \"false\" - name: \"X_CSI_VSPHERE_ENABLED\" value: \"false\" # X_CSI_VSPHERE_PORTGROUP: An existing portGroup that driver will use for vSphere # recommended format: csi-x-VC-PG, x can be anything of user choice # Allowed value: valid existing port group on the array # Default value: \"\" \u003cempty\u003e - name: \"X_CSI_VSPHERE_PORTGROUP\" value: \"\" # X_CSI_VSPHERE_HOSTNAME: An existing host(initiator group)/ host group(cascaded intiator group) that driver will use for vSphere # this host/host group should contain initiators from all the ESXs/ESXi host where the cluster is deployed # recommended format: csi-x-VC-HN, x can be anything of user choice # Allowed value: valid existing host(initiator group)/ host group(cascaded intiator group) on the array # Default value: \"\" \u003cempty\u003e - name: \"X_CSI_VSPHERE_HOSTNAME\" value: \"\" Edit the Secret file vcenter-creds here with required values. Example:\napiVersion: v1 kind: Secret metadata: name: vcenter-creds # Set driver namespace namespace: powermax type: Opaque data: # set username to the base64 encoded username username: YWRtaW4= # set password to the base64 encoded password password: YWRtaW4= These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with vCenter privileges. 3. 4. Run following command to create the configmap\nkubectl create -f vcenter-secret.yaml Note: Name of the secret should always be vcenter-creds.\n","categories":"","description":"Installing Dell CSI Driver for PowerMax via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for PowerMax via Dell CSM Operator\n","ref":"/csm-docs/v1/deployment/csmoperator/drivers/powermax/","tags":"","title":"PowerMax"},{"body":"Configuring PowerMax CSI Driver with CSM for Authorization Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow these steps to configure the CSI Drivers to work with the Authorization sidecar:\nApply the secret containing the tenant token data into the driver namespace. It’s assumed that the Kubernetes administrator has the token secret manifest, generated by your storage administrator via Generate a Token, saved in /tmp/token.yaml.\n#It is assumed that array type powermax has the namespace “powermax”.\nkubectl apply -f /tmp/token.yaml -n powermax Edit these parameters in samples/secret/karavi-authorization-config.json file in CSI PowerMax driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\nParameter Description Required Default username Username for connecting to the backend storage array. This parameter is ignored. No - password Password for connecting to to the backend storage array. This parameter is ignored. No - intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes - endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400 systemID System ID of the backend storage array. Yes \" \" skipCertificateValidation A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml Create the karavi-authorization-config secret using this command:\nkubectl -n powermax create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f - Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n powermax create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f - Otherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n powermax create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f - Enable CSM Authorization in the driver installation applicable to your installation method.\nHelm\nIn Install the Driver where you edit samples/secret/secret.yaml with the credentials of the PowerMax, you can leave these with the default values as they will be ignored.\nRefer to the Install the Driver section to edit the parameters in my-powermax-settings.yaml file to configure the driver to communicate with the CSM Authorization sidecar.\nUpdate global.storageArrays.endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate global.managementServers.endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate authorization.enabled to true.\nUpdate authorization.sidecarProxyImage to the image of the CSM Authorization sidecar. In most cases, you can leave the default value.\nUpdate authorization.proxyHost to the hostname of the CSM Authorization Proxy Server.\nUpdate authorization.skipCertificateValidation to true or false depending on if you want to disable or enable certificate validation of the CSM Authorization Proxy Server.\nExample:\nglobal: storageArrays: - storageArrayId: \"123456789\" endpoint: https://localhost:9400 managementServers: - endpoint: https://localhost:9400 authorization: enabled: true # sidecarProxyImage: the container image used for the csm-authorization-sidecar. # Default value: dellemc/csm-authorization-sidecar:v1.7.0 sidecarProxyImage: dellemc/csm-authorization-sidecar:v1.7.0 # proxyHost: hostname of the csm-authorization server # Default value: None proxyHost: csm-authorization.com # skipCertificateValidation: certificate validation of the csm-authorization server # Allowed Values: # \"true\" - TLS certificate verification will be skipped # \"false\" - TLS certificate will be verified # Default value: \"true\" skipCertificateValidation: true Install the Dell CSI PowerMax driver following the appropriate documenation for your installation method.\n(Optional) Install dellctl to perform Kubernetes administrator commands for additional capabilities (e.g., list volumes). Please refer to the dellctl documentation page for the installation steps and command list.\n","categories":"","description":"Enabling CSM Authorization for PowerMax CSI Driver\n","excerpt":"Enabling CSM Authorization for PowerMax CSI Driver\n","ref":"/csm-docs/v2/authorization/configuration/powermax/","tags":"","title":"PowerMax"},{"body":" Linked Proxy mode for CSI reverse proxy is no longer actively maintained or supported. It will be deprecated in CSM 1.9 (Driver Version 2.9.0). It is highly recommended that you use stand alone mode going forward. CSI Driver for Dell PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, see the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\nCSI Driver for Dell PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support- CSI PowerMax ReverseProxy, which maximizes CSI driver and Unisphere performance Kubernetes External Resizer, which resizes the volume (optional) Kubernetes External health monitor, which provides volume health status (optional) Dell CSI Replicator, which provides Replication capability. (optional) Dell CSI Migrator, which provides migrating capability within and across arrays (optional) Node rescanner, which rescans the node for new data paths after migration The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\nCSI Driver for Dell PowerMax Kubernetes Node Registrar, which handles the driver registration Prerequisites The following requirements must be met before installing CSI Driver for Dell PowerMax:\nInstall Kubernetes or OpenShift (see supported versions) Install Helm 3 Fibre Channel requirements iSCSI requirements Auto RDM for vSphere over FC requirements Certificate validation for Unisphere REST API calls Mount propagation is enabled on container runtime that is being used Linux multipathing requirements If using Snapshot feature, satisfy all Volume Snapshot requirements If enabling CSM for Authorization, please refer to the Authorization deployment steps first If using Powerpath , install the PowerPath for Linux requirements Prerequisite for CSI Reverse Proxy CSI PowerMax Reverse Proxy is an HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be an X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nStarting from v2.7.0 , these secrets will be created automatically using the below tls.key and tls.cert contents provided in values.yaml file. For this , we need to install cert-manager using below command which manages the certs and secrets .\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml Here is an example showing how to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 Install Helm 3 Install Helm 3 on the master node before you install CSI Driver for Dell PowerMax.\nSteps\nRun the command to install Helm 3.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Fibre Channel Requirements CSI Driver for Dell PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install CSI Driver:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs. iSCSI Requirements The CSI Driver for Dell PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\nAll Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell PowerMax array. All the port group names supplied to the driver must exist on each Dell PowerMax with the same name. For more information about configuring iSCSI, see Dell Host Connectivity guide.\nAuto RDM for vSphere over FC requirements The CSI Driver for Dell PowerMax supports auto RDM for vSphere over FC. These requirements are applicable for the clusters deployed on ESX/ESXi using virtualized environement.\nSet up the environment as follows:\nRequires VMware vCenter management software to manage all ESX/ESXis where the cluster is hosted.\nAdd all FC array ports zoned to the ESX/ESXis to a port group where the cluster is hosted .\nAdd initiators from all ESX/ESXis to a host(initiator group) where the cluster is hosted.\nEdit samples/secret/vcenter-secret.yaml file, to point to the correct namespace, and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with vCenter privileges.\nCreate the secret by running the below command,\nkubectl create -f samples/secret/vcenter-secret.yaml Certificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\nTo fetch the certificate, run\nopenssl s_client -showcerts -connect [Unisphere IP]:8443 \u003c/dev/null 2\u003e /dev/null | openssl x509 -outform PEM \u003e ca_cert.pem NOTE: The IP address varies for each user.\nTo create the secret, run\nkubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax Ports in the port group There are no restrictions to how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell PowerMax to ensure that you have multiple paths to your data volumes.\nLinux multipathing requirements CSI Driver for Dell PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\nAll the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file. As a best practice, use the following options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 PowerPath for Linux requirements CSI Driver for Dell PowerMax supports PowerPath for Linux. Configure Linux PowerPath before installing the CSI Driver.\nSet up the PowerPath for Linux as follows:\nAll the nodes must have the PowerPath package installed . Download the PowerPath archive for the environment from Dell Online Support. Untar the PowerPath archive, Copy the RPM package into a temporary folder and Install PowerPath using rpm -ivh DellEMCPower.LINUX-\u003cversion\u003e-\u003cbuild\u003e.\u003cplatform\u003e.x86_64.rpm Start the PowerPath service using systemctl start PowerPath Note: Do not install Dell PowerPath if multi-path software is already installed, as they cannot co-exist with native multi-path software.\n(Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\n(Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication: enabled: true Replication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in the csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\nRun git clone -b v2.7.0 https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the samples/secret/secret.yaml file,to point to the correct namespace, and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with PowerMax privileges. Create the secret by running kubectl create -f samples/secret/secret.yaml Copy the default values.yaml file cd helm \u0026\u0026 cp csi-powermax/values.yaml my-powermax-settings.yaml Ensure the unisphere have 10.0 REST endpoint support by clicking on Unisphere -\u003e Help (?) -\u003e About in Unisphere for PowerMax GUI. Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml Parameter Description Required Default global This section refers to configuration options for both CSI PowerMax Driver and Reverse Proxy - - defaultCredentialsSecret This secret name refers to:\n1. The Unisphere credentials if the driver is installed without proxy or with proxy in Linked mode.\n2. The proxy credentials if the driver is installed with proxy in StandAlone mode.\n3. The default Unisphere credentials if credentialsSecret is not specified for a management server. Yes powermax-creds storageArrays This section refers to the list of arrays managed by the driver and Reverse Proxy in StandAlone mode. - - storageArrayId This refers to PowerMax Symmetrix ID. Yes 000000000001 endpoint This refers to the URL of the Unisphere server managing storageArrayId. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes if Reverse Proxy mode is StandAlone https://primary-1.unisphe.re:8443 backupEndpoint This refers to the URL of the backup Unisphere server managing storageArrayId, if Reverse Proxy is installed in StandAlone mode. If authorization is enabled, backupEndpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes https://backup-1.unisphe.re:8443 managementServers This section refers to the list of configurations for Unisphere servers managing powermax arrays. - - endpoint This refers to the URL of the Unisphere server. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes https://primary-1.unisphe.re:8443 credentialsSecret This refers to the user credentials for endpoint Yes primary-1-secret skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. No “True” certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty limits This refers to various limits for Reverse Proxy No - maxActiveRead This refers to the maximum concurrent READ request handled by the reverse proxy. No 5 maxActiveWrite This refers to the maximum concurrent WRITE request handled by the reverse proxy. No 4 maxOutStandingRead This refers to maximum queued READ request when reverse proxy receives more than maxActiveRead requests. No 50 maxOutStandingWrite This refers to maximum queued WRITE request when reverse proxy receives more than maxActiveWrite requests. No 50 kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes “ABC” logLevel CSI driver log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes “debug” logFormat CSI driver log format. Allowed values: “TEXT” or “JSON”. Yes “TEXT” kubeletConfigDir kubelet config directory path. Ensure that the config.yaml file is present at this path. Yes /var/lib/kubelet defaultFsType Used to set the default FS type for external provisioner Yes ext4 portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only “PortGroup1, PortGroup2, PortGroup3” skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No “True” transportProtocol Set the preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty nodeNameTemplate Used to specify a template that will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, leave this value empty. No Empty modifyHostName Change any existing host names. When nodenametemplate is set, it changes the name to the specified format else it uses driver default host name format. No false powerMaxDebug Enables low level and http traffic logging between the CSI driver and Unisphere. Don’t enable this unless asked to do so by the support team. No false enableCHAP Determine if the driver is going to configure SCSI node databases on the nodes with the CHAP credentials. If enabled, the CHAP secret must be provided in the credentials secret and set to the key “chapsecret” No false fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” version Current version of the driver. Don’t modify this value as this value will be used by the install script. Yes v2.3.0 images Defines the container images used by the driver. - - driverRepository Defines the registry of the container image used for the driver. Yes dellemc controller Allows configuration of the controller-specific parameters. - - controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2 volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s” snapshot.enabled Enable/Disable volume snapshot feature Yes true snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot” resizer.enabled Enable/Disable volume expansion feature Yes true healthMonitor.enabled Allows to enable/disable volume health monitor No false healthMonitor.interval Interval of monitoring volume health condition No 60s nodeSelector Define node selection constraints for pods of controller deployment No tolerations Define tolerations for the controller deployment, if required No node Allows configuration of the node-specific parameters. - - tolerations Add tolerations as per requirement No - nodeSelector Add node selectors as per requirement No - healthMonitor.enabled Allows to enable/disable volume health monitor No false topologyControl.enabled Allows to enable/disable topology control to filter topology keys No false csireverseproxy This section refers to the configuration options for CSI PowerMax Reverse Proxy - - image This refers to the image of the CSI PowerMax Reverse Proxy container. Yes dellemc/csipowermax-reverseproxy:v2.4.0 tlsSecret This refers to the TLS secret of the Reverse Proxy Server. Yes csirevproxy-tls-secret deployAsSidecar If set to true, the Reverse Proxy is installed as a sidecar to the driver’s controller pod otherwise it is installed as a separate deployment. Yes “True” port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation Yes 2222 mode This refers to the installation mode of Reverse Proxy. It can be set to:\n1. Linked: In this mode, the Reverse Proxy communicates with a primary or a backup Unisphere managing the same set of arrays.\n2. StandAlone: In this mode, the Reverse Proxy communicates with multiple arrays managed by different Unispheres. Yes “StandAlone” certManager Auto-create TLS certificate for csi-reverseproxy - - selfSignedCert Set selfSignedCert to use a self-signed certificate No true certificateFile certificateFile has tls.key content in encoded format No tls.crt.encoded64 privateKeyFile privateKeyFile has tls.key content in encoded format No tls.key.encoded64 authorization Authorization is an optional feature to apply credential shielding of the backend PowerMax. - - enabled A boolean that enables/disables authorization feature. No false sidecarProxyImage Image for csm-authorization-sidecar. No \" \" proxyHost Hostname of the csm-authorization server. No Empty skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization proxy server. No true migration Migration is an optional feature to enable migration between storage classes - - enabled A boolean that enables/disables migration feature. No false image Image for dell-csi-migrator sidecar. No \" \" nodeRescanSidecarImage Image for node rescan sidecar which rescans nodes for identifying new paths. No \" \" migrationPrefix enables migration sidecar to read required information from the storage class fields No migration.storage.dell.com replication Replication is an optional feature to enable replication \u0026 disaster recovery capabilities of PowerMax to Kubernetes clusters. - - enabled A boolean that enables/disables replication feature. No false image Image for dell-csi-replicator sidecar. No \" \" replicationContextPrefix enables side cars to read required information from the volume context No powermax replicationPrefix Determine if replication is enabled No replication.storage.dell.com vSphere This section refers to the configuration options for VMware virtualized environment support via RDM - - enabled A boolean that enables/disables VMware virtualized environment support. No false fcPortGroup Existing portGroup that driver will use for vSphere. Yes \"\" fcHostGroup Existing host(initiator group)/hostgroup(cascaded initiator group) that driver will use for vSphere. Yes \"\" vCenterHost URL/endpoint of the vCenter where all the ESX are present Yes \"\" vCenterCredSecret Secret name for the vCenter credentials. Yes \"\" Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml Or you can also install the driver using standalone helm chart using the command helm install --values my-powermax-settings.yaml --namespace powermax powermax ./csi-powermax Note:\nFor detailed instructions on how to run the install scripts, see the readme document in the dell-csi-helm-installer folder. There are a set of samples provided here to help you configure the driver with reverse proxy This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option In order to enable authorization, there should be an authorization proxy server already installed. PowerMax Array username must have role as StorageAdmin to be able to perform CRUD operations. If the user is using complex K8s version like “v1.23.3-mirantis-1”, use this kubeVersion check in helm Chart file. kubeVersion: “\u003e= 1.23.0-0 \u003c 1.27.0-0”. User should provide all boolean values with double-quotes. This applies only for values.yaml. Example: “true”/“false”. controllerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails. Endpoint should not have any special character at the end apart from port number. Storage Classes A wide set of annotated storage class manifests has been provided in the samples/storageclass folder. Please use these samples to create new storage classes to provision storage.\nVolume Snapshot Class Starting with CSI PowerMax v1.7.0, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nSample values file The following sections have useful snippets from values.yaml file which provides more information on how to configure the CSI PowerMax driver along with CSI PowerMax ReverseProxy in various modes\nCSI PowerMax driver with Proxy in Linked mode In this mode, the CSI PowerMax ReverseProxy acts as a passthrough for the RESTAPI calls and only provides limited functionality such as rate limiting, backup Unisphere server. The CSI PowerMax driver is still responsible for the authentication with the Unisphere server.\nThe first endpoint in the list of management servers is the primary Unisphere server and if you provide a second endpoint, then it will be considered as the backup Unisphere’s endpoint.\nglobal: defaultCredentialsSecret: powermax-creds storageArrays: - storageArrayId: \"000000000001\" - storageArrayId: \"000000000002\" managementServers: - endpoint: https://primary-unisphere:8443 skipCertificateValidation: false certSecret: primary-cert limits: maxActiveRead: 5 maxActiveWrite: 4 maxOutStandingRead: 50 maxOutStandingWrite: 50 - endpoint: https://backup-unisphere:8443 # \"csireverseproxy\" refers to the subchart csireverseproxy csireverseproxy: # Set enabled to true if you want to use proxy image: dellemc/csipowermax-reverseproxy:v2.4.0 tlsSecret: csirevproxy-tls-secret deployAsSidecar: true port: 2222 mode: Linked Note: Since the driver is still responsible for authentication when used with Proxy in Linked mode, the credentials for both primary and backup Unisphere need to be the same.\nCSI PowerMax driver with Proxy in StandAlone mode This is the most advanced configuration which provides you with the capability to connect to Multiple Unisphere servers. You can specify primary and backup Unisphere servers for each storage array. If you have different credentials for your Unisphere servers, you can also specify different credential secrets.\nglobal: defaultCredentialsSecret: powermax-creds storageArrays: - storageArrayId: \"000000000001\" endpoint: https://primary-1.unisphe.re:8443 backupEndpoint: https://backup-1.unisphe.re:8443 - storageArrayId: \"000000000002\" endpoint: https://primary-2.unisphe.re:8443 backupEndpoint: https://backup-2.unisphe.re:8443 managementServers: - endpoint: https://primary-1.unisphe.re:8443 credentialsSecret: primary-1-secret skipCertificateValidation: false certSecret: primary-cert limits: maxActiveRead: 5 maxActiveWrite: 4 maxOutStandingRead: 50 maxOutStandingWrite: 50 - endpoint: https://backup-1.unisphe.re:8443 credentialsSecret: backup-1-secret skipCertificateValidation: true - endpoint: https://primary-2.unisphe.re:8443 credentialsSecret: primary-2-secret skipCertificateValidation: true - endpoint: https://backup-2.unisphe.re:8443 credentialsSecret: backup-2-secret skipCertificateValidation: true # \"csireverseproxy\" refers to the subchart csireverseproxy csireverseproxy: image: dellemc/csipowermax-reverseproxy:v2.4.0 tlsSecret: csirevproxy-tls-secret deployAsSidecar: true port: 2222 mode: StandAlone Note: If the credential secret is missing from any management server details, the installer will try to use the defaultCredentialsSecret\n","categories":"","description":"Installing CSI Driver for PowerMax via Helm\n","excerpt":"Installing CSI Driver for PowerMax via Helm\n","ref":"/csm-docs/v2/csidriver/installation/helm/powermax/","tags":"","title":"PowerMax"},{"body":" The Dell CSI Operator is no longer actively maintained or supported. Dell CSI Operator has been replaced with Dell CSM Operator. If you are currently using Dell CSI Operator, refer to the operator migration documentation to migrate from Dell CSI Operator to Dell CSM Operator.\nLinked Proxy mode for CSI reverse proxy is no longer actively maintained or supported. It will be deprecated in CSM 1.9. It is highly recommended that you use stand alone mode going forward. Installing CSI Driver for PowerMax via Operator CSI Driver for Dell PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Fibre Channel Requirements CSI Driver for Dell PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install CSI Driver:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs. iSCSI Requirements The CSI Driver for Dell PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\nAll Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell PowerMax array. All the port group names supplied to the driver must exist on each Dell PowerMax with the same name. For more information about configuring iSCSI, see Dell Host Connectivity guide.\nAuto RDM for vSphere over FC requirements The CSI Driver for Dell PowerMax supports auto RDM for vSphere over FC. These requirements are applicable for the clusters deployed on ESX/ESXi using virtualized environement.\nSet up the environment as follows:\nRequires VMware vCenter management software to manage all ESX/ESXis where the cluster is hosted.\nAdd all FC array ports zoned to the ESX/ESXis to a port group where the cluster is hosted .\nAdd initiators from all ESX/ESXis to a host(initiator group)/host group(cascaded initiator group) where the cluster is hosted.\nCreate a secret which contains vCenter privileges. Follow the steps here to create the same.\nNote: Hostgroups support with vSphere environment will be only available on csm-operator.\nLinux multipathing requirements CSI Driver for Dell PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\nAll the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file. As a best practice, use these options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 PowerPath for Linux requirements CSI Driver for Dell PowerMax supports PowerPath for Linux. Configure Linux PowerPath before installing the CSI Driver.\nFollow this procedure to set up PowerPath for Linux:\nAll the nodes must have the PowerPath package installed . Download the PowerPath archive for the environment from Dell Online Support. Untar the PowerPath archive, Copy the RPM package into a temporary folder and Install PowerPath using rpm -ivh DellEMCPower.LINUX-\u003cversion\u003e-\u003cbuild\u003e.\u003cplatform\u003e.x86_64.rpm Start the PowerPath service using systemctl start PowerPath Note: Do not install Dell PowerPath if multi-path software is already installed, as they cannot co-exist with native multi-path software.\nCreate secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. See the detailed documentation on how to create this secret here.\nInstall Driver Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\nCreate PowerMax credentials: Create a file called powermax-creds.yaml with the following content:\napiVersion: v1 kind: Secret metadata: name: powermax-creds # Replace driver-namespace with the namespace where driver is being deployed namespace: \u003cdriver-namespace\u003e type: Opaque data: # set username to the base64 encoded username username: \u003cbase64 username\u003e # set password to the base64 encoded password password: \u003cbase64 password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards) # chapsecret: \u003cbase64 CHAP secret\u003e Replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run the kubectl create -f powermax-creds.yaml command to create the secret.\nCreate a Custom Resource (CR) for PowerMax using the sample files provided here.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:\nParameter Description Required Default replicas Controls the number of controller Pods you deploy. If controller Pods are greater than the number of available nodes, excess Pods will become stuck in pending. The default is 2 which allows for Controller high availability. Yes 2 fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” Common parameters for node and controller X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended to all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443 X_CSI_TRANSPORT_PROTOCOL Choose which transport protocol to use (ISCSI, FC, auto or None) Yes auto X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No - X_CSI_MANAGED_ARRAYS List of comma-separated array ID(s) which will be managed by the driver Yes - X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Yes powermax-reverseproxy X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4 X_CSI_IG_MODIFY_HOSTNAME Change any existing host names. When nodenametemplate is set, it changes the name to the specified format else it uses driver default host name format. No false X_CSI_IG_NODENAME_TEMPLATE Provide a template for the CSI driver to use while creating the Host/IG on the array for the nodes in the cluster. It is of the format a-b-c-%foo%-xyz where foo will be replaced by host name of each node in the cluster. No - X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature see the related documentation No - X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller and Node plugin. Provides details of volume status, usage and volume condition. As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar No false X_CSI_VSPHERE_ENABLED Enable VMware virtualized environment support via RDM No false X_CSI_VSPHERE_PORTGROUP Existing portGroup that driver will use for vSphere Yes \"\" X_CSI_VSPHERE_HOSTNAME Existing host(initiator group)/host group(cascaded initiator group) that driver will use for vSphere Yes \"\" X_CSI_VCenter_HOST URL/endpoint of the vCenter where all the ESX are present Yes \"\" Node parameters X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature see the related documentation No false X_CSI_TOPOLOGY_CONTROL_ENABLED Enable/Disabe topology control. It filters out arrays, associated transport protocol available to each node and creates topology keys based on any such user input. No false Execute the following command to create the PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.\nNote - If CSI driver is getting installed using OCP UI , create these two configmaps manually using the command oc create -f \u003cconfigfilename\u003e\nConfigmap name powermax-config-params apiVersion: v1 kind: ConfigMap metadata: name: powermax-config-params namespace: test-powermax data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"debug\" CSI_LOG_FORMAT: \"JSON\" Configmap name node-topology-config apiVersion: v1 kind: ConfigMap metadata: name: node-topology-config namespace: test-powermax data: topologyConfig.yaml: | allowedConnections: - nodeName: \"node1\" rules: - \"000000000001:FC\" - \"000000000002:FC\" - nodeName: \"*\" rules: - \"000000000002:FC\" deniedConnections: - nodeName: \"node2\" rules: - \"000000000002:*\" - nodeName: \"node3\" rules: - \"*:*\" CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is component that will be installed along with the CSI PowerMax driver. For more details on this feature see the related documentation.\nDeployment and ClusterIP service will be created by dell-csi-operator.\nPre-requisites Create a TLS secret that holds an SSL certificate and a private key which is required by the reverse proxy server. Use a tool such as openssl to generate this secret using the example below:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt --key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs config : This section contains the details of the Reverse Proxy configuration mode : This value is set to Linked by default. Do not change this value linkConfig : This section contains the configuration of the Linked mode primary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if the primary Unisphere is unreachable url : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false standAloneConfig : This section contains the configuration of the StandAlone mode. Refer to the sample below for the detailed config Note: Only one of the Linked or StandAlone configurations needs to be supplied. The appropriate mode needs to be set in the spec as well.\nHere is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion: storage.dell.com/v1 kind: CSIPowerMaxRevProxy metadata: name: powermax-reverseproxy # \u003c- Name of the CSIPowerMaxRevProxy object namespace: test-powermax # \u003c- Set the namespace to where you will install the CSI PowerMax driver spec: # Image for CSI PowerMax ReverseProxy image: dellemc/csipowermax-reverseproxy:v2.3.0 # \u003c- CSI PowerMax Reverse Proxy image imagePullPolicy: Always # TLS secret which contains SSL certificate and private key for the Reverse Proxy server tlsSecret: csirevproxy-tls-secret config: mode: Linked linkConfig: primary: url: https://0.0.0.0:8443 #Unisphere URL skipCertificateValidation: true # This setting determines if client side Unisphere certificate validation is to be skipped certSecret: \"\" # Provide this value if skipCertificateValidation is set to false backup: # This is an optional field and lets you configure a backup unisphere which can be used by proxy server url: https://0.0.0.0:8443 #Unisphere URL skipCertificateValidation: true standAloneConfig: # Set mode to \"StandAlone\" in order to use this config storageArrays: - storageArrayId: \"000000000001\" # Unisphere server managing the PowerMax array primaryURL: https://unisphere-1-addr:8443 # proxyCredentialSecrets are used by the clients of the proxy to connect to it # If using proxy in the stand alone mode, then the driver must be provided the same secret. # The format of the proxy credential secret are exactly the same as the unisphere credential secret # For using the proxy with the driver, use the same proxy credential secrets for # all the managed storage arrays proxyCredentialSecrets: - proxy-creds - storageArrayId: \"000000000002\" primaryURL: https://unisphere-2-addr:8443 # An optional backup Unisphere server managing the same array # This can be used by the proxy to fall back to in case the primary # Unisphere is inaccessible temporarily backupURL: unisphere-3-addr:8443 proxyCredentialSecrets: - proxy-creds managementServers: - url: https://unisphere-1-addr:8443 # Secret containing the credentials of the Unisphere server arrayCredentialSecret: unsiphere-1-creds skipCertificateValidation: true - url: https://unisphere-2-addr:8443 arrayCredentialSecret: unsiphere-2-creds skipCertificateValidation: true - url: https://unisphere-3-addr:8443 arrayCredentialSecret: unsiphere-3-creds skipCertificateValidation: true Installation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service:\nkubectl create -f powermax_reverseproxy.yaml You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u003cnamespace\u003e kubectl get svc -n \u003cnamespace\u003e There is a new sample file - powermax_revproxy_standalone_with_driver.yaml in the samples folder which enables installation of CSI PowerMax ReverseProxy in StandAlone mode along with the CSI PowerMax driver. This mode enables the CSI PowerMax driver to connect to multiple Unisphere servers for managing multiple PowerMax arrays. Please follow the same steps described above to install ReverseProxy with this new sample file.\nDynamic Logging Configuration This feature is introduced in CSI Driver for powermax version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Sample CRD file for powermax You can find the sample CRD file here\nNote:\nKubelet config dir path is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. Volume Health Monitoring This feature is introduced in CSI Driver for PowerMax version 2.2.0.\nOperator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator.\nTo enable this feature, set X_CSI_HEALTH_MONITOR_ENABLED to true in the driver manifest under controller and node section. Also, install the external-health-monitor from sideCars section for controller plugin. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" Support for custom topology keys This feature is introduced in CSI Driver for PowerMax version 2.3.0.\nOperator based installation Support for custom topology keys is optional and by default this feature is disabled for drivers when installed via operator.\nX_CSI_TOPOLOGY_CONTROL_ENABLED provides a way to filter topology keys on a node based on array and transport protocol. If enabled, user can create custom topology keys by editing node-topology-config configmap.\nTo enable this feature, set X_CSI_TOPOLOGY_CONTROL_ENABLED to true in the driver manifest under node section. # X_CSI_TOPOLOGY_CONTROL_ENABLED provides a way to filter topology keys on a node based on array and transport protocol # if enabled, user can create custom topology keys by editing node-topology-config configmap. # Allowed values: # true: enable the filtration based on config map # false: disable the filtration based on config map # Default value: false - name: X_CSI_TOPOLOGY_CONTROL_ENABLED value: \"false\" Edit the sample config map “node-topology-config” present in sample CRD with appropriate values: Parameter Description allowedConnections List of node, array and protocol info for user allowed configuration allowedConnections.nodeName Name of the node on which user wants to apply given rules allowedConnections.rules List of StorageArrayID:TransportProtocol pair deniedConnections List of node, array and protocol info for user denied configuration deniedConnections.nodeName Name of the node on which user wants to apply given rules deniedConnections.rules List of StorageArrayID:TransportProtocol pair Note: Name of the configmap should always be node-topology-config.\nSupport for auto RDM for vSphere over FC This feature is introduced in CSI Driver for PowerMax version 2.5.0.\nOperator based installation Support for auto RDM for vSphere over FC feature is optional and by default this feature is disabled for drivers when installed via operator.\nTo enable this feature, set X_CSI_VSPHERE_ENABLED to true in the driver manifest under controller and node section.\n# VMware/vSphere virtualization support # set X_CSI_VSPHERE_ENABLED to true, if you to enable VMware virtualized environment support via RDM # Allowed values: # \"true\" - vSphere volumes are enabled # \"false\" - vSphere volumes are disabled # Default value: \"false\" - name: \"X_CSI_VSPHERE_ENABLED\" value: \"false\" # X_CSI_VSPHERE_PORTGROUP: An existing portGroup that driver will use for vSphere # recommended format: csi-x-VC-PG, x can be anything of user choice # Allowed value: valid existing port group on the array # Default value: \"\" \u003cempty\u003e - name: \"X_CSI_VSPHERE_PORTGROUP\" value: \"\" # X_CSI_VSPHERE_HOSTNAME: An existing host(initiator group)/ host group(cascaded intiator group) that driver will use for vSphere # this host/host group should contain initiators from all the ESXs/ESXi host where the cluster is deployed # recommended format: csi-x-VC-HN, x can be anything of user choice # Allowed value: valid existing host(initiator group)/ host group(cascaded intiator group) on the array # Default value: \"\" \u003cempty\u003e - name: \"X_CSI_VSPHERE_HOSTNAME\" value: \"\" Edit the section in the driver manifest having the sample for the following Secret with required values.\napiVersion: v1 kind: Secret metadata: name: vcenter-creds # Set driver namespace namespace: test-powermax type: Opaque data: # set username to the base64 encoded username username: YWRtaW4= # set password to the base64 encoded password password: YWRtaW4= These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with vCenter privileges.\n","categories":"","description":"Installing CSI Driver for PowerMax via Operator\n","excerpt":"Installing CSI Driver for PowerMax via Operator\n","ref":"/csm-docs/v2/csidriver/installation/operator/powermax/","tags":"","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use CSI Driver for Dell PowerMax. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with a different number of volumes in a given namespace using the storageclass provided. To test the installation of the CSI driver, perform these tests:\nVolume clone test Volume test Snapshot test Volume test Use this procedure to perform a volume test.\nCreate a namespace with the name test.\nRun the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\nRun the starttest.sh script and provide it with a test name. The following sample command can be used to run the 2vols test:\n./starttest.sh -t 2vols -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e This script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\nRun the /stoptest.sh -t 2vols -n \u003ctest_namespace\u003e script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\nNOTE: Helm tests have been designed assuming that users have created storageclass names like storageclass-name and storageclass-name-xfs. You can use kubectl get sc to check for the storageclass names.\nVolume clone test Use this procedure to perform a volume clone test.\nCreate a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: volumeclonetest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e This script does the following:\nInstalls a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum, and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test. Snapshot test Use this procedure to perform a snapshot test.\nCreate a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e This script does the following:\nInstalls a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot of that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test. NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass. You must update the snapshot class name in the file snap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\nVolume Expansion test Use this procedure to perform a volume expansion test.\nCreate a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e This script does the following:\nInstalls a helm chart that creates a Pod with a container, creates one PVC, and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC, and then recalculates the checksum Cleans up all the resources that were created as part of the test Note: This is not applicable for replicated volumes.\nSetting Application Prefix Application prefix is the name of the application that can be used to group the PowerMax volumes. We can use it while naming storage group. To set the application prefix for PowerMax, please refer to the sample storage class https://github.com/dell/csi-powermax/blob/main/samples/storageclass/powermax.yaml.\n# Name of application to be used to group volumes # This is used in naming storage group # Optional: true, Default value: None # Examples: APP, app, sanity, tests ApplicationPrefix: \u003capplication prefix\u003e Note: Supported length of storage group for PowerMax is 64 characters. Storage group name is of the format “csi-clusterprefix-application prefix-SLO name-SRP name-SG”. Based on the other inputs like clusterprefix,SLO name and SRP name maximum length of the ApplicationPrefix can vary.\nConsuming existing volumes with static provisioning Use this procedure to consume existing volumes with static provisioning.\nOpen your Unisphere for PowerMax, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, storage class is assumed as ‘powermax’, cluster prefix as ‘ABC’ and volume’s internal name as ‘00001’, array ID as ‘000000000001’, volume ID as ‘1abc23456’. The volume-handle should be in the format of csi-clusterPrefix-volumeNamePrefix-id-arrayID-volumeID. apiVersion: v1 kind: PersistentVolume metadata: name: pvol namespace: test spec: accessModes: - ReadWriteOnce capacity: storage: 8Gi csi: driver: csi-powermax.dellemc.com volumeHandle: csi-ABC-pmax-1abc23456-000000000001-00001 persistentVolumeReclaimPolicy: Retain storageClassName: powermax volumeMode: Filesystem Create PersistentVolumeClaim to use this PersistentVolume. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvc namespace: test spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: powermax volumeMode: Filesystem volumeName: pvol Then use this PVC as a volume in a pod. apiVersion: v1 kind: ServiceAccount metadata: name: powermaxtest namespace: test --- kind: StatefulSet apiVersion: apps/v1 metadata: name: powermaxtest namespace: test spec: selector: matchLabels: app: powermaxtest serviceName: staticprovisioning template: metadata: labels: app: powermaxtest spec: serviceAccount: powermaxtest containers: - name: test image: docker.io/centos:latest command: [ \"/bin/sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data\" name: pvc volumes: - name: pvc persistentVolumeClaim: claimName: pvc After the pod becomes Ready and Running, you can start to use this pod and volume. Note: CSI driver for PowerMax will create the necessary objects like Storage group, HostID and Masking View. They must not be created manually.\nSetting QoS parameters for throttling performance and bandwidth Use this procedure to set QoS parameters for throttling performance and bandwidth\nCreate storage class with the following parameters set. # Following params are for HostLimits, set them only if you want to set IOLimits # HostLimitName uniquely identifies given set of limits on a storage class # This is used in naming storage group, max of 3 letter # Optional: true # Example: \"HL1\", \"HL2\" #HostLimitName: \"HL1\" # The MBs per Second Host IO limit for the storage class # Optional: true, Default: \"\" # Examples: 100, 200, NOLIMIT #HostIOLimitMBSec: \"\" # The IOs per Second Host IO limit for the storage class # Optional: true, Default: \"\" # Examples: 100, 200, NOLIMIT #HostIOLimitIOSec: \"\" # distribution of the Host IO limits for the storage class # Optional: true, Default: \"\" # Allowed values: Never\",\"Always\" or \"OnFailure\" only #DynamicDistribution: \"\" Use the above storage class to create the PVC and provision the volume to the pod.\nOnce the pod becones Ready and Running, you will see the QoS parameters applied for throttling performance and bandwidth.\n","categories":"","description":"Tests to validate PowerMax CSI Driver installation","excerpt":"Tests to validate PowerMax CSI Driver installation","ref":"/csm-docs/v2/csidriver/installation/test/powermax/","tags":"","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v2.7.0 Linked Proxy mode for CSI reverse proxy is no longer actively maintained or supported. It will be deprecated in CSM 1.9. It is highly recommended that you use stand alone mode going forward. Note: Starting from CSI v2.4.0, Only Unisphere 10.0 REST endpoints are supported. It is mandatory that Unisphere should be updated to 10.0. Please find the instructions here.\nNew Features/Changes Added support for OpenShift 4.12 Added support for PowerMax v10.0.1 array Migrated image registry from k8s.gcr.io to registry.k8s.io Added support for Amazon EKS Anywhere Added support for Kubernetes 1.27 Added support for read only mount option for block volumes Added support for host groups for vSphere environment Added support to delete volumes on target array when it is set to Delete in storage class Added support for setting up QoS parameters for throttling performance and bandwidth at Storage Group level Added support for CSM Operator for PowerMax Driver Added support to create reverseproxy certs automatically Fixed Issues There are no fixed issues in this release.\nKnown Issues Issue Workaround Unable to update Host: A problem occurred modifying the host resource This issue occurs when the nodes do not have unique hostnames or when an IP address/FQDN with same sub-domains are used as hostnames. The workaround is to use unique hostnames or FQDN with unique sub-domains When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for PowerMax CSI driver","excerpt":"Release notes for PowerMax CSI driver","ref":"/csm-docs/v2/csidriver/release/powermax/","tags":"","title":"PowerMax"},{"body":" Symptoms Prevention, Resolution or Workaround kubectl describe pod powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e indicates that the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or log in to the docker registry kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver cannot authenticate Check your secret’s username and password kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver failed to connect to the U4P because it could not verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.23.0 \u003c 1.27.0 which is incompatible with Kubernetes V1.23.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which are not supported. When a node goes down, the block volumes attached to the node cannot be attached to another node 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. When attempting a driver upgrade, you see: spec.fsGroupPolicy: Invalid value: \"xxx\": field is immutable You cannot upgrade between drivers with different fsGroupPolicies. See upgrade documentation for more details Ater the migration group is in “migrated” state but unable to move to “commit ready” state because the new paths are not being discovered on the cluster nodes. Run the following commands manually on the cluster nodes rescan-scsi-bus.sh -i rescan-scsi-bus.sh -a Failed to fetch details for array: 000000000000. [Unauthorized]\" Please make sure that correct encrypted username and password in secret files are used, also ensure whether the RBAC is enabled for the user Error looking up volume for idempotence check: Not Found or Get Volume step fails for: (000000000000) symID with error (Invalid Response from API) Make sure that Unisphere endpoint doesn’t end with front slash FailedPrecondition desc = no topology keys could be generate Make sure that FC or iSCSI connectivity to the arrays are proper CreateHost failed with error initiator is already part of different host. Update modifyHostName to true in values.yaml Or Remove the initiator from existing host kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs says connection refused and the reverseproxy logs says “Failed to setup server.(secrets \"secret-name\" not found)” Make sure the given secret exist on the cluster nodestage is failing with error Error invalid IQN Target iqn.EMC.0648.SE1F 1. Update initiator name to full default name , ex: iqn.1993-08.org.debian:01:e9afae962192 2.Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed and it should be full default name. Volume mount is failing on few OS(ex:VMware Virtual Platform) during node publish with error wrong fs type, bad option, bad superblock 1. Check the multipath configuration(if enabled) 2. Edit Vm Advanced settings-\u003ehardware and add the param disk.enableUUID=true and reboot the node ","categories":"","description":"Troubleshooting PowerMax Driver","excerpt":"Troubleshooting PowerMax Driver","ref":"/csm-docs/v2/csidriver/troubleshooting/powermax/","tags":"","title":"PowerMax"},{"body":"The CSI Driver for Dell PowerMax can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nPrerequisites The CSI Driver for Dell PowerMax can create PVC with different storage protocols access :\ndirect Fiber Channel direct iSCSI Fiber Channel via VMware Raw Device Mapping In most cases, you will use one protocol only; therefore you should comply with the according prerequisites and not the others. Listing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using this command:\nkubectl get csm --all-namespaces Fibre Channel Requirements CSI Driver for Dell PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install CSI Driver:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs. iSCSI Requirements The CSI Driver for Dell PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\nAll Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell PowerMax array. All the port group names supplied to the driver must exist on each Dell PowerMax with the same name. For more information about configuring iSCSI, see Dell Host Connectivity guide.\niscsi-daemon MachineConfig To configure iSCSI in Red Hat OpenShift clusters, you can create a MachineConfig object using the console or oc to ensure that the iSCSI daemon starts on all the Red Hat CoreOS nodes. Here is an example of a MachineConfig object:\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: 99-iscsid labels: machineconfiguration.openshift.io/role: worker spec: config: ignition: version: 3.2.0 systemd: units: - name: \"iscsid.service\" enabled: true Once the MachineConfig object has been deployed, CoreOS will ensure that iscsid.service starts automatically.\nAlternatively, you can check the status of the iSCSI service by entering the following command on each worker node in the cluster:\nsudo systemctl status iscsid The service should be up and running (i.e. should be active state).\nIf the iscsid.service is not running, then perform the following steps on each worker node in the cluster\nLogin to worker nodes and check if the file /etc/iscsi/initiatorname.iscsi has been created properly If the file doesn’t exist or it doesn’t contain a valid ISCSI IQN, then make sure it exists with valid entries Ensure that iscsid service is running - Enable sudo systemctl enable iscsid \u0026 restart sudo systemctl restart iscsid iscsid if necessary. Note: If your worker nodes are running Red Hat CoreOS, make sure that automatic ISCSI login at boot is configured. Please contact RedHat for more details. Linux multipathing requirements CSI Driver for Dell PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\nAll the nodes must have the Device Mapper Multipathing package installed. NOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file. As a best practice, use these options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 multipathd MachineConfig If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\nuser_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0 Use the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: workers-multipath-conf-default labels: machineconfiguration.openshift.io/role: worker spec: config: ignition: version: 3.2.0 storage: files: - contents: source: data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0K verification: {} filesystem: root mode: 400 path: /etc/multipath.conf After deploying thisMachineConfig object, CoreOS will start multipath service automatically. Alternatively, you can check the status of the multipath service by entering the following command in each worker nodes. sudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally, you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer to official documentation of the multipath configuration.\nPowerPath for Linux requirements CSI Driver for Dell PowerMax supports PowerPath for Linux. Configure Linux PowerPath before installing the CSI Driver.\nFollow this procedure to set up PowerPath for Linux:\nAll the nodes must have the PowerPath package installed . Download the PowerPath archive for the environment from Dell Online Support. Untar the PowerPath archive, Copy the RPM package into a temporary folder and Install PowerPath using rpm -ivh DellEMCPower.LINUX-\u003cversion\u003e-\u003cbuild\u003e.\u003cplatform\u003e.x86_64.rpm Start the PowerPath service using systemctl start PowerPath Auto RDM for vSphere over FC requirements The CSI Driver for Dell PowerMax supports auto RDM for vSphere over FC. These requirements are applicable for the clusters deployed on ESX/ESXi using virtualized environement.\nSet up the environment as follows:\nRequires VMware vCenter management software to manage all ESX/ESXis where the cluster is hosted.\nAdd all FC array ports zoned to the ESX/ESXis to a port group where the cluster is hosted .\nAdd initiators from all ESX/ESXis to a host(initiator group)/host group(cascaded initiator group) where the cluster is hosted.\nCreate a secret which contains vCenter privileges. Follow the steps here to create the same.\nInstallation (Optional) Create secret for client-side TLS verification Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. See the detailed documentation on how to create this secret here.\nInstall Driver Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\nCreate PowerMax credentials: Create a file called powermax-creds.yaml with the following content:\napiVersion: v1 kind: Secret metadata: name: powermax-creds # Replace driver-namespace with the namespace where driver is being deployed namespace: \u003cdriver-namespace\u003e type: Opaque data: # set username to the base64 encoded username username: \u003cbase64 username\u003e # set password to the base64 encoded password password: \u003cbase64 password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards) # chapsecret: \u003cbase64 CHAP secret\u003e Replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run the kubectl create -f powermax-creds.yaml command to create the secret.\nCreate a configmap using sample here. Fill in the appropriate values for driver configuration. Example: config.yaml\nmode: StandAlone # Mode for the reverseproxy, should not be changed port: 2222 # Port on which reverseproxy will listen logLevel: debug logFormat: text standAloneConfig: storageArrays: - storageArrayId: \"000000000001\" # arrayID primaryURL: https://primary-1.unisphe.re:8443 # primary unisphere for arrayID backupURL: https://backup-1.unisphe.re:8443 # backup unisphere for arrayID proxyCredentialSecrets: - proxy-secret-11 # credential secret for primary unisphere, e.g., powermax-creds - proxy-secret-12 # credential secret for backup unisphere, e.g., powermax-creds - storageArrayId: \"000000000002\" primaryURL: https://primary-2.unisphe.re:8443 backupURL: https://backup-2.unisphe.re:8443 proxyCredentialSecrets: - proxy-secret-21 - proxy-secret-22 managementServers: - url: https://primary-1.unisphe.re:8443 # primary unisphere endpoint arrayCredentialSecret: primary-1-secret # primary credential secret e.g., powermax-creds skipCertificateValidation: true - url: https://backup-1.unisphe.re:8443 # backup unisphere endpoint arrayCredentialSecret: backup-1-secret # backup credential secret e.g., powermax-creds skipCertificateValidation: false # value false, to verify unisphere certificate and provide certSecret certSecret: primary-certs # unisphere verification certificate - url: https://primary-2.unisphe.re:8443 arrayCredentialSecret: primary-2-secret skipCertificateValidation: true - url: https://backup-2.unisphe.re:8443 arrayCredentialSecret: backup-2-secret skipCertificateValidation: false certSecret: primary-certs After editing the file, run this command to create a secret called powermax-reverseproxy-config. If you are using a different namespace/secret name, just substitute those into the command.\nkubectl create configmap powermax-reverseproxy-config --from-file config.yaml -n powermax Create a CR (Custom Resource) for PowerMax using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:\nParameter Description Required Default dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet replicas Controls the number of controller Pods you deploy. If controller Pods are greater than the number of available nodes, excess Pods will become stuck in pending. The default is 2 which allows for Controller high availability. Yes 2 fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” Common parameters for node and controller X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended to all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443 X_CSI_TRANSPORT_PROTOCOL Choose which transport protocol to use (ISCSI, FC, auto or None) Yes auto X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No - X_CSI_MANAGED_ARRAYS List of comma-separated array ID(s) which will be managed by the driver Yes - X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Yes csipowermax-reverseproxy X_CSI_IG_MODIFY_HOSTNAME Change any existing host names. When nodenametemplate is set, it changes the name to the specified format else it uses driver default host name format. No false X_CSI_IG_NODENAME_TEMPLATE Provide a template for the CSI driver to use while creating the Host/IG on the array for the nodes in the cluster. It is of the format a-b-c-%foo%-xyz where foo will be replaced by host name of each node in the cluster. No - X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature see the related documentation No - X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller and Node plugin. Provides details of volume status, usage and volume condition. As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar No false X_CSI_VSPHERE_ENABLED Enable VMware virtualized environment support via RDM No false X_CSI_VSPHERE_PORTGROUP Existing portGroup that driver will use for vSphere Yes \"\" X_CSI_VSPHERE_HOSTNAME Existing host(initiator group)/host group(cascaded initiator group) that driver will use for vSphere Yes \"\" X_CSI_VCenter_HOST URL/endpoint of the vCenter where all the ESX are present Yes \"\" Node parameters X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature see the related documentation No false X_CSI_TOPOLOGY_CONTROL_ENABLED Enable/Disabe topology control. It filters out arrays, associated transport protocol available to each node and creates topology keys based on any such user input. No false CSI Reverseproxy Module X_CSI_REVPROXY_TLS_SECRET Name of TLS secret defined in config map Yes “csirevproxy-tls-secret” X_CSI_REVPROXY_PORT Port number where reverseproxy will listen as defined in config map Yes “2222” X_CSI_CONFIG_MAP_NAME Name of config map as created for CSI PowerMax Yes “powermax-reverseproxy-config” Execute the following command to create the PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.\nThe mandatory module CSI PowerMax Reverseproxy will be installed automatically with the same command.\nOther features to enable Dynamic Logging Configuration This feature is introduced in CSI Driver for powermax version 2.0.0.\nAs part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Volume Health Monitoring This feature is introduced in CSI Driver for PowerMax version 2.2.0.\nVolume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via CSM operator.\nTo enable this feature, set X_CSI_HEALTH_MONITOR_ENABLED to true in the driver manifest under controller and node section. Also, install the external-health-monitor from sideCars section for controller plugin. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" Support for custom topology keys This feature is introduced in CSI Driver for PowerMax version 2.3.0.\nSupport for custom topology keys is optional and by default this feature is disabled for drivers when installed via CSM operator.\nX_CSI_TOPOLOGY_CONTROL_ENABLED provides a way to filter topology keys on a node based on array and transport protocol. If enabled, user can create custom topology keys by editing node-topology-config configmap.\nTo enable this feature, set X_CSI_TOPOLOGY_CONTROL_ENABLED to true in the driver manifest under node section.\n# X_CSI_TOPOLOGY_CONTROL_ENABLED provides a way to filter topology keys on a node based on array and transport protocol # if enabled, user can create custom topology keys by editing node-topology-config configmap. # Allowed values: # true: enable the filtration based on config map # false: disable the filtration based on config map # Default value: false - name: X_CSI_TOPOLOGY_CONTROL_ENABLED value: \"false\" Edit the sample config map “node-topology-config” as described here with appropriate values: Example:\nkind: ConfigMap metadata: name: node-topology-config namespace: powermax data: topologyConfig.yaml: | allowedConnections: - nodeName: \"node1\" rules: - \"000000000001:FC\" - \"000000000002:FC\" - nodeName: \"*\" rules: - \"000000000002:FC\" deniedConnections: - nodeName: \"node2\" rules: - \"000000000002:*\" - nodeName: \"node3\" rules: - \"*:*\" Parameter Description allowedConnections List of node, array and protocol info for user allowed configuration allowedConnections.nodeName Name of the node on which user wants to apply given rules allowedConnections.rules List of StorageArrayID:TransportProtocol pair deniedConnections List of node, array and protocol info for user denied configuration deniedConnections.nodeName Name of the node on which user wants to apply given rules deniedConnections.rules List of StorageArrayID:TransportProtocol pair Run following command to create the configmap\nkubectl create -f topologyConfig.yaml Note: Name of the configmap should always be node-topology-config.\nSupport for auto RDM for vSphere over FC This feature is introduced in CSI Driver for PowerMax version 2.5.0.\nSupport for auto RDM for vSphere over FC feature is optional and by default this feature is disabled for drivers when installed via CSM operator.\nTo enable this feature, set X_CSI_VSPHERE_ENABLED to true in the driver manifest under controller and node section.\n# VMware/vSphere virtualization support # set X_CSI_VSPHERE_ENABLED to true, if you to enable VMware virtualized environment support via RDM # Allowed values: # \"true\" - vSphere volumes are enabled # \"false\" - vSphere volumes are disabled # Default value: \"false\" - name: \"X_CSI_VSPHERE_ENABLED\" value: \"false\" # X_CSI_VSPHERE_PORTGROUP: An existing portGroup that driver will use for vSphere # recommended format: csi-x-VC-PG, x can be anything of user choice # Allowed value: valid existing port group on the array # Default value: \"\" \u003cempty\u003e - name: \"X_CSI_VSPHERE_PORTGROUP\" value: \"\" # X_CSI_VSPHERE_HOSTNAME: An existing host(initiator group)/ host group(cascaded intiator group) that driver will use for vSphere # this host/host group should contain initiators from all the ESXs/ESXi host where the cluster is deployed # recommended format: csi-x-VC-HN, x can be anything of user choice # Allowed value: valid existing host(initiator group)/ host group(cascaded intiator group) on the array # Default value: \"\" \u003cempty\u003e - name: \"X_CSI_VSPHERE_HOSTNAME\" value: \"\" Edit the Secret file vcenter-creds here with required values. Example:\napiVersion: v1 kind: Secret metadata: name: vcenter-creds # Set driver namespace namespace: powermax type: Opaque data: # set username to the base64 encoded username username: YWRtaW4= # set password to the base64 encoded password password: YWRtaW4= These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with vCenter privileges. 3. 4. Run following command to create the configmap\nkubectl create -f vcenter-secret.yaml Note: Name of the secret should always be vcenter-creds.\n","categories":"","description":"Installing Dell CSI Driver for PowerMax via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for PowerMax via Dell CSM Operator\n","ref":"/csm-docs/v2/deployment/csmoperator/drivers/powermax/","tags":"","title":"PowerMax"},{"body":"Configuring PowerMax CSI Driver with CSM for Authorization Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow these steps to configure the CSI Drivers to work with the Authorization sidecar:\nApply the secret containing the token data into the driver namespace. It’s assumed that the Kubernetes administrator has the token secret manifest saved in /tmp/token.yaml.\n# It is assumed that array type powermax has the namespace \"powermax\". kubectl apply -f /tmp/token.yaml -n powermax Edit these parameters in samples/secret/karavi-authorization-config.json file in CSI PowerMax driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\nParameter Description Required Default username Username for connecting to the backend storage array. This parameter is ignored. No - password Password for connecting to to the backend storage array. This parameter is ignored. No - intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes - endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400 systemID System ID of the backend storage array. Yes \" \" skipCertificateValidation A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml Create the karavi-authorization-config secret using this command:\nkubectl -n powermax create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f -\nNote:\nCreate the driver secret as you would normally except update/add the connection information for communicating with the sidecar instead of the backend storage array and scrub the username and password Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n powermax create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f -\nOtherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n powermax create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f -\nPlease refer to step 8 in the installation steps for PowerMax to edit the parameters in my-powermax-settings.yaml to communicate with the sidecar.\nUpdate endpoint to match the endpoint in samples/secret/karavi-authorization-config.json.\nEnable CSM for Authorization and provide the proxyHost address\nInstall the CSI PowerMax driver\n","categories":"","description":"Enabling CSM Authorization for PowerMax CSI Driver\n","excerpt":"Enabling CSM Authorization for PowerMax CSI Driver\n","ref":"/csm-docs/v3/authorization/configuration/powermax/","tags":"","title":"PowerMax"},{"body":"CSI Driver for Dell PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, see the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\nCSI Driver for Dell PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support- CSI PowerMax ReverseProxy, which maximizes CSI driver and Unisphere performance Kubernetes External Resizer, which resizes the volume (optional) Kubernetes External health monitor, which provides volume health status (optional) Dell CSI Replicator, which provides Replication capability. (optional) Dell CSI Migrator, which provides migrating capability within and across arrays (optional) Node rescanner, which rescans the node for new data paths after migration The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\nCSI Driver for Dell PowerMax Kubernetes Node Registrar, which handles the driver registration Prerequisites The following requirements must be met before installing CSI Driver for Dell PowerMax:\nInstall Kubernetes or OpenShift (see supported versions) Install Helm 3 Fibre Channel requirements iSCSI requirements Auto RDM for vSphere over FC requirements Certificate validation for Unisphere REST API calls Mount propagation is enabled on container runtime that is being used Linux multipathing requirements If using Snapshot feature, satisfy all Volume Snapshot requirements If enabling CSM for Authorization, please refer to the Authorization deployment steps first If using Powerpath , install the PowerPath for Linux requirements Prerequisite for CSI Reverse Proxy CSI PowerMax Reverse Proxy is an HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be an X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nHere is an example showing how to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Install Helm 3 Install Helm 3 on the master node before you install CSI Driver for Dell PowerMax.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.\nFibre Channel Requirements CSI Driver for Dell PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install CSI Driver:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs. iSCSI Requirements The CSI Driver for Dell PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\nAll Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell PowerMax array. All the port group names supplied to the driver must exist on each Dell PowerMax with the same name. For more information about configuring iSCSI, see Dell Host Connectivity guide.\nAuto RDM for vSphere over FC requirements The CSI Driver for Dell PowerMax supports auto RDM for vSphere over FC. These requirements are applicable for the clusters deployed on ESX/ESXi using virtualized environement.\nSet up the environment as follows:\nRequires VMware vCenter management software to manage all ESX/ESXis where the cluster is hosted.\nAdd all FC array ports zoned to the ESX/ESXis to a port group where the cluster is hosted .\nAdd initiators from all ESX/ESXis to a host(initiator group) where the cluster is hosted.\nEdit samples/secret/vcenter-secret.yaml file, to point to the correct namespace, and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with vCenter privileges.\nNote: Initiators from all ESX/ESXi should be part of a single host(initiator group) and not hostgroup(cascaded intitiator group).\nCreate the secret by running the below command, kubectl create -f samples/secret/vcenter-secret.yaml.\nCertificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\nTo fetch the certificate, run openssl s_client -showcerts -connect [Unisphere IP]:8443 \u003c/dev/null 2\u003e /dev/null | openssl x509 -outform PEM \u003e ca_cert.pem\nNOTE: The IP address varies for each user.\nTo create the secret, run kubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax\nPorts in the port group There are no restrictions to how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell PowerMax to ensure that you have multiple paths to your data volumes.\nLinux multipathing requirements CSI Driver for Dell PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\nAll the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file. As a best practice, use the following options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 PowerPath for Linux requirements CSI Driver for Dell PowerMax supports PowerPath for Linux. Configure Linux PowerPath before installing the CSI Driver.\nSet up the PowerPath for Linux as follows:\nAll the nodes must have the PowerPath package installed . Download the PowerPath archive for the environment from Dell Online Support. Untar the PowerPath archive, Copy the RPM package into a temporary folder and Install PowerPath using rpm -ivh DellEMCPower.LINUX-\u003cversion\u003e-\u003cbuild\u003e.\u003cplatform\u003e.x86_64.rpm Start the PowerPath service using systemctl start PowerPath Note: Do not install Dell PowerPath if multi-path software is already installed, as they cannot co-exist with native multi-path software.\n(Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\n(Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication: enabled: true Replication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in the csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\nRun git clone -b v2.6.0 https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the samples/secret/secret.yaml file,to point to the correct namespace, and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with PowerMax privileges. Create the secret by running kubectl create -f samples/secret/secret.yaml. Create a TLS secret with the name - csireverseproxy-tls-secret which holds an SSL certificate and the corresponding private key in the namespace where you are installing the driver. Copy the default values.yaml file cd helm \u0026\u0026 cp csi-powermax/values.yaml my-powermax-settings.yaml Ensure the unisphere have 10.0 REST endpoint support by clicking on Unisphere -\u003e Help (?) -\u003e About in Unisphere for PowerMax GUI. Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml Parameter Description Required Default global This section refers to configuration options for both CSI PowerMax Driver and Reverse Proxy - - defaultCredentialsSecret This secret name refers to:\n1. The Unisphere credentials if the driver is installed without proxy or with proxy in Linked mode.\n2. The proxy credentials if the driver is installed with proxy in StandAlone mode.\n3. The default Unisphere credentials if credentialsSecret is not specified for a management server. Yes powermax-creds storageArrays This section refers to the list of arrays managed by the driver and Reverse Proxy in StandAlone mode. - - storageArrayId This refers to PowerMax Symmetrix ID. Yes 000000000001 endpoint This refers to the URL of the Unisphere server managing storageArrayId. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes if Reverse Proxy mode is StandAlone https://primary-1.unisphe.re:8443 backupEndpoint This refers to the URL of the backup Unisphere server managing storageArrayId, if Reverse Proxy is installed in StandAlone mode. If authorization is enabled, backupEndpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes https://backup-1.unisphe.re:8443 managementServers This section refers to the list of configurations for Unisphere servers managing powermax arrays. - - endpoint This refers to the URL of the Unisphere server. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes https://primary-1.unisphe.re:8443 credentialsSecret This refers to the user credentials for endpoint Yes primary-1-secret skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. No “True” certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty limits This refers to various limits for Reverse Proxy No - maxActiveRead This refers to the maximum concurrent READ request handled by the reverse proxy. No 5 maxActiveWrite This refers to the maximum concurrent WRITE request handled by the reverse proxy. No 4 maxOutStandingRead This refers to maximum queued READ request when reverse proxy receives more than maxActiveRead requests. No 50 maxOutStandingWrite This refers to maximum queued WRITE request when reverse proxy receives more than maxActiveWrite requests. No 50 kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes “ABC” logLevel CSI driver log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes “debug” logFormat CSI driver log format. Allowed values: “TEXT” or “JSON”. Yes “TEXT” kubeletConfigDir kubelet config directory path. Ensure that the config.yaml file is present at this path. Yes /var/lib/kubelet defaultFsType Used to set the default FS type for external provisioner Yes ext4 portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only “PortGroup1, PortGroup2, PortGroup3” skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No “True” transportProtocol Set the preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty nodeNameTemplate Used to specify a template that will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, leave this value empty. No Empty modifyHostName Change any existing host names. When nodenametemplate is set, it changes the name to the specified format else it uses driver default host name format. No false powerMaxDebug Enables low level and http traffic logging between the CSI driver and Unisphere. Don’t enable this unless asked to do so by the support team. No false enableCHAP Determine if the driver is going to configure SCSI node databases on the nodes with the CHAP credentials. If enabled, the CHAP secret must be provided in the credentials secret and set to the key “chapsecret” No false fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” version Current version of the driver. Don’t modify this value as this value will be used by the install script. Yes v2.3.0 images Defines the container images used by the driver. - - driverRepository Defines the registry of the container image used for the driver. Yes dellemc controller Allows configuration of the controller-specific parameters. - - controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2 volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s” snapshot.enabled Enable/Disable volume snapshot feature Yes true snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot” resizer.enabled Enable/Disable volume expansion feature Yes true healthMonitor.enabled Allows to enable/disable volume health monitor No false healthMonitor.interval Interval of monitoring volume health condition No 60s nodeSelector Define node selection constraints for pods of controller deployment No tolerations Define tolerations for the controller deployment, if required No node Allows configuration of the node-specific parameters. - - tolerations Add tolerations as per requirement No - nodeSelector Add node selectors as per requirement No - healthMonitor.enabled Allows to enable/disable volume health monitor No false topologyControl.enabled Allows to enable/disable topology control to filter topology keys No false csireverseproxy This section refers to the configuration options for CSI PowerMax Reverse Proxy - - image This refers to the image of the CSI Powermax Reverse Proxy container. Yes dellemc/csipowermax-reverseproxy:v2.4.0 tlsSecret This refers to the TLS secret of the Reverse Proxy Server. Yes csirevproxy-tls-secret deployAsSidecar If set to true, the Reverse Proxy is installed as a sidecar to the driver’s controller pod otherwise it is installed as a separate deployment. Yes “True” port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation Yes 2222 mode This refers to the installation mode of Reverse Proxy. It can be set to:\n1. Linked: In this mode, the Reverse Proxy communicates with a primary or a backup Unisphere managing the same set of arrays.\n2. StandAlone: In this mode, the Reverse Proxy communicates with multiple arrays managed by different Unispheres. Yes “StandAlone” authorization Authorization is an optional feature to apply credential shielding of the backend PowerMax. - - enabled A boolean that enables/disables authorization feature. No false sidecarProxyImage Image for csm-authorization-sidecar. No \" \" proxyHost Hostname of the csm-authorization server. No Empty skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. No true migration Migration is an optional feature to enable migration between storage classes - - enabled A boolean that enables/disables migration feature. No false image Image for dell-csi-migrator sidecar. No \" \" nodeRescanSidecarImage Image for node rescan sidecar which rescans nodes for identifying new paths. No \" \" migrationPrefix enables migration sidecar to read required information from the storage class fields No migration.storage.dell.com replication Replication is an optional feature to enable replication \u0026 disaster recovery capabilities of PowerMax to Kubernetes clusters. - - enabled A boolean that enables/disables replication feature. No false image Image for dell-csi-replicator sidecar. No \" \" replicationContextPrefix enables side cars to read required information from the volume context No powermax replicationPrefix Determine if replication is enabled No replication.storage.dell.com vSphere This section refers to the configuration options for VMware virtualized environment support via RDM - - enabled A boolean that enables/disables VMware virtualized environment support. No false fcPortGroup Existing portGroup that driver will use for vSphere. Yes \"\" fcHostGroup Existing host(initiator group) that driver will use for vSphere. Yes \"\" vCenterHost URL/endpoint of the vCenter where all the ESX are present Yes \"\" vCenterCredSecret Secret name for the vCenter credentials. Yes \"\" Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml Or you can also install the driver using standalone helm chart using the command helm install --values my-powermax-settings.yaml --namespace powermax powermax ./csi-powermax Note:\nFor detailed instructions on how to run the install scripts, see the readme document in the dell-csi-helm-installer folder. There are a set of samples provided here to help you configure the driver with reverse proxy This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option In order to enable authorization, there should be an authorization proxy server already installed. PowerMax Array username must have role as StorageAdmin to be able to perform CRUD operations. If the user is using complex K8s version like “v1.22.3-mirantis-1”, use below kubeVersion check in helm Chart file. kubeVersion: “\u003e= 1.22.0-0 \u003c 1.25.0-0”. User should provide all boolean values with double-quotes. This applies only for values.yaml. Example: “true”/“false”. controllerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails. Endpoint should not have any special character at the end apart from port number. Storage Classes A wide set of annotated storage class manifests has been provided in the samples/storageclass folder. Please use these samples to create new storage classes to provision storage.\nVolume Snapshot Class Starting with CSI PowerMax v1.7.0, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nSample values file The following sections have useful snippets from values.yaml file which provides more information on how to configure the CSI PowerMax driver along with CSI PowerMax ReverseProxy in various modes\nCSI PowerMax driver with Proxy in Linked mode In this mode, the CSI PowerMax ReverseProxy acts as a passthrough for the RESTAPI calls and only provides limited functionality such as rate limiting, backup Unisphere server. The CSI PowerMax driver is still responsible for the authentication with the Unisphere server.\nThe first endpoint in the list of management servers is the primary Unisphere server and if you provide a second endpoint, then it will be considered as the backup Unisphere’s endpoint.\nglobal: defaultCredentialsSecret: powermax-creds storageArrays: - storageArrayId: \"000000000001\" - storageArrayId: \"000000000002\" managementServers: - endpoint: https://primary-unisphere:8443 skipCertificateValidation: false certSecret: primary-cert limits: maxActiveRead: 5 maxActiveWrite: 4 maxOutStandingRead: 50 maxOutStandingWrite: 50 - endpoint: https://backup-unisphere:8443 # \"csireverseproxy\" refers to the subchart csireverseproxy csireverseproxy: # Set enabled to true if you want to use proxy image: dellemc/csipowermax-reverseproxy:v2.4.0 tlsSecret: csirevproxy-tls-secret deployAsSidecar: true port: 2222 mode: Linked Note: Since the driver is still responsible for authentication when used with Proxy in Linked mode, the credentials for both primary and backup Unisphere need to be the same.\nCSI PowerMax driver with Proxy in StandAlone mode This is the most advanced configuration which provides you with the capability to connect to Multiple Unisphere servers. You can specify primary and backup Unisphere servers for each storage array. If you have different credentials for your Unisphere servers, you can also specify different credential secrets.\nglobal: defaultCredentialsSecret: powermax-creds storageArrays: - storageArrayId: \"000000000001\" endpoint: https://primary-1.unisphe.re:8443 backupEndpoint: https://backup-1.unisphe.re:8443 - storageArrayId: \"000000000002\" endpoint: https://primary-2.unisphe.re:8443 backupEndpoint: https://backup-2.unisphe.re:8443 managementServers: - endpoint: https://primary-1.unisphe.re:8443 credentialsSecret: primary-1-secret skipCertificateValidation: false certSecret: primary-cert limits: maxActiveRead: 5 maxActiveWrite: 4 maxOutStandingRead: 50 maxOutStandingWrite: 50 - endpoint: https://backup-1.unisphe.re:8443 credentialsSecret: backup-1-secret skipCertificateValidation: true - endpoint: https://primary-2.unisphe.re:8443 credentialsSecret: primary-2-secret skipCertificateValidation: true - endpoint: https://backup-2.unisphe.re:8443 credentialsSecret: backup-2-secret skipCertificateValidation: true # \"csireverseproxy\" refers to the subchart csireverseproxy csireverseproxy: image: dellemc/csipowermax-reverseproxy:v2.4.0 tlsSecret: csirevproxy-tls-secret deployAsSidecar: true port: 2222 mode: StandAlone Note: If the credential secret is missing from any management server details, the installer will try to use the defaultCredentialsSecret\n","categories":"","description":"Installing CSI Driver for PowerMax via Helm\n","excerpt":"Installing CSI Driver for PowerMax via Helm\n","ref":"/csm-docs/v3/csidriver/installation/helm/powermax/","tags":"","title":"PowerMax"},{"body":"Installing CSI Driver for PowerMax via Operator CSI Driver for Dell PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Fibre Channel Requirements CSI Driver for Dell PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install CSI Driver:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs. iSCSI Requirements The CSI Driver for Dell PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\nAll Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell PowerMax array. All the port group names supplied to the driver must exist on each Dell PowerMax with the same name. For more information about configuring iSCSI, see Dell Host Connectivity guide.\nAuto RDM for vSphere over FC requirements The CSI Driver for Dell PowerMax supports auto RDM for vSphere over FC. These requirements are applicable for the clusters deployed on ESX/ESXi using virtualized environement.\nSet up the environment as follows:\nRequires VMware vCenter management software to manage all ESX/ESXis where the cluster is hosted.\nAdd all FC array ports zoned to the ESX/ESXis to a port group where the cluster is hosted .\nAdd initiators from all ESX/ESXis to a host(initiator group) where the cluster is hosted.\nCreate a secret which contains vCenter privileges. Follow the steps here to create the same.\nLinux multipathing requirements CSI Driver for Dell PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\nAll the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file. As a best practice, use these options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 PowerPath for Linux requirements CSI Driver for Dell PowerMax supports PowerPath for Linux. Configure Linux PowerPath before installing the CSI Driver.\nFollow this procedure to set up PowerPath for Linux:\nAll the nodes must have the PowerPath package installed . Download the PowerPath archive for the environment from Dell Online Support. Untar the PowerPath archive, Copy the RPM package into a temporary folder and Install PowerPath using rpm -ivh DellEMCPower.LINUX-\u003cversion\u003e-\u003cbuild\u003e.\u003cplatform\u003e.x86_64.rpm Start the PowerPath service using systemctl start PowerPath Note: Do not install Dell PowerPath if multi-path software is already installed, as they cannot co-exist with native multi-path software.\nCreate secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. See the detailed documentation on how to create this secret here.\nInstall Driver Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\nCreate PowerMax credentials: Create a file called powermax-creds.yaml with the following content:\napiVersion: v1 kind: Secret metadata: name: powermax-creds # Replace driver-namespace with the namespace where driver is being deployed namespace: \u003cdriver-namespace\u003e type: Opaque data: # set username to the base64 encoded username username: \u003cbase64 username\u003e # set password to the base64 encoded password password: \u003cbase64 password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards) # chapsecret: \u003cbase64 CHAP secret\u003e Replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run the kubectl create -f powermax-creds.yaml command to create the secret.\nCreate a Custom Resource (CR) for PowerMax using the sample files provided here.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:\nParameter Description Required Default replicas Controls the number of controller Pods you deploy. If controller Pods are greater than the number of available nodes, excess Pods will become stuck in pending. The default is 2 which allows for Controller high availability. Yes 2 fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” Common parameters for node and controller X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended to all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443 X_CSI_TRANSPORT_PROTOCOL Choose which transport protocol to use (ISCSI, FC, auto or None) Yes auto X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No - X_CSI_MANAGED_ARRAYS List of comma-separated array ID(s) which will be managed by the driver Yes - X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Yes powermax-reverseproxy X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4 X_CSI_IG_MODIFY_HOSTNAME Change any existing host names. When nodenametemplate is set, it changes the name to the specified format else it uses driver default host name format. No false X_CSI_IG_NODENAME_TEMPLATE Provide a template for the CSI driver to use while creating the Host/IG on the array for the nodes in the cluster. It is of the format a-b-c-%foo%-xyz where foo will be replaced by host name of each node in the cluster. No - X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature see the related documentation No - X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller and Node plugin. Provides details of volume status, usage and volume condition. As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar No false X_CSI_VSPHERE_ENABLED Enable VMware virtualized environment support via RDM No false X_CSI_VSPHERE_PORTGROUP Existing portGroup that driver will use for vSphere Yes \"\" X_CSI_VSPHERE_HOSTGROUP Existing host(initiator group) that driver will use for vSphere Yes \"\" X_CSI_VCenter_HOST URL/endpoint of the vCenter where all the ESX are present Yes \"\" Node parameters X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature see the related documentation No false X_CSI_TOPOLOGY_CONTROL_ENABLED Enable/Disabe topology control. It filters out arrays, associated transport protocol available to each node and creates topology keys based on any such user input. No false Execute the following command to create the PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.\nNote - If CSI driver is getting installed using OCP UI , create these two configmaps manually using the command oc create -f \u003cconfigfilename\u003e\nConfigmap name powermax-config-params apiVersion: v1 kind: ConfigMap metadata: name: powermax-config-params namespace: test-powermax data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"debug\" CSI_LOG_FORMAT: \"JSON\" Configmap name node-topology-config kind: ConfigMap metadata: name: node-topology-config namespace: test-powermax data: topologyConfig.yaml: | allowedConnections: - nodeName: \"node1\" rules: - \"000000000001:FC\" - \"000000000002:FC\" - nodeName: \"*\" rules: - \"000000000002:FC\" deniedConnections: - nodeName: \"node2\" rules: - \"000000000002:*\" - nodeName: \"node3\" rules: - \"*:*\" CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is component that will be installed along with the CSI PowerMax driver. For more details on this feature see the related documentation.\nDeployment and ClusterIP service will be created by dell-csi-operator.\nPre-requisites Create a TLS secret that holds an SSL certificate and a private key which is required by the reverse proxy server. Use a tool such as openssl to generate this secret using the example below:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt --key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs config : This section contains the details of the Reverse Proxy configuration mode : This value is set to Linked by default. Do not change this value linkConfig : This section contains the configuration of the Linked mode primary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if the primary Unisphere is unreachable url : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false standAloneConfig : This section contains the configuration of the StandAlone mode. Refer to the sample below for the detailed config Note: Only one of the Linked or StandAlone configurations needs to be supplied. The appropriate mode needs to be set in the spec as well.\nHere is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion: storage.dell.com/v1 kind: CSIPowerMaxRevProxy metadata: name: powermax-reverseproxy # \u003c- Name of the CSIPowerMaxRevProxy object namespace: test-powermax # \u003c- Set the namespace to where you will install the CSI PowerMax driver spec: # Image for CSI PowerMax ReverseProxy image: dellemc/csipowermax-reverseproxy:v2.3.0 # \u003c- CSI PowerMax Reverse Proxy image imagePullPolicy: Always # TLS secret which contains SSL certificate and private key for the Reverse Proxy server tlsSecret: csirevproxy-tls-secret config: mode: Linked linkConfig: primary: url: https://0.0.0.0:8443 #Unisphere URL skipCertificateValidation: true # This setting determines if client side Unisphere certificate validation is to be skipped certSecret: \"\" # Provide this value if skipCertificateValidation is set to false backup: # This is an optional field and lets you configure a backup unisphere which can be used by proxy server url: https://0.0.0.0:8443 #Unisphere URL skipCertificateValidation: true standAloneConfig: # Set mode to \"StandAlone\" in order to use this config storageArrays: - storageArrayId: \"000000000001\" # Unisphere server managing the PowerMax array primaryURL: https://unisphere-1-addr:8443 # proxyCredentialSecrets are used by the clients of the proxy to connect to it # If using proxy in the stand alone mode, then the driver must be provided the # same secret. # The format of the proxy credential secret are exactly the same as the unisphere credential secret # For using the proxy with the driver, use the same proxy credential secrets for # all the managed storage arrays proxyCredentialSecrets: - proxy-creds - storageArrayId: \"000000000002\" primaryURL: https://unisphere-2-addr:8443 # An optional backup Unisphere server managing the same array # This can be used by the proxy to fall back to in case the primary # Unisphere is inaccessible temporarily backupURL: unisphere-3-addr:8443 proxyCredentialSecrets: - proxy-creds managementServers: - url: https://unisphere-1-addr:8443 # Secret containing the credentials of the Unisphere server arrayCredentialSecret: unsiphere-1-creds skipCertificateValidation: true - url: https://unisphere-2-addr:8443 arrayCredentialSecret: unsiphere-2-creds skipCertificateValidation: true - url: https://unisphere-3-addr:8443 arrayCredentialSecret: unsiphere-3-creds skipCertificateValidation: true Installation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service:\nkubectl create -f powermax_reverseproxy.yaml You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u003cnamespace\u003e kubectl get svc -n \u003cnamespace\u003e There is a new sample file - powermax_revproxy_standalone_with_driver.yaml in the samples folder which enables installation of CSI PowerMax ReverseProxy in StandAlone mode along with the CSI PowerMax driver. This mode enables the CSI PowerMax driver to connect to multiple Unisphere servers for managing multiple PowerMax arrays. Please follow the same steps described above to install ReverseProxy with this new sample file.\nDynamic Logging Configuration This feature is introduced in CSI Driver for powermax version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Sample CRD file for powermax You can find the sample CRD file here\nNote:\nKubelet config dir path is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. Volume Health Monitoring This feature is introduced in CSI Driver for PowerMax version 2.2.0.\nOperator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator.\nTo enable this feature, set X_CSI_HEALTH_MONITOR_ENABLED to true in the driver manifest under controller and node section. Also, install the external-health-monitor from sideCars section for controller plugin. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" Support for custom topology keys This feature is introduced in CSI Driver for PowerMax version 2.3.0.\nOperator based installation Support for custom topology keys is optional and by default this feature is disabled for drivers when installed via operator.\nX_CSI_TOPOLOGY_CONTROL_ENABLED provides a way to filter topology keys on a node based on array and transport protocol. If enabled, user can create custom topology keys by editing node-topology-config configmap.\nTo enable this feature, set X_CSI_TOPOLOGY_CONTROL_ENABLED to true in the driver manifest under node section. # X_CSI_TOPOLOGY_CONTROL_ENABLED provides a way to filter topology keys on a node based on array and transport protocol # if enabled, user can create custom topology keys by editing node-topology-config configmap. # Allowed values: # true: enable the filtration based on config map # false: disable the filtration based on config map # Default value: false - name: X_CSI_TOPOLOGY_CONTROL_ENABLED value: \"false\" Edit the sample config map “node-topology-config” present in sample CRD with appropriate values: Parameter Description allowedConnections List of node, array and protocol info for user allowed configuration allowedConnections.nodeName Name of the node on which user wants to apply given rules allowedConnections.rules List of StorageArrayID:TransportProtocol pair deniedConnections List of node, array and protocol info for user denied configuration deniedConnections.nodeName Name of the node on which user wants to apply given rules deniedConnections.rules List of StorageArrayID:TransportProtocol pair Note: Name of the configmap should always be node-topology-config.\nSupport for auto RDM for vSphere over FC This feature is introduced in CSI Driver for PowerMax version 2.5.0.\nOperator based installation Support for auto RDM for vSphere over FC feature is optional and by default this feature is disabled for drivers when installed via operator.\nTo enable this feature, set X_CSI_VSPHERE_ENABLED to true in the driver manifest under controller and node section.\n# VMware/vSphere virtualization support # set X_CSI_VSPHERE_ENABLED to true, if you to enable VMware virtualized environment support via RDM # Allowed values: # \"true\" - vSphere volumes are enabled # \"false\" - vSphere volumes are disabled # Default value: \"false\" - name: \"X_CSI_VSPHERE_ENABLED\" value: \"false\" # X_CSI_VSPHERE_PORTGROUP: An existing portGroup that driver will use for vSphere # recommended format: csi-x-VC-PG, x can be anything of user choice # Allowed value: valid existing port group on the array # Default value: \"\" \u003cempty\u003e - name: \"X_CSI_VSPHERE_PORTGROUP\" value: \"\" # X_CSI_VSPHERE_HOSTGROUP: An existing host(initiator group) that driver will use for vSphere # this hostGroup should contain initiators from all the ESXs/ESXi host where the cluster is deployed # recommended format: csi-x-VC-HG, x can be anything of user choice # Allowed value: valid existing host on the array # Default value: \"\" \u003cempty\u003e - name: \"X_CSI_VSPHERE_HOSTGROUP\" value: \"\" Edit the section in the driver manifest having the sample for the following Secret with required values.\napiVersion: v1 kind: Secret metadata: name: vcenter-creds # Set driver namespace namespace: test-powermax type: Opaque data: # set username to the base64 encoded username username: YWRtaW4= # set password to the base64 encoded password password: YWRtaW4= These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with vCenter privileges.\n","categories":"","description":"Installing CSI Driver for PowerMax via Operator\n","excerpt":"Installing CSI Driver for PowerMax via Operator\n","ref":"/csm-docs/v3/csidriver/installation/operator/powermax/","tags":"","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use CSI Driver for Dell PowerMax. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with a different number of volumes in a given namespace using the storageclass provided. To test the installation of the CSI driver, perform these tests:\nVolume clone test Volume test Snapshot test Volume test Use this procedure to perform a volume test.\nCreate a namespace with the name test.\nRun the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\nRun the starttest.sh script and provide it with a test name. The following sample command can be used to run the 2vols test: ./starttest.sh -t 2vols -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e\nThis script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\nRun the /stoptest.sh -t 2vols -n \u003ctest_namespace\u003e script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\nNOTE: Helm tests have been designed assuming that users have created storageclass names like storageclass-name and storageclass-name-xfs. You can use kubectl get sc to check for the storageclass names.\nVolume clone test Use this procedure to perform a volume clone test.\nCreate a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: bash volumeclonetest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e This script does the following:\nInstalls a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum, and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test. Snapshot test Use this procedure to perform a snapshot test.\nCreate a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e This script does the following:\nInstalls a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot of that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test. NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass. You must update the snapshot class name in the file snap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\nVolume Expansion test Use this procedure to perform a volume expansion test.\nCreate a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e This script does the following:\nInstalls a helm chart that creates a Pod with a container, creates one PVC, and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC, and then recalculates the checksum Cleans up all the resources that were created as part of the test Note: This is not applicable for replicated volumes.\nSetting Application Prefix Application prefix is the name of the application that can be used to group the PowerMax volumes. We can use it while naming storage group. To set the application prefix for PowerMax, please refer to the sample storage class https://github.com/dell/csi-powermax/blob/main/samples/storageclass/powermax.yaml.\n# Name of application to be used to group volumes # This is used in naming storage group # Optional: true, Default value: None # Examples: APP, app, sanity, tests ApplicationPrefix: \u003capplication prefix\u003e Note: Supported length of storage group for PowerMax is 64 characters. Storage group name is of the format “csi-clusterprefix-application prefix-SLO name-SRP name-SG”. Based on the other inputs like clusterprefix,SLO name and SRP name maximum length of the ApplicationPrefix can vary.\nConsuming existing volumes with static provisioning Use this procedure to consume existing volumes with static provisioning.\nOpen your Unisphere for Powermax, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, storage class is assumed as ‘powermax’, cluster prefix as ‘ABC’ and volume’s internal name as ‘00001’, array ID as ‘000000000001’, volume ID as ‘1abc23456’. The volume-handle should be in the format of csi-clusterPrefix-volumeNamePrefix-id-arrayID-volumeID. apiVersion: v1 kind: PersistentVolume metadata: name: pvol namespace: test spec: accessModes: - ReadWriteOnce capacity: storage: 8Gi csi: driver: csi-powermax.dellemc.com volumeHandle: csi-ABC-pmax-1abc23456-000000000001-00001 persistentVolumeReclaimPolicy: Retain storageClassName: powermax volumeMode: Filesystem Create PersistentVolumeClaim to use this PersistentVolume. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvc namespace: test spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: powermax volumeMode: Filesystem volumeName: pvol Then use this PVC as a volume in a pod. apiVersion: v1 kind: ServiceAccount metadata: name: powermaxtest namespace: test --- kind: StatefulSet apiVersion: apps/v1 metadata: name: powermaxtest namespace: test spec: selector: matchLabels: app: powermaxtest serviceName: staticprovisioning template: metadata: labels: app: powermaxtest spec: serviceAccount: powermaxtest containers: - name: test image: docker.io/centos:latest command: [ \"/bin/sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data\" name: pvc volumes: - name: pvc persistentVolumeClaim: claimName: pvc After the pod becomes Ready and Running, you can start to use this pod and volume. Note: CSI driver for PowerMax will create the necessary objects like Storage group, HostID and Masking View. They must not be created manually.\n","categories":"","description":"Tests to validate PowerMax CSI Driver installation","excerpt":"Tests to validate PowerMax CSI Driver installation","ref":"/csm-docs/v3/csidriver/installation/test/powermax/","tags":"","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v2.6.0 Note: Starting from CSI v2.4.0, Only Unisphere 10.0 REST endpoints are supported. It is mandatory that Unisphere should be updated to 10.0. Please find the instructions here.\nNew Features/Changes Added support for RKE 1.4.2. Added support to cleanup powerpath dead paths Added support for Kubernetes 1.26 Added support to clone the replicated volumes Added support to restore the snapshot of metro volumes Added support for MKE 3.6.1 Added support for user array migration between arrays Added support for Observability Added support for generating manifest file via CSM Installation wizard Fixed Issues There are no fixed issues in this release.\nKnown Issues Issue Workaround Unable to update Host: A problem occurred modifying the host resource This issue occurs when the nodes do not have unique hostnames or when an IP address/FQDN with same sub-domains are used as hostnames. The workaround is to use unique hostnames or FQDN with unique sub-domains When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for PowerMax CSI driver","excerpt":"Release notes for PowerMax CSI driver","ref":"/csm-docs/v3/csidriver/release/powermax/","tags":"","title":"PowerMax"},{"body":" Symptoms Prevention, Resolution or Workaround kubectl describe pod powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e indicates that the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or log in to the docker registry kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver cannot authenticate Check your secret’s username and password kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver failed to connect to the U4P because it could not verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.22.0 \u003c 1.25.0 which is incompatible with Kubernetes V1.22.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. When a node goes down, the block volumes attached to the node cannot be attached to another node 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. When attempting a driver upgrade, you see: spec.fsGroupPolicy: Invalid value: \"xxx\": field is immutable You cannot upgrade between drivers with different fsGroupPolicies. See upgrade documentation for more details Ater the migration group is in “migrated” state but unable to move to “commit ready” state because the new paths are not being discovered on the cluster nodes. Run the following commands manually on the cluster nodes rescan-scsi-bus.sh -i rescan-scsi-bus.sh -a Failed to fetch details for array: 000000000000. [Unauthorized]\" Please make sure that correct encrypted username and password in secret files are used, also ensure whether the RBAC is enabled for the user Error looking up volume for idempotence check: Not Found or Get Volume step fails for: (000000000000) symID with error (Invalid Response from API) Make sure that Unisphere endpoint doesn’t end with front slash FailedPrecondition desc = no topology keys could be generate Make sure that FC or iSCSI connectivity to the arrays are proper CreateHost failed with error initiator is already part of different host. Update modifyHostName to true in values.yaml Or Remove the initiator from existing host kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs says connection refused and the reverseproxy logs says “Failed to setup server.(secrets \"secret-name\" not found)” Make sure the given secret exist on the cluster nodestage is failing with error Error invalid IQN Target iqn.EMC.0648.SE1F 1. Update initiator name to full default name , ex: iqn.1993-08.org.debian:01:e9afae962192 2.Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed and it should be full default name. Volume mount is failing on few OS(ex:VMware Virtual Platform) during node publish with error wrong fs type, bad option, bad superblock 1. Check the multipath configuration(if enabled) 2. Edit Vm Advanced settings-\u003ehardware and add the param disk.enableUUID=true and reboot the node ","categories":"","description":"Troubleshooting PowerMax Driver","excerpt":"Troubleshooting PowerMax Driver","ref":"/csm-docs/v3/csidriver/troubleshooting/powermax/","tags":"","title":"PowerMax"},{"body":"Configuring PowerScale CSI Driver with CSM for Authorization Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow these steps to configure the CSI Drivers to work with the Authorization sidecar:\nApply the secret containing the token data into the driver namespace. It’s assumed that the Kubernetes administrator has the token secret manifest, generated by your storage administrator via Generate a Token, saved in /tmp/token.yaml.\n#It is assumed that array type powerscale has the namespace “isilon”.\nkubectl apply -f /tmp/token.yaml -n isilon Edit these parameters in samples/secret/karavi-authorization-config.json file in CSI PowerScale driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\nParameter Description Required Default username Username for connecting to the backend storage array. This parameter is ignored. No - password Password for connecting to to the backend storage array. This parameter is ignored. No - intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes - endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400 systemID Cluster name of the backend storage array. Yes \" \" skipCertificateValidation A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml Create the karavi-authorization-config secret using this command:\nkubectl -n isilon create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f - Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n isilon create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f - Otherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n isilon create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f - Prepare the driver configuration secret, applicable to your driver installation method, to communicate with the CSM Authorization sidecar.\nHelm\nRefer to the Install the Driver section to edit the parameters in samples/secret/secret.yaml file to configure the driver to communicate with the CSM Authorization sidecar.\nUpdate endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate mountEndpoint to the PowerScale OneFS API server. For example, 10.0.0.1.\nUpdate skipCertificateValidation to true.\nThe username and password can be any value since they will be ignored.\nExample:\nisilonClusters: - clusterName: \"cluster1\" username: \"ignored\" password: \"ignored\" isDefault: true endpoint: localhost endpointPort: 9400 mountEndpoint: 10.0.0.1 skipCertificateValidation: true Operator\nRefer to the Prerequisite section to prepare the secret.yaml file to configure the driver to communicate with the CSM Authorization sidecar.\nUpdate endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate mountEndpoint to the PowerScale OneFS API server. For example, 10.0.0.1.\nUpdate skipCertificateValidation to true.\nThe username and password can be any value since they will be ignored.\nExample:\nisilonClusters: - clusterName: \"cluster1\" username: \"ignored\" password: \"ignored\" isDefault: true endpoint: localhost endpointPort: 9400 mountEndpoint: 10.0.0.1 skipCertificateValidation: true Enable CSM Authorization in the driver installation applicable to your installation method.\nHelm\nRefer to the Install the Driver section to edit the parameters in my-isilon-settings.yaml file to enable CSM Authorization.\nUpdate authorization.enabled to true.\nUpdate images.authorization to the image of the CSM Authorization sidecar. In most cases, you can leave the default value.\nUpdate authorization.proxyHost to the hostname of the CSM Authorization Proxy Server.\nUpdate authorization.skipCertificateValidation to true or false depending on if you want to disable or enable certificate validation of the CSM Authorization Proxy Server.\nExample:\nauthorization: enabled: true # sidecarProxyImage: the container image used for the csm-authorization-sidecar. # Default value: dellemc/csm-authorization-sidecar:v1.9.0 sidecarProxyImage: dellemc/csm-authorization-sidecar:v1.9.0 # proxyHost: hostname of the csm-authorization server # Default value: None proxyHost: csm-authorization.com # skipCertificateValidation: certificate validation of the csm-authorization server # Allowed Values: # \"true\" - TLS certificate verification will be skipped # \"false\" - TLS certificate will be verified # Default value: \"true\" skipCertificateValidation: true Operator\nRefer to the Install Driver section to edit the parameters in the Custom Resource to enable CSM Authorization.\nUnder modules, enable the module named authorization:\nUpdate the enabled field to true.\nUpdate the image to the image of the CSM Authorization sidecar. In most cases, you can leave the default value.\nUpdate the PROXY_HOST environment value to the hostname of the CSM Authorization Proxy Server.\nUpdate the SKIP_CERTIFICATE_VALIDATION environment value to true or false depending on if you want to disable or enable certificate validation of the CSM Authorization Proxy Server.\nmodules: # Authorization: enable csm-authorization for RBAC - name: authorization # enable: Enable/Disable csm-authorization enabled: true configVersion: v1.9.0 components: - name: karavi-authorization-proxy image: dellemc/csm-authorization-sidecar:v1.9.0 envs: # proxyHost: hostname of the csm-authorization server - name: \"PROXY_HOST\" value: \"csm-authorization.com\" # skipCertificateValidation: Enable/Disable certificate validation of the csm-authorization server - name: \"SKIP_CERTIFICATE_VALIDATION\" value: \"true\" Install the Dell CSI PowerScale driver following the appropriate documenation for your installation method.\n(Optional) Install dellctl to perform Kubernetes administrator commands for additional capabilities (e.g., list volumes). Please refer to the dellctl documentation page for the installation steps and command list.\n","categories":"","description":"Enabling CSM Authorization for PowerScale CSI Driver\n","excerpt":"Enabling CSM Authorization for PowerScale CSI Driver\n","ref":"/csm-docs/docs/authorization/configuration/powerscale/","tags":"","title":"PowerScale"},{"body":"The CSI Driver for Dell PowerScale can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nPrerequisites The following are requirements to be met before installing the CSI Driver for Dell PowerScale:\nInstall Kubernetes or OpenShift (see supported versions) Install Helm 3 Mount propagation is enabled on container runtime that is being used nfs-utils package must be installed on nodes that will mount volumes If using Snapshot feature, satisfy all Volume Snapshot requirements If enabling CSM for Authorization, please refer to the Authorization deployment steps first If enabling CSM for Replication, please refer to the Replication deployment steps first If enabling CSM for Resiliency, please refer to the Resiliency deployment steps first If enabling Encryption, please refer to the Encryption deployment steps first Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerScale.\nSteps\nRun the command to install Helm 3.0.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash (Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\n(Optional) Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via helm.\nIf enabled capacity metrics (used \u0026 free capacity, used \u0026 free inodes) for PowerScale PV will be expose in Kubernetes metrics API.\nTo enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\ncontroller: healthMonitor: # enabled: Enable/Disable health monitor of CSI volumes # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: None enabled: false # interval: Interval of monitoring volume health condition # Allowed values: Number followed by unit (s,m,h) # Examples: 60s, 5m, 1h # Default value: 60s interval: 60s node: healthMonitor: # enabled: Enable/Disable health monitor of CSI volumes- volume usage, volume condition # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: None enabled: false NOTE: To enable this feature to existing driver OR enable this feature while upgrading the driver versions, follow either of the way.\nReinstall of Driver Upgrade the driver smoothly with “–upgrade” option (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication: enabled: true Replication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in the csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\nRun git clone -b v2.9.1 https://github.com/dell/csi-powerscale.git to clone the git repository.\nEnsure that you have created the namespace where you want to install the driver. You can run kubectl create namespace isilon to create a new one. The use of “isilon” as the namespace is just an example. You can choose any name for the namespace.\nCollect information from the PowerScale Systems like IP address, IsiPath, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml.\nDownload wget -O my-isilon-settings.yaml https://raw.githubusercontent.com/dell/helm-charts/csi-isilon-2.9.1/charts/csi-isilon/values.yaml into cd ../dell-csi-helm-installer to customize settings for installation.\nEdit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\nParameter Description Required Default images List all the images used by the CSI driver and CSM. If you use a private repository, change the registries accordingly. Yes \"\" logLevel CSI driver log level No “debug” certSecretCount Defines the number of certificate secrets, which the user is going to create for SSL authentication. (isilon-cert-0..isilon-cert-(n-1)); Minimum value should be 1. Yes 1 allowedNetworks Defines the list of networks that can be used for NFS I/O traffic, CIDR format must be used. No [ ] maxIsilonVolumesPerNode Defines the default value for a maximum number of volumes that the controller can publish to the node. If the value is zero CO SHALL decide how many volumes of this type can be published by the controller to the node. This limit is applicable to all the nodes in the cluster for which node label ‘max-isilon-volumes-per-node’ is not set. Yes 0 imagePullPolicy Defines the policy to determine if the image should be pulled prior to starting the container Yes IfNotPresent verbose Indicates what content of the OneFS REST API message should be logged in debug level logs Yes 1 kubeletConfigDir Specify kubelet config dir path Yes “/var/lib/kubelet” enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish a connection to Array. This requires enableCustomTopology to be enabled. No false fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity.enabled Enable/Disable storage capacity tracking No true storageCapacity.pollInterval Configure how often the driver checks for changed capacity No 5m podmonAPIPort Defines the port which csi-driver will use within the cluster to support podmon No 8083 maxPathLen Defines the maximum length of path for a volume No 192 controller Configure controller pod specific parameters controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2 volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s” snapshot.enabled Enable/Disable volume snapshot feature Yes true snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot” resizer.enabled Enable/Disable volume expansion feature Yes true healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume status, volume condition Yes false healthMonitor.interval Interval of monitoring volume health condition Yes 60s nodeSelector Define node selection constraints for pods of controller deployment No tolerations Define tolerations for the controller deployment, if required No leader-election-lease-duration Duration, that non-leader candidates will wait to force acquire leadership No 20s leader-election-renew-deadline Duration, that the acting leader will retry refreshing leadership before giving up No 15s leader-election-retry-period Duration, the LeaderElector clients should wait between tries of actions No 5s node Configure node pod specific parameters nodeSelector Define node selection constraints for pods of node daemonset No tolerations Define tolerations for the node daemonset, if required No dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition Yes false PLATFORM ATTRIBUTES endpointPort Define the HTTPs port number of the PowerScale OneFS API server. If authorization is enabled, endpointPort should be the HTTPS localhost port that the authorization sidecar will listen on. This value acts as a default value for endpointPort, if not specified for a cluster config in secret. No 8080 skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. This value acts as a default value for skipCertificateValidation, if not specified for a cluster config in secret. No true isiAuthType Indicates the authentication method to be used. If set to 1 then it follows as session-based authentication else basic authentication. If authorization.enabled=true, this value must be set to 1. No 0 isiAccessZone Define the name of the access zone a volume can be created in. If storageclass is missing with AccessZone parameter, then value of isiAccessZone is used for the same. No System enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. No true isiPath Define the base path for the volumes to be created on PowerScale cluster. This value acts as a default value for isiPath, if not specified for a cluster config in secret No /ifs/data/csi ignoreUnresolvableHosts Allows new host to add to existing export list though any of the existing hosts from the same exports are unresolvable/doesn’t exist anymore. No false noProbeOnStart Define whether the controller/node plugin should probe all the PowerScale clusters during driver initialization No false autoProbe Specify if automatically probe the PowerScale cluster if not done already during CSI calls No true authorization Authorization is an optional feature to apply credential shielding of the backend PowerScale. - - enabled A boolean that enables/disables authorization feature. If enabled, isiAuthType must be set to 1. No false proxyHost Hostname of the csm-authorization server. No Empty skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization proxy server. No true podmon Podmon is an optional feature to enable application pods to be resilient to node failure. - - enabled A boolean that enables/disables podmon feature. No false encryption Encryption is an optional feature to apply encryption to CSI volumes. - - enabled A boolean that enables/disables Encryption feature. No false NOTE:\nControllerCount parameter value must not exceed the number of nodes in the Kubernetes cluster. Otherwise, some of the controller pods remain in a “Pending” state till new nodes are available for scheduling. The installer exits with a WARNING on the same. Whenever the certSecretCount parameter changes in my-isilon-setting.yaml user needs to reinstall the driver. In order to enable authorization, there should be an authorization proxy server already installed. If you are using custom images, update each attributes under the images field in my-isilon-setting.yaml to make sure that they are pointing to the correct image repository and version. Edit following parameters in samples/secret/secret.yaml file and update/add connection/authentication information for one or more PowerScale clusters. If replication feature is enabled, ensure the secret includes all the PowerScale clusters involved in replication.\nParameter Description Required Default clusterName Logical name of PoweScale cluster against which volume CRUD operations are performed through this secret. Yes - username username for connecting to PowerScale OneFS API server Yes - password password for connecting to PowerScale OneFS API server Yes - endpoint HTTPS endpoint of the PowerScale OneFS API server. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes - isDefault Indicates if this is a default cluster (would be used by storage classes without ClusterName parameter). Only one of the cluster config should be marked as default. No false Optional parameters Following parameters are Optional. If specified will override default values from values.yaml. skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. No default value from values.yaml ignoreUnresolvableHosts Allows new host to add to existing export list though any of the existing hosts from the same exports are unresolvable/doesn’t exist anymore. No default value from values.yaml endpointPort Specify the HTTPs port number of the PowerScale OneFS API server No default value from values.yaml isiPath The base path for the volumes to be created on PowerScale cluster. Note: IsiPath parameter in storageclass, if present will override this attribute. No default value from values.yaml mountEndpoint Endpoint of the PowerScale OneFS API server, for example, 10.0.0.1. This must be specified if CSM-Authorization is enabled. No - User privileges The username specified in secret.yaml must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\nPrivilege Type ISI_PRIV_LOGIN_PAPI Read Only ISI_PRIV_NFS Read Write ISI_PRIV_QUOTA Read Write ISI_PRIV_SNAPSHOT Read Write ISI_PRIV_IFS_RESTORE Read Only ISI_PRIV_NS_IFS_ACCESS Read Only ISI_PRIV_IFS_BACKUP Read Only ISI_PRIV_SYNCIQ Read Write ISI_PRIV_STATISTICS Read Only Create isilon-creds secret using the following command: kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\nNOTE:\nIf any key/value is present in all my-isilon-settings.yaml, secret, and storageClass, then the values provided in storageClass parameters take precedence. The user has to validate the yaml syntax and array-related key/values while replacing or appending the isilon-creds secret. The driver will continue to use previous values in case of an error found in the yaml file. For the key isiIP/endpoint, the user can give either IP address or FQDN. Also, the user can prefix ‘https’ (For example, https://192.168.1.1) with the value. The isilon-creds secret has a mountEndpoint parameter which should only be updated and used when Authorization is enabled. Install OneFS CA certificates by following the instructions from the next section, if you want to validate OneFS API server’s certificates. If not, create an empty secret using the following command and an empty secret must be created for the successful installation of CSI Driver for Dell PowerScale.\nkubectl create -f empty-secret.yaml This command will create a new secret called isilon-certs-0 in isilon namespace.\nInstall the driver using csi-install.sh bash script and default yaml by running\ncd dell-csi-helm-installer \u0026\u0026 wget -O my-isilon-settings.yaml https://raw.githubusercontent.com/dell/helm-charts/csi-isilon-2.9.1/charts/csi-isilon/values.yaml \u0026\u0026 ./csi-install.sh --namespace isilon --values my-isilon-settings.yaml Certificate validation for OneFS REST API calls The CSI driver exposes an install parameter ‘skipCertificateValidation’ which determines if the driver performs client-side verification of the OneFS certificates. The ‘skipCertificateValidation’ parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the ‘skipCertificateValidation’ is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the ‘skipCertificateValidation’ parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem To create the certs secret, run kubectl create secret generic isilon-certs-0 --from-file=cert-0=ca_cert_0.pem -n isilon Use the following command to replace the secret kubectl create secret generic isilon-certs-0 -n isilon --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - NOTES:\nThe OneFS IP can be with or without a port, depends upon the configuration of OneFS API server. The commands are based on the namespace ‘isilon’ It is highly recommended that ca_cert.pem file(s) having the naming convention as ca_cert_number.pem (example: ca_cert_0, ca_cert_1), where this number starts from 0 and grows as the number of OneFS arrays grows. The cert secret created out of these pem files must have the naming convention as isilon-certs-number (example: isilon-certs-0, isilon-certs-1, and so on.); The number must start from zero and must grow in incremental order. The number of the secrets created out of pem files should match certSecretCount value in myvalues.yaml or my-isilon-settings.yaml. Dynamic update of array details via secret.yaml CSI Driver for Dell PowerScale now provides supports for Multi cluster. Now users can link the single CSI Driver to multiple OneFS Clusters by updating secret.yaml. Users can now update the isilon-creds secret by editing the secret.yaml and executing the following command\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Note: Updating isilon-certs-x secrets is a manual process, unlike isilon-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nStorage Classes The CSI driver for Dell PowerScale version 1.5 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A sample storage class manifest is available at samples/storageclass/isilon.yaml. Use this sample manifest to create a storageclass to provision storage; uncomment/ update the manifest as per the requirements.\nWhat happens to my existing storage classes? Upgrading from CSI PowerScale v2.3 driver: The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nNOTE:\nAt least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es): Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create secondary storage class:\nThere are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\nCopy the storageclass.yaml to second_storageclass.yaml (This is just an example, you can rename to file you require.) Edit the second_storageclass.yaml yaml file and update following parameters: Update the name parameter to you require metadata: name: isilon-new Cluster name of 2nd array looks like this in the secret file.( Under /samples/secret/secret.yaml) - clusterName: \"cluster2\" username: \"user name\" password: \"Password\" endpoint: \"10.X.X.X\" endpointPort: \"8080 Use same clusterName ↑ in the second_storageclass.yaml # Optional: true ClusterName: \"cluster2\" Note: These are two essential parameters that you need to change in the “second_storageclass.yaml” file and other parameters that you change as required. Save the second_storageclass.yaml file\nCreate your 2nd storage class by using kubectl:\nkubectl create -f \u003cpath_to_second_storageclass_file\u003e Use newly created storage class isilon-new for volumes to spin up on cluster2\nPVC example\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: isilon-new Volume Snapshot Class Starting CSI PowerScale v1.6, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. Sample volume snapshot class manifests are available at samples/volumesnapshotclass/. Use these sample manifests to create a volumesnapshotclass for creating volume snapshots; uncomment/ update the manifests as per the requirements.\nSilent Mount Re-tries (v2.6.0) There are race conditions, when completing the ControllerPublish call to populate the client to volumes export list takes longer time than usual due to background NFS refresh process on OneFS wouldn’t have completed at same time, resulted in error:“mount failed” with initial attempts and might log success after few re-tries. This unnecessarily logs false positive “mount failed” error logs and to overcome this scenario Driver does silent mount re-tries attempts after every two sec. (five attempts max) for every NodePublish Call and allows successful mount within five re-tries without logging any mount error messages. “mount failed” will be logged once these five mount retrial attempts are exhausted and still client is not populated to export list.\nMount Re-tries handles below scenarios:\nAccess denied by server while mounting (NFSv3) No such file or directory (NFSv4) Sample:\nlevel=error clusterName=powerscale runid=10 msg=\"mount failed: exit status 32 mounting arguments: -t nfs -o rw XX.XX.XX.XX:/ifs/data/csi/k8s-ac7b91962d /var/lib/kubelet/pods/9f72096a-a7dc-4517-906c-20697f9d7375/volumes/kubernetes.io~csi/k8s-ac7b91962d/mount output: mount.nfs: access denied by server while mounting XX.XX.XX.XX:/ifs/data/csi/k8s-ac7b91962d ","categories":"","description":"Installing CSI Driver for PowerScale via Helm\n","excerpt":"Installing CSI Driver for PowerScale via Helm\n","ref":"/csm-docs/docs/csidriver/installation/helm/isilon/","tags":"","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\nCreating a storage class:\nCreate a file storageclass.yaml using sample yaml file located at samples/storageclass/isilon.yaml. Update/uncomment the attributes in this sample file as per the requirements.\nExecute the following command to create a storage class:\nkubectl create -f $PWD/storageclass.yaml Result: After executing the above command storage class will be created in the default namespace, and the user can see the storage class by executing\nkubectl get sc Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at samples/persistentvolumeclaim/pvc.yaml\nExecute the following command to create volume:\nkubectl create -f $PWD/pvc.yaml Result: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing the command kubectl get pvc.\nNote: The status of the volume can be either Bound or Pending depending on the VolumeBindingMode specified on the storage class.\nAttach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at samples/pod/.\nExecute the following command to mount the volume to the Kubernetes node:\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, a new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for the host to be part of clients/rootclients field of export created for volume and used by nginx application.\nCreate Snapshot\nVolumeSnapshotClass is needed for creating the volume snapshots. Starting from v1.6, CSI Driver for PowerScale will not create any default Volume Snapshot class.\nSo the user has to create a volume snapshot class. The required sample files are present under samples/volumesnapshotclass/. Choose the file based on Kubernetes version.\nExecute either one of the following commands to create a volume snapshot class.\nkubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1.yaml` OR `kubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1beta1.yaml The above-said command will create a volume snapshotclass with the name isilon-snapclass.\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snapshot-of-test-pvc.yaml. The sample file for snapshot creation is located at samples/volumesnapshot/.\nExecute the following command to create snapshot:\nkubectl create -f samples/volumesnapshot/snapshot-of-test-pvc.yaml The spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is test-pvc, then the created snapshot is named snapshot-of-test-pvc. Verify the PowerScale system for the newly created snapshot.\nNote:\nUser can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. User has to make sure that the IsiPath in the parameters section of the volume snapshot class is matching with a corresponding storage class. Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in the spec dataSource field.\nThe sample file for volume creation from the snapshot is located at samples/persistentvolumeclaim/pvc-from-snapshot.yaml .\nExecute the following command to create a snapshot:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-snapshot.yaml Verify the PowerScale system for newly created volume from the snapshot.\nDelete Snapshot\nExecute the following commands to delete the snapshot:\nkubectl get volumesnapshot kubectl delete volumesnapshot snapshot-of-test-pvc Create a new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in the spec dataSource field.\nThe sample file for volume creation from volume is located at samples/persistentvolumeclaim/pvc-from-pvc.yaml\nExecute the following command to create a pvc from another pvc:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-pvc.yaml Verify the PowerScale system for newly created volume from volume.\nTo Unattach the volume from Host\nDelete the nginx application to Unattach the volume from the host:\nkubectl delete -f nginx.yaml To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc ","categories":"","description":"Tests to validate PowerScale CSI Driver installation","excerpt":"Tests to validate PowerScale CSI Driver installation","ref":"/csm-docs/docs/csidriver/installation/test/powerscale/","tags":"","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v2.9.1 New Features/Changes #947 - [FEATURE]: Support for Kubernetes 1.28 #1066 - [FEATURE]: Support for Openshift 4.14 #851 - [FEATURE]: Helm Chart Enhancement - Container Images Configurable in values.yaml #905 - [FEATURE]: Add support for CSI Spec 1.6 #996 - [FEATURE]: Dell CSI to Dell CSM Operator Migration Process Fixed Issues #771 - [BUG]: Gopowerscale unit test fails #990 - [BUG]: X_CSI_AUTH_TYPE cannot be set in CSM Operator #999 - [BUG]: Volume health fails because it looks to a wrong path #1014 - [BUG]: Missing error check for os.Stat call during volume publish #1046 - [BUG]:Is cert-csi expansion expected to successfully run with enableQuota: false on PowerScale? #1053 - [BUG]: make gosec is erroring out - Repos PowerMax,PowerStore,PowerScale (gosec is installed) #1061 - [BUG]: Golint is not installing with go get command #1110 - [BUG]: Multi Controller defect - sidecars timeout #1103 - [BUG]: CSM Operator doesn’t apply fSGroupPolicy value to CSIDriver Object Known Issues Issue Resolution or workaround, if known Storage capacity tracking does not return MaximumVolumeSize parameter. PowerScale is purely NFS based meaning it has no actual volumes. Therefore MaximumVolumeSize cannot be implemented if there is no volume creation. CSI PowerScale 2.9.1 is compliant with CSI 1.6 specification since the field MaximumVolumeSize is optional. If the length of the nodeID exceeds 128 characters, the driver fails to update the CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn’t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in the CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future Kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581 Note: In kubernetes 1.22 this limit has been relaxed to 192 characters. If some older NFS exports /terminated worker nodes still in NFS export client list, CSI driver tries to add a new worker node it fails (For RWX volume). User need to manually clean the export client list from old entries to make successful addition of new worker nodes. Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100 fsGroupPolicy may not work as expected without root privileges for NFS onlyhttps://github.com/kubernetes/examples/issues/260 To get the desired behavior set “RootClientEnabled” = “true” in the storage class parameter Driver logs shows “VendorVersion=2.3.0+dirty” Update the driver to csi-powerscale 2.4.0 PowerScale 9.5.0, Driver installation fails with session based auth, “HTTP/1.1 401 Unauthorized” Fix is available in PowerScale \u003e= 9.5.0.4 If the volume limit is exhausted and there are pending pods and PVCs due to exceed max volume count, the pending PVCs will be bound to PVs and the pending pods will be scheduled to nodes when the driver pods are restarted. It is advised not to have any pending pods or PVCs once the volume limit per node is exhausted on a CSI Driver. There is an open issue reported with kubenetes at https://github.com/kubernetes/kubernetes/issues/95911 with the same behavior. Note Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for PowerScale CSI driver","excerpt":"Release notes for PowerScale CSI driver","ref":"/csm-docs/docs/csidriver/release/powerscale/","tags":"","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\nSymptoms Prevention, Resolution or Workaround The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret’s username and password for corresponding cluster The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn’t verify the certificates Check the isilon-certs- secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \"true\" for insecure connection. SSL validation is recommended in the production environment. The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver error: create volume failed, Access denied. create directory as requested This situation can happen when the user who created the base path is different from the user configured for the driver. Make sure the user used to deploy CSI-Driver must have enough rights on the base path (i.e. isiPath) to perform all operations. Volume/filesystem is allowed to mount by any host in the network, though that host is not a part of the export of that particular volume under /ifs directory “Dell PowerScale: OneFS NFS Design Considerations and Best Practices”: There is a default shared directory (ifs) of OneFS, which lets clients running Windows, UNIX, Linux, or Mac OS X access the same directories and files. It is recommended to disable the ifs shared directory in a production environment and create dedicated NFS exports and SMB shares for your workload. Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class is not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same. While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\n* ISI_PRIV_STATISTICS\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700. If the hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to driver version 1.4.0 or later there is a possibility of “localhost” as a stale entry in export Recommended setup: User should not map a hostname to loopback IP in /etc/hosts file Driver node pod is in “CrashLoopBackOff” as “Node ID” generated is not with proper FQDN. This might be due to “dnsPolicy” implemented on the driver node pod which may differ with different networks. This parameter is configurable in both helm and Operator installer and the user can try with different “dnsPolicy” according to the environment. The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver Authentication failed. Trying to re-authenticate when using Session-based authentication The issue has been resolved from OneFS 9.3 onwards, for OneFS versions prior to 9.3 for session-based authentication either smart connect can be created against a single node of Isilon or CSI Driver can be installed/pointed to a particular node of the Isilon else basic authentication can be used by setting isiAuthType in values.yaml to 0 When an attempt is made to create more than one ReadOnly PVC from the same volume snapshot, the second and subsequent requests result in PVCs in state Pending, with a warning another RO volume from this snapshot is already present. This is because the driver allows only one RO volume from a specific snapshot at any point in time. This is to allow faster creation(within a few seconds) of a RO PVC from a volume snapshot irrespective of the size of the volume snapshot. Wait for the deletion of the first RO PVC created from the same volume snapshot. Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.22.0 \u003c 1.25.0 which is incompatible with Kubernetes V1.22.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. Standby controller pod is in crashloopbackoff state Scale down the replica count of the controller pod’s deployment to 1 using kubectl scale deployment \u003cdeployment_name\u003e --replicas=1 -n \u003cdriver_namespace\u003e Driver install fails because of the incompatible helm values file specified in dell-csi-helm-installer - expected: v2.9.x, found: v2.8.0. Change driver version in each file in dell/csi-powerscale/dell-csi-helm-installer from 2.8.0 to 2.9.x ","categories":"","description":"Troubleshooting PowerScale Driver","excerpt":"Troubleshooting PowerScale Driver","ref":"/csm-docs/docs/csidriver/troubleshooting/powerscale/","tags":"","title":"PowerScale"},{"body":"Installing CSI Driver for PowerScale via Dell CSM Operator The CSI Driver for Dell PowerScale can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using the following command:\nkubectl get csm --all-namespaces Prerequisite Create namespace. Execute kubectl create namespace isilon to create the isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘isilon’.\nCreate isilon-creds secret by creating a yaml file called secret.yaml with the following content:\nisilonClusters: # logical name of PowerScale Cluster - clusterName: \"cluster1\" # username for connecting to PowerScale OneFS API server # Default value: None username: \"user\" # password for connecting to PowerScale OneFS API server password: \"password\" # HTTPS endpoint of the PowerScale OneFS API server # Default value: None # Examples: \"1.2.3.4\", \"https://1.2.3.4\", \"https://abc.myonefs.com\" endpoint: \"1.2.3.4\" # Is this a default cluster (would be used by storage classes without ClusterName parameter) # Allowed values: # true: mark this cluster config as default # false: mark this cluster config as not default # Default value: false isDefault: true # Specify whether the PowerScale OneFS API server's certificate chain and host name should be verified. # Allowed values: # true: skip OneFS API server's certificate verification # false: verify OneFS API server's certificates # Default value: default value specified in values.yaml # skipCertificateValidation: true # The base path for the volumes to be created on PowerScale cluster # This will be used if a storage class does not have the IsiPath parameter specified. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Default value: default value specified in values.yaml # Examples: \"/ifs/data/csi\", \"/ifs/engineering\" # isiPath: \"/ifs/data/csi\" # The permissions for isi volume directory path # This will be used if a storage class does not have the IsiVolumePathPermissions parameter specified. # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" # isiVolumePathPermissions: \"0777\" - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" endpointPort: \"8080\" Replace the values for the given keys as per your environment.\nIf replication feature is enabled, ensure the secret includes all the PowerScale clusters involved in replication.\nAfter creating the secret.yaml, the following command can be used to create the secret,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml Use the following command to replace or update the secret\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f - Note: The user needs to validate the YAML syntax and array related key/values while replacing the isilon-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\nCreate isilon-certs-n secret. Please refer this section for creating cert-secrets.\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion: v1 kind: Secret metadata: name: isilon-certs-0 namespace: isilon type: Opaque data: cert-0: \"\" Execute command: kubectl create -f empty-secret.yaml\nInstall Driver Follow all the prerequisites above\nCreate a CR (Custom Resource) for PowerScale using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\nParameter Description Required Default dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet storageCapacity Enable/Disable storage capacity tracking feature No false Common parameters for node and controller CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock X_CSI_ISI_SKIP_CERTIFICATE_VALIDATION Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true X_CSI_ISI_PATH Base path for the volumes to be created Yes X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true X_CSI_ISI_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes X_CSI_ISI_VOLUME_PATH_PERMISSIONS The permissions for isi volume directory path Yes 0777 X_CSI_ISI_AUTH_TYPE Indicates the authentication method to be used. If set to 1 then it follows as session-based authentication else basic authentication. If CSM Authorization is enabled, this value must be set to 1. No 0 Controller parameters X_CSI_MODE Driver starting mode No controller X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes Node parameters X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes 0 X_CSI_MODE Driver starting mode No node Execute the following command to create PowerScale custom resource:\nkubectl create -f \u003cinput_sample_file.yaml\u003e This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\nVerify the CSI Driver installation\nNote :\n“Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. ","categories":"","description":"Installing Dell CSI Driver for PowerScale via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for PowerScale via Dell CSM Operator\n","ref":"/csm-docs/docs/deployment/csmoperator/drivers/powerscale/","tags":"","title":"PowerScale"},{"body":"Configuring PowerScale CSI Driver with CSM for Authorization Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow these steps to configure the CSI Drivers to work with the Authorization sidecar:\nApply the secret containing the token data into the driver namespace. It’s assumed that the Kubernetes administrator has the token secret manifest, generated by your storage administrator via Generate a Token, saved in /tmp/token.yaml.\n#It is assumed that array type powerscale has the namespace “isilon”.\nkubectl apply -f /tmp/token.yaml -n isilon Edit these parameters in samples/secret/karavi-authorization-config.json file in CSI PowerScale driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\nParameter Description Required Default username Username for connecting to the backend storage array. This parameter is ignored. No - password Password for connecting to to the backend storage array. This parameter is ignored. No - intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes - endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400 systemID Cluster name of the backend storage array. Yes \" \" skipCertificateValidation A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml Create the karavi-authorization-config secret using this command:\nkubectl -n isilon create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f - Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n isilon create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f - Otherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n isilon create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f - Prepare the driver configuration secret, applicable to your driver installation method, to communicate with the CSM Authorization sidecar.\nHelm\nRefer to the Install the Driver section to edit the parameters in samples/secret/secret.yaml file to configure the driver to communicate with the CSM Authorization sidecar.\nUpdate endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate mountEndpoint to the PowerScale OneFS API server. For example, 10.0.0.1.\nUpdate skipCertificateValidation to true.\nThe username and password can be any value since they will be ignored.\nExample:\nisilonClusters: - clusterName: \"cluster1\" username: \"ignored\" password: \"ignored\" isDefault: true endpoint: localhost endpointPort: 9400 mountEndpoint: 10.0.0.1 skipCertificateValidation: true Operator\nRefer to the Prerequisite section to prepare the secret.yaml file to configure the driver to communicate with the CSM Authorization sidecar.\nUpdate endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate mountEndpoint to the PowerScale OneFS API server. For example, 10.0.0.1.\nUpdate skipCertificateValidation to true.\nThe username and password can be any value since they will be ignored.\nExample:\nisilonClusters: - clusterName: \"cluster1\" username: \"ignored\" password: \"ignored\" isDefault: true endpoint: localhost endpointPort: 9400 mountEndpoint: 10.0.0.1 skipCertificateValidation: true Enable CSM Authorization in the driver installation applicable to your installation method.\nHelm\nRefer to the Install the Driver section to edit the parameters in my-isilon-settings.yaml file to enable CSM Authorization.\nUpdate authorization.enabled to true.\nUpdate authorization.sidecarProxyImage to the image of the CSM Authorization sidecar. In most cases, you can leave the default value.\nUpdate authorization.proxyHost to the hostname of the CSM Authorization Proxy Server.\nUpdate authorization.skipCertificateValidation to true or false depending on if you want to disable or enable certificate validation of the CSM Authorization Proxy Server.\nExample:\nauthorization: enabled: true # sidecarProxyImage: the container image used for the csm-authorization-sidecar. # Default value: dellemc/csm-authorization-sidecar:v1.8.0 sidecarProxyImage: dellemc/csm-authorization-sidecar:v1.8.0 # proxyHost: hostname of the csm-authorization server # Default value: None proxyHost: csm-authorization.com # skipCertificateValidation: certificate validation of the csm-authorization server # Allowed Values: # \"true\" - TLS certificate verification will be skipped # \"false\" - TLS certificate will be verified # Default value: \"true\" skipCertificateValidation: true Operator\nRefer to the Install Driver section to edit the parameters in the Custom Resource to enable CSM Authorization.\nUnder modules, enable the module named authorization:\nUpdate the enabled field to true.\nUpdate the image to the image of the CSM Authorization sidecar. In most cases, you can leave the default value.\nUpdate the PROXY_HOST environment value to the hostname of the CSM Authorization Proxy Server.\nUpdate the SKIP_CERTIFICATE_VALIDATION environment value to true or false depending on if you want to disable or enable certificate validation of the CSM Authorization Proxy Server.\nmodules: # Authorization: enable csm-authorization for RBAC - name: authorization # enable: Enable/Disable csm-authorization enabled: true configVersion: v1.8.0 components: - name: karavi-authorization-proxy image: dellemc/csm-authorization-sidecar:v1.8.0 envs: # proxyHost: hostname of the csm-authorization server - name: \"PROXY_HOST\" value: \"csm-authorization.com\" # skipCertificateValidation: Enable/Disable certificate validation of the csm-authorization server - name: \"SKIP_CERTIFICATE_VALIDATION\" value: \"true\" Install the Dell CSI PowerScale driver following the appropriate documenation for your installation method.\n(Optional) Install dellctl to perform Kubernetes administrator commands for additional capabilities (e.g., list volumes). Please refer to the dellctl documentation page for the installation steps and command list.\n","categories":"","description":"Enabling CSM Authorization for PowerScale CSI Driver\n","excerpt":"Enabling CSM Authorization for PowerScale CSI Driver\n","ref":"/csm-docs/v1/authorization/configuration/powerscale/","tags":"","title":"PowerScale"},{"body":"The CSI Driver for Dell PowerScale can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\nCSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\nCSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration Prerequisites The following are requirements to be met before installing the CSI Driver for Dell PowerScale:\nInstall Kubernetes or OpenShift (see supported versions) Install Helm 3 Mount propagation is enabled on container runtime that is being used nfs-utils package must be installed on nodes that will mount volumes If using Snapshot feature, satisfy all Volume Snapshot requirements If enabling CSM for Authorization, please refer to the Authorization deployment steps first If enabling CSM for Replication, please refer to the Replication deployment steps first If enabling CSM for Resiliency, please refer to the Resiliency deployment steps first If enabling Encryption, please refer to the Encryption deployment steps first Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerScale.\nSteps\nRun the command to install Helm 3.0.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash (Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\n(Optional) Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via helm.\nIf enabled capacity metrics (used \u0026 free capacity, used \u0026 free inodes) for PowerScale PV will be expose in Kubernetes metrics API.\nTo enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\ncontroller: healthMonitor: # enabled: Enable/Disable health monitor of CSI volumes # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: None enabled: false # interval: Interval of monitoring volume health condition # Allowed values: Number followed by unit (s,m,h) # Examples: 60s, 5m, 1h # Default value: 60s interval: 60s node: healthMonitor: # enabled: Enable/Disable health monitor of CSI volumes- volume usage, volume condition # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: None enabled: false NOTE: To enable this feature to existing driver OR enable this feature while upgrading the driver versions, follow either of the way.\nReinstall of Driver Upgrade the driver smoothly with “–upgrade” option (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication: enabled: true Replication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in the csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\nRun git clone -b v2.8.0 https://github.com/dell/csi-powerscale.git to clone the git repository.\nEnsure that you have created the namespace where you want to install the driver. You can run kubectl create namespace isilon to create a new one. The use of “isilon” as the namespace is just an example. You can choose any name for the namespace.\nCollect information from the PowerScale Systems like IP address, IsiPath, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml.\nDownload wget -O my-isilon-settings.yaml https://raw.githubusercontent.com/dell/helm-charts/csi-isilon-2.8.0/charts/csi-isilon/values.yaml into cd ../dell-csi-helm-installer to customize settings for installation.\nEdit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\nParameter Description Required Default driverRepository Set to give the repository containing the driver image (used as part of the image name). Yes dellemc logLevel CSI driver log level No “debug” certSecretCount Defines the number of certificate secrets, which the user is going to create for SSL authentication. (isilon-cert-0..isilon-cert-(n-1)); Minimum value should be 1. Yes 1 allowedNetworks Defines the list of networks that can be used for NFS I/O traffic, CIDR format must be used. No [ ] maxIsilonVolumesPerNode Defines the default value for a maximum number of volumes that the controller can publish to the node. If the value is zero CO SHALL decide how many volumes of this type can be published by the controller to the node. This limit is applicable to all the nodes in the cluster for which node label ‘max-isilon-volumes-per-node’ is not set. Yes 0 imagePullPolicy Defines the policy to determine if the image should be pulled prior to starting the container Yes IfNotPresent verbose Indicates what content of the OneFS REST API message should be logged in debug level logs Yes 1 kubeletConfigDir Specify kubelet config dir path Yes “/var/lib/kubelet” enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish a connection to Array. This requires enableCustomTopology to be enabled. No false fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity.enabled Enable/Disable storage capacity tracking No true storageCapacity.pollInterval Configure how often the driver checks for changed capacity No 5m podmonAPIPort Defines the port which csi-driver will use within the cluster to support podmon No 8083 maxPathLen Defines the maximum length of path for a volume No 192 controller Configure controller pod specific parameters controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2 volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s” snapshot.enabled Enable/Disable volume snapshot feature Yes true snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot” resizer.enabled Enable/Disable volume expansion feature Yes true healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume status, volume condition Yes false healthMonitor.interval Interval of monitoring volume health condition Yes 60s nodeSelector Define node selection constraints for pods of controller deployment No tolerations Define tolerations for the controller deployment, if required No leader-election-lease-duration Duration, that non-leader candidates will wait to force acquire leadership No 20s leader-election-renew-deadline Duration, that the acting leader will retry refreshing leadership before giving up No 15s leader-election-retry-period Duration, the LeaderElector clients should wait between tries of actions No 5s node Configure node pod specific parameters nodeSelector Define node selection constraints for pods of node daemonset No tolerations Define tolerations for the node daemonset, if required No dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition Yes false PLATFORM ATTRIBUTES endpointPort Define the HTTPs port number of the PowerScale OneFS API server. If authorization is enabled, endpointPort should be the HTTPS localhost port that the authorization sidecar will listen on. This value acts as a default value for endpointPort, if not specified for a cluster config in secret. No 8080 skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. This value acts as a default value for skipCertificateValidation, if not specified for a cluster config in secret. No true isiAuthType Indicates the authentication method to be used. If set to 1 then it follows as session-based authentication else basic authentication No 0 isiAccessZone Define the name of the access zone a volume can be created in. If storageclass is missing with AccessZone parameter, then value of isiAccessZone is used for the same. No System enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. No true isiPath Define the base path for the volumes to be created on PowerScale cluster. This value acts as a default value for isiPath, if not specified for a cluster config in secret No /ifs/data/csi ignoreUnresolvableHosts Allows new host to add to existing export list though any of the existing hosts from the same exports are unresolvable/doesn’t exist anymore. No false noProbeOnStart Define whether the controller/node plugin should probe all the PowerScale clusters during driver initialization No false autoProbe Specify if automatically probe the PowerScale cluster if not done already during CSI calls No true authorization Authorization is an optional feature to apply credential shielding of the backend PowerScale. - - enabled A boolean that enables/disables authorization feature. No false sidecarProxyImage Image for csm-authorization-sidecar. No \" \" proxyHost Hostname of the csm-authorization server. No Empty skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization proxy server. No true podmon Podmon is an optional feature to enable application pods to be resilient to node failure. - - enabled A boolean that enables/disables podmon feature. No false image image for podmon. No \" \" encryption Encryption is an optional feature to apply encryption to CSI volumes. - - enabled A boolean that enables/disables Encryption feature. No false image Encryption driver image name. No “dellemc/csm-encryption:v0.3.0” NOTE:\nControllerCount parameter value must not exceed the number of nodes in the Kubernetes cluster. Otherwise, some of the controller pods remain in a “Pending” state till new nodes are available for scheduling. The installer exits with a WARNING on the same. Whenever the certSecretCount parameter changes in my-isilon-setting.yaml user needs to reinstall the driver. In order to enable authorization, there should be an authorization proxy server already installed. If you are using a custom image, check the version and driverRepository fields in my-isilon-setting.yaml to make sure that they are pointing to the correct image repository and driver version. These two fields are spliced together to form the image name, as shown here: /csi-isilon: Edit following parameters in samples/secret/secret.yaml file and update/add connection/authentication information for one or more PowerScale clusters.\nParameter Description Required Default clusterName Logical name of PoweScale cluster against which volume CRUD operations are performed through this secret. Yes - username username for connecting to PowerScale OneFS API server Yes - password password for connecting to PowerScale OneFS API server Yes - endpoint HTTPS endpoint of the PowerScale OneFS API server. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes - isDefault Indicates if this is a default cluster (would be used by storage classes without ClusterName parameter). Only one of the cluster config should be marked as default. No false Optional parameters Following parameters are Optional. If specified will override default values from values.yaml. skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. No default value from values.yaml ignoreUnresolvableHosts Allows new host to add to existing export list though any of the existing hosts from the same exports are unresolvable/doesn’t exist anymore. No default value from values.yaml endpointPort Specify the HTTPs port number of the PowerScale OneFS API server No default value from values.yaml isiPath The base path for the volumes to be created on PowerScale cluster. Note: IsiPath parameter in storageclass, if present will override this attribute. No default value from values.yaml mountEndpoint Endpoint of the PowerScale OneFS API server, for example, 10.0.0.1. This must be specified if CSM-Authorization is enabled. No - User privileges The username specified in secret.yaml must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\nPrivilege Type ISI_PRIV_LOGIN_PAPI Read Only ISI_PRIV_NFS Read Write ISI_PRIV_QUOTA Read Write ISI_PRIV_SNAPSHOT Read Write ISI_PRIV_IFS_RESTORE Read Only ISI_PRIV_NS_IFS_ACCESS Read Only ISI_PRIV_IFS_BACKUP Read Only ISI_PRIV_SYNCIQ Read Write Create isilon-creds secret using the following command: kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\nNOTE:\nIf any key/value is present in all my-isilon-settings.yaml, secret, and storageClass, then the values provided in storageClass parameters take precedence. The user has to validate the yaml syntax and array-related key/values while replacing or appending the isilon-creds secret. The driver will continue to use previous values in case of an error found in the yaml file. For the key isiIP/endpoint, the user can give either IP address or FQDN. Also, the user can prefix ‘https’ (For example, https://192.168.1.1) with the value. The isilon-creds secret has a mountEndpoint parameter which should only be updated and used when Authorization is enabled. Install OneFS CA certificates by following the instructions from the next section, if you want to validate OneFS API server’s certificates. If not, create an empty secret using the following command and an empty secret must be created for the successful installation of CSI Driver for Dell PowerScale.\nkubectl create -f empty-secret.yaml This command will create a new secret called isilon-certs-0 in isilon namespace.\nInstall the driver using csi-install.sh bash script and default yaml by running\ncd dell-csi-helm-installer \u0026\u0026 wget -O my-isilon-settings.yaml https://raw.githubusercontent.com/dell/helm-charts/csi-isilon-2.8.0/charts/csi-isilon/values.yaml \u0026\u0026 ./csi-install.sh --namespace isilon --values my-isilon-settings.yaml Certificate validation for OneFS REST API calls The CSI driver exposes an install parameter ‘skipCertificateValidation’ which determines if the driver performs client-side verification of the OneFS certificates. The ‘skipCertificateValidation’ parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the ‘skipCertificateValidation’ is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the ‘skipCertificateValidation’ parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem To create the certs secret, run kubectl create secret generic isilon-certs-0 --from-file=cert-0=ca_cert_0.pem -n isilon Use the following command to replace the secret kubectl create secret generic isilon-certs-0 -n isilon --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - NOTES:\nThe OneFS IP can be with or without a port, depends upon the configuration of OneFS API server. The commands are based on the namespace ‘isilon’ It is highly recommended that ca_cert.pem file(s) having the naming convention as ca_cert_number.pem (example: ca_cert_0, ca_cert_1), where this number starts from 0 and grows as the number of OneFS arrays grows. The cert secret created out of these pem files must have the naming convention as isilon-certs-number (example: isilon-certs-0, isilon-certs-1, and so on.); The number must start from zero and must grow in incremental order. The number of the secrets created out of pem files should match certSecretCount value in myvalues.yaml or my-isilon-settings.yaml. Dynamic update of array details via secret.yaml CSI Driver for Dell PowerScale now provides supports for Multi cluster. Now users can link the single CSI Driver to multiple OneFS Clusters by updating secret.yaml. Users can now update the isilon-creds secret by editing the secret.yaml and executing the following command\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Note: Updating isilon-certs-x secrets is a manual process, unlike isilon-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nStorage Classes The CSI driver for Dell PowerScale version 1.5 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A sample storage class manifest is available at samples/storageclass/isilon.yaml. Use this sample manifest to create a storageclass to provision storage; uncomment/ update the manifest as per the requirements.\nWhat happens to my existing storage classes? Upgrading from CSI PowerScale v2.3 driver: The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nNOTE:\nAt least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es): Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create secondary storage class:\nThere are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\nCopy the storageclass.yaml to second_storageclass.yaml (This is just an example, you can rename to file you require.) Edit the second_storageclass.yaml yaml file and update following parameters: Update the name parameter to you require metadata: name: isilon-new Cluster name of 2nd array looks like this in the secret file.( Under /samples/secret/secret.yaml) - clusterName: \"cluster2\" username: \"user name\" password: \"Password\" endpoint: \"10.X.X.X\" endpointPort: \"8080 Use same clusterName ↑ in the second_storageclass.yaml # Optional: true ClusterName: \"cluster2\" Note: These are two essential parameters that you need to change in the “second_storageclass.yaml” file and other parameters that you change as required. Save the second_storageclass.yaml file\nCreate your 2nd storage class by using kubectl:\nkubectl create -f \u003cpath_to_second_storageclass_file\u003e Use newly created storage class isilon-new for volumes to spin up on cluster2\nPVC example\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: isilon-new Volume Snapshot Class Starting CSI PowerScale v1.6, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. Sample volume snapshot class manifests are available at samples/volumesnapshotclass/. Use these sample manifests to create a volumesnapshotclass for creating volume snapshots; uncomment/ update the manifests as per the requirements.\nSilent Mount Re-tries (v2.6.0) There are race conditions, when completing the ControllerPublish call to populate the client to volumes export list takes longer time than usual due to background NFS refresh process on OneFS wouldn’t have completed at same time, resulted in error:“mount failed” with initial attempts and might log success after few re-tries. This unnecessarily logs false positive “mount failed” error logs and to overcome this scenario Driver does silent mount re-tries attempts after every two sec. (five attempts max) for every NodePublish Call and allows successful mount within five re-tries without logging any mount error messages. “mount failed” will be logged once these five mount retrial attempts are exhausted and still client is not populated to export list.\nMount Re-tries handles below scenarios:\nAccess denied by server while mounting (NFSv3) No such file or directory (NFSv4) Sample:\nlevel=error clusterName=powerscale runid=10 msg=\"mount failed: exit status 32 mounting arguments: -t nfs -o rw XX.XX.XX.XX:/ifs/data/csi/k8s-ac7b91962d /var/lib/kubelet/pods/9f72096a-a7dc-4517-906c-20697f9d7375/volumes/kubernetes.io~csi/k8s-ac7b91962d/mount output: mount.nfs: access denied by server while mounting XX.XX.XX.XX:/ifs/data/csi/k8s-ac7b91962d ","categories":"","description":"Installing CSI Driver for PowerScale via Helm\n","excerpt":"Installing CSI Driver for PowerScale via Helm\n","ref":"/csm-docs/v1/csidriver/installation/helm/isilon/","tags":"","title":"PowerScale"},{"body":" The Dell CSI Operator is no longer actively maintained or supported. Dell CSI Operator has been replaced with Dell CSM Operator. If you are currently using Dell CSI Operator, refer to the operator migration documentation to migrate from Dell CSI Operator to Dell CSM Operator.\nInstalling CSI Driver for PowerScale via Operator The CSI Driver for Dell PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nNote: MKE (Mirantis Kubernetes Engine) does not support the installation of CSI-PowerScale via Operator.\nListing installed drivers with the CSI Isilon CRD User can query for CSI-PowerScale driver using the following command:\nkubectl get csiisilon --all-namespaces Install Driver Create namespace.\nExecute kubectl create namespace isilon to create the isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘isilon’.\nCreate isilon-creds secret by using secret.yaml file format only.\n2.1 Create a yaml file called secret.yaml with the following content:\nisilonClusters: # logical name of PowerScale Cluster - clusterName: \"cluster1\" # username for connecting to PowerScale OneFS API server # Default value: None username: \"user\" # password for connecting to PowerScale OneFS API server password: \"password\" # HTTPS endpoint of the PowerScale OneFS API server # Default value: None # Examples: \"1.2.3.4\", \"https://1.2.3.4\", \"https://abc.myonefs.com\" endpoint: \"1.2.3.4\" # Is this a default cluster (would be used by storage classes without ClusterName parameter) # Allowed values: # true: mark this cluster config as default # false: mark this cluster config as not default # Default value: false isDefault: true # Specify whether the PowerScale OneFS API server's certificate chain and host name should be verified. # Allowed values: # true: skip OneFS API server's certificate verification # false: verify OneFS API server's certificates # Default value: default value specified in values.yaml # skipCertificateValidation: true # The base path for the volumes to be created on PowerScale cluster # This will be used if a storage class does not have the IsiPath parameter specified. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Default value: default value specified in values.yaml # Examples: \"/ifs/data/csi\", \"/ifs/engineering\" # isiPath: \"/ifs/data/csi\" # The permissions for isi volume directory path # This will be used if a storage class does not have the IsiVolumePathPermissions parameter specified. # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" # isiVolumePathPermissions: \"0777\" - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" endpointPort: \"8080\" Replace the values for the given keys as per your environment. After creating the secret.yaml, the following command can be used to create the secret,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml Use the following command to replace or update the secret\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Note: The user needs to validate the YAML syntax and array related key/values while replacing the isilon-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\nCreate isilon-certs-n secret. Please refer this section for creating cert-secrets.\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion: v1 kind: Secret metadata: name: isilon-certs-0 namespace: isilon type: Opaque data: cert-0: \"\" Execute command:\nkubectl create -f empty-secret.yaml Create a CR (Custom Resource) for PowerScale using the sample files provided here.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\nParameter Description Required Default dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity Enable/Disable storage capacity tracking feature No true X_CSI_MAX_PATH_LIMIT Defines the maximum length of path for a volume No 192 Common parameters for node and controller CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock X_CSI_ISI_SKIP_CERTIFICATE_VALIDATION Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true X_CSI_ISI_PATH Base path for the volumes to be created Yes X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true X_CSI_ISI_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes X_CSI_ISI_VOLUME_PATH_PERMISSIONS The permissions for isi volume directory path Yes 0777 X_CSI_ISI_AUTH_TYPE Indicates the authentication method to be used. If set to 1 then it follows as session-based authentication else basic authentication No 0 Controller parameters X_CSI_MODE Driver starting mode No controller X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes nodeSelector Define node selection constraints for pods of controller deployment No X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin. Provides details of volume status and volume condition. As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar No false Node parameters X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes 0 X_CSI_MODE Driver starting mode No node X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from node plugin. Provides details of volume usage No false Side car parameters leader-election-lease-duration Duration, that non-leader candidates will wait to force acquire leadership No 20s leader-election-renew-deadline Duration, that the acting leader will retry refreshing leadership before giving up No 15s leader-election-retry-period Duration, the LeaderElector clients should wait between tries of actions No 5s Execute the following command to create PowerScale custom resource:\nkubectl create -f \u003cinput_sample_file.yaml\u003e This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\nNote :\nFrom CSI-PowerScale v1.6.0 and higher, Storage class and VolumeSnapshotClass will not be created as part of driver deployment. The user has to create Storageclass and Volume Snapshot Class. “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. Volume Health Monitoring This feature is introduced in CSI Driver for PowerScale version 2.1.0.\nOperator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" ","categories":"","description":"Installing CSI Driver for PowerScale via Operator\n","excerpt":"Installing CSI Driver for PowerScale via Operator\n","ref":"/csm-docs/v1/csidriver/installation/operator/isilon/","tags":"","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\nCreating a storage class:\nCreate a file storageclass.yaml using sample yaml file located at samples/storageclass/isilon.yaml. Update/uncomment the attributes in this sample file as per the requirements.\nExecute the following command to create a storage class:\nkubectl create -f $PWD/storageclass.yaml Result: After executing the above command storage class will be created in the default namespace, and the user can see the storage class by executing\nkubectl get sc Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at samples/persistentvolumeclaim/pvc.yaml\nExecute the following command to create volume:\nkubectl create -f $PWD/pvc.yaml Result: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing the command kubectl get pvc.\nNote: The status of the volume can be either Bound or Pending depending on the VolumeBindingMode specified on the storage class.\nAttach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at samples/pod/.\nExecute the following command to mount the volume to the Kubernetes node:\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, a new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for the host to be part of clients/rootclients field of export created for volume and used by nginx application.\nCreate Snapshot\nVolumeSnapshotClass is needed for creating the volume snapshots. Starting from v1.6, CSI Driver for PowerScale will not create any default Volume Snapshot class.\nSo the user has to create a volume snapshot class. The required sample files are present under samples/volumesnapshotclass/. Choose the file based on Kubernetes version.\nExecute either one of the following commands to create a volume snapshot class.\nkubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1.yaml` OR `kubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1beta1.yaml The above-said command will create a volume snapshotclass with the name isilon-snapclass.\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snapshot-of-test-pvc.yaml. The sample file for snapshot creation is located at samples/volumesnapshot/.\nExecute the following command to create snapshot:\nkubectl create -f samples/volumesnapshot/snapshot-of-test-pvc.yaml The spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is test-pvc, then the created snapshot is named snapshot-of-test-pvc. Verify the PowerScale system for the newly created snapshot.\nNote:\nUser can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. User has to make sure that the IsiPath in the parameters section of the volume snapshot class is matching with a corresponding storage class. Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in the spec dataSource field.\nThe sample file for volume creation from the snapshot is located at samples/persistentvolumeclaim/pvc-from-snapshot.yaml .\nExecute the following command to create a snapshot:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-snapshot.yaml Verify the PowerScale system for newly created volume from the snapshot.\nDelete Snapshot\nExecute the following commands to delete the snapshot:\nkubectl get volumesnapshot kubectl delete volumesnapshot snapshot-of-test-pvc Create a new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in the spec dataSource field.\nThe sample file for volume creation from volume is located at samples/persistentvolumeclaim/pvc-from-pvc.yaml\nExecute the following command to create a pvc from another pvc:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-pvc.yaml Verify the PowerScale system for newly created volume from volume.\nTo Unattach the volume from Host\nDelete the nginx application to Unattach the volume from the host:\nkubectl delete -f nginx.yaml To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc ","categories":"","description":"Tests to validate PowerScale CSI Driver installation","excerpt":"Tests to validate PowerScale CSI Driver installation","ref":"/csm-docs/v1/csidriver/installation/test/powerscale/","tags":"","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v2.8.0 New Features/Changes #724 - [FEATURE]: CSM support for Openshift 4.13 #877 - [FEATURE]: Make standalone helm chart available from helm repository : https://dell.github.io/dell/helm-charts #950 - [FEATURE]: PowerScale 9.5.0.4 support #967 - [FEATURE]: SLES15 SP4 support in csi powerscale #922 - [FEATURE]: Use ubi9 micro as base image Fixed Issues #916 - [BUG]: Remove references to deprecated io/ioutil package #487 - [BUG]: Powerscale CSI driver RO PVC-from-snapshot wrong zone Known Issues Issue Resolution or workaround, if known If the length of the nodeID exceeds 128 characters, the driver fails to update the CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn’t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in the CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future Kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581 Note: In kubernetes 1.22 this limit has been relaxed to 192 characters. If some older NFS exports /terminated worker nodes still in NFS export client list, CSI driver tries to add a new worker node it fails (For RWX volume). User need to manually clean the export client list from old entries to make successful addition of new worker nodes. Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100 fsGroupPolicy may not work as expected without root privileges for NFS onlyhttps://github.com/kubernetes/examples/issues/260 To get the desired behavior set “RootClientEnabled” = “true” in the storage class parameter Driver logs shows “VendorVersion=2.3.0+dirty” Update the driver to csi-powerscale 2.4.0 PowerScale 9.5.0, Driver installation fails with session based auth, “HTTP/1.1 401 Unauthorized” Fix is available in PowerScale \u003e= 9.5.0.4 If the volume limit is exhausted and there are pending pods and PVCs due to exceed max volume count, the pending PVCs will be bound to PVs and the pending pods will be scheduled to nodes when the driver pods are restarted. It is advised not to have any pending pods or PVCs once the volume limit per node is exhausted on a CSI Driver. There is an open issue reported with kubenetes at https://github.com/kubernetes/kubernetes/issues/95911 with the same behavior. Note Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for PowerScale CSI driver","excerpt":"Release notes for PowerScale CSI driver","ref":"/csm-docs/v1/csidriver/release/powerscale/","tags":"","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\nSymptoms Prevention, Resolution or Workaround The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret’s username and password for corresponding cluster The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn’t verify the certificates Check the isilon-certs- secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \"true\" for insecure connection. SSL validation is recommended in the production environment. The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver error: create volume failed, Access denied. create directory as requested This situation can happen when the user who created the base path is different from the user configured for the driver. Make sure the user used to deploy CSI-Driver must have enough rights on the base path (i.e. isiPath) to perform all operations. Volume/filesystem is allowed to mount by any host in the network, though that host is not a part of the export of that particular volume under /ifs directory “Dell PowerScale: OneFS NFS Design Considerations and Best Practices”: There is a default shared directory (ifs) of OneFS, which lets clients running Windows, UNIX, Linux, or Mac OS X access the same directories and files. It is recommended to disable the ifs shared directory in a production environment and create dedicated NFS exports and SMB shares for your workload. Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class is not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same. While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700. If the hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to driver version 1.4.0 or later there is a possibility of “localhost” as a stale entry in export Recommended setup: User should not map a hostname to loopback IP in /etc/hosts file Driver node pod is in “CrashLoopBackOff” as “Node ID” generated is not with proper FQDN. This might be due to “dnsPolicy” implemented on the driver node pod which may differ with different networks. This parameter is configurable in both helm and Operator installer and the user can try with different “dnsPolicy” according to the environment. The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver Authentication failed. Trying to re-authenticate when using Session-based authentication The issue has been resolved from OneFS 9.3 onwards, for OneFS versions prior to 9.3 for session-based authentication either smart connect can be created against a single node of Isilon or CSI Driver can be installed/pointed to a particular node of the Isilon else basic authentication can be used by setting isiAuthType in values.yaml to 0 When an attempt is made to create more than one ReadOnly PVC from the same volume snapshot, the second and subsequent requests result in PVCs in state Pending, with a warning another RO volume from this snapshot is already present. This is because the driver allows only one RO volume from a specific snapshot at any point in time. This is to allow faster creation(within a few seconds) of a RO PVC from a volume snapshot irrespective of the size of the volume snapshot. Wait for the deletion of the first RO PVC created from the same volume snapshot. Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.22.0 \u003c 1.25.0 which is incompatible with Kubernetes V1.22.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. ","categories":"","description":"Troubleshooting PowerScale Driver","excerpt":"Troubleshooting PowerScale Driver","ref":"/csm-docs/v1/csidriver/troubleshooting/powerscale/","tags":"","title":"PowerScale"},{"body":"Installing CSI Driver for PowerScale via Dell CSM Operator The CSI Driver for Dell PowerScale can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using the following command:\nkubectl get csm --all-namespaces Prerequisite Create namespace. Execute kubectl create namespace isilon to create the isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘isilon’.\nCreate isilon-creds secret by creating a yaml file called secret.yaml with the following content:\nisilonClusters: # logical name of PowerScale Cluster - clusterName: \"cluster1\" # username for connecting to PowerScale OneFS API server # Default value: None username: \"user\" # password for connecting to PowerScale OneFS API server password: \"password\" # HTTPS endpoint of the PowerScale OneFS API server # Default value: None # Examples: \"1.2.3.4\", \"https://1.2.3.4\", \"https://abc.myonefs.com\" endpoint: \"1.2.3.4\" # Is this a default cluster (would be used by storage classes without ClusterName parameter) # Allowed values: # true: mark this cluster config as default # false: mark this cluster config as not default # Default value: false isDefault: true # Specify whether the PowerScale OneFS API server's certificate chain and host name should be verified. # Allowed values: # true: skip OneFS API server's certificate verification # false: verify OneFS API server's certificates # Default value: default value specified in values.yaml # skipCertificateValidation: true # The base path for the volumes to be created on PowerScale cluster # This will be used if a storage class does not have the IsiPath parameter specified. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Default value: default value specified in values.yaml # Examples: \"/ifs/data/csi\", \"/ifs/engineering\" # isiPath: \"/ifs/data/csi\" # The permissions for isi volume directory path # This will be used if a storage class does not have the IsiVolumePathPermissions parameter specified. # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" # isiVolumePathPermissions: \"0777\" - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" endpointPort: \"8080\" Replace the values for the given keys as per your environment. After creating the secret.yaml, the following command can be used to create the secret,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml Use the following command to replace or update the secret\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f - Note: The user needs to validate the YAML syntax and array related key/values while replacing the isilon-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\nCreate isilon-certs-n secret. Please refer this section for creating cert-secrets.\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion: v1 kind: Secret metadata: name: isilon-certs-0 namespace: isilon type: Opaque data: cert-0: \"\" Execute command: kubectl create -f empty-secret.yaml\nInstall Driver Follow all the prerequisites above\nCreate a CR (Custom Resource) for PowerScale using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\nParameter Description Required Default dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet storageCapacity Enable/Disable storage capacity tracking feature No false Common parameters for node and controller CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock X_CSI_ISI_SKIP_CERTIFICATE_VALIDATION Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true X_CSI_ISI_PATH Base path for the volumes to be created Yes X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true X_CSI_ISI_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes X_CSI_ISI_VOLUME_PATH_PERMISSIONS The permissions for isi volume directory path Yes 0777 Controller parameters X_CSI_MODE Driver starting mode No controller X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes Node parameters X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes 0 X_CSI_MODE Driver starting mode No node Execute the following command to create PowerScale custom resource:\nkubectl create -f \u003cinput_sample_file.yaml\u003e This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\nVerify the CSI Driver installation\nNote :\n“Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. ","categories":"","description":"Installing Dell CSI Driver for PowerScale via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for PowerScale via Dell CSM Operator\n","ref":"/csm-docs/v1/deployment/csmoperator/drivers/powerscale/","tags":"","title":"PowerScale"},{"body":"Configuring PowerScale CSI Driver with CSM for Authorization Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow these steps to configure the CSI Drivers to work with the Authorization sidecar:\nApply the secret containing the token data into the driver namespace. It’s assumed that the Kubernetes administrator has the token secret manifest, generated by your storage administrator via Generate a Token, saved in /tmp/token.yaml.\n#It is assumed that array type powerscale has the namespace “isilon”.\nkubectl apply -f /tmp/token.yaml -n isilon Edit these parameters in samples/secret/karavi-authorization-config.json file in CSI PowerScale driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\nParameter Description Required Default username Username for connecting to the backend storage array. This parameter is ignored. No - password Password for connecting to to the backend storage array. This parameter is ignored. No - intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes - endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400 systemID Cluster name of the backend storage array. Yes \" \" skipCertificateValidation A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml Create the karavi-authorization-config secret using this command:\nkubectl -n isilon create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f - Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n isilon create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f - Otherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n isilon create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f - Prepare the driver configuration secret, applicable to your driver installation method, to communicate with the CSM Authorization sidecar.\nHelm\nRefer to the Install the Driver section to edit the parameters in samples/secret/secret.yaml file to configure the driver to communicate with the CSM Authorization sidecar.\nUpdate endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate mountEndpoint to the PowerScale OneFS API server. For example, 10.0.0.1.\nUpdate skipCertificateValidation to true.\nThe username and password can be any value since they will be ignored.\nExample:\nisilonClusters: - clusterName: \"cluster1\" username: \"ignored\" password: \"ignored\" isDefault: true endpoint: localhost endpointPort: 9400 mountEndpoint: 10.0.0.1 skipCertificateValidation: true Operator\nRefer to the Prerequisite section to prepare the secret.yaml file to configure the driver to communicate with the CSM Authorization sidecar.\nUpdate endpoint to match the localhost endpoint in samples/secret/karavi-authorization-config.json.\nUpdate mountEndpoint to the PowerScale OneFS API server. For example, 10.0.0.1.\nUpdate skipCertificateValidation to true.\nThe username and password can be any value since they will be ignored.\nExample:\nisilonClusters: - clusterName: \"cluster1\" username: \"ignored\" password: \"ignored\" isDefault: true endpoint: localhost endpointPort: 9400 mountEndpoint: 10.0.0.1 skipCertificateValidation: true Enable CSM Authorization in the driver installation applicable to your installation method.\nHelm\nRefer to the Install the Driver section to edit the parameters in my-isilon-settings.yaml file to enable CSM Authorization.\nUpdate authorization.enabled to true.\nUpdate authorization.sidecarProxyImage to the image of the CSM Authorization sidecar. In most cases, you can leave the default value.\nUpdate authorization.proxyHost to the hostname of the CSM Authorization Proxy Server.\nUpdate authorization.skipCertificateValidation to true or false depending on if you want to disable or enable certificate validation of the CSM Authorization Proxy Server.\nExample:\nauthorization: enabled: true # sidecarProxyImage: the container image used for the csm-authorization-sidecar. # Default value: dellemc/csm-authorization-sidecar:v1.7.0 sidecarProxyImage: dellemc/csm-authorization-sidecar:v1.7.0 # proxyHost: hostname of the csm-authorization server # Default value: None proxyHost: csm-authorization.com # skipCertificateValidation: certificate validation of the csm-authorization server # Allowed Values: # \"true\" - TLS certificate verification will be skipped # \"false\" - TLS certificate will be verified # Default value: \"true\" skipCertificateValidation: true Operator\nRefer to the Install Driver section to edit the parameters in the Custom Resource to enable CSM Authorization.\nUnder modules, enable the module named authorization:\nUpdate the enabled field to true.\nUpdate the image to the image of the CSM Authorization sidecar. In most cases, you can leave the default value.\nUpdate the PROXY_HOST environment value to the hostname of the CSM Authorization Proxy Server.\nUpdate the SKIP_CERTIFICATE_VALIDATION environment value to true or false depending on if you want to disable or enable certificate validation of the CSM Authorization Proxy Server.\nmodules: # Authorization: enable csm-authorization for RBAC - name: authorization # enable: Enable/Disable csm-authorization enabled: true configVersion: v1.7.0 components: - name: karavi-authorization-proxy image: dellemc/csm-authorization-sidecar:v1.6.0 envs: # proxyHost: hostname of the csm-authorization server - name: \"PROXY_HOST\" value: \"csm-authorization.com\" # skipCertificateValidation: Enable/Disable certificate validation of the csm-authorization server - name: \"SKIP_CERTIFICATE_VALIDATION\" value: \"true\" Install the Dell CSI PowerScale driver following the appropriate documenation for your installation method.\n(Optional) Install dellctl to perform Kubernetes administrator commands for additional capabilities (e.g., list volumes). Please refer to the dellctl documentation page for the installation steps and command list.\n","categories":"","description":"Enabling CSM Authorization for PowerScale CSI Driver\n","excerpt":"Enabling CSM Authorization for PowerScale CSI Driver\n","ref":"/csm-docs/v2/authorization/configuration/powerscale/","tags":"","title":"PowerScale"},{"body":"The CSI Driver for Dell PowerScale can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\nCSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\nCSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration Prerequisites The following are requirements to be met before installing the CSI Driver for Dell PowerScale:\nInstall Kubernetes or OpenShift (see supported versions) Install Helm 3 Mount propagation is enabled on container runtime that is being used nfs-utils package must be installed on nodes that will mount volumes If using Snapshot feature, satisfy all Volume Snapshot requirements If enabling CSM for Authorization, please refer to the Authorization deployment steps first If enabling CSM for Replication, please refer to the Replication deployment steps first If enabling CSM for Resiliency, please refer to the Resiliency deployment steps first If enabling Encryption, please refer to the Encryption deployment steps first Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerScale.\nSteps\nRun the command to install Helm 3.0.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash (Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\n(Optional) Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via helm.\nIf enabled capacity metrics (used \u0026 free capacity, used \u0026 free inodes) for PowerScale PV will be expose in Kubernetes metrics API.\nTo enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\ncontroller: healthMonitor: # enabled: Enable/Disable health monitor of CSI volumes # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: None enabled: false # interval: Interval of monitoring volume health condition # Allowed values: Number followed by unit (s,m,h) # Examples: 60s, 5m, 1h # Default value: 60s interval: 60s node: healthMonitor: # enabled: Enable/Disable health monitor of CSI volumes- volume usage, volume condition # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: None enabled: false (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication: enabled: true Replication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in the csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\nRun git clone -b v2.7.0 https://github.com/dell/csi-powerscale.git to clone the git repository.\nEnsure that you have created the namespace where you want to install the driver. You can run kubectl create namespace isilon to create a new one. The use of “isilon” as the namespace is just an example. You can choose any name for the namespace.\nCollect information from the PowerScale Systems like IP address, IsiPath, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml.\nCopy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation.\nEdit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\nParameter Description Required Default driverRepository Set to give the repository containing the driver image (used as part of the image name). Yes dellemc logLevel CSI driver log level No “debug” certSecretCount Defines the number of certificate secrets, which the user is going to create for SSL authentication. (isilon-cert-0..isilon-cert-(n-1)); Minimum value should be 1. Yes 1 allowedNetworks Defines the list of networks that can be used for NFS I/O traffic, CIDR format must be used. No [ ] maxIsilonVolumesPerNode Defines the default value for a maximum number of volumes that the controller can publish to the node. If the value is zero CO SHALL decide how many volumes of this type can be published by the controller to the node. This limit is applicable to all the nodes in the cluster for which node label ‘max-isilon-volumes-per-node’ is not set. Yes 0 imagePullPolicy Defines the policy to determine if the image should be pulled prior to starting the container Yes IfNotPresent verbose Indicates what content of the OneFS REST API message should be logged in debug level logs Yes 1 kubeletConfigDir Specify kubelet config dir path Yes “/var/lib/kubelet” enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish a connection to Array. This requires enableCustomTopology to be enabled. No false fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity.enabled Enable/Disable storage capacity tracking No true storageCapacity.pollInterval Configure how often the driver checks for changed capacity No 5m podmonAPIPort Defines the port which csi-driver will use within the cluster to support podmon No 8083 maxPathLen Defines the maximum length of path for a volume No 192 controller Configure controller pod specific parameters controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2 volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s” snapshot.enabled Enable/Disable volume snapshot feature Yes true snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot” resizer.enabled Enable/Disable volume expansion feature Yes true healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume status, volume condition Yes false healthMonitor.interval Interval of monitoring volume health condition Yes 60s nodeSelector Define node selection constraints for pods of controller deployment No tolerations Define tolerations for the controller deployment, if required No leader-election-lease-duration Duration, that non-leader candidates will wait to force acquire leadership No 20s leader-election-renew-deadline Duration, that the acting leader will retry refreshing leadership before giving up No 15s leader-election-retry-period Duration, the LeaderElector clients should wait between tries of actions No 5s node Configure node pod specific parameters nodeSelector Define node selection constraints for pods of node daemonset No tolerations Define tolerations for the node daemonset, if required No dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition Yes false PLATFORM ATTRIBUTES endpointPort Define the HTTPs port number of the PowerScale OneFS API server. If authorization is enabled, endpointPort should be the HTTPS localhost port that the authorization sidecar will listen on. This value acts as a default value for endpointPort, if not specified for a cluster config in secret. No 8080 skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. This value acts as a default value for skipCertificateValidation, if not specified for a cluster config in secret. No true isiAuthType Indicates the authentication method to be used. If set to 1 then it follows as session-based authentication else basic authentication No 0 isiAccessZone Define the name of the access zone a volume can be created in. If storageclass is missing with AccessZone parameter, then value of isiAccessZone is used for the same. No System enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. No true isiPath Define the base path for the volumes to be created on PowerScale cluster. This value acts as a default value for isiPath, if not specified for a cluster config in secret No /ifs/data/csi ignoreUnresolvableHosts Allows new host to add to existing export list though any of the existing hosts from the same exports are unresolvable/doesn’t exist anymore. No false noProbeOnStart Define whether the controller/node plugin should probe all the PowerScale clusters during driver initialization No false autoProbe Specify if automatically probe the PowerScale cluster if not done already during CSI calls No true authorization Authorization is an optional feature to apply credential shielding of the backend PowerScale. - - enabled A boolean that enables/disables authorization feature. No false sidecarProxyImage Image for csm-authorization-sidecar. No \" \" proxyHost Hostname of the csm-authorization server. No Empty skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization proxy server. No true podmon Podmon is an optional feature to enable application pods to be resilient to node failure. - - enabled A boolean that enables/disables podmon feature. No false image image for podmon. No \" \" encryption Encryption is an optional feature to apply encryption to CSI volumes. - - enabled A boolean that enables/disables Encryption feature. No false image Encryption driver image name. No “dellemc/csm-encryption:v0.3.0” NOTE:\nControllerCount parameter value must not exceed the number of nodes in the Kubernetes cluster. Otherwise, some of the controller pods remain in a “Pending” state till new nodes are available for scheduling. The installer exits with a WARNING on the same. Whenever the certSecretCount parameter changes in my-isilon-setting.yaml user needs to reinstall the driver. In order to enable authorization, there should be an authorization proxy server already installed. If you are using a custom image, check the version and driverRepository fields in my-isilon-setting.yaml to make sure that they are pointing to the correct image repository and driver version. These two fields are spliced together to form the image name, as shown here: /csi-isilon:v Edit following parameters in samples/secret/secret.yaml file and update/add connection/authentication information for one or more PowerScale clusters.\nParameter Description Required Default clusterName Logical name of PoweScale cluster against which volume CRUD operations are performed through this secret. Yes - username username for connecting to PowerScale OneFS API server Yes - password password for connecting to PowerScale OneFS API server Yes - endpoint HTTPS endpoint of the PowerScale OneFS API server. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes - isDefault Indicates if this is a default cluster (would be used by storage classes without ClusterName parameter). Only one of the cluster config should be marked as default. No false Optional parameters Following parameters are Optional. If specified will override default values from values.yaml. skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. No default value from values.yaml ignoreUnresolvableHosts Allows new host to add to existing export list though any of the existing hosts from the same exports are unresolvable/doesn’t exist anymore. No default value from values.yaml endpointPort Specify the HTTPs port number of the PowerScale OneFS API server No default value from values.yaml isiPath The base path for the volumes to be created on PowerScale cluster. Note: IsiPath parameter in storageclass, if present will override this attribute. No default value from values.yaml mountEndpoint Endpoint of the PowerScale OneFS API server, for example, 10.0.0.1. This must be specified if CSM-Authorization is enabled. No - User privileges The username specified in secret.yaml must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\nPrivilege Type ISI_PRIV_LOGIN_PAPI Read Only ISI_PRIV_NFS Read Write ISI_PRIV_QUOTA Read Write ISI_PRIV_SNAPSHOT Read Write ISI_PRIV_IFS_RESTORE Read Only ISI_PRIV_NS_IFS_ACCESS Read Only ISI_PRIV_IFS_BACKUP Read Only ISI_PRIV_SYNCIQ Read Write Create isilon-creds secret using the following command: kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\nNOTE:\nIf any key/value is present in all my-isilon-settings.yaml, secret, and storageClass, then the values provided in storageClass parameters take precedence. The user has to validate the yaml syntax and array-related key/values while replacing or appending the isilon-creds secret. The driver will continue to use previous values in case of an error found in the yaml file. For the key isiIP/endpoint, the user can give either IP address or FQDN. Also, the user can prefix ‘https’ (For example, https://192.168.1.1) with the value. The isilon-creds secret has a mountEndpoint parameter which should only be updated and used when Authorization is enabled. Install OneFS CA certificates by following the instructions from the next section, if you want to validate OneFS API server’s certificates. If not, create an empty secret using the following command and an empty secret must be created for the successful installation of CSI Driver for Dell PowerScale.\nkubectl create -f empty-secret.yaml This command will create a new secret called isilon-certs-0 in isilon namespace.\nInstall the driver using csi-install.sh bash script by running\ncd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace isilon --values ../helm/my-isilon-settings.yaml (assuming that the current working directory is ‘helm’ and my-isilon-settings.yaml is also present under ‘helm’ directory)\nCertificate validation for OneFS REST API calls The CSI driver exposes an install parameter ‘skipCertificateValidation’ which determines if the driver performs client-side verification of the OneFS certificates. The ‘skipCertificateValidation’ parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the ‘skipCertificateValidation’ is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the ‘skipCertificateValidation’ parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem To create the certs secret, run kubectl create secret generic isilon-certs-0 --from-file=cert-0=ca_cert_0.pem -n isilon Use the following command to replace the secret kubectl create secret generic isilon-certs-0 -n isilon --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - NOTES:\nThe OneFS IP can be with or without a port, depends upon the configuration of OneFS API server. The commands are based on the namespace ‘isilon’ It is highly recommended that ca_cert.pem file(s) having the naming convention as ca_cert_number.pem (example: ca_cert_0, ca_cert_1), where this number starts from 0 and grows as the number of OneFS arrays grows. The cert secret created out of these pem files must have the naming convention as isilon-certs-number (example: isilon-certs-0, isilon-certs-1, and so on.); The number must start from zero and must grow in incremental order. The number of the secrets created out of pem files should match certSecretCount value in myvalues.yaml or my-isilon-settings.yaml. Dynamic update of array details via secret.yaml CSI Driver for Dell PowerScale now provides supports for Multi cluster. Now users can link the single CSI Driver to multiple OneFS Clusters by updating secret.yaml. Users can now update the isilon-creds secret by editing the secret.yaml and executing the following command\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Note: Updating isilon-certs-x secrets is a manual process, unlike isilon-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nStorage Classes The CSI driver for Dell PowerScale version 1.5 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A sample storage class manifest is available at samples/storageclass/isilon.yaml. Use this sample manifest to create a storageclass to provision storage; uncomment/ update the manifest as per the requirements.\nWhat happens to my existing storage classes? Upgrading from CSI PowerScale v2.3 driver: The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nNOTE:\nAt least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es): Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create secondary storage class:\nThere are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\nCopy the storageclass.yaml to second_storageclass.yaml ( This is just an example, you can rename to file you require. ) Edit the second_storageclass.yaml yaml file and update following parameters: Update the name parameter to you require metadata: name: isilon-new Cluster name of 2nd array looks like this in the secret file.( Under /samples/secret/secret.yaml) - clusterName: \"cluster2\" username: \"user name\" password: \"Password\" endpoint: \"10.X.X.X\" endpointPort: \"8080 Use same clusterName ↑ in the second_storageclass.yaml # Optional: true ClusterName: \"cluster2\" Note: These are two essential parameters that you need to change in the “second_storageclass.yaml” file and other parameters that you change as required. Save the second_storageclass.yaml file\nCreate your 2nd storage class by using kubectl:\nkubectl create -f \u003cpath_to_second_storageclass_file\u003e Use newly created storage class isilon-new for volumes to spin up on cluster2\nPVC example\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: isilon-new Volume Snapshot Class Starting CSI PowerScale v1.6, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. Sample volume snapshot class manifests are available at samples/volumesnapshotclass/. Use these sample manifests to create a volumesnapshotclass for creating volume snapshots; uncomment/ update the manifests as per the requirements.\nSilent Mount Re-tries (v2.6.0) There are race conditions, when completing the ControllerPublish call to populate the client to volumes export list takes longer time than usual due to background NFS refresh process on OneFS wouldn’t have completed at same time, resulted in error:“mount failed” with initial attempts and might log success after few re-tries. This unnecessarily logs false positive “mount failed” error logs and to overcome this scenario Driver does silent mount re-tries attempts after every two sec. (five attempts max) for every NodePublish Call and allows successful mount within five re-tries without logging any mount error messages. “mount failed” will be logged once these five mount retrial attempts are exhausted and still client is not populated to export list.\nMount Re-tries handles below scenarios:\nAccess denied by server while mounting (NFSv3) No such file or directory (NFSv4) Sample:\nlevel=error clusterName=powerscale runid=10 msg=\"mount failed: exit status 32 mounting arguments: -t nfs -o rw XX.XX.XX.XX:/ifs/data/csi/k8s-ac7b91962d /var/lib/kubelet/pods/9f72096a-a7dc-4517-906c-20697f9d7375/volumes/kubernetes.io~csi/k8s-ac7b91962d/mount output: mount.nfs: access denied by server while mounting XX.XX.XX.XX:/ifs/data/csi/k8s-ac7b91962d ","categories":"","description":"Installing CSI Driver for PowerScale via Helm\n","excerpt":"Installing CSI Driver for PowerScale via Helm\n","ref":"/csm-docs/v2/csidriver/installation/helm/isilon/","tags":"","title":"PowerScale"},{"body":" The Dell CSI Operator is no longer actively maintained or supported. Dell CSI Operator has been replaced with Dell CSM Operator. If you are currently using Dell CSI Operator, refer to the operator migration documentation to migrate from Dell CSI Operator to Dell CSM Operator.\nInstalling CSI Driver for PowerScale via Operator The CSI Driver for Dell PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nNote: MKE (Mirantis Kubernetes Engine) does not support the installation of CSI-PowerScale via Operator.\nListing installed drivers with the CSI Isilon CRD User can query for CSI-PowerScale driver using the following command:\nkubectl get csiisilon --all-namespaces Install Driver Create namespace.\nExecute kubectl create namespace isilon to create the isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘isilon’.\nCreate isilon-creds secret by using secret.yaml file format only.\n2.1 Create a yaml file called secret.yaml with the following content:\nisilonClusters: # logical name of PowerScale Cluster - clusterName: \"cluster1\" # username for connecting to PowerScale OneFS API server # Default value: None username: \"user\" # password for connecting to PowerScale OneFS API server password: \"password\" # HTTPS endpoint of the PowerScale OneFS API server # Default value: None # Examples: \"1.2.3.4\", \"https://1.2.3.4\", \"https://abc.myonefs.com\" endpoint: \"1.2.3.4\" # Is this a default cluster (would be used by storage classes without ClusterName parameter) # Allowed values: # true: mark this cluster config as default # false: mark this cluster config as not default # Default value: false isDefault: true # Specify whether the PowerScale OneFS API server's certificate chain and host name should be verified. # Allowed values: # true: skip OneFS API server's certificate verification # false: verify OneFS API server's certificates # Default value: default value specified in values.yaml # skipCertificateValidation: true # The base path for the volumes to be created on PowerScale cluster # This will be used if a storage class does not have the IsiPath parameter specified. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Default value: default value specified in values.yaml # Examples: \"/ifs/data/csi\", \"/ifs/engineering\" # isiPath: \"/ifs/data/csi\" # The permissions for isi volume directory path # This will be used if a storage class does not have the IsiVolumePathPermissions parameter specified. # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" # isiVolumePathPermissions: \"0777\" - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" endpointPort: \"8080\" Replace the values for the given keys as per your environment. After creating the secret.yaml, the following command can be used to create the secret,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml Use the following command to replace or update the secret\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Note: The user needs to validate the YAML syntax and array related key/values while replacing the isilon-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\nCreate isilon-certs-n secret. Please refer this section for creating cert-secrets.\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion: v1 kind: Secret metadata: name: isilon-certs-0 namespace: isilon type: Opaque data: cert-0: \"\" Execute command:\nkubectl create -f empty-secret.yaml Create a CR (Custom Resource) for PowerScale using the sample files provided here.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\nParameter Description Required Default dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity Enable/Disable storage capacity tracking feature No true X_CSI_MAX_PATH_LIMIT Defines the maximum length of path for a volume No 192 Common parameters for node and controller CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock X_CSI_ISI_SKIP_CERTIFICATE_VALIDATION Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true X_CSI_ISI_PATH Base path for the volumes to be created Yes X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true X_CSI_ISI_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes X_CSI_ISI_VOLUME_PATH_PERMISSIONS The permissions for isi volume directory path Yes 0777 X_CSI_ISI_AUTH_TYPE Indicates the authentication method to be used. If set to 1 then it follows as session-based authentication else basic authentication No 0 Controller parameters X_CSI_MODE Driver starting mode No controller X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes nodeSelector Define node selection constraints for pods of controller deployment No X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin. Provides details of volume status and volume condition. As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar No false Node parameters X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes 0 X_CSI_MODE Driver starting mode No node X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from node plugin. Provides details of volume usage No false Side car parameters leader-election-lease-duration Duration, that non-leader candidates will wait to force acquire leadership No 20s leader-election-renew-deadline Duration, that the acting leader will retry refreshing leadership before giving up No 15s leader-election-retry-period Duration, the LeaderElector clients should wait between tries of actions No 5s Execute the following command to create PowerScale custom resource:\nkubectl create -f \u003cinput_sample_file.yaml\u003e This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\nNote :\nFrom CSI-PowerScale v1.6.0 and higher, Storage class and VolumeSnapshotClass will not be created as part of driver deployment. The user has to create Storageclass and Volume Snapshot Class. “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. Volume Health Monitoring This feature is introduced in CSI Driver for PowerScale version 2.1.0.\nOperator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" ","categories":"","description":"Installing CSI Driver for PowerScale via Operator\n","excerpt":"Installing CSI Driver for PowerScale via Operator\n","ref":"/csm-docs/v2/csidriver/installation/operator/isilon/","tags":"","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\nCreating a storage class:\nCreate a file storageclass.yaml using sample yaml file located at samples/storageclass/isilon.yaml. Update/uncomment the attributes in this sample file as per the requirements.\nExecute the following command to create a storage class:\nkubectl create -f $PWD/storageclass.yaml Result: After executing the above command storage class will be created in the default namespace, and the user can see the storage class by executing\nkubectl get sc Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at samples/persistentvolumeclaim/pvc.yaml\nExecute the following command to create volume:\nkubectl create -f $PWD/pvc.yaml Result: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing the command kubectl get pvc.\nNote: The status of the volume can be either Bound or Pending depending on the VolumeBindingMode specified on the storage class.\nAttach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at samples/pod/.\nExecute the following command to mount the volume to the Kubernetes node:\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, a new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for the host to be part of clients/rootclients field of export created for volume and used by nginx application.\nCreate Snapshot\nVolumeSnapshotClass is needed for creating the volume snapshots. Starting from v1.6, CSI Driver for PowerScale will not create any default Volume Snapshot class.\nSo the user has to create a volume snapshot class. The required sample files are present under samples/volumesnapshotclass/. Choose the file based on Kubernetes version.\nExecute either one of the following commands to create a volume snapshot class.\nkubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1.yaml` OR `kubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1beta1.yaml The above-said command will create a volume snapshotclass with the name isilon-snapclass.\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snapshot-of-test-pvc.yaml. The sample file for snapshot creation is located at samples/volumesnapshot/.\nExecute the following command to create snapshot:\nkubectl create -f samples/volumesnapshot/snapshot-of-test-pvc.yaml The spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is test-pvc, then the created snapshot is named snapshot-of-test-pvc. Verify the PowerScale system for the newly created snapshot.\nNote:\nUser can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. User has to make sure that the IsiPath in the parameters section of the volume snapshot class is matching with a corresponding storage class. Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in the spec dataSource field.\nThe sample file for volume creation from the snapshot is located at samples/persistentvolumeclaim/pvc-from-snapshot.yaml .\nExecute the following command to create a snapshot:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-snapshot.yaml Verify the PowerScale system for newly created volume from the snapshot.\nDelete Snapshot\nExecute the following commands to delete the snapshot:\nkubectl get volumesnapshot kubectl delete volumesnapshot snapshot-of-test-pvc Create a new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in the spec dataSource field.\nThe sample file for volume creation from volume is located at samples/persistentvolumeclaim/pvc-from-pvc.yaml\nExecute the following command to create a pvc from another pvc:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-pvc.yaml Verify the PowerScale system for newly created volume from volume.\nTo Unattach the volume from Host\nDelete the nginx application to Unattach the volume from the host:\nkubectl delete -f nginx.yaml To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc ","categories":"","description":"Tests to validate PowerScale CSI Driver installation","excerpt":"Tests to validate PowerScale CSI Driver installation","ref":"/csm-docs/v2/csidriver/installation/test/powerscale/","tags":"","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v2.7.0 New Features/Changes Allow user to set Quota limit parameters from the PVC request in CSI PowerScale CSI Spec 1.5: Storage capacity tracking feature Added support for Kubernetes 1.27 Added support for OpenShift 4.12 Migrated image registry from k8s.gcr.io to registry.k8s.io CSM Operator: Support install of Resiliency module Fixed Issues Known Issues Issue Resolution or workaround, if known If the length of the nodeID exceeds 128 characters, the driver fails to update the CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn’t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in the CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future Kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581 Note: In kubernetes 1.22 this limit has been relaxed to 192 characters. If some older NFS exports /terminated worker nodes still in NFS export client list, CSI driver tries to add a new worker node it fails (For RWX volume). User need to manually clean the export client list from old entries to make successful addition of new worker nodes. Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100 fsGroupPolicy may not work as expected without root privileges for NFS onlyhttps://github.com/kubernetes/examples/issues/260 To get the desired behavior set “RootClientEnabled” = “true” in the storage class parameter Driver logs shows “VendorVersion=2.3.0+dirty” Update the driver to csi-powerscale 2.4.0 Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for PowerScale CSI driver","excerpt":"Release notes for PowerScale CSI driver","ref":"/csm-docs/v2/csidriver/release/powerscale/","tags":"","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\nSymptoms Prevention, Resolution or Workaround The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret’s username and password for corresponding cluster The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn’t verify the certificates Check the isilon-certs- secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \"true\" for insecure connection. SSL validation is recommended in the production environment. The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver error: create volume failed, Access denied. create directory as requested This situation can happen when the user who created the base path is different from the user configured for the driver. Make sure the user used to deploy CSI-Driver must have enough rights on the base path (i.e. isiPath) to perform all operations. Volume/filesystem is allowed to mount by any host in the network, though that host is not a part of the export of that particular volume under /ifs directory “Dell PowerScale: OneFS NFS Design Considerations and Best Practices”: There is a default shared directory (ifs) of OneFS, which lets clients running Windows, UNIX, Linux, or Mac OS X access the same directories and files. It is recommended to disable the ifs shared directory in a production environment and create dedicated NFS exports and SMB shares for your workload. Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class is not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same. While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700. If the hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to driver version 1.4.0 or later there is a possibility of “localhost” as a stale entry in export Recommended setup: User should not map a hostname to loopback IP in /etc/hosts file Driver node pod is in “CrashLoopBackOff” as “Node ID” generated is not with proper FQDN. This might be due to “dnsPolicy” implemented on the driver node pod which may differ with different networks. This parameter is configurable in both helm and Operator installer and the user can try with different “dnsPolicy” according to the environment. The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver Authentication failed. Trying to re-authenticate when using Session-based authentication The issue has been resolved from OneFS 9.3 onwards, for OneFS versions prior to 9.3 for session-based authentication either smart connect can be created against a single node of Isilon or CSI Driver can be installed/pointed to a particular node of the Isilon else basic authentication can be used by setting isiAuthType in values.yaml to 0 When an attempt is made to create more than one ReadOnly PVC from the same volume snapshot, the second and subsequent requests result in PVCs in state Pending, with a warning another RO volume from this snapshot is already present. This is because the driver allows only one RO volume from a specific snapshot at any point in time. This is to allow faster creation(within a few seconds) of a RO PVC from a volume snapshot irrespective of the size of the volume snapshot. Wait for the deletion of the first RO PVC created from the same volume snapshot. While attaching a ReadOnly PVC from a volume snapshot to a pod, the mount operation will fail with error mounting ... failed, reason given by server: No such file or directory, if RO volume’s access zone(non System access zone) on Isilon is configured with a dedicated service IP(which is same as AzServiceIP storage class parameter). This operation results in accessing the snapshot base directory(/ifs) and results in overstepping the RO volume’s access zone’s base directory, which the OneFS doesn’t allow. Provide a service ip that belongs to RO volume’s access zone which set the highest level /ifs as its zone base directory. Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.22.0 \u003c 1.25.0 which is incompatible with Kubernetes V1.22.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. ","categories":"","description":"Troubleshooting PowerScale Driver","excerpt":"Troubleshooting PowerScale Driver","ref":"/csm-docs/v2/csidriver/troubleshooting/powerscale/","tags":"","title":"PowerScale"},{"body":"Installing CSI Driver for PowerScale via Dell CSM Operator The CSI Driver for Dell PowerScale can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using the following command:\nkubectl get csm --all-namespaces Prerequisite Create namespace. Execute kubectl create namespace isilon to create the isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘isilon’.\nCreate isilon-creds secret by creating a yaml file called secret.yaml with the following content:\nisilonClusters: # logical name of PowerScale Cluster - clusterName: \"cluster1\" # username for connecting to PowerScale OneFS API server # Default value: None username: \"user\" # password for connecting to PowerScale OneFS API server password: \"password\" # HTTPS endpoint of the PowerScale OneFS API server # Default value: None # Examples: \"1.2.3.4\", \"https://1.2.3.4\", \"https://abc.myonefs.com\" endpoint: \"1.2.3.4\" # Is this a default cluster (would be used by storage classes without ClusterName parameter) # Allowed values: # true: mark this cluster config as default # false: mark this cluster config as not default # Default value: false isDefault: true # Specify whether the PowerScale OneFS API server's certificate chain and host name should be verified. # Allowed values: # true: skip OneFS API server's certificate verification # false: verify OneFS API server's certificates # Default value: default value specified in values.yaml # skipCertificateValidation: true # The base path for the volumes to be created on PowerScale cluster # This will be used if a storage class does not have the IsiPath parameter specified. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Default value: default value specified in values.yaml # Examples: \"/ifs/data/csi\", \"/ifs/engineering\" # isiPath: \"/ifs/data/csi\" # The permissions for isi volume directory path # This will be used if a storage class does not have the IsiVolumePathPermissions parameter specified. # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" # isiVolumePathPermissions: \"0777\" - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" endpointPort: \"8080\" Replace the values for the given keys as per your environment. After creating the secret.yaml, the following command can be used to create the secret,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml Use the following command to replace or update the secret\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f - Note: The user needs to validate the YAML syntax and array related key/values while replacing the isilon-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\nCreate isilon-certs-n secret. Please refer this section for creating cert-secrets.\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion: v1 kind: Secret metadata: name: isilon-certs-0 namespace: isilon type: Opaque data: cert-0: \"\" Execute command: kubectl create -f empty-secret.yaml\nInstall Driver Follow all the prerequisites above\nCreate a CR (Custom Resource) for PowerScale using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\nParameter Description Required Default dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet storageCapacity Enable/Disable storage capacity tracking feature No false Common parameters for node and controller CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock X_CSI_ISI_SKIP_CERTIFICATE_VALIDATION Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true X_CSI_ISI_PATH Base path for the volumes to be created Yes X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true X_CSI_ISI_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes X_CSI_ISI_VOLUME_PATH_PERMISSIONS The permissions for isi volume directory path Yes 0777 Controller parameters X_CSI_MODE Driver starting mode No controller X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes Node parameters X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes 0 X_CSI_MODE Driver starting mode No node Execute the following command to create PowerScale custom resource:\nkubectl create -f \u003cinput_sample_file.yaml\u003e This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\nVerify the CSI Driver installation\nNote :\n“Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. ","categories":"","description":"Installing Dell CSI Driver for PowerScale via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for PowerScale via Dell CSM Operator\n","ref":"/csm-docs/v2/deployment/csmoperator/drivers/powerscale/","tags":"","title":"PowerScale"},{"body":"Configuring PowerScale CSI Driver with CSM for Authorization Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow these steps to configure the CSI Drivers to work with the Authorization sidecar:\nApply the secret containing the token data into the driver namespace. It’s assumed that the Kubernetes administrator has the token secret manifest saved in /tmp/token.yaml.\n# It is assumed that array type powerscale has the namespace \"isilon\". kubectl apply -f /tmp/token.yaml -n isilon Edit these parameters in samples/secret/karavi-authorization-config.json file in CSI PowerScale driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\nParameter Description Required Default username Username for connecting to the backend storage array. This parameter is ignored. No - password Password for connecting to to the backend storage array. This parameter is ignored. No - intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes - endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400 systemID System ID of the backend storage array. Yes \" \" skipCertificateValidation A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml Create the karavi-authorization-config secret using this command:\nkubectl -n isilon create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f -\nNote:\nCreate the driver secret as you would normally except update/add the connection information for communicating with the sidecar instead of the backend storage array and scrub the username and password The systemID will be the clusterName of the array. The isilon-creds secret has a mountEndpoint parameter which must be set to the hostname or IP address of the PowerScale OneFS API server, for example, 10.0.0.1. Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n isilon create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f -\nOtherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n isilon create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f -\nPlease refer to step 5 in the installation steps for PowerScale to edit the parameters in my-isilon-settings.yaml to communicate with the sidecar.\nUpdate endpointPort to match the endpoint port number in samples/secret/karavi-authorization-config.json\nNotes:\nIn my-isilon-settings.yaml, endpointPort acts as a default value. If endpointPort is not specified in my-isilon-settings.yaml, then it should be specified in the endpoint parameter of samples/secret/secret.yaml. Enable CSM for Authorization and provide the proxyHost address\nPlease refer to step 6 in the installation steps for PowerScale to edit the parameters in samples/secret/secret.yaml file to communicate with the sidecar.\nUpdate endpoint to match the endpoint in samples/secret/karavi-authorization-config.json\nNotes:\nOnly add the endpoint port if it has not been set in my-isilon-settings.yaml. The isilon-creds secret has a mountEndpoint parameter which must be set to the hostname or IP address of the PowerScale OneFS API server, for example, 10.0.0.1. Create the isilon-creds secret using this command:\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl apply -f -\nInstall the CSI PowerScale driver\n","categories":"","description":"Enabling CSM Authorization for PowerScale CSI Driver\n","excerpt":"Enabling CSM Authorization for PowerScale CSI Driver\n","ref":"/csm-docs/v3/authorization/configuration/powerscale/","tags":"","title":"PowerScale"},{"body":"The CSI Driver for Dell PowerScale can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\nCSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\nCSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration Prerequisites The following are requirements to be met before installing the CSI Driver for Dell PowerScale:\nInstall Kubernetes or OpenShift (see supported versions) Install Helm 3 Mount propagation is enabled on container runtime that is being used nfs-utils package must be installed on nodes that will mount volumes If using Snapshot feature, satisfy all Volume Snapshot requirements If enabling CSM for Authorization, please refer to the Authorization deployment steps first If enabling CSM for Replication, please refer to the Replication deployment steps first If enabling CSM for Resiliency, please refer to the Resiliency deployment steps first If enabling Encryption, please refer to the Encryption deployment steps first Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerScale.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\n(Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\n(Optional) Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via helm.\nIf enabled capacity metrics (used \u0026 free capacity, used \u0026 free inodes) for PowerScale PV will be expose in Kubernetes metrics API.\nTo enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\ncontroller: healthMonitor: # enabled: Enable/Disable health monitor of CSI volumes # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: None enabled: false # interval: Interval of monitoring volume health condition # Allowed values: Number followed by unit (s,m,h) # Examples: 60s, 5m, 1h # Default value: 60s interval: 60s node: healthMonitor: # enabled: Enable/Disable health monitor of CSI volumes- volume usage, volume condition # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: None enabled: false (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication: enabled: true Replication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in the csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\nRun git clone -b v2.6.1 https://github.com/dell/csi-powerscale.git to clone the git repository.\nEnsure that you have created the namespace where you want to install the driver. You can run kubectl create namespace isilon to create a new one. The use of “isilon” as the namespace is just an example. You can choose any name for the namespace.\nCollect information from the PowerScale Systems like IP address, IsiPath, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml.\nCopy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation.\nEdit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\nParameter Description Required Default driverRepository Set to give the repository containing the driver image (used as part of the image name). Yes dellemc logLevel CSI driver log level No “debug” certSecretCount Defines the number of certificate secrets, which the user is going to create for SSL authentication. (isilon-cert-0..isilon-cert-(n-1)); Minimum value should be 1. Yes 1 allowedNetworks Defines the list of networks that can be used for NFS I/O traffic, CIDR format must be used. No [ ] maxIsilonVolumesPerNode Defines the default value for a maximum number of volumes that the controller can publish to the node. If the value is zero CO SHALL decide how many volumes of this type can be published by the controller to the node. This limit is applicable to all the nodes in the cluster for which node label ‘max-isilon-volumes-per-node’ is not set. Yes 0 imagePullPolicy Defines the policy to determine if the image should be pulled prior to starting the container Yes IfNotPresent verbose Indicates what content of the OneFS REST API message should be logged in debug level logs Yes 1 kubeletConfigDir Specify kubelet config dir path Yes “/var/lib/kubelet” enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish a connection to Array. This requires enableCustomTopology to be enabled. No false fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” podmonAPIPort Defines the port which csi-driver will use within the cluster to support podmon No 8083 maxPathLen Defines the maximum length of path for a volume No 192 controller Configure controller pod specific parameters controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2 volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s” snapshot.enabled Enable/Disable volume snapshot feature Yes true snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot” resizer.enabled Enable/Disable volume expansion feature Yes true healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume status, volume condition Yes false healthMonitor.interval Interval of monitoring volume health condition Yes 60s nodeSelector Define node selection constraints for pods of controller deployment No tolerations Define tolerations for the controller deployment, if required No leader-election-lease-duration Duration, that non-leader candidates will wait to force acquire leadership No 20s leader-election-renew-deadline Duration, that the acting leader will retry refreshing leadership before giving up No 15s leader-election-retry-period Duration, the LeaderElector clients should wait between tries of actions No 5s node Configure node pod specific parameters nodeSelector Define node selection constraints for pods of node daemonset No tolerations Define tolerations for the node daemonset, if required No dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition Yes false PLATFORM ATTRIBUTES endpointPort Define the HTTPs port number of the PowerScale OneFS API server. If authorization is enabled, endpointPort should be the HTTPS localhost port that the authorization sidecar will listen on. This value acts as a default value for endpointPort, if not specified for a cluster config in secret. No 8080 skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. This value acts as a default value for skipCertificateValidation, if not specified for a cluster config in secret. No true isiAuthType Indicates the authentication method to be used. If set to 1 then it follows as session-based authentication else basic authentication No 0 isiAccessZone Define the name of the access zone a volume can be created in. If storageclass is missing with AccessZone parameter, then value of isiAccessZone is used for the same. No System enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. No true isiPath Define the base path for the volumes to be created on PowerScale cluster. This value acts as a default value for isiPath, if not specified for a cluster config in secret No /ifs/data/csi ignoreUnresolvableHosts Allows new host to add to existing export list though any of the existing hosts from the same exports are unresolvable/doesn’t exist anymore. No false noProbeOnStart Define whether the controller/node plugin should probe all the PowerScale clusters during driver initialization No false autoProbe Specify if automatically probe the PowerScale cluster if not done already during CSI calls No true authorization Authorization is an optional feature to apply credential shielding of the backend PowerScale. - - enabled A boolean that enables/disables authorization feature. No false sidecarProxyImage Image for csm-authorization-sidecar. No \" \" proxyHost Hostname of the csm-authorization server. No Empty skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. No true podmon Podmon is an optional feature to enable application pods to be resilient to node failure. - - enabled A boolean that enables/disables podmon feature. No false image image for podmon. No \" \" encryption Encryption is an optional feature to apply encryption to CSI volumes. - - enabled A boolean that enables/disables Encryption feature. No false image Encryption driver image name. No “dellemc/csm-encryption:v0.3.0” NOTE:\nControllerCount parameter value must not exceed the number of nodes in the Kubernetes cluster. Otherwise, some of the controller pods remain in a “Pending” state till new nodes are available for scheduling. The installer exits with a WARNING on the same. Whenever the certSecretCount parameter changes in my-isilon-setting.yaml user needs to reinstall the driver. In order to enable authorization, there should be an authorization proxy server already installed. If you are using a custom image, check the version and driverRepository fields in my-isilon-setting.yaml to make sure that they are pointing to the correct image repository and driver version. These two fields are spliced together to form the image name, as shown here: /csi-isilon:v Edit following parameters in samples/secret/secret.yaml file and update/add connection/authentication information for one or more PowerScale clusters.\nParameter Description Required Default clusterName Logical name of PoweScale cluster against which volume CRUD operations are performed through this secret. Yes - username username for connecting to PowerScale OneFS API server Yes - password password for connecting to PowerScale OneFS API server Yes - endpoint HTTPS endpoint of the PowerScale OneFS API server. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes - isDefault Indicates if this is a default cluster (would be used by storage classes without ClusterName parameter). Only one of the cluster config should be marked as default. No false Optional parameters Following parameters are Optional. If specified will override default values from values.yaml. skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. No default value from values.yaml ignoreUnresolvableHosts Allows new host to add to existing export list though any of the existing hosts from the same exports are unresolvable/doesn’t exist anymore. No default value from values.yaml endpointPort Specify the HTTPs port number of the PowerScale OneFS API server No default value from values.yaml isiPath The base path for the volumes to be created on PowerScale cluster. Note: IsiPath parameter in storageclass, if present will override this attribute. No default value from values.yaml mountEndpoint Endpoint of the PowerScale OneFS API server, for example, 10.0.0.1. This must be specified if CSM-Authorization is enabled. No - User privileges The username specified in secret.yaml must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\nPrivilege Type ISI_PRIV_LOGIN_PAPI Read Only ISI_PRIV_NFS Read Write ISI_PRIV_QUOTA Read Write ISI_PRIV_SNAPSHOT Read Write ISI_PRIV_IFS_RESTORE Read Only ISI_PRIV_NS_IFS_ACCESS Read Only ISI_PRIV_IFS_BACKUP Read Only ISI_PRIV_SYNCIQ Read Write Create isilon-creds secret using the following command: kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\nNOTE:\nIf any key/value is present in all my-isilon-settings.yaml, secret, and storageClass, then the values provided in storageClass parameters take precedence. The user has to validate the yaml syntax and array-related key/values while replacing or appending the isilon-creds secret. The driver will continue to use previous values in case of an error found in the yaml file. For the key isiIP/endpoint, the user can give either IP address or FQDN. Also, the user can prefix ‘https’ (For example, https://192.168.1.1) with the value. The isilon-creds secret has a mountEndpoint parameter which should only be updated and used when Authorization is enabled. Install OneFS CA certificates by following the instructions from the next section, if you want to validate OneFS API server’s certificates. If not, create an empty secret using the following command and an empty secret must be created for the successful installation of CSI Driver for Dell PowerScale.\nkubectl create -f empty-secret.yaml This command will create a new secret called isilon-certs-0 in isilon namespace.\nInstall the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace isilon --values ../helm/my-isilon-settings.yaml (assuming that the current working directory is ‘helm’ and my-isilon-settings.yaml is also present under ‘helm’ directory)\nCertificate validation for OneFS REST API calls The CSI driver exposes an install parameter ‘skipCertificateValidation’ which determines if the driver performs client-side verification of the OneFS certificates. The ‘skipCertificateValidation’ parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the ‘skipCertificateValidation’ is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the ‘skipCertificateValidation’ parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem To create the certs secret, run kubectl create secret generic isilon-certs-0 --from-file=cert-0=ca_cert_0.pem -n isilon Use the following command to replace the secret kubectl create secret generic isilon-certs-0 -n isilon --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - NOTES:\nThe OneFS IP can be with or without a port, depends upon the configuration of OneFS API server. The commands are based on the namespace ‘isilon’ It is highly recommended that ca_cert.pem file(s) having the naming convention as ca_cert_number.pem (example: ca_cert_0, ca_cert_1), where this number starts from 0 and grows as the number of OneFS arrays grows. The cert secret created out of these pem files must have the naming convention as isilon-certs-number (example: isilon-certs-0, isilon-certs-1, and so on.); The number must start from zero and must grow in incremental order. The number of the secrets created out of pem files should match certSecretCount value in myvalues.yaml or my-isilon-settings.yaml. Dynamic update of array details via secret.yaml CSI Driver for Dell PowerScale now provides supports for Multi cluster. Now users can link the single CSI Driver to multiple OneFS Clusters by updating secret.yaml. Users can now update the isilon-creds secret by editing the secret.yaml and executing the following command\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f -\nNote: Updating isilon-certs-x secrets is a manual process, unlike isilon-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nStorage Classes The CSI driver for Dell PowerScale version 1.5 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A sample storage class manifest is available at samples/storageclass/isilon.yaml. Use this sample manifest to create a storageclass to provision storage; uncomment/ update the manifest as per the requirements.\nWhat happens to my existing storage classes? Upgrading from CSI PowerScale v2.3 driver: The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nNOTE:\nAt least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es): Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create secondary storage class:\nThere are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\nCopy the storageclass.yaml to second_storageclass.yaml ( This is just an example, you can rename to file you require. ) Edit the second_storageclass.yaml yaml file and update following parameters: Update the name parameter to you require\nmetadata: name: isilon-new Cluster name of 2nd array looks like this in the secret file.( Under /samples/secret/secret.yaml)\n- clusterName: \"cluster2\" username: \"user name\" password: \"Password\" endpoint: \"10.X.X.X\" endpointPort: \"8080 Use same clusterName ↑ in the second_storageclass.yaml\n# Optional: true ClusterName: \"cluster2\" Note: These are two essential parameters that you need to change in the “second_storageclass.yaml” file and other parameters that you change as required.\nSave the second_storageclass.yaml file Create your 2nd storage class by using kubectl: kubectl create -f \u003cpath_to_second_storageclass_file\u003e Use newly created storage class isilon-new for volumes to spin up on cluster2\nPVC example\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: isilon-new Volume Snapshot Class Starting CSI PowerScale v1.6, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. Sample volume snapshot class manifests are available at samples/volumesnapshotclass/. Use these sample manifests to create a volumesnapshotclass for creating volume snapshots; uncomment/ update the manifests as per the requirements.\nSilent Mount Re-tries (v2.6.0) There are race conditions, when completing the ControllerPublish call to populate the client to volumes export list takes longer time than usual due to background NFS refresh process on OneFS wouldn’t have completed at same time, resulted in error:“mount failed” with initial attempts and might log success after few re-tries. This unnecessarily logs false positive “mount failed” error logs and to overcome this scenario Driver does silent mount re-tries attempts after every two sec. (five attempts max) for every NodePublish Call and allows successful mount within five re-tries without logging any mount error messages. “mount failed” will be logged once these five mount retrial attempts are exhausted and still client is not populated to export list.\nMount Re-tries handles below scenarios:\nAccess denied by server while mounting (NFSv3) No such file or directory (NFSv4) Sample:\nlevel=error clusterName=powerscale runid=10 msg=\"mount failed: exit status 32 mounting arguments: -t nfs -o rw XX.XX.XX.XX:/ifs/data/csi/k8s-ac7b91962d /var/lib/kubelet/pods/9f72096a-a7dc-4517-906c-20697f9d7375/volumes/kubernetes.io~csi/k8s-ac7b91962d/mount output: mount.nfs: access denied by server while mounting XX.XX.XX.XX:/ifs/data/csi/k8s-ac7b91962d ","categories":"","description":"Installing CSI Driver for PowerScale via Helm\n","excerpt":"Installing CSI Driver for PowerScale via Helm\n","ref":"/csm-docs/v3/csidriver/installation/helm/isilon/","tags":"","title":"PowerScale"},{"body":"Installing CSI Driver for PowerScale via Operator The CSI Driver for Dell PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nNote: MKE (Mirantis Kubernetes Engine) does not support the installation of CSI-PowerScale via Operator.\nListing installed drivers with the CSI Isilon CRD User can query for CSI-PowerScale driver using the following command: kubectl get csiisilon --all-namespaces\nInstall Driver Create namespace.\nExecute kubectl create namespace isilon to create the isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘isilon’.\nCreate isilon-creds secret by using secret.yaml file format only.\n2.1 Create a yaml file called secret.yaml with the following content:\nisilonClusters: # logical name of PowerScale Cluster - clusterName: \"cluster1\" # username for connecting to PowerScale OneFS API server # Default value: None username: \"user\" # password for connecting to PowerScale OneFS API server password: \"password\" # HTTPS endpoint of the PowerScale OneFS API server # Default value: None # Examples: \"1.2.3.4\", \"https://1.2.3.4\", \"https://abc.myonefs.com\" endpoint: \"1.2.3.4\" # Is this a default cluster (would be used by storage classes without ClusterName parameter) # Allowed values: # true: mark this cluster config as default # false: mark this cluster config as not default # Default value: false isDefault: true # Specify whether the PowerScale OneFS API server's certificate chain and host name should be verified. # Allowed values: # true: skip OneFS API server's certificate verification # false: verify OneFS API server's certificates # Default value: default value specified in values.yaml # skipCertificateValidation: true # The base path for the volumes to be created on PowerScale cluster # This will be used if a storage class does not have the IsiPath parameter specified. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Default value: default value specified in values.yaml # Examples: \"/ifs/data/csi\", \"/ifs/engineering\" # isiPath: \"/ifs/data/csi\" # The permissions for isi volume directory path # This will be used if a storage class does not have the IsiVolumePathPermissions parameter specified. # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" # isiVolumePathPermissions: \"0777\" - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" endpointPort: \"8080\" Replace the values for the given keys as per your environment. After creating the secret.yaml, the following command can be used to create the secret, kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\nUse the following command to replace or update the secret\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f -\nNote: The user needs to validate the YAML syntax and array related key/values while replacing the isilon-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\nCreate isilon-certs-n secret. Please refer this section for creating cert-secrets.\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion: v1 kind: Secret metadata: name: isilon-certs-0 namespace: isilon type: Opaque data: cert-0: \"\" Execute command: kubectl create -f empty-secret.yaml\nCreate a CR (Custom Resource) for PowerScale using the sample files provided here.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\nParameter Description Required Default dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” X_CSI_MAX_PATH_LIMIT Defines the maximum length of path for a volume No 192 Common parameters for node and controller CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock X_CSI_ISI_SKIP_CERTIFICATE_VALIDATION Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true X_CSI_ISI_PATH Base path for the volumes to be created Yes X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true X_CSI_ISI_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes X_CSI_ISI_VOLUME_PATH_PERMISSIONS The permissions for isi volume directory path Yes 0777 X_CSI_ISI_AUTH_TYPE Indicates the authentication method to be used. If set to 1 then it follows as session-based authentication else basic authentication No 0 Controller parameters X_CSI_MODE Driver starting mode No controller X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes nodeSelector Define node selection constraints for pods of controller deployment No X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin. Provides details of volume status and volume condition. As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar No false Node parameters X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes 0 X_CSI_MODE Driver starting mode No node X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from node plugin. Provides details of volume usage No false Side car parameters leader-election-lease-duration Duration, that non-leader candidates will wait to force acquire leadership No 20s leader-election-renew-deadline Duration, that the acting leader will retry refreshing leadership before giving up No 15s leader-election-retry-period Duration, the LeaderElector clients should wait between tries of actions No 5s Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\nNote :\nFrom CSI-PowerScale v1.6.0 and higher, Storage class and VolumeSnapshotClass will not be created as part of driver deployment. The user has to create Storageclass and Volume Snapshot Class. “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. Volume Health Monitoring This feature is introduced in CSI Driver for PowerScale version 2.1.0.\nOperator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" ","categories":"","description":"Installing CSI Driver for PowerScale via Operator\n","excerpt":"Installing CSI Driver for PowerScale via Operator\n","ref":"/csm-docs/v3/csidriver/installation/operator/isilon/","tags":"","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\nCreating a storage class:\nCreate a file storageclass.yaml using sample yaml file located at samples/storageclass/isilon.yaml. Update/uncomment the attributes in this sample file as per the requirements.\nExecute the following command to create a storage class:\nkubectl create -f $PWD/storageclass.yaml Result: After executing the above command storage class will be created in the default namespace, and the user can see the storage class by executing kubectl get sc.\nCreating a volume:\nCreate a file pvc.yaml using sample yaml files located at samples/persistentvolumeclaim/pvc.yaml\nExecute the following command to create volume:\nkubectl create -f $PWD/pvc.yaml\nResult: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing the command kubectl get pvc.\nNote: The status of the volume can be either Bound or Pending depending on the VolumeBindingMode specified on the storage class.\nAttach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at samples/pod/.\nExecute the following command to mount the volume to the Kubernetes node:\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, a new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for the host to be part of clients/rootclients field of export created for volume and used by nginx application.\nCreate Snapshot\nVolumeSnapshotClass is needed for creating the volume snapshots. Starting from v1.6, CSI Driver for PowerScale will not create any default Volume Snapshot class.\nSo the user has to create a volume snapshot class. The required sample files are present under samples/volumesnapshotclass/. Choose the file based on Kubernetes version.\nExecute either one of the following commands to create a volume snapshot class.\nkubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1.yaml OR kubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1beta1.yaml\nThe above-said command will create a volume snapshotclass with the name isilon-snapclass.\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snapshot-of-test-pvc.yaml. The sample file for snapshot creation is located at samples/volumesnapshot/.\nExecute the following command to create snapshot:\nkubectl create -f samples/volumesnapshot/snapshot-of-test-pvc.yaml\nThe spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is test-pvc, then the created snapshot is named snapshot-of-test-pvc. Verify the PowerScale system for the newly created snapshot.\nNote:\nUser can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. User has to make sure that the IsiPath in the parameters section of the volume snapshot class is matching with a corresponding storage class. Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in the spec dataSource field.\nThe sample file for volume creation from the snapshot is located at samples/persistentvolumeclaim/pvc-from-snapshot.yaml .\nExecute the following command to create a snapshot:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-snapshot.yaml Verify the PowerScale system for newly created volume from the snapshot.\nDelete Snapshot\nExecute the following commands to delete the snapshot:\nkubectl get volumesnapshot kubectl delete volumesnapshot snapshot-of-test-pvc Create a new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in the spec dataSource field.\nThe sample file for volume creation from volume is located at samples/persistentvolumeclaim/pvc-from-pvc.yaml\nExecute the following command to create a pvc from another pvc:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-pvc.yaml Verify the PowerScale system for newly created volume from volume.\nTo Unattach the volume from Host\nDelete the nginx application to Unattach the volume from the host:\nkubectl delete -f nginx.yaml\nTo delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc ","categories":"","description":"Tests to validate PowerScale CSI Driver installation","excerpt":"Tests to validate PowerScale CSI Driver installation","ref":"/csm-docs/v3/csidriver/installation/test/powerscale/","tags":"","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v2.6.1 New Features/Changes Added support for Kubernetes 1.26 Added support for Ubuntu 22.04 Added support for MKE 3.6.x Added support for RKE 1.4.1 Fixed Issues Github ID Description 753 Replication: Incorrect quota set on the target PV/directory when Quota is enabled. Known Issues Issue Resolution or workaround, if known If the length of the nodeID exceeds 128 characters, the driver fails to update the CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn’t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in the CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future Kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581 Note: In kubernetes 1.22 this limit has been relaxed to 192 characters. If some older NFS exports /terminated worker nodes still in NFS export client list, CSI driver tries to add a new worker node it fails (For RWX volume). User need to manually clean the export client list from old entries to make successful addition of new worker nodes. Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100 fsGroupPolicy may not work as expected without root privileges for NFS onlyhttps://github.com/kubernetes/examples/issues/260 To get the desired behavior set “RootClientEnabled” = “true” in the storage class parameter Driver logs shows “VendorVersion=2.3.0+dirty” Update the driver to csi-powerscale 2.4.0 Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for PowerScale CSI driver","excerpt":"Release notes for PowerScale CSI driver","ref":"/csm-docs/v3/csidriver/release/powerscale/","tags":"","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\nSymptoms Prevention, Resolution or Workaround The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret’s username and password for corresponding cluster The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn’t verify the certificates Check the isilon-certs- secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \"true\" for insecure connection. SSL validation is recommended in the production environment. The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver error: create volume failed, Access denied. create directory as requested This situation can happen when the user who created the base path is different from the user configured for the driver. Make sure the user used to deploy CSI-Driver must have enough rights on the base path (i.e. isiPath) to perform all operations. Volume/filesystem is allowed to mount by any host in the network, though that host is not a part of the export of that particular volume under /ifs directory “Dell PowerScale: OneFS NFS Design Considerations and Best Practices”: There is a default shared directory (ifs) of OneFS, which lets clients running Windows, UNIX, Linux, or Mac OS X access the same directories and files. It is recommended to disable the ifs shared directory in a production environment and create dedicated NFS exports and SMB shares for your workload. Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class is not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same. While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700. If the hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to driver version 1.4.0 or later there is a possibility of “localhost” as a stale entry in export Recommended setup: User should not map a hostname to loopback IP in /etc/hosts file Driver node pod is in “CrashLoopBackOff” as “Node ID” generated is not with proper FQDN. This might be due to “dnsPolicy” implemented on the driver node pod which may differ with different networks. This parameter is configurable in both helm and Operator installer and the user can try with different “dnsPolicy” according to the environment. The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver Authentication failed. Trying to re-authenticate when using Session-based authentication The issue has been resolved from OneFS 9.3 onwards, for OneFS versions prior to 9.3 for session-based authentication either smart connect can be created against a single node of Isilon or CSI Driver can be installed/pointed to a particular node of the Isilon else basic authentication can be used by setting isiAuthType in values.yaml to 0 When an attempt is made to create more than one ReadOnly PVC from the same volume snapshot, the second and subsequent requests result in PVCs in state Pending, with a warning another RO volume from this snapshot is already present. This is because the driver allows only one RO volume from a specific snapshot at any point in time. This is to allow faster creation(within a few seconds) of a RO PVC from a volume snapshot irrespective of the size of the volume snapshot. Wait for the deletion of the first RO PVC created from the same volume snapshot. While attaching a ReadOnly PVC from a volume snapshot to a pod, the mount operation will fail with error mounting ... failed, reason given by server: No such file or directory, if RO volume’s access zone(non System access zone) on Isilon is configured with a dedicated service IP(which is same as AzServiceIP storage class parameter). This operation results in accessing the snapshot base directory(/ifs) and results in overstepping the RO volume’s access zone’s base directory, which the OneFS doesn’t allow. Provide a service ip that belongs to RO volume’s access zone which set the highest level /ifs as its zone base directory. Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.22.0 \u003c 1.25.0 which is incompatible with Kubernetes V1.22.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. ","categories":"","description":"Troubleshooting PowerScale Driver","excerpt":"Troubleshooting PowerScale Driver","ref":"/csm-docs/v3/csidriver/troubleshooting/powerscale/","tags":"","title":"PowerScale"},{"body":"Installing CSI Driver for PowerScale via Dell CSM Operator The CSI Driver for Dell PowerScale can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nNote: MKE (Mirantis Kubernetes Engine) does not support the installation of CSI-PowerScale via Operator.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using the following command: kubectl get csm --all-namespaces\nPrerequisite Create namespace. Execute kubectl create namespace isilon to create the isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘isilon’.\nCreate isilon-creds secret by creating a yaml file called secret.yaml with the following content:\nisilonClusters: # logical name of PowerScale Cluster - clusterName: \"cluster1\" # username for connecting to PowerScale OneFS API server # Default value: None username: \"user\" # password for connecting to PowerScale OneFS API server password: \"password\" # HTTPS endpoint of the PowerScale OneFS API server # Default value: None # Examples: \"1.2.3.4\", \"https://1.2.3.4\", \"https://abc.myonefs.com\" endpoint: \"1.2.3.4\" # Is this a default cluster (would be used by storage classes without ClusterName parameter) # Allowed values: # true: mark this cluster config as default # false: mark this cluster config as not default # Default value: false isDefault: true # Specify whether the PowerScale OneFS API server's certificate chain and host name should be verified. # Allowed values: # true: skip OneFS API server's certificate verification # false: verify OneFS API server's certificates # Default value: default value specified in values.yaml # skipCertificateValidation: true # The base path for the volumes to be created on PowerScale cluster # This will be used if a storage class does not have the IsiPath parameter specified. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Default value: default value specified in values.yaml # Examples: \"/ifs/data/csi\", \"/ifs/engineering\" # isiPath: \"/ifs/data/csi\" # The permissions for isi volume directory path # This will be used if a storage class does not have the IsiVolumePathPermissions parameter specified. # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" # isiVolumePathPermissions: \"0777\" - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" endpointPort: \"8080\" Replace the values for the given keys as per your environment. After creating the secret.yaml, the following command can be used to create the secret, kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\nUse the following command to replace or update the secret\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the YAML syntax and array related key/values while replacing the isilon-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\nCreate isilon-certs-n secret. Please refer this section for creating cert-secrets.\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion: v1 kind: Secret metadata: name: isilon-certs-0 namespace: isilon type: Opaque data: cert-0: \"\" Execute command: kubectl create -f empty-secret.yaml\nInstall Driver Follow all the prerequisites above\nCreate a CR (Custom Resource) for PowerScale using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\nParameter Description Required Default dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet Common parameters for node and controller CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock X_CSI_ISI_SKIP_CERTIFICATE_VALIDATION Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true X_CSI_ISI_PATH Base path for the volumes to be created Yes X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true X_CSI_ISI_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes X_CSI_ISI_VOLUME_PATH_PERMISSIONS The permissions for isi volume directory path Yes 0777 Controller parameters X_CSI_MODE Driver starting mode No controller X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes Node parameters X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes 0 X_CSI_MODE Driver starting mode No node Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\nVerify the CSI Driver installation\nNote :\n“Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. ","categories":"","description":"Installing Dell CSI Driver for PowerScale via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for PowerScale via Dell CSM Operator\n","ref":"/csm-docs/v3/deployment/csmoperator/drivers/powerscale/","tags":"","title":"PowerScale"},{"body":"The CSI Driver for Dell PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nPrerequisites The following are requirements to be met before installing the CSI Driver for Dell PowerStore:\nInstall Kubernetes or OpenShift (see supported versions) Install Helm 3.x If you plan to use either the Fibre Channel or iSCSI or NVMe/TCP or NVMe/FC protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator or Set up the NVMe Initiator sections below. You can use NFS volumes without FC or iSCSI or NVMe/TCP or NVMe/FC configuration. You can use either the Fibre Channel or iSCSI or NVMe/TCP or NVMe/FC protocol, but you do not need all the four.\nIf you want to use preconfigured iSCSI/FC hosts be sure to check that they are not part of any host group\nLinux native multipathing requirements Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements Nonsecure registries are defined in Docker or other container runtimes, for CSI drivers that are hosted in a non-secure location. You can access your cluster with kubectl and helm. Ensure that your nodes support mounting NFS volumes. Install Helm 3.x Install Helm 3.x on the master node before you install the CSI Driver for Dell PowerStore.\nSteps\nRun the command to install Helm 3.x.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Fibre Channel requirements Dell PowerStore supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell PowerStore:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done. Set up the iSCSI Initiator The CSI Driver for Dell PowerStore v1.4 and higher supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\nEnsure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi. For information about configuring iSCSI, see Dell PowerStore documentation on Dell Support.\nSet up the NVMe Initiator If you want to use the protocol, set up the NVMe initiators as follows:\nThe driver requires NVMe management command-line interface (nvme-cli) to use configure, edit, view or start the NVMe client and target. The nvme-cli utility provides a command-line and interactive shell option. The NVMe CLI tool is installed in the host using the below command. sudo apt install nvme-cli Requirements for NVMeTCP\nModules including the nvme, nvme_core, nvme_fabrics, and nvme_tcp are required for using NVMe over Fabrics using TCP. Load the NVMe and NVMe-OF Modules using the below commands: modprobe nvme modprobe nvme_tcp The NVMe modules may not be available after a node reboot. Loading the modules at startup is recommended. Requirements for NVMeFC\nNVMeFC Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done. NOTE:\nDo not load the nvme_tcp module for NVMeFC Linux multipathing requirements Dell PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell PowerStore.\nSet up Linux multipathing as follows:\nEnsure that all nodes have the Device Mapper Multipathing package installed. You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\nEnable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes. multipathd MachineConfig If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\nuser_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0 Use the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: workers-multipath-conf-default labels: machineconfiguration.openshift.io/role: worker spec: config: ignition: version: 3.2.0 storage: files: - contents: source: data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0K verification: {} filesystem: root mode: 400 path: /etc/multipath.conf After deploying thisMachineConfig object, CoreOS will start multipath service automatically. Alternatively, you can check the status of the multipath service by entering the following command in each worker nodes. sudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally, you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer to official documentation of the multipath configuration.\n(Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\nVolume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via helm. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\ncontroller: healthMonitor: # enabled: Enable/Disable health monitor of CSI volumes # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: None enabled: false # interval: Interval of monitoring volume health condition # Allowed values: Number followed by unit (s,m,h) # Examples: 60s, 5m, 1h # Default value: 60s interval: 60s node: healthMonitor: # enabled: Enable/Disable health monitor of CSI volumes- volume usage, volume condition # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: None enabled: false (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication: enabled: true Replication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\nRun git clone -b v2.9.1 https://github.com/dell/csi-powerstore.git to clone the git repository.\nEnsure that you have created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one. “csi-powerstore” is just an example. You can choose any name for the namespace. But make sure to align to the same namespace during the whole installation.\nEdit samples/secret/secret.yaml file and configure connection information for your PowerStore arrays changing following parameters:\nendpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what transport protocol we should use (FC, ISCSI, NVMeTCP, NVMeFC, None, or auto). nasName: defines what NAS should be used for NFS volumes. nfsAcls (Optional): defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. Add more blocks similar to above for each PowerStore array if necessary. If replication feature is enabled, ensure the secret includes all the PowerStore arrays involved in replication.\nUser Privileges The username specified in secret.yaml must be from the authentication providers of PowerStore. The user must have the correct user role to perform the actions. The minimum requirement is Storage Operator.\nCreate the secret by running\nkubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml Create storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\nIf you do not specify arrayID parameter in the storage class then the array that was specified as the default would be used for provisioning volumes.\nDownload the default values.yaml file\ncd dell-csi-helm-installer \u0026\u0026 wget -O my-powerstore-settings.yaml https://github.com/dell/helm-charts/raw/csi-powerstore-2.9.1/charts/csi-powerstore/values.yaml Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:\nParameter Description Required Default images List all the images used by the CSI driver and CSM. If you use a private repository, change the registries accordingly. Yes \"\" logLevel Defines CSI driver log level No “debug” logFormat Defines CSI driver log format No “JSON” externalAccess Defines additional entries for hostAccess of NFS volumes, single IP address and subnet are valid entries No \" \" kubeletConfigDir Defines kubelet config path for cluster Yes “/var/lib/kubelet” maxPowerstoreVolumesPerNode Defines the default value for maximum number of volumes that the controller can publish to the node. If the value is zero, then CO shall decide how many volumes of this type can be published by the controller to the node. This limit is applicable to all the nodes in the cluster for which the node label ‘max-powerstore-volumes-per-node’ is not set. No 0 imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Yes “IfNotPresent” nfsAcls Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777” connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No False controller.controllerCount Defines number of replicas of controller deployment Yes 2 controller.volumeNamePrefix Defines the string added to each volume that the CSI driver creates No “csivol” controller.snapshot.enabled Allows to enable/disable snapshotter sidecar with driver installation for snapshot feature No “true” controller.snapshot.snapNamePrefix Defines prefix to apply to the names of a created snapshots No “csisnap” controller.resizer.enabled Allows to enable/disable resizer sidecar with driver installation for volume expansion feature No “true” controller.healthMonitor.enabled Allows to enable/disable volume health monitor No false controller.healthMonitor.interval Interval of monitoring volume health condition No 60s controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \" \" controller.tolerations Defines tolerations that would be applied to controller deployment Yes \" \" node.nodeNamePrefix Defines the string added to each node that the CSI driver registers No “csi-node” node.nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No “/etc/machine-id” node.healthMonitor.enabled Allows to enable/disable volume health monitor No false node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \" \" node.tolerations Defines tolerations that would be applied to node daemonset Yes \" \" fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” controller.vgsnapshot.enabled Allows to enable/disable the volume group snapshot feature No “true” version To use any driver version No Latest driver version allowAutoRoundOffFilesystemSize Allows the controller to round off filesystem to 3Gi which is the minimum supported value No false storageCapacity.enabled Allows to enable/disable storage capacity tracking feature No true storageCapacity.pollInterval Configure how often the driver checks for changed capacity No 5m podmon.enabled Allows to enable/disable Resiliency feature No false Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n csi-powerstore NOTE:\nFor detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using node.nodeNamePrefix and the ID read from the file pointed to by node.nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. A hostname the driver uses for registration of adapters is in the form \u003cnodeNamePrefix\u003e-\u003cnodeID\u003e-\u003cnodeIP\u003e. By default, these are csi-node and the machine ID read from the file /etc/machine-id. To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver. Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option. Storage Classes The CSI driver for Dell PowerStore version 1.3 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class:\nThere are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\nEdit the sample storage class yaml file and update following parameters: arrayID: specifies what storage cluster the driver should use, if not specified driver will use storage cluster specified as default in samples/secret/secret.yaml csi.storage.k8s.io/fstype: specifies what filesystem type driver should use, possible variants ext3, ext4, xfs, nfs, if not specified driver will use ext4 by default. nfsAcls (Optional): defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. allowedTopologies (Optional): If you want you can also add topology constraints. allowedTopologies: - matchLabelExpressions: - key: csi-powerstore.dellemc.com/12.34.56.78-iscsi # replace \"-iscsi\" with \"-fc\", \"-nvmetcp\" or \"-nvmefc\" or \"-nfs\" at the end to use FC, NVMeTCP, NVMeFC or NFS enabled hosts # replace \"12.34.56.78\" with PowerStore endpoint IP values: - \"true\" Create your storage class by using kubectl: kubectl create -f \u003cpath_to_storageclass_file\u003e NOTE: Deleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerStore v1.4.0, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nDynamically update the powerstore secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\nkubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\n","categories":"","description":"Installing CSI Driver for PowerStore via Helm\n","excerpt":"Installing CSI Driver for PowerStore via Helm\n","ref":"/csm-docs/docs/csidriver/installation/helm/powerstore/","tags":"","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs, and nfs storage classes and automatically mounts them to the pod.\nIt assumes that you’ve created the same basic three storage classes from samples/storageclass folder without changing their names. If you’ve created different storage classes please edit tests/simple/simple.yaml and change PersistentVolumeClaim definitions to point to correct storage classes.\nSteps\nTo run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/ You can find all the created resources in testpowerstore namespace.\nCheck if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\nGo into the created container and verify that everything is mounted correctly.\nAfter verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/ ","categories":"","description":"Tests to validate PowerStore CSI Driver installation","excerpt":"Tests to validate PowerStore CSI Driver installation","ref":"/csm-docs/docs/csidriver/installation/test/powerstore/","tags":"","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v2.9.1 New Features/Changes #947 - [FEATURE]: Support for Kubernetes 1.28 #1066 - [FEATURE]: Support for Openshift 4.14 #851 - [FEATURE]: Helm Chart Enhancement - Container Images Configurable in values.yaml #905 - [FEATURE]: Add support for CSI Spec 1.6 #996 - [FEATURE]: Dell CSI to Dell CSM Operator Migration Process #1031 - [FEATURE]: Update to the latest UBI Micro image for CSM Fixed Issues #1006 - [BUG]: Too many login sessions in gopowerstore client causes unexpected session termination in UI #1014 - [BUG]: Missing error check for os.Stat call during volume publish #1053 - [BUG]: make gosec is erroring out - Repos PowerMax,PowerStore,PowerScale (gosec is installed) #1061 - [BUG]: Golint is not installing with go get command #1108 - [BUG]: Volumes failing to mount when customer using NVMeTCP on Powerstore #1103 - [BUG]: CSM Operator doesn’t apply fSGroupPolicy value to CSIDriver Object Known Issues Issue Resolution or workaround, if known Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100 fsGroupPolicy may not work as expected without root privileges for NFS onlyhttps://github.com/kubernetes/examples/issues/260 To get the desired behavior set “allowRoot: “true” in the storage class parameter If the NVMeFC pod is not getting created and the host looses the ssh connection, causing the driver pods to go to error state remove the nvme_tcp module from the host incase of NVMeFC connection When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. When driver node pods enter CrashLoopBackOff and PVC remains in pending state with one of the following events:\n1. failed to provision volume with StorageClass \u003cstorage-class-name\u003e: error generating accessibility requirements: no available topology found 2. waiting for a volume to be created, either by external provisioner “csi-powerstore.dellemc.com” or manually created by system administrator. Check whether all array details present in the secret file are valid and remove any invalid entries if present. Redeploy the driver. If an ephemeral pod is not being created in OpenShift 4.13 and is failing with the error “error when creating pod: the pod uses an inline volume provided by CSIDriver csi-powerstore.dellemc.com, and the namespace has a pod security enforcement level that is lower than privileged.” This issue occurs because OpenShift 4.13 introduced the CSI Volume Admission plugin to restrict the use of a CSI driver capable of provisioning CSI ephemeral volumes during pod admission (https://docs.openshift.com/container-platform/4.13/storage/container_storage_interface/ephemeral-storage-csi-inline.html). Therefore, an additional label “security.openshift.io/csi-ephemeral-volume-profile” needs to be added to the CSIDriver object to support inline ephemeral volumes. In OpenShift 4.13, the root user is not allowed to perform write operations on NFS shares, when root squashing is enabled. The workaround for this issue is to disable root squashing by setting allowRoot: “true” in the NFS storage class. If the volume limit is exhausted and there are pending pods and PVCs due to exceed max volume count, the pending PVCs will be bound to PVs, and the pending pods will be scheduled to nodes when the driver pods are restarted. It is advised not to have any pending pods or PVCs once the volume limit per node is exhausted on a CSI Driver. There is an open issue reported with Kubenetes at https://github.com/kubernetes/kubernetes/issues/95911 with the same behavior. If two separate networks are configured for ISCSI and NVMeTCP, the driver may encounter difficulty identifying the second network (e.g., NVMeTCP). This is a known issue, and the workaround involves creating a single network on the array to serve both ISCSI and NVMeTCP purposes. Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for PowerStore CSI driver","excerpt":"Release notes for PowerStore CSI driver","ref":"/csm-docs/docs/csidriver/release/powerstore/","tags":"","title":"PowerStore"},{"body":" Symptoms Prevention, Resolution or Workaround When you run the command kubectl describe pods powerstore-controller-\u003csuffix\u003e –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command. The kubectl logs -n csi-powerstore powerstore-node-\u003csuffix\u003e driver logs show that the driver can’t connect to PowerStore API. Check if you’ve created a secret with correct credentials Installation of the driver on Kubernetes supported versions fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.21/v1.22/v1.23 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements If PVC is not getting created and getting the following error in PVC description: failed to provision volume with StorageClass \"powerstore-iscsi\": rpc error: code = Internal desc = : Unknown error: Check if you’ve created a secret with correct credentials If the NVMeFC pod is not getting created and the host looses the ssh connection, causing the driver pods to go to error state remove the nvme_tcp module from the host incase of NVMeFC connection When a node goes down, the block volumes attached to the node cannot be attached to another node 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. If the pod creation for NVMe takes time when the connections between the host and the array are more than 2 and considerable volumes are mounted on the host Reduce the number of connections between the host and the array to 2. Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.22.0 \u003c 1.25.0 which is incompatible with Kubernetes V1.22.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. If two separate networks are configured for ISCSI and NVMeTCP, the driver may encounter difficulty identifying the second network (e.g., NVMeTCP). This is a known issue, and the workaround involves creating a single network on the array to serve both ISCSI and NVMeTCP purposes. Unable to provision PVC’s via driver Ensure that the NAS name matches the one provided on the array side. Unable to install or upgrade the driver Ensure that the firewall is configured to grant adequate permissions for downloading images from the registry. Faulty paths in the multipath Ensure that the configuration of the multipath is correct and connectivity to the underlying hardware is intact. Unable to install or upgrade the driver due to minimum Kubernetes version or Openshift version Currently CSM only supports n, n-1, n-2 version of Kubernetes and Openshift, if you still wanted to continue with existing version update the verify.sh to continue. Volumes are not getting deleted on the array when PV’s are deleted Ensure persistentVolumeReclaimPolicy is set to Delete. ","categories":"","description":"Troubleshooting PowerStore Driver","excerpt":"Troubleshooting PowerStore Driver","ref":"/csm-docs/docs/csidriver/troubleshooting/powerstore/","tags":"","title":"PowerStore"},{"body":"Installing CSI Driver for PowerStore via Dell CSM Operator The CSI Driver for Dell PowerStore can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using the following command:\nkubectl get csm --all-namespaces Prerequisite Fibre Channel requirements Dell PowerStore supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell PowerStore:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done. Set up the iSCSI Initiator The CSI Driver for Dell PowerStore v1.4 and higher supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\nEnsure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi. For information about configuring iSCSI, see Dell PowerStore documentation on Dell Support.\nSet up the NVMe Initiator If you want to use the protocol, set up the NVMe initiators as follows:\nThe driver requires NVMe management command-line interface (nvme-cli) to use configure, edit, view or start the NVMe client and target. The nvme-cli utility provides a command-line and interactive shell option. The NVMe CLI tool is installed in the host using the below command. sudo apt install nvme-cli Requirements for NVMeTCP\nModules including the nvme, nvme_core, nvme_fabrics, and nvme_tcp are required for using NVMe over Fabrics using TCP. Load the NVMe and NVMe-OF Modules using the below commands: modprobe nvme modprobe nvme_tcp The NVMe modules may not be available after a node reboot. Loading the modules at startup is recommended. Requirements for NVMeFC\nNVMeFC Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done. NOTE:\nDo not load the nvme_tcp module for NVMeFC Linux multipathing requirements Dell PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell PowerStore.\nSet up Linux multipathing as follows:\nEnsure that all nodes have the Device Mapper Multipathing package installed. You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\nEnable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes. multipathd MachineConfig If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\nuser_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0 Use the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: workers-multipath-conf-default labels: machineconfiguration.openshift.io/role: worker spec: config: ignition: version: 3.2.0 storage: files: - contents: source: data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0K verification: {} filesystem: root mode: 400 path: /etc/multipath.conf After deploying thisMachineConfig object, CoreOS will start multipath service automatically. Alternatively, you can check the status of the multipath service by entering the following command in each worker nodes. sudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally, you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer to official documentation of the multipath configuration.\n(Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\n(Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in sample.yaml\nreplication: enabled: true Replication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nCreate namespace. Execute kubectl create namespace powerstore to create the powerstore namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘powerstore’.\nCreate a file called config.yaml that has Powerstore array connection details with the following content\narrays: - endpoint: \"https://10.0.0.1/api/rest\" # full URL path to the PowerStore API globalID: \"unique\" # unique id of the PowerStore array username: \"user\" # username for connecting to API password: \"password\" # password for connecting to API skipCertificateValidation: true # indicates if client side validation of (management)server's certificate can be skipped isDefault: true # treat current array as a default (would be used by storage classes without arrayID parameter) blockProtocol: \"auto\" # what SCSI transport protocol use on node side (FC, ISCSI, NVMeTCP, NVMeFC, None, or auto) nasName: \"nas-server\" # what NAS should be used for NFS volumes nfsAcls: \"0777\" # (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. # NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. Change the parameters with relevant values for your PowerStore array. Add more blocks similar to above for each PowerStore array if necessary.\nIf replication feature is enabled, ensure the secret includes all the PowerStore arrays involved in replication.\nUser Privileges The username specified in config.yaml must be from the authentication providers of PowerStore. The user must have the correct user role to perform the actions. The minimum requirement is Storage Operator.\nCreate Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion: v1 kind: Secret metadata: name: powerstore-config namespace: powerstore type: Opaque data: config: CONFIG_YAML Combine both files and create Kubernetes secret by running the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Install Driver Follow all the prerequisites above\nCreate a CR (Custom Resource) for PowerStore using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:\nParameter Description Required Default replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be in pending state until new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2 namespace Specifies namespace where the driver will be installed Yes “powerstore” fsGroupPolicy Defines which FS Group policy mode to be used. Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity Enable/Disable storage capacity tracking feature No false Common parameters for node and controller X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node” X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter” Controller parameters X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No empty X_CSI_NFS_ACLS Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777” Node parameters X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false Execute the following command to create PowerStore custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e This command will deploy the CSI PowerStore driver in the namespace specified in the input YAML file.\nNext, the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e Verify the CSI Driver installation Note :\n“Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Snapshotter and resizer sidecars are not optional. They are defaults with Driver installation. ","categories":"","description":"Installing Dell CSI Driver for PowerStore via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for PowerStore via Dell CSM Operator\n","ref":"/csm-docs/docs/deployment/csmoperator/drivers/powerstore/","tags":"","title":"PowerStore"},{"body":"The CSI Driver for Dell PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\nCSI Driver for Dell PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers (Optional) Kubernetes External Snapshotter, which provides snapshot support (Optional) Kubernetes External Resizer, which resizes the volume The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\nCSI Driver for Dell PowerStore Kubernetes Node Registrar, which handles the driver registration Prerequisites The following are requirements to be met before installing the CSI Driver for Dell PowerStore:\nInstall Kubernetes or OpenShift (see supported versions) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI or NVMe/TCP or NVMe/FC protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator or Set up the NVMe Initiator sections below. You can use NFS volumes without FC or iSCSI or NVMe/TCP or NVMe/FC configuration. You can use either the Fibre Channel or iSCSI or NVMe/TCP or NVMe/FC protocol, but you do not need all the four.\nIf you want to use preconfigured iSCSI/FC hosts be sure to check that they are not part of any host group\nLinux native multipathing requirements Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements Nonsecure registries are defined in Docker or other container runtimes, for CSI drivers that are hosted in a non-secure location. You can access your cluster with kubectl and helm. Ensure that your nodes support mounting NFS volumes. Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerStore.\nSteps\nRun the command to install Helm 3.0.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Fibre Channel requirements Dell PowerStore supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell PowerStore:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done. Set up the iSCSI Initiator The CSI Driver for Dell PowerStore v1.4 and higher supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\nEnsure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi. For information about configuring iSCSI, see Dell PowerStore documentation on Dell Support.\nSet up the NVMe Initiator If you want to use the protocol, set up the NVMe initiators as follows:\nThe driver requires NVMe management command-line interface (nvme-cli) to use configure, edit, view or start the NVMe client and target. The nvme-cli utility provides a command-line and interactive shell option. The NVMe CLI tool is installed in the host using the below command. sudo apt install nvme-cli Requirements for NVMeTCP\nModules including the nvme, nvme_core, nvme_fabrics, and nvme_tcp are required for using NVMe over Fabrics using TCP. Load the NVMe and NVMe-OF Modules using the below commands: modprobe nvme modprobe nvme_tcp The NVMe modules may not be available after a node reboot. Loading the modules at startup is recommended. Requirements for NVMeFC\nNVMeFC Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done. NOTE:\nDo not load the nvme_tcp module for NVMeFC Linux multipathing requirements Dell PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell PowerStore.\nSet up Linux multipathing as follows:\nEnsure that all nodes have the Device Mapper Multipathing package installed. You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\nEnable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes. (Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\nVolume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via helm. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\ncontroller: healthMonitor: # enabled: Enable/Disable health monitor of CSI volumes # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: None enabled: false # interval: Interval of monitoring volume health condition # Allowed values: Number followed by unit (s,m,h) # Examples: 60s, 5m, 1h # Default value: 60s interval: 60s node: healthMonitor: # enabled: Enable/Disable health monitor of CSI volumes- volume usage, volume condition # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: None enabled: false (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication: enabled: true Replication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\nRun git clone -b v2.8.0 https://github.com/dell/csi-powerstore.git to clone the git repository.\nEnsure that you have created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one. “csi-powerstore” is just an example. You can choose any name for the namespace. But make sure to align to the same namespace during the whole installation.\nEdit samples/secret/secret.yaml file and configure connection information for your PowerStore arrays changing following parameters:\nendpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what transport protocol we should use (FC, ISCSI, NVMeTCP, NVMeFC, None, or auto). nasName: defines what NAS should be used for NFS volumes. nfsAcls (Optional): defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. Add more blocks similar to above for each PowerStore array if necessary.\nUser Privileges The username specified in secret.yaml must be from the authentication providers of PowerStore. The user must have the correct user role to perform the actions. The minimum requirement is Storage Operator.\nCreate the secret by running\nkubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml Create storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\nIf you do not specify arrayID parameter in the storage class then the array that was specified as the default would be used for provisioning volumes.\nDownload the default values.yaml file\ncd dell-csi-helm-installer \u0026\u0026 wget -O my-powerstore-settings.yaml https://github.com/dell/helm-charts/raw/csi-powerstore-2.8.0/charts/csi-powerstore/values.yaml Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:\nParameter Description Required Default logLevel Defines CSI driver log level No “debug” logFormat Defines CSI driver log format No “JSON” externalAccess Defines additional entries for hostAccess of NFS volumes, single IP address and subnet are valid entries No \" \" kubeletConfigDir Defines kubelet config path for cluster Yes “/var/lib/kubelet” maxPowerstoreVolumesPerNode Defines the default value for maximum number of volumes that the controller can publish to the node. If the value is zero, then CO shall decide how many volumes of this type can be published by the controller to the node. This limit is applicable to all the nodes in the cluster for which the node label ‘max-powerstore-volumes-per-node’ is not set. No 0 imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Yes “IfNotPresent” nfsAcls Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777” connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No False controller.controllerCount Defines number of replicas of controller deployment Yes 2 controller.volumeNamePrefix Defines the string added to each volume that the CSI driver creates No “csivol” controller.snapshot.enabled Allows to enable/disable snapshotter sidecar with driver installation for snapshot feature No “true” controller.snapshot.snapNamePrefix Defines prefix to apply to the names of a created snapshots No “csisnap” controller.resizer.enabled Allows to enable/disable resizer sidecar with driver installation for volume expansion feature No “true” controller.healthMonitor.enabled Allows to enable/disable volume health monitor No false controller.healthMonitor.interval Interval of monitoring volume health condition No 60s controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \" \" controller.tolerations Defines tolerations that would be applied to controller deployment Yes \" \" node.nodeNamePrefix Defines the string added to each node that the CSI driver registers No “csi-node” node.nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No “/etc/machine-id” node.healthMonitor.enabled Allows to enable/disable volume health monitor No false node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \" \" node.tolerations Defines tolerations that would be applied to node daemonset Yes \" \" fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” controller.vgsnapshot.enabled Allows to enable/disable the volume group snapshot feature No “true” images.driverRepository To use an image from custom repository No dockerhub version To use any driver version No Latest driver version allowAutoRoundOffFilesystemSize Allows the controller to round off filesystem to 3Gi which is the minimum supported value No false storageCapacity.enabled Allows to enable/disable storage capacity tracking feature No true storageCapacity.pollInterval Configure how often the driver checks for changed capacity No 5m podmon.enabled Allows to enable/disable Resiliency feature No false podmon.image Sidecar image for resiliency No - Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n csi-powerstore NOTE:\nFor detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using node.nodeNamePrefix and the ID read from the file pointed to by node.nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. A hostname the driver uses for registration of adapters is in the form \u003cnodeNamePrefix\u003e-\u003cnodeID\u003e-\u003cnodeIP\u003e. By default, these are csi-node and the machine ID read from the file /etc/machine-id. To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver. Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option. Storage Classes The CSI driver for Dell PowerStore version 1.3 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class:\nThere are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\nEdit the sample storage class yaml file and update following parameters: arrayID: specifies what storage cluster the driver should use, if not specified driver will use storage cluster specified as default in samples/secret/secret.yaml csi.storage.k8s.io/fstype: specifies what filesystem type driver should use, possible variants ext3, ext4, xfs, nfs, if not specified driver will use ext4 by default. nfsAcls (Optional): defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. allowedTopologies (Optional): If you want you can also add topology constraints. allowedTopologies: - matchLabelExpressions: - key: csi-powerstore.dellemc.com/12.34.56.78-iscsi # replace \"-iscsi\" with \"-fc\", \"-nvmetcp\" or \"-nvmefc\" or \"-nfs\" at the end to use FC, NVMeTCP, NVMeFC or NFS enabled hosts # replace \"12.34.56.78\" with PowerStore endpoint IP values: - \"true\" Create your storage class by using kubectl: kubectl create -f \u003cpath_to_storageclass_file\u003e NOTE: Deleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerStore v1.4.0, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nDynamically update the powerstore secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\nkubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\n","categories":"","description":"Installing CSI Driver for PowerStore via Helm\n","excerpt":"Installing CSI Driver for PowerStore via Helm\n","ref":"/csm-docs/v1/csidriver/installation/helm/powerstore/","tags":"","title":"PowerStore"},{"body":" The Dell CSI Operator is no longer actively maintained or supported. Dell CSI Operator has been replaced with Dell CSM Operator. If you are currently using Dell CSI Operator, refer to the operator migration documentation to migrate from Dell CSI Operator to Dell CSM Operator.\nInstalling CSI Driver for PowerStore via Operator The CSI Driver for Dell PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note: The deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver Create namespace:\nRun kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\nCreate PowerStore array connection config:\nCreate a file called config.yaml with the following content\narrays: - endpoint: \"https://10.0.0.1/api/rest\" # full URL path to the PowerStore API globalID: \"unique\" # unique id of the PowerStore array username: \"user\" # username for connecting to API password: \"password\" # password for connecting to API skipCertificateValidation: true # indicates if client side validation of (management)server's certificate can be skipped isDefault: true # treat current array as a default (would be used by storage classes without arrayID parameter) blockProtocol: \"auto\" # what SCSI transport protocol use on node side (FC, ISCSI, NVMeTCP, NVMeFC, None, or auto) nasName: \"nas-server\" # what NAS should be used for NFS volumes nfsAcls: \"0777\" # (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. # NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. Change the parameters with relevant values for your PowerStore array.\nAdd more blocks similar to above for each PowerStore array if necessary.\nUser Privileges The username specified in config.yaml must be from the authentication providers of PowerStore. The user must have the correct user role to perform the actions. The minimum requirement is Storage Operator.\nCreate Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion: v1 kind: Secret metadata: name: powerstore-config namespace: \u003cdriver-namespace\u003e type: Opaque data: config: CONFIG_YAML Combine both files and create Kubernetes secret by running the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Create a Custom Resource (CR) for PowerStore using the sample files provided here.\nBelow is a sample CR:\napiVersion: storage.dell.com/v1 kind: CSIPowerStore metadata: name: test-powerstore namespace: test-powerstore spec: driver: configVersion: v2.7.0 replicas: 2 dnsPolicy: ClusterFirstWithHostNet forceUpdate: false fsGroupPolicy: ReadWriteOnceWithFSType storageCapacity: true common: image: \"dellemc/csi-powerstore:v2.7.0\" imagePullPolicy: IfNotPresent envs: - name: X_CSI_POWERSTORE_NODE_NAME_PREFIX value: \"csi\" - name: X_CSI_FC_PORTS_FILTER_FILE_PATH value: \"/etc/fc-ports-filter\" sideCars: - name: external-health-monitor args: [\"--monitor-interval=60s\"] - name: provisioner args: [\"--capacity-poll-interval=5m\"] controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" - name: X_CSI_NFS_ACLS value: \"0777\" nodeSelector: node-role.kubernetes.io/master: \"\" tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" node: envs: - name: \"X_CSI_POWERSTORE_ENABLE_CHAP\" value: \"true\" - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" - name: X_CSI_POWERSTORE_MAX_VOLUMES_PER_NODE value: \"0\" nodeSelector: node-role.kubernetes.io/worker: \"\" tolerations: - key: \"node-role.kubernetes.io/worker\" operator: \"Exists\" effect: \"NoSchedule\" --- apiVersion: v1 kind: ConfigMap metadata: name: powerstore-config-params namespace: test-powerstore data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"debug\" CSI_LOG_FORMAT: \"JSON\" Users must configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values: Parameter Description Required Default replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be pending state till new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2 namespace Specifies namespace where the drive will be installed Yes “test-powerstore” fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity Enable/Disable storage capacity tracking feature No true Common parameters for node and controller X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node” X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter” Controller parameters X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No \" \" X_CSI_NFS_ACLS Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777” Node parameters X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false X_CSI_POWERSTORE_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node No 0 Execute the following command to create PowerStore custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerStore driver. After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\nsideCars: # Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\". - name: external-health-monitor args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin- volume status, volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin- volume usage, volume condition # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params Note :\n“Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. ","categories":"","description":"Installing CSI Driver for PowerStore via Operator\n","excerpt":"Installing CSI Driver for PowerStore via Operator\n","ref":"/csm-docs/v1/csidriver/installation/operator/powerstore/","tags":"","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs, and nfs storage classes and automatically mounts them to the pod.\nIt assumes that you’ve created the same basic three storage classes from samples/storageclass folder without changing their names. If you’ve created different storage classes please edit tests/simple/simple.yaml and change PersistentVolumeClaim definitions to point to correct storage classes.\nSteps\nTo run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/ You can find all the created resources in testpowerstore namespace.\nCheck if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\nGo into the created container and verify that everything is mounted correctly.\nAfter verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/ ","categories":"","description":"Tests to validate PowerStore CSI Driver installation","excerpt":"Tests to validate PowerStore CSI Driver installation","ref":"/csm-docs/v1/csidriver/installation/test/powerstore/","tags":"","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v2.8.0 New Features/Changes #724 - [FEATURE]: CSM support for Openshift 4.13 #877 - [FEATURE]: Make standalone helm chart available from helm repository : https://dell.github.io/dell/helm-charts #878 - [FEATURE]: CSI 1.5 spec support: Implement Volume Limits #879 - [FEATURE]: Configurable Volume Attributes use recommended naming convention / #922 - [FEATURE]: Use ubi9 micro as base image Fixed Issues #916 - [BUG]: Remove references to deprecated io/ioutil package #928 - [BUG]: PowerStore Replication - Delete RG request hangs Known Issues Issue Resolution or workaround, if known Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100 fsGroupPolicy may not work as expected without root privileges for NFS onlyhttps://github.com/kubernetes/examples/issues/260 To get the desired behavior set “allowRoot: “true” in the storage class parameter If the NVMeFC pod is not getting created and the host looses the ssh connection, causing the driver pods to go to error state remove the nvme_tcp module from the host incase of NVMeFC connection When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. When driver node pods enter CrashLoopBackOff and PVC remains in pending state with one of the following events:\n1. failed to provision volume with StorageClass \u003cstorage-class-name\u003e: error generating accessibility requirements: no available topology found 2. waiting for a volume to be created, either by external provisioner “csi-powerstore.dellemc.com” or manually created by system administrator. Check whether all array details present in the secret file are valid and remove any invalid entries if present. Redeploy the driver. If an ephemeral pod is not being created in OpenShift 4.13 and is failing with the error “error when creating pod: the pod uses an inline volume provided by CSIDriver csi-powerstore.dellemc.com, and the namespace has a pod security enforcement level that is lower than privileged.” This issue occurs because OpenShift 4.13 introduced the CSI Volume Admission plugin to restrict the use of a CSI driver capable of provisioning CSI ephemeral volumes during pod admission (https://docs.openshift.com/container-platform/4.13/storage/container_storage_interface/ephemeral-storage-csi-inline.html). Therefore, an additional label “security.openshift.io/csi-ephemeral-volume-profile” needs to be added to the CSIDriver object to support inline ephemeral volumes. In OpenShift 4.13, the root user is not allowed to perform write operations on NFS shares, when root squashing is enabled. The workaround for this issue is to disable root squashing by setting allowRoot: “true” in the NFS storage class. If the volume limit is exhausted and there are pending pods and PVCs due to exceed max volume count, the pending PVCs will be bound to PVs, and the pending pods will be scheduled to nodes when the driver pods are restarted. It is advised not to have any pending pods or PVCs once the volume limit per node is exhausted on a CSI Driver. There is an open issue reported with Kubenetes at https://github.com/kubernetes/kubernetes/issues/95911 with the same behavior. Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for PowerStore CSI driver","excerpt":"Release notes for PowerStore CSI driver","ref":"/csm-docs/v1/csidriver/release/powerstore/","tags":"","title":"PowerStore"},{"body":" Symptoms Prevention, Resolution or Workaround When you run the command kubectl describe pods powerstore-controller-\u003csuffix\u003e –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command. The kubectl logs -n csi-powerstore powerstore-node-\u003csuffix\u003e driver logs show that the driver can’t connect to PowerStore API. Check if you’ve created a secret with correct credentials Installation of the driver on Kubernetes supported versions fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.21/v1.22/v1.23 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements If PVC is not getting created and getting the following error in PVC description: failed to provision volume with StorageClass \"powerstore-iscsi\": rpc error: code = Internal desc = : Unknown error: Check if you’ve created a secret with correct credentials If the NVMeFC pod is not getting created and the host looses the ssh connection, causing the driver pods to go to error state remove the nvme_tcp module from the host incase of NVMeFC connection When a node goes down, the block volumes attached to the node cannot be attached to another node 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. If the pod creation for NVMe takes time when the connections between the host and the array are more than 2 and considerable volumes are mounted on the host Reduce the number of connections between the host and the array to 2. Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.22.0 \u003c 1.25.0 which is incompatible with Kubernetes V1.22.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. ","categories":"","description":"Troubleshooting PowerStore Driver","excerpt":"Troubleshooting PowerStore Driver","ref":"/csm-docs/v1/csidriver/troubleshooting/powerstore/","tags":"","title":"PowerStore"},{"body":"Installing CSI Driver for PowerStore via Dell CSM Operator The CSI Driver for Dell PowerStore can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using the following command:\nkubectl get csm --all-namespaces Prerequisite Create namespace. Execute kubectl create namespace powerstore to create the powerstore namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘powerstore’.\nCreate a file called config.yaml that has Powerstore array connection details with the following content\narrays: - endpoint: \"https://10.0.0.1/api/rest\" # full URL path to the PowerStore API globalID: \"unique\" # unique id of the PowerStore array username: \"user\" # username for connecting to API password: \"password\" # password for connecting to API skipCertificateValidation: true # indicates if client side validation of (management)server's certificate can be skipped isDefault: true # treat current array as a default (would be used by storage classes without arrayID parameter) blockProtocol: \"auto\" # what SCSI transport protocol use on node side (FC, ISCSI, NVMeTCP, NVMeFC, None, or auto) nasName: \"nas-server\" # what NAS should be used for NFS volumes nfsAcls: \"0777\" # (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. # NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. Change the parameters with relevant values for your PowerStore array. Add more blocks similar to above for each PowerStore array if necessary.\nUser Privileges The username specified in config.yaml must be from the authentication providers of PowerStore. The user must have the correct user role to perform the actions. The minimum requirement is Storage Operator.\nCreate Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion: v1 kind: Secret metadata: name: powerstore-config namespace: powerstore type: Opaque data: config: CONFIG_YAML Combine both files and create Kubernetes secret by running the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Install Driver Follow all the prerequisites above\nCreate a CR (Custom Resource) for PowerStore using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:\nParameter Description Required Default replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be in pending state until new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2 namespace Specifies namespace where the driver will be installed Yes “powerstore” fsGroupPolicy Defines which FS Group policy mode to be used. Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity Enable/Disable storage capacity tracking feature No false Common parameters for node and controller X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node” X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter” Controller parameters X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No empty X_CSI_NFS_ACLS Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777” Node parameters X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false Execute the following command to create PowerStore custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e This command will deploy the CSI PowerStore driver in the namespace specified in the input YAML file.\nNext, the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e Verify the CSI Driver installation Note :\n“Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Snapshotter and resizer sidecars are not optional. They are defaults with Driver installation. ","categories":"","description":"Installing Dell CSI Driver for PowerStore via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for PowerStore via Dell CSM Operator\n","ref":"/csm-docs/v1/deployment/csmoperator/drivers/powerstore/","tags":"","title":"PowerStore"},{"body":"The CSI Driver for Dell PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\nCSI Driver for Dell PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers (Optional) Kubernetes External Snapshotter, which provides snapshot support (Optional) Kubernetes External Resizer, which resizes the volume The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\nCSI Driver for Dell PowerStore Kubernetes Node Registrar, which handles the driver registration Prerequisites The following are requirements to be met before installing the CSI Driver for Dell PowerStore:\nInstall Kubernetes or OpenShift (see supported versions) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI or NVMe/TCP or NVMe/FC protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator or Set up the NVMe Initiator sections below. You can use NFS volumes without FC or iSCSI or NVMe/TCP or NVMe/FC configuration. You can use either the Fibre Channel or iSCSI or NVMe/TCP or NVMe/FC protocol, but you do not need all the four.\nIf you want to use preconfigured iSCSI/FC hosts be sure to check that they are not part of any host group\nLinux native multipathing requirements Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements Nonsecure registries are defined in Docker or other container runtimes, for CSI drivers that are hosted in a non-secure location. You can access your cluster with kubectl and helm. Ensure that your nodes support mounting NFS volumes. Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerStore.\nSteps\nRun the command to install Helm 3.0.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Fibre Channel requirements Dell PowerStore supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell PowerStore:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done. Set up the iSCSI Initiator The CSI Driver for Dell PowerStore v1.4 and higher supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\nEnsure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi. For information about configuring iSCSI, see Dell PowerStore documentation on Dell Support.\nSet up the NVMe Initiator If you want to use the protocol, set up the NVMe initiators as follows:\nThe driver requires NVMe management command-line interface (nvme-cli) to use configure, edit, view or start the NVMe client and target. The nvme-cli utility provides a command-line and interactive shell option. The NVMe CLI tool is installed in the host using the below command. sudo apt install nvme-cli Requirements for NVMeTCP\nModules including the nvme, nvme_core, nvme_fabrics, and nvme_tcp are required for using NVMe over Fabrics using TCP. Load the NVMe and NVMe-OF Modules using the below commands: modprobe nvme modprobe nvme_tcp The NVMe modules may not be available after a node reboot. Loading the modules at startup is recommended. Requirements for NVMeFC\nNVMeFC Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done. NOTE:\nDo not load the nvme_tcp module for NVMeFC Linux multipathing requirements Dell PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell PowerStore.\nSet up Linux multipathing as follows:\nEnsure that all nodes have the Device Mapper Multipathing package installed. You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\nEnable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes. (Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\nVolume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via helm. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\ncontroller: healthMonitor: # enabled: Enable/Disable health monitor of CSI volumes # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: None enabled: false # interval: Interval of monitoring volume health condition # Allowed values: Number followed by unit (s,m,h) # Examples: 60s, 5m, 1h # Default value: 60s interval: 60s node: healthMonitor: # enabled: Enable/Disable health monitor of CSI volumes- volume usage, volume condition # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: None enabled: false (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication: enabled: true Replication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\nRun git clone -b v2.7.0 https://github.com/dell/csi-powerstore.git to clone the git repository.\nEnsure that you have created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one. “csi-powerstore” is just an example. You can choose any name for the namespace. But make sure to align to the same namespace during the whole installation.\nEdit samples/secret/secret.yaml file and configure connection information for your PowerStore arrays changing following parameters:\nendpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what transport protocol we should use (FC, ISCSI, NVMeTCP, NVMeFC, None, or auto). nasName: defines what NAS should be used for NFS volumes. nfsAcls (Optional): defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. Add more blocks similar to above for each PowerStore array if necessary.\nUser Privileges The username specified in secret.yaml must be from the authentication providers of PowerStore. The user must have the correct user role to perform the actions. The minimum requirement is Storage Operator.\nCreate the secret by running\nkubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml Create storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\nIf you do not specify arrayID parameter in the storage class then the array that was specified as the default would be used for provisioning volumes.\nCopy the default values.yaml file\ncd dell-csi-helm-installer \u0026\u0026 cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:\nParameter Description Required Default logLevel Defines CSI driver log level No “debug” logFormat Defines CSI driver log format No “JSON” externalAccess Defines additional entries for hostAccess of NFS volumes, single IP address and subnet are valid entries No \" \" kubeletConfigDir Defines kubelet config path for cluster Yes “/var/lib/kubelet” imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Yes “IfNotPresent” nfsAcls Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777” connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No False controller.controllerCount Defines number of replicas of controller deployment Yes 2 controller.volumeNamePrefix Defines the string added to each volume that the CSI driver creates No “csivol” controller.snapshot.enabled Allows to enable/disable snapshotter sidecar with driver installation for snapshot feature No “true” controller.snapshot.snapNamePrefix Defines prefix to apply to the names of a created snapshots No “csisnap” controller.resizer.enabled Allows to enable/disable resizer sidecar with driver installation for volume expansion feature No “true” controller.healthMonitor.enabled Allows to enable/disable volume health monitor No false controller.healthMonitor.interval Interval of monitoring volume health condition No 60s controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \" \" controller.tolerations Defines tolerations that would be applied to controller deployment Yes \" \" node.nodeNamePrefix Defines the string added to each node that the CSI driver registers No “csi-node” node.nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No “/etc/machine-id” node.healthMonitor.enabled Allows to enable/disable volume health monitor No false node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \" \" node.tolerations Defines tolerations that would be applied to node daemonset Yes \" \" fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” controller.vgsnapshot.enabled Allows to enable/disable the volume group snapshot feature No “true” images.driverRepository To use an image from custom repository No dockerhub version To use any driver version No Latest driver version allowAutoRoundOffFilesystemSize Allows the controller to round off filesystem to 3Gi which is the minimum supported value No false storageCapacity.enabled Allows to enable/disable storage capacity tracking feature No true storageCapacity.pollInterval Configure how often the driver checks for changed capacity No 5m podmon.enabled Allows to enable/disable Resiliency feature No false podmon.image Sidecar image for resiliency No - Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n csi-powerstore NOTE:\nFor detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using node.nodeNamePrefix and the ID read from the file pointed to by node.nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. A hostname the driver uses for registration of adapters is in the form \u003cnodeNamePrefix\u003e-\u003cnodeID\u003e-\u003cnodeIP\u003e. By default, these are csi-node and the machine ID read from the file /etc/machine-id. To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver. Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option. Storage Classes The CSI driver for Dell PowerStore version 1.3 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class:\nThere are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\nEdit the sample storage class yaml file and update following parameters: arrayID: specifies what storage cluster the driver should use, if not specified driver will use storage cluster specified as default in samples/secret/secret.yaml csi.storage.k8s.io/fstype: specifies what filesystem type driver should use, possible variants ext3, ext4, xfs, nfs, if not specified driver will use ext4 by default. nfsAcls (Optional): defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. allowedTopologies (Optional): If you want you can also add topology constraints. allowedTopologies: - matchLabelExpressions: - key: csi-powerstore.dellemc.com/12.34.56.78-iscsi # replace \"-iscsi\" with \"-fc\", \"-nvmetcp\" or \"-nvmefc\" or \"-nfs\" at the end to use FC, NVMeTCP, NVMeFC or NFS enabled hosts # replace \"12.34.56.78\" with PowerStore endpoint IP values: - \"true\" Create your storage class by using kubectl: kubectl create -f \u003cpath_to_storageclass_file\u003e NOTE: Deleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerStore v1.4.0, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nDynamically update the powerstore secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\nkubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\n","categories":"","description":"Installing CSI Driver for PowerStore via Helm\n","excerpt":"Installing CSI Driver for PowerStore via Helm\n","ref":"/csm-docs/v2/csidriver/installation/helm/powerstore/","tags":"","title":"PowerStore"},{"body":" The Dell CSI Operator is no longer actively maintained or supported. Dell CSI Operator has been replaced with Dell CSM Operator. If you are currently using Dell CSI Operator, refer to the operator migration documentation to migrate from Dell CSI Operator to Dell CSM Operator.\nInstalling CSI Driver for PowerStore via Operator The CSI Driver for Dell PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note: The deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver Create namespace:\nRun kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\nCreate PowerStore array connection config:\nCreate a file called config.yaml with the following content\narrays: - endpoint: \"https://10.0.0.1/api/rest\" # full URL path to the PowerStore API globalID: \"unique\" # unique id of the PowerStore array username: \"user\" # username for connecting to API password: \"password\" # password for connecting to API skipCertificateValidation: true # indicates if client side validation of (management)server's certificate can be skipped isDefault: true # treat current array as a default (would be used by storage classes without arrayID parameter) blockProtocol: \"auto\" # what SCSI transport protocol use on node side (FC, ISCSI, NVMeTCP, NVMeFC, None, or auto) nasName: \"nas-server\" # what NAS should be used for NFS volumes nfsAcls: \"0777\" # (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. # NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. Change the parameters with relevant values for your PowerStore array.\nAdd more blocks similar to above for each PowerStore array if necessary.\nUser Privileges The username specified in config.yaml must be from the authentication providers of PowerStore. The user must have the correct user role to perform the actions. The minimum requirement is Storage Operator.\nCreate Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion: v1 kind: Secret metadata: name: powerstore-config namespace: \u003cdriver-namespace\u003e type: Opaque data: config: CONFIG_YAML Combine both files and create Kubernetes secret by running the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Create a Custom Resource (CR) for PowerStore using the sample files provided here.\nBelow is a sample CR:\napiVersion: storage.dell.com/v1 kind: CSIPowerStore metadata: name: test-powerstore namespace: test-powerstore spec: driver: configVersion: v2.7.0 replicas: 2 dnsPolicy: ClusterFirstWithHostNet forceUpdate: false fsGroupPolicy: ReadWriteOnceWithFSType storageCapacity: true common: image: \"dellemc/csi-powerstore:v2.7.0\" imagePullPolicy: IfNotPresent envs: - name: X_CSI_POWERSTORE_NODE_NAME_PREFIX value: \"csi\" - name: X_CSI_FC_PORTS_FILTER_FILE_PATH value: \"/etc/fc-ports-filter\" sideCars: - name: external-health-monitor args: [\"--monitor-interval=60s\"] - name: provisioner args: [\"--capacity-poll-interval=5m\"] controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" - name: X_CSI_NFS_ACLS value: \"0777\" nodeSelector: node-role.kubernetes.io/master: \"\" tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" node: envs: - name: \"X_CSI_POWERSTORE_ENABLE_CHAP\" value: \"true\" - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" nodeSelector: node-role.kubernetes.io/worker: \"\" tolerations: - key: \"node-role.kubernetes.io/worker\" operator: \"Exists\" effect: \"NoSchedule\" --- apiVersion: v1 kind: ConfigMap metadata: name: powerstore-config-params namespace: test-powerstore data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"debug\" CSI_LOG_FORMAT: \"JSON\" Users must configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values: Parameter Description Required Default replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be pending state till new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2 namespace Specifies namespace where the drive will be installed Yes “test-powerstore” fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity Enable/Disable storage capacity tracking feature No true Common parameters for node and controller X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node” X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter” Controller parameters X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No \" \" X_CSI_NFS_ACLS Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777” Node parameters X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false Execute the following command to create PowerStore custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerStore driver. After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\nsideCars: # Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\". - name: external-health-monitor args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin- volume status, volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin- volume usage, volume condition # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params Note :\n“Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. ","categories":"","description":"Installing CSI Driver for PowerStore via Operator\n","excerpt":"Installing CSI Driver for PowerStore via Operator\n","ref":"/csm-docs/v2/csidriver/installation/operator/powerstore/","tags":"","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs, and nfs storage classes and automatically mounts them to the pod.\nIt assumes that you’ve created the same basic three storage classes from samples/storageclass folder without changing their names. If you’ve created different storage classes please edit tests/simple/simple.yaml and change PersistentVolumeClaim definitions to point to correct storage classes.\nSteps\nTo run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/ You can find all the created resources in testpowerstore namespace.\nCheck if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\nGo into the created container and verify that everything is mounted correctly.\nAfter verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/ ","categories":"","description":"Tests to validate PowerStore CSI Driver installation","excerpt":"Tests to validate PowerStore CSI Driver installation","ref":"/csm-docs/v2/csidriver/installation/test/powerstore/","tags":"","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v2.7.0 New Features/Changes CSI PowerStore - Add support for PowerStore Medusa (v3.5) array Allow FQDN for the endpoint in CSI-PowerStore CSM Operator: Support install of Resiliency module Migrate image registry from k8s.gcr.io to registry.k8s.io CSM support for Kubernetes 1.27 Add upgrade support of csi-powerstore driver in CSM-Operator CSM support for Openshift 4.12 Update to the latest UBI/UBI Micro images for CSM Fixed Issues Storage Capacity Tracking not working in CSI-PowerStore when installed using CSM Operator CHAP is set to true in the CSI-PowerStore sample file in CSI Operator Unable to delete application pod when CSI PowerStore is installed using CSM Operator Known Issues Issue Resolution or workaround, if known Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100 fsGroupPolicy may not work as expected without root privileges for NFS onlyhttps://github.com/kubernetes/examples/issues/260 To get the desired behavior set “allowRoot: “true” in the storage class parameter If the NVMeFC pod is not getting created and the host looses the ssh connection, causing the driver pods to go to error state remove the nvme_tcp module from the host incase of NVMeFC connection When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. When driver node pods enter CrashLoopBackOff and PVC remains in pending state with one of the following events:\n1. failed to provision volume with StorageClass \u003cstorage-class-name\u003e: error generating accessibility requirements: no available topology found 2. waiting for a volume to be created, either by external provisioner “csi-powerstore.dellemc.com” or manually created by system administrator. Check whether all array details present in the secret file are valid and remove any invalid entries if present. Redeploy the driver. Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for PowerStore CSI driver","excerpt":"Release notes for PowerStore CSI driver","ref":"/csm-docs/v2/csidriver/release/powerstore/","tags":"","title":"PowerStore"},{"body":" Symptoms Prevention, Resolution or Workaround When you run the command kubectl describe pods powerstore-controller-\u003csuffix\u003e –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command. The kubectl logs -n csi-powerstore powerstore-node-\u003csuffix\u003e driver logs show that the driver can’t connect to PowerStore API. Check if you’ve created a secret with correct credentials Installation of the driver on Kubernetes supported versions fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.21/v1.22/v1.23 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements If PVC is not getting created and getting the following error in PVC description: failed to provision volume with StorageClass \"powerstore-iscsi\": rpc error: code = Internal desc = : Unknown error: Check if you’ve created a secret with correct credentials If the NVMeFC pod is not getting created and the host looses the ssh connection, causing the driver pods to go to error state remove the nvme_tcp module from the host incase of NVMeFC connection When a node goes down, the block volumes attached to the node cannot be attached to another node 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. If the pod creation for NVMe takes time when the connections between the host and the array are more than 2 and considerable volumes are mounted on the host Reduce the number of connections between the host and the array to 2. Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.22.0 \u003c 1.25.0 which is incompatible with Kubernetes V1.22.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. ","categories":"","description":"Troubleshooting PowerStore Driver","excerpt":"Troubleshooting PowerStore Driver","ref":"/csm-docs/v2/csidriver/troubleshooting/powerstore/","tags":"","title":"PowerStore"},{"body":"Installing CSI Driver for PowerStore via Dell CSM Operator The CSI Driver for Dell PowerStore can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using the following command:\nkubectl get csm --all-namespaces Prerequisite Create namespace. Execute kubectl create namespace powerstore to create the powerstore namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘powerstore’.\nCreate a file called config.yaml that has Powerstore array connection details with the following content\narrays: - endpoint: \"https://10.0.0.1/api/rest\" # full URL path to the PowerStore API globalID: \"unique\" # unique id of the PowerStore array username: \"user\" # username for connecting to API password: \"password\" # password for connecting to API skipCertificateValidation: true # indicates if client side validation of (management)server's certificate can be skipped isDefault: true # treat current array as a default (would be used by storage classes without arrayID parameter) blockProtocol: \"auto\" # what SCSI transport protocol use on node side (FC, ISCSI, NVMeTCP, NVMeFC, None, or auto) nasName: \"nas-server\" # what NAS should be used for NFS volumes nfsAcls: \"0777\" # (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. # NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. Change the parameters with relevant values for your PowerStore array. Add more blocks similar to above for each PowerStore array if necessary.\nUser Privileges The username specified in config.yaml must be from the authentication providers of PowerStore. The user must have the correct user role to perform the actions. The minimum requirement is Storage Operator.\nCreate Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion: v1 kind: Secret metadata: name: powerstore-config namespace: powerstore type: Opaque data: config: CONFIG_YAML Combine both files and create Kubernetes secret by running the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Install Driver Follow all the prerequisites above\nCreate a CR (Custom Resource) for PowerStore using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:\nParameter Description Required Default replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be in pending state until new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2 namespace Specifies namespace where the driver will be installed Yes “powerstore” fsGroupPolicy Defines which FS Group policy mode to be used. Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity Enable/Disable storage capacity tracking feature No false Common parameters for node and controller X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node” X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter” Controller parameters X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No empty X_CSI_NFS_ACLS Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777” Node parameters X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false Execute the following command to create PowerStore custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e This command will deploy the CSI PowerStore driver in the namespace specified in the input YAML file.\nNext, the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e Verify the CSI Driver installation Note :\n“Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Snapshotter and resizer sidecars are not optional. They are defaults with Driver installation. ","categories":"","description":"Installing Dell CSI Driver for PowerStore via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for PowerStore via Dell CSM Operator\n","ref":"/csm-docs/v2/deployment/csmoperator/drivers/powerstore/","tags":"","title":"PowerStore"},{"body":"The CSI Driver for Dell PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\nCSI Driver for Dell PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers (Optional) Kubernetes External Snapshotter, which provides snapshot support (Optional) Kubernetes External Resizer, which resizes the volume The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\nCSI Driver for Dell PowerStore Kubernetes Node Registrar, which handles the driver registration Prerequisites The following are requirements to be met before installing the CSI Driver for Dell PowerStore:\nInstall Kubernetes or OpenShift (see supported versions) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI or NVMe/TCP or NVMe/FC protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator or Set up the NVMe Initiator sections below. You can use NFS volumes without FC or iSCSI or NVMe/TCP or NVMe/FC configuration. You can use either the Fibre Channel or iSCSI or NVMe/TCP or NVMe/FC protocol, but you do not need all the four.\nIf you want to use preconfigured iSCSI/FC hosts be sure to check that they are not part of any host group\nLinux native multipathing requirements Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements Nonsecure registries are defined in Docker or other container runtimes, for CSI drivers that are hosted in a non-secure location. You can access your cluster with kubectl and helm. Ensure that your nodes support mounting NFS volumes. Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerStore.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell PowerStore supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell PowerStore:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done. Set up the iSCSI Initiator The CSI Driver for Dell PowerStore v1.4 and higher supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\nEnsure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi. For information about configuring iSCSI, see Dell PowerStore documentation on Dell Support.\nSet up the NVMe Initiator If you want to use the protocol, set up the NVMe initiators as follows:\nThe driver requires NVMe management command-line interface (nvme-cli) to use configure, edit, view or start the NVMe client and target. The nvme-cli utility provides a command-line and interactive shell option. The NVMe CLI tool is installed in the host using the below command. sudo apt install nvme-cli Requirements for NVMeTCP\nModules including the nvme, nvme_core, nvme_fabrics, and nvme_tcp are required for using NVMe over Fabrics using TCP. Load the NVMe and NVMe-OF Modules using the below commands: modprobe nvme modprobe nvme_tcp The NVMe modules may not be available after a node reboot. Loading the modules at startup is recommended. Requirements for NVMeFC\nNVMeFC Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done. NOTE:\nDo not load the nvme_tcp module for NVMeFC Linux multipathing requirements Dell PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell PowerStore.\nSet up Linux multipathing as follows:\nEnsure that all nodes have the Device Mapper Multipathing package installed. You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\nEnable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes. (Optional) Volume Snapshot Requirements For detailed snapshot setup procedure, click here.\nVolume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via helm. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\ncontroller: healthMonitor: # enabled: Enable/Disable health monitor of CSI volumes # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: None enabled: false # interval: Interval of monitoring volume health condition # Allowed values: Number followed by unit (s,m,h) # Examples: 60s, 5m, 1h # Default value: 60s interval: 60s node: healthMonitor: # enabled: Enable/Disable health monitor of CSI volumes- volume usage, volume condition # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: None enabled: false (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication: enabled: true Replication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\nRun git clone -b v2.6.0 https://github.com/dell/csi-powerstore.git to clone the git repository.\nEnsure that you have created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one. “csi-powerstore” is just an example. You can choose any name for the namespace. But make sure to align to the same namespace during the whole installation.\nEdit samples/secret/secret.yaml file and configure connection information for your PowerStore arrays changing following parameters:\nendpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what transport protocol we should use (FC, ISCSI, NVMeTCP, NVMeFC, None, or auto). nasName: defines what NAS should be used for NFS volumes. nfsAcls (Optional): defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. Add more blocks similar to above for each PowerStore array if necessary.\nCreate the secret by running kubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml\nCreate storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\nIf you do not specify arrayID parameter in the storage class then the array that was specified as the default would be used for provisioning volumes.\nCopy the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml\nEdit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:\nParameter Description Required Default logLevel Defines CSI driver log level No “debug” logFormat Defines CSI driver log format No “JSON” externalAccess Defines additional entries for hostAccess of NFS volumes, single IP address and subnet are valid entries No \" \" kubeletConfigDir Defines kubelet config path for cluster Yes “/var/lib/kubelet” imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Yes “IfNotPresent” nfsAcls Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777” connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No False controller.controllerCount Defines number of replicas of controller deployment Yes 2 controller.volumeNamePrefix Defines the string added to each volume that the CSI driver creates No “csivol” controller.snapshot.enabled Allows to enable/disable snapshotter sidecar with driver installation for snapshot feature No “true” controller.snapshot.snapNamePrefix Defines prefix to apply to the names of a created snapshots No “csisnap” controller.resizer.enabled Allows to enable/disable resizer sidecar with driver installation for volume expansion feature No “true” controller.healthMonitor.enabled Allows to enable/disable volume health monitor No false controller.healthMonitor.interval Interval of monitoring volume health condition No 60s controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \" \" controller.tolerations Defines tolerations that would be applied to controller deployment Yes \" \" node.nodeNamePrefix Defines the string added to each node that the CSI driver registers No “csi-node” node.nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No “/etc/machine-id” node.healthMonitor.enabled Allows to enable/disable volume health monitor No false node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \" \" node.tolerations Defines tolerations that would be applied to node daemonset Yes \" \" fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” controller.vgsnapshot.enabled To enable or disable the volume group snapshot feature No “true” images.driverRepository To use an image from custom repository No dockerhub version To use any driver version No Latest driver version allowAutoRoundOffFilesystemSize Allows the controller to round off filesystem to 3Gi which is the minimum supported value No false storageCapacity.enabled Enable/Disable storage capacity tracking No true storageCapacity.pollInterval Configure how often the driver checks for changed capacity No 5m Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n csi-powerstore NOTE:\nFor detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using node.nodeNamePrefix and the ID read from the file pointed to by node.nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. A hostname the driver uses for registration of adapters is in the form \u003cnodeNamePrefix\u003e-\u003cnodeID\u003e-\u003cnodeIP\u003e. By default, these are csi-node and the machine ID read from the file /etc/machine-id. To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver. Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option. Storage Classes The CSI driver for Dell PowerStore version 1.3 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class:\nThere are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\nEdit the sample storage class yaml file and update following parameters: arrayID: specifies what storage cluster the driver should use, if not specified driver will use storage cluster specified as default in samples/secret/secret.yaml csi.storage.k8s.io/fstype: specifies what filesystem type driver should use, possible variants ext3, ext4, xfs, nfs, if not specified driver will use ext4 by default. nfsAcls (Optional): defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. allowedTopologies (Optional): If you want you can also add topology constraints. allowedTopologies: - matchLabelExpressions: - key: csi-powerstore.dellemc.com/12.34.56.78-iscsi # replace \"-iscsi\" with \"-fc\", \"-nvmetcp\" or \"-nvmefc\" or \"-nfs\" at the end to use FC, NVMeTCP, NVMeFC or NFS enabled hosts # replace \"12.34.56.78\" with PowerStore endpoint IP values: - \"true\" Create your storage class by using kubectl: kubectl create -f \u003cpath_to_storageclass_file\u003e NOTE: Deleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerStore v1.4.0, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nDynamically update the powerstore secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\nkubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\n","categories":"","description":"Installing CSI Driver for PowerStore via Helm\n","excerpt":"Installing CSI Driver for PowerStore via Helm\n","ref":"/csm-docs/v3/csidriver/installation/helm/powerstore/","tags":"","title":"PowerStore"},{"body":"Installing CSI Driver for PowerStore via Operator The CSI Driver for Dell PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note: The deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver Create namespace:\nRun kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\nCreate PowerStore array connection config:\nCreate a file called config.yaml with the following content\narrays: - endpoint: \"https://10.0.0.1/api/rest\" # full URL path to the PowerStore API globalID: \"unique\" # unique id of the PowerStore array username: \"user\" # username for connecting to API password: \"password\" # password for connecting to API skipCertificateValidation: true # indicates if client side validation of (management)server's certificate can be skipped isDefault: true # treat current array as a default (would be used by storage classes without arrayID parameter) blockProtocol: \"auto\" # what SCSI transport protocol use on node side (FC, ISCSI, NVMeTCP, NVMeFC, None, or auto) nasName: \"nas-server\" # what NAS should be used for NFS volumes nfsAcls: \"0777\" # (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. # NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. Change the parameters with relevant values for your PowerStore array.\nAdd more blocks similar to above for each PowerStore array if necessary.\nCreate Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion: v1 kind: Secret metadata: name: powerstore-config namespace: \u003cdriver-namespace\u003e type: Opaque data: config: CONFIG_YAML Combine both files and create Kubernetes secret by running the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Create a Custom Resource (CR) for PowerStore using the sample files provided here.\nBelow is a sample CR:\napiVersion: storage.dell.com/v1 kind: CSIPowerStore metadata: name: test-powerstore namespace: test-powerstore spec: driver: configVersion: v2.6.0 replicas: 2 dnsPolicy: ClusterFirstWithHostNet forceUpdate: false fsGroupPolicy: ReadWriteOnceWithFSType storageCapacity: true common: image: \"dellemc/csi-powerstore:v2.6.0\" imagePullPolicy: IfNotPresent envs: - name: X_CSI_POWERSTORE_NODE_NAME_PREFIX value: \"csi\" - name: X_CSI_FC_PORTS_FILTER_FILE_PATH value: \"/etc/fc-ports-filter\" sideCars: - name: external-health-monitor args: [\"--monitor-interval=60s\"] - name: provisioner args: [\"--capacity-poll-interval=5m\"] controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" - name: X_CSI_NFS_ACLS value: \"0777\" nodeSelector: node-role.kubernetes.io/master: \"\" tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" node: envs: - name: \"X_CSI_POWERSTORE_ENABLE_CHAP\" value: \"true\" - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" nodeSelector: node-role.kubernetes.io/worker: \"\" tolerations: - key: \"node-role.kubernetes.io/worker\" operator: \"Exists\" effect: \"NoSchedule\" --- apiVersion: v1 kind: ConfigMap metadata: name: powerstore-config-params namespace: test-powerstore data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"debug\" CSI_LOG_FORMAT: \"JSON\" Users must configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values: Parameter Description Required Default replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be pending state till new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2 namespace Specifies namespace where the drive will be installed Yes “test-powerstore” fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity Enable/Disable storage capacity tracking feature No true Common parameters for node and controller X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node” X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter” Controller parameters X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No \" \" X_CSI_NFS_ACLS Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777” Node parameters X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false Execute the following command to create PowerStore custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerStore driver. After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\nsideCars: # Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\". - name: external-health-monitor args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin- volume status, volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin- volume usage, volume condition # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params Note :\n“Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. ","categories":"","description":"Installing CSI Driver for PowerStore via Operator\n","excerpt":"Installing CSI Driver for PowerStore via Operator\n","ref":"/csm-docs/v3/csidriver/installation/operator/powerstore/","tags":"","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs, and nfs storage classes and automatically mounts them to the pod.\nIt assumes that you’ve created the same basic three storage classes from samples/storageclass folder without changing their names. If you’ve created different storage classes please edit tests/simple/simple.yaml and change PersistentVolumeClaim definitions to point to correct storage classes.\nSteps\nTo run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/ You can find all the created resources in testpowerstore namespace.\nCheck if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\nGo into the created container and verify that everything is mounted correctly.\nAfter verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/ ","categories":"","description":"Tests to validate PowerStore CSI Driver installation","excerpt":"Tests to validate PowerStore CSI Driver installation","ref":"/csm-docs/v3/csidriver/installation/test/powerstore/","tags":"","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v2.6.0 New Features/Changes Added support for Resiliency Added support for Kubernetes 1.26 Added support for MKE 3.6.x Added support for RKE 1.4.1 Fixed Issues Multiple iSCSI network support Create volume successful but unable to map volumes to a hosts Can’t find IP in X_CSI_POWERSTORE_EXTERNAL_ACCESS for NFS provisioning Known Issues Issue Resolution or workaround, if known Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100 fsGroupPolicy may not work as expected without root privileges for NFS onlyhttps://github.com/kubernetes/examples/issues/260 To get the desired behavior set “allowRoot: “true” in the storage class parameter If the NVMeFC pod is not getting created and the host looses the ssh connection, causing the driver pods to go to error state remove the nvme_tcp module from the host incase of NVMeFC connection When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for PowerStore CSI driver","excerpt":"Release notes for PowerStore CSI driver","ref":"/csm-docs/v3/csidriver/release/powerstore/","tags":"","title":"PowerStore"},{"body":" Symptoms Prevention, Resolution or Workaround When you run the command kubectl describe pods powerstore-controller-\u003csuffix\u003e –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command. The kubectl logs -n csi-powerstore powerstore-node-\u003csuffix\u003e driver logs show that the driver can’t connect to PowerStore API. Check if you’ve created a secret with correct credentials Installation of the driver on Kubernetes supported versions fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.21/v1.22/v1.23 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements If PVC is not getting created and getting the following error in PVC description: failed to provision volume with StorageClass \"powerstore-iscsi\": rpc error: code = Internal desc = : Unknown error: Check if you’ve created a secret with correct credentials If the NVMeFC pod is not getting created and the host looses the ssh connection, causing the driver pods to go to error state remove the nvme_tcp module from the host incase of NVMeFC connection When a node goes down, the block volumes attached to the node cannot be attached to another node 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node. If the pod creation for NVMe takes time when the connections between the host and the array are more than 2 and considerable volumes are mounted on the host Reduce the number of connections between the host and the array to 2. Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.22.0 \u003c 1.25.0 which is incompatible with Kubernetes V1.22.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. ","categories":"","description":"Troubleshooting PowerStore Driver","excerpt":"Troubleshooting PowerStore Driver","ref":"/csm-docs/v3/csidriver/troubleshooting/powerstore/","tags":"","title":"PowerStore"},{"body":"Installing CSI Driver for PowerStore via Dell CSM Operator The CSI Driver for Dell PowerStore can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nNote: MKE (Mirantis Kubernetes Engine) does not support the installation of CSI-PowerStore via Operator.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using the following command: kubectl get csm --all-namespaces\nPrerequisite Create namespace. Execute kubectl create namespace test-powerstore to create the test-powerstore namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ’test-powerstore'.\nCreate a file called config.yaml that has Powerstore array connection details with the following content\narrays: - endpoint: \"https://10.0.0.1/api/rest\" # full URL path to the PowerStore API globalID: \"unique\" # unique id of the PowerStore array username: \"user\" # username for connecting to API password: \"password\" # password for connecting to API skipCertificateValidation: true # indicates if client side validation of (management)server's certificate can be skipped isDefault: true # treat current array as a default (would be used by storage classes without arrayID parameter) blockProtocol: \"auto\" # what SCSI transport protocol use on node side (FC, ISCSI, NVMeTCP, NVMeFC, None, or auto) nasName: \"nas-server\" # what NAS should be used for NFS volumes nfsAcls: \"0777\" # (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. # NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares. Change the parameters with relevant values for your PowerStore array. Add more blocks similar to above for each PowerStore array if necessary.\nCreate Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion: v1 kind: Secret metadata: name: test-powerstore-config namespace: test-powerstore type: Opaque data: config: CONFIG_YAML Combine both files and create Kubernetes secret by running the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Install Driver Follow all the prerequisites above\nCreate a CR (Custom Resource) for PowerStore using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:\nParameter Description Required Default replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be in pending state until new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2 namespace Specifies namespace where the driver will be installed Yes “test-powerstore” fsGroupPolicy Defines which FS Group policy mode to be used. Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity Enable/Disable storage capacity tracking feature No false Common parameters for node and controller X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node” X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter” Controller parameters X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No empty X_CSI_NFS_ACLS Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777” Node parameters X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false Execute the following command to create PowerStore custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. This command will deploy the CSI PowerStore driver in the namespace specified in the input YAML file\nNext, the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e Verify the CSI Driver installation\nNote :\n“Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Snapshotter and resizer sidecars are not optional. They are defaults with Driver installation. ","categories":"","description":"Installing Dell CSI Driver for PowerStore via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for PowerStore via Dell CSM Operator\n","ref":"/csm-docs/v3/deployment/csmoperator/drivers/powerstore/","tags":"","title":"PowerStore"},{"body":"Configuring the CSM for Authorization Proxy Server The storage administrator must first configure Authorization with the following via karavictl:\nKaravictl admin token Storage systems Tenants Roles Role bindings Note:\nThe address of the Authorization proxy-server must be specified when executing karavictl. For the RPM deployment, the address is the DNS-hostname of the machine where the RPM is installed. For the Helm/Operator deployment, the address is the Ingress host of the proxy-server with the port of the exposed Ingress Controller. Configuring Admin Token An admin token is required for executing karavictl commands, with the exception of admin token and cluster-info. For example, to generate an admin token and redirect the output to a file:\n$ karavictl admin token --name admin --access-token-expiration 30s --refresh-token-expiration 120m \u003e admintoken.yaml $ Enter JWT Signing Secret: $ cat admintoken.yaml { \"Access\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJjc20iLCJleHAiOjE2ODIzNDg0MzEsImdyb3VwIjoiYWRtaW4iLCJpc3MiOiJjb20uZGVsbC5jc20iLCJyb2xlcyI6IiIsInN1YiI6ImNzbS1hZG1pbiJ9.OxTL48c1VLKSY6oVnYw_jmQ7XHX4UEfwIRkfLQh9beA\", \"Refresh\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJjc20iLCJleHAiOjE2ODQ5NDAzNzEsImdyb3VwIjoiYWRtaW4iLCJpc3MiOiJjb20uZGVsbC5jc20iLCJyb2xlcyI6IiIsInN1YiI6ImNzbS1hZG1pbiJ9._ELmuc2qprZPeuW22wISiw0pvuM6rhyabDOybakqs68\" } Alternatively, the JWT signing secret can be specified with the CLI.\n$ karavictl admin token --name admin --jwt-signing-secret supersecret --access-token-expiration 30s --refresh-token-expiration 120m \u003e admintoken.yaml $ cat admintoken.yaml { \"Access\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJjc20iLCJleHAiOjE2ODIzNDg2MTEsImdyb3VwIjoiYWRtaW4iLCJpc3MiOiJjb20uZGVsbC5jc20iLCJyb2xlcyI6IiIsInN1YiI6ImNzbS1hZG1pbiJ9.C6c9DrlOE95_soFm0YEyzs08ye2TL_koYsp4qJFEglI\", \"Refresh\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJjc20iLCJleHAiOjE2ODIzNTU3ODEsImdyb3VwIjoiYWRtaW4iLCJpc3MiOiJjb20uZGVsbC5jc20iLCJyb2xlcyI6IiIsInN1YiI6ImNzbS1hZG1pbiJ9.XMcOVIuJ56JhuJrfGqQ_DUqXDyHLxrOrkvQJUxAOst4\" } Note:\nThe karavictl admin token command is an exception where you do not need to specify the address of the proxy-server. Configuring Storage A storage entity in CSM Authorization consists of the storage type (PowerFlex, PowerMax, PowerScale), the system ID, the API endpoint, and the credentials. For example, to create PowerFlex storage:\n#RPM Deployment\nkaravictl storage create --type powerflex --endpoint ${powerflexIP} --system-id ${systemID} --user ${user} --password ${password} --array-insecure --insecure --addr DNS-hostname --admin-token admintoken.yaml #Helm/Operator Deployment\nkaravictl storage create --type powerflex --endpoint ${powerflexIP} --system-id ${systemID} --user ${user} --password ${password} --array-insecure --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml Note:\nThe insecure flag specifies to skip certificate validation when connecting to the Authorization proxy-server. The addr flag is the address of the Authorization proxy-server. The array-insecure flag specifies to skip certificate validation when proxy-service connects to the backend storage array. The powerflexIP is the API endpoint of your PowerFlex. You can find the systemID at the https://\u003cpowerflex_gui_address\u003e/dashboard/performance near the System title. The user and password arguments are credentials to the powerflex UI. Run karavictl storage create --help for help. Configuring Tenants A tenant is a Kubernetes cluster that a role will be bound to. For example, to create a tenant named Finance: #RPM Deployment\nkaravictl tenant create --name Finance --insecure --addr DNS-hostname --admin-token admintoken.yaml #Helm/Operator Deployment\nkaravictl tenant create --name Finance --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml Note:\nThe insecure flag specifies to skip certificate validation when connecting to the Authorization proxy-server. The addr flag is the address of the Authorization proxy-server. Run karavictl tenant create --help for help. For the Powerflex Pre-approved Guid feature, the approvesdc boolean flag is true by default. If the approvesdc flag is false for a tenant, the proxy server will deny the requests to approve SDC if the SDCs are already in not-approved state. Inorder to change this flag for an already created tenant, see tenant update command in CLI section. #RPM Deployment\nkaravictl tenant create --name Finance --approvesdc=false --insecure --addr DNS-hostname --admin-token admintoken.yaml #Helm/Operator Deployment\nkaravictl tenant create --name Finance --approvesdc=false --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml Configuring Roles A role consists of a name, the storage to use, and the quota limit for the storage pool to be used. For example, to create a role named FinanceRole using the PowerFlex storage created above with a quota limit of 100GB in storage pool myStoragePool:\n#RPM Deployment\nkaravictl role create --role=FinanceRole=powerflex=${systemID}=myStoragePool=100GB --insecure --addr DNS-hostname --admin-token admintoken.yaml #Helm/Operator Deployment\nkaravictl role create --role=FinanceRole=powerflex=${systemID}=myStoragePool=100GB --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml Note:\nThe insecure flag specifies to skip certificate validation when connecting to the Authorization proxy-server. The addr flag is the address of the Authorization proxy-server. Run karavictl role create --help for help. Configuring Role Bindings A role binding binds a role to a tenant. For example, to bind the FinanceRole to the Finance tenant:\n#RPM Deployment\nkaravictl rolebinding create --tenant Finance --role FinanceRole --insecure --addr DNS-hostname --admin-token admintoken.yaml #Helm/Operator Deployment\nkaravictl rolebinding create --tenant Finance --role FinanceRole --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml Note:\nThe insecure flag specifies to skip certificate validation when connecting to the Authorization proxy-server. The addr flag is the address of the Authorization proxy-server. Run karavictl rolebinding create --help for help. Generate a Token Once rolebindings are created, an access/refresh token pair can be created for the tenant. The storage admin is responsible for generating and sending the token to the Kubernetes tenant admin.\n#RPM Deployment\nkaravictl generate token --tenant Finance --insecure --addr DNS-hostname --admin-token admintoken.yaml \u003e token.yaml #Helm/Operator Deployment\nkaravictl generate token --tenant Finance --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml \u003e token.yaml token.yaml will have a Kubernetes secret manifest that looks like this:\napiVersion: v1 data: access: ZXlKaGJHY2lPaUpJVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SmhkV1FpT2lKamMyMGlMQ0psZUhBaU9qRTJPREl3TVRBeU5UTXNJbWR5YjNWd0lqb2labTl2SWl3aWFYTnpJam9pWTI5dExtUmxiR3d1WTNOdElpd2ljbTlzWlhNaU9pSmlZWElpTENKemRXSWlPaUpqYzIwdGRHVnVZVzUwSW4wLjlSYkJISzJUS2dZbVdDX0paazBoSXV0N0daSDV4NGVjQVk2ekdaUDNvUWs= refresh: ZXlKaGJHY2lPaUpJVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SmhkV1FpT2lKamMyMGlMQ0psZUhBaU9qRTJPRFEyTURJeE9UTXNJbWR5YjNWd0lqb2labTl2SWl3aWFYTnpJam9pWTI5dExtUmxiR3d1WTNOdElpd2ljbTlzWlhNaU9pSmlZWElpTENKemRXSWlPaUpqYzIwdGRHVnVZVzUwSW4wLkxQcDQzbXktSVJudTFjdmZRcko4M0pMdTR2NXlWQlRDV2NjWFpfWjROQkU= kind: Secret metadata: creationTimestamp: null name: proxy-authz-tokens type: Opaque This secret must be applied in the driver namespace.\nNote:\nThe insecure flag specifies to skip certificate validation when connecting to the Authorization proxy-server. The addr flag is the address of the Authorization proxy-server. Run karavictl generate token --help for help. ","categories":"","description":"Configuring the CSM for Authorization Proxy Server\n","excerpt":"Configuring the CSM for Authorization Proxy Server\n","ref":"/csm-docs/docs/authorization/configuration/proxy-server/","tags":"","title":"Proxy Server"},{"body":"Configuring the CSM for Authorization Proxy Server The storage administrator must first configure Authorization with the following via karavictl:\nKaravictl admin token Storage systems Tenants Roles Role bindings Note:\nThe address of the Authorization proxy-server must be specified when executing karavictl. For the RPM deployment, the address is the DNS-hostname of the machine where the RPM is installed. For the Helm/Operator deployment, the address is the Ingress host of the proxy-server with the port of the exposed Ingress Controller. Configuring Admin Token An admin token is required for executing karavictl commands, with the exception of admin token and cluster-info. For example, to generate an admin token and redirect the output to a file:\n$ karavictl admin token --name admin --access-token-expiration 30s --refresh-token-expiration 120m \u003e admintoken.yaml $ Enter JWT Signing Secret: $ cat admintoken.yaml { \"Access\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJjc20iLCJleHAiOjE2ODIzNDg0MzEsImdyb3VwIjoiYWRtaW4iLCJpc3MiOiJjb20uZGVsbC5jc20iLCJyb2xlcyI6IiIsInN1YiI6ImNzbS1hZG1pbiJ9.OxTL48c1VLKSY6oVnYw_jmQ7XHX4UEfwIRkfLQh9beA\", \"Refresh\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJjc20iLCJleHAiOjE2ODQ5NDAzNzEsImdyb3VwIjoiYWRtaW4iLCJpc3MiOiJjb20uZGVsbC5jc20iLCJyb2xlcyI6IiIsInN1YiI6ImNzbS1hZG1pbiJ9._ELmuc2qprZPeuW22wISiw0pvuM6rhyabDOybakqs68\" } Alternatively, the JWT signing secret can be specified with the CLI.\n$ karavictl admin token --name admin --jwt-signing-secret supersecret --access-token-expiration 30s --refresh-token-expiration 120m \u003e admintoken.yaml $ cat admintoken.yaml { \"Access\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJjc20iLCJleHAiOjE2ODIzNDg2MTEsImdyb3VwIjoiYWRtaW4iLCJpc3MiOiJjb20uZGVsbC5jc20iLCJyb2xlcyI6IiIsInN1YiI6ImNzbS1hZG1pbiJ9.C6c9DrlOE95_soFm0YEyzs08ye2TL_koYsp4qJFEglI\", \"Refresh\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJjc20iLCJleHAiOjE2ODIzNTU3ODEsImdyb3VwIjoiYWRtaW4iLCJpc3MiOiJjb20uZGVsbC5jc20iLCJyb2xlcyI6IiIsInN1YiI6ImNzbS1hZG1pbiJ9.XMcOVIuJ56JhuJrfGqQ_DUqXDyHLxrOrkvQJUxAOst4\" } Note:\nThe karavictl admin token command is an exception where you do not need to specify the address of the proxy-server. Configuring Storage A storage entity in CSM Authorization consists of the storage type (PowerFlex, PowerMax, PowerScale), the system ID, the API endpoint, and the credentials. For example, to create PowerFlex storage:\n#RPM Deployment\nkaravictl storage create --type powerflex --endpoint ${powerflexIP} --system-id ${systemID} --user ${user} --password ${password} --array-insecure --insecure --addr DNS-hostname --admin-token admintoken.yaml #Helm/Operator Deployment\nkaravictl storage create --type powerflex --endpoint ${powerflexIP} --system-id ${systemID} --user ${user} --password ${password} --array-insecure --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml Note:\nThe insecure flag specifies to skip certificate validation when connecting to the Authorization proxy-server. The addr flag is the address of the Authorization proxy-server. The array-insecure flag specifies to skip certificate validation when proxy-service connects to the backend storage array. The powerflexIP is the API endpoint of your PowerFlex. You can find the systemID at the https://\u003cpowerflex_gui_address\u003e/dashboard/performance near the System title. The user and password arguments are credentials to the powerflex UI. Run karavictl storage create --help for help. Configuring Tenants A tenant is a Kubernetes cluster that a role will be bound to. For example, to create a tenant named Finance: #RPM Deployment\nkaravictl tenant create --name Finance --insecure --addr DNS-hostname --admin-token admintoken.yaml #Helm/Operator Deployment\nkaravictl tenant create --name Finance --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml Note:\nThe insecure flag specifies to skip certificate validation when connecting to the Authorization proxy-server. The addr flag is the address of the Authorization proxy-server. Run karavictl tenant create --help for help. For the Powerflex Pre-approved Guid feature, the approvesdc boolean flag is true by default. If the approvesdc flag is false for a tenant, the proxy server will deny the requests to approve SDC if the SDCs are already in not-approved state. Inorder to change this flag for an already created tenant, see tenant update command in CLI section. #RPM Deployment\nkaravictl tenant create --name Finance --approvesdc=false --insecure --addr DNS-hostname --admin-token admintoken.yaml #Helm/Operator Deployment\nkaravictl tenant create --name Finance --approvesdc=false --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml Configuring Roles A role consists of a name, the storage to use, and the quota limit for the storage pool to be used. For example, to create a role named FinanceRole using the PowerFlex storage created above with a quota limit of 100GB in storage pool myStoragePool:\n#RPM Deployment\nkaravictl role create --role=FinanceRole=powerflex=${systemID}=myStoragePool=100GB --insecure --addr DNS-hostname --admin-token admintoken.yaml #Helm/Operator Deployment\nkaravictl role create --role=FinanceRole=powerflex=${systemID}=myStoragePool=100GB --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml Note:\nThe insecure flag specifies to skip certificate validation when connecting to the Authorization proxy-server. The addr flag is the address of the Authorization proxy-server. Run karavictl role create --help for help. Configuring Role Bindings A role binding binds a role to a tenant. For example, to bind the FinanceRole to the Finance tenant:\n#RPM Deployment\nkaravictl rolebinding create --tenant Finance --role FinanceRole --insecure --addr DNS-hostname --admin-token admintoken.yaml #Helm/Operator Deployment\nkaravictl rolebinding create --tenant Finance --role FinanceRole --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml Note:\nThe insecure flag specifies to skip certificate validation when connecting to the Authorization proxy-server. The addr flag is the address of the Authorization proxy-server. Run karavictl rolebinding create --help for help. Generate a Token Once rolebindings are created, an access/refresh token pair can be created for the tenant. The storage admin is responsible for generating and sending the token to the Kubernetes tenant admin.\n#RPM Deployment\nkaravictl generate token --tenant Finance --insecure --addr DNS-hostname --admin-token admintoken.yaml \u003e token.yaml #Helm/Operator Deployment\nkaravictl generate token --tenant Finance --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml \u003e token.yaml token.yaml will have a Kubernetes secret manifest that looks like this:\napiVersion: v1 data: access: ZXlKaGJHY2lPaUpJVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SmhkV1FpT2lKamMyMGlMQ0psZUhBaU9qRTJPREl3TVRBeU5UTXNJbWR5YjNWd0lqb2labTl2SWl3aWFYTnpJam9pWTI5dExtUmxiR3d1WTNOdElpd2ljbTlzWlhNaU9pSmlZWElpTENKemRXSWlPaUpqYzIwdGRHVnVZVzUwSW4wLjlSYkJISzJUS2dZbVdDX0paazBoSXV0N0daSDV4NGVjQVk2ekdaUDNvUWs= refresh: ZXlKaGJHY2lPaUpJVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SmhkV1FpT2lKamMyMGlMQ0psZUhBaU9qRTJPRFEyTURJeE9UTXNJbWR5YjNWd0lqb2labTl2SWl3aWFYTnpJam9pWTI5dExtUmxiR3d1WTNOdElpd2ljbTlzWlhNaU9pSmlZWElpTENKemRXSWlPaUpqYzIwdGRHVnVZVzUwSW4wLkxQcDQzbXktSVJudTFjdmZRcko4M0pMdTR2NXlWQlRDV2NjWFpfWjROQkU= kind: Secret metadata: creationTimestamp: null name: proxy-authz-tokens type: Opaque This secret must be applied in the driver namespace.\nNote:\nThe insecure flag specifies to skip certificate validation when connecting to the Authorization proxy-server. The addr flag is the address of the Authorization proxy-server. Run karavictl generate token --help for help. ","categories":"","description":"Configuring the CSM for Authorization Proxy Server\n","excerpt":"Configuring the CSM for Authorization Proxy Server\n","ref":"/csm-docs/v1/authorization/configuration/proxy-server/","tags":"","title":"Proxy Server"},{"body":"Configuring the CSM for Authorization Proxy Server The storage administrator must first configure Authorization with the following via karavictl:\nKaravictl admin token Storage systems Tenants Roles Role bindings Note:\nThe address of the Authorization proxy-server must be specified when executing karavictl. For the RPM deployment, the address is the DNS-hostname of the machine where the RPM is installed. For the Helm/Operator deployment, the address is the Ingress host of the proxy-server with the port of the exposed Ingress Controller. Configuring Admin Token An admin token is required for executing karavictl commands, with the exception of admin token and cluster-info. For example, to generate an admin token and redirect the output to a file:\n$ karavictl admin token --name admin --access-token-expiration 30s --refresh-token-expiration 120m \u003e admintoken.yaml $ Enter JWT Signing Secret: $ cat admintoken.yaml { \"Access\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJjc20iLCJleHAiOjE2ODIzNDg0MzEsImdyb3VwIjoiYWRtaW4iLCJpc3MiOiJjb20uZGVsbC5jc20iLCJyb2xlcyI6IiIsInN1YiI6ImNzbS1hZG1pbiJ9.OxTL48c1VLKSY6oVnYw_jmQ7XHX4UEfwIRkfLQh9beA\", \"Refresh\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJjc20iLCJleHAiOjE2ODQ5NDAzNzEsImdyb3VwIjoiYWRtaW4iLCJpc3MiOiJjb20uZGVsbC5jc20iLCJyb2xlcyI6IiIsInN1YiI6ImNzbS1hZG1pbiJ9._ELmuc2qprZPeuW22wISiw0pvuM6rhyabDOybakqs68\" } Alternatively, the JWT signing secret can be specified with the CLI.\n$ karavictl admin token --name admin --jwt-signing-secret supersecret --access-token-expiration 30s --refresh-token-expiration 120m \u003e admintoken.yaml $ cat admintoken.yaml { \"Access\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJjc20iLCJleHAiOjE2ODIzNDg2MTEsImdyb3VwIjoiYWRtaW4iLCJpc3MiOiJjb20uZGVsbC5jc20iLCJyb2xlcyI6IiIsInN1YiI6ImNzbS1hZG1pbiJ9.C6c9DrlOE95_soFm0YEyzs08ye2TL_koYsp4qJFEglI\", \"Refresh\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJjc20iLCJleHAiOjE2ODIzNTU3ODEsImdyb3VwIjoiYWRtaW4iLCJpc3MiOiJjb20uZGVsbC5jc20iLCJyb2xlcyI6IiIsInN1YiI6ImNzbS1hZG1pbiJ9.XMcOVIuJ56JhuJrfGqQ_DUqXDyHLxrOrkvQJUxAOst4\" } Note:\nThe karavictl admin token command is an exception where you do not need to specify the address of the proxy-server. Configuring Storage A storage entity in CSM Authorization consists of the storage type (PowerFlex, PowerMax, PowerScale), the system ID, the API endpoint, and the credentials. For example, to create PowerFlex storage:\n#RPM Deployment\nkaravictl storage create --type powerflex --endpoint ${powerflexIP} --system-id ${systemID} --user ${user} --password ${password} --array-insecure --insecure --addr DNS-hostname --admin-token admintoken.yaml #Helm/Operator Deployment\nkaravictl storage create --type powerflex --endpoint ${powerflexIP} --system-id ${systemID} --user ${user} --password ${password} --array-insecure --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml Note:\nThe insecure flag specifies to skip certificate validation when connecting to the Authorization proxy-server. The addr flag is the address of the Authorization proxy-server. The array-insecure flag specifies to skip certificate validation when proxy-service connects to the backend storage array. The powerflexIP is the API endpoint of your PowerFlex. You can find the systemID at the https://\u003cpowerflex_gui_address\u003e/dashboard/performance near the System title. The user and password arguments are credentials to the powerflex UI. Run karavictl storage create --help for help. Configuring Tenants A tenant is a Kubernetes cluster that a role will be bound to. For example, to create a tenant named Finance: #RPM Deployment\nkaravictl tenant create --name Finance --insecure --addr DNS-hostname --admin-token admintoken.yaml #Helm/Operator Deployment\nkaravictl tenant create --name Finance --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml Note:\nThe insecure flag specifies to skip certificate validation when connecting to the Authorization proxy-server. The addr flag is the address of the Authorization proxy-server. Run karavictl tenant create --help for help. For the Powerflex Pre-approved Guid feature, the approvesdc boolean flag is true by default. If the approvesdc flag is false for a tenant, the proxy server will deny the requests to approve SDC if the SDCs are already in not-approved state. Inorder to change this flag for an already created tenant, see tenant update command in CLI section. #RPM Deployment\nkaravictl tenant create --name Finance --approvesdc=false --insecure --addr DNS-hostname --admin-token admintoken.yaml #Helm/Operator Deployment\nkaravictl tenant create --name Finance --approvesdc=false --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml Configuring Roles A role consists of a name, the storage to use, and the quota limit for the storage pool to be used. For example, to create a role named FinanceRole using the PowerFlex storage created above with a quota limit of 100GB in storage pool myStoragePool:\n#RPM Deployment\nkaravictl role create --role=FinanceRole=powerflex=${systemID}=myStoragePool=100GB --insecure --addr DNS-hostname --admin-token admintoken.yaml #Helm/Operator Deployment\nkaravictl role create --role=FinanceRole=powerflex=${systemID}=myStoragePool=100GB --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml Note:\nThe insecure flag specifies to skip certificate validation when connecting to the Authorization proxy-server. The addr flag is the address of the Authorization proxy-server. Run karavictl role create --help for help. Configuring Role Bindings A role binding binds a role to a tenant. For example, to bind the FinanceRole to the Finance tenant:\n#RPM Deployment\nkaravictl rolebinding create --tenant Finance --role FinanceRole --insecure --addr DNS-hostname --admin-token admintoken.yaml #Helm/Operator Deployment\nkaravictl rolebinding create --tenant Finance --role FinanceRole --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml Note:\nThe insecure flag specifies to skip certificate validation when connecting to the Authorization proxy-server. The addr flag is the address of the Authorization proxy-server. Run karavictl rolebinding create --help for help. Generate a Token Once rolebindings are created, an access/refresh token pair can be created for the tenant. The storage admin is responsible for generating and sending the token to the Kubernetes tenant admin.\n#RPM Deployment\nkaravictl generate token --tenant Finance --insecure --addr DNS-hostname --admin-token admintoken.yaml \u003e token.yaml #Helm/Operator Deployment\nkaravictl generate token --tenant Finance --insecure --addr csm-authorization.com:\u003cingress-controller-port\u003e --admin-token admintoken.yaml \u003e token.yaml token.yaml will have a Kubernetes secret manifest that looks like this:\napiVersion: v1 data: access: ZXlKaGJHY2lPaUpJVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SmhkV1FpT2lKamMyMGlMQ0psZUhBaU9qRTJPREl3TVRBeU5UTXNJbWR5YjNWd0lqb2labTl2SWl3aWFYTnpJam9pWTI5dExtUmxiR3d1WTNOdElpd2ljbTlzWlhNaU9pSmlZWElpTENKemRXSWlPaUpqYzIwdGRHVnVZVzUwSW4wLjlSYkJISzJUS2dZbVdDX0paazBoSXV0N0daSDV4NGVjQVk2ekdaUDNvUWs= refresh: ZXlKaGJHY2lPaUpJVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SmhkV1FpT2lKamMyMGlMQ0psZUhBaU9qRTJPRFEyTURJeE9UTXNJbWR5YjNWd0lqb2labTl2SWl3aWFYTnpJam9pWTI5dExtUmxiR3d1WTNOdElpd2ljbTlzWlhNaU9pSmlZWElpTENKemRXSWlPaUpqYzIwdGRHVnVZVzUwSW4wLkxQcDQzbXktSVJudTFjdmZRcko4M0pMdTR2NXlWQlRDV2NjWFpfWjROQkU= kind: Secret metadata: creationTimestamp: null name: proxy-authz-tokens type: Opaque This secret must be applied in the driver namespace.\nNote:\nThe insecure flag specifies to skip certificate validation when connecting to the Authorization proxy-server. The addr flag is the address of the Authorization proxy-server. Run karavictl generate token --help for help. ","categories":"","description":"Configuring the CSM for Authorization Proxy Server\n","excerpt":"Configuring the CSM for Authorization Proxy Server\n","ref":"/csm-docs/v2/authorization/configuration/proxy-server/","tags":"","title":"Proxy Server"},{"body":" The CSM Authorization karavictl CLI is no longer actively maintained or supported. It will be deprecated in CSM 2.0.\nConfiguring the CSM for Authorization Proxy Server The storage administrator must first configure the proxy server with the following:\nStorage systems Tenants Roles Bind roles to tenants Note:\nThe RPM deployment will use the address and port of the server (i.e. grpc.:443). The Helm deployment will use the address and port of the Ingress hosts for the storage, tenant, and role services. Configuring Storage A storage entity in CSM Authorization consists of the storage type (PowerFlex, PowerMax, PowerScale), the system ID, the API endpoint, and the credentials. For example, to create PowerFlex storage:\n# RPM Deployment karavictl storage create --type powerflex --endpoint ${powerflexIP} --system-id ${systemID} --user ${user} --password ${password} --array-insecure # Helm Deployment karavictl storage create --type powerflex --endpoint ${powerflexIP} --system-id ${systemID} --user ${user} --password ${password} --insecure --array-insecure --addr storage.csm-authorization.com:\u003cingress-nginx-controller-port\u003e Note:\nThe insecure flag specifies to skip certificate validation when connecting to the CSM Authorization storage service. The array-insecure flag specifies to skip certificate validation when proxy-service connects to the backend storage array. Run karavictl storage create --help for help. The powerflexIP is the endpoint to your powerflex machine. You can find the systemID at the https://\u003cyour_powerflex_ip_address\u003e/dashboard/performance near the System title. The user and password arguments are credentials to the powerflex UI. Configuring Tenants A tenant is a Kubernetes cluster that a role will be bound to. For example, to create a tenant named Finance:\n# RPM Deployment karavictl tenant create --name Finance --insecure --addr grpc.\u003cDNS-hostname\u003e:443 # Helm Deployment karavictl tenant create --name Finance --insecure --addr tenant.csm-authorization.com:\u003cingress-nginx-controller-port\u003e Note:\nThe insecure flag specifies to skip certificate validation when connecting to the tenant service. Run karavictl tenant create --help for help. DNS-hostname refers to the hostname of the system in which the CSM for Authorization server will be installed. This hostname can be found by running nslookup \u003cIP_address\u003e For the Powerflex Pre-approved Guid feature, the approvesdc boolean flag is true by default. If the approvesdc flag is false for a tenant, the proxy server will deny the requests to approve SDC if the SDCs are already in not-approved state. Inorder to change this flag for an already created tenant, see tenant update command in CLI section. # RPM Deployment karavictl tenant create --name Finance --approvesdc=false --insecure --addr grpc.DNS-hostname:443 # Helm Deployment karavictl tenant create --name Finance --approvesdc=false --insecure --addr tenant.csm-authorization.com:\u003cingress-nginx-controller-port\u003e Configuring Roles A role consists of a name, the storage to use, and the quota limit for the storage pool to be used. For example, to create a role named FinanceRole using the PowerFlex storage created above with a quota limit of 100GB in storage pool myStoragePool:\n# RPM Deployment karavictl role create --role=FinanceRole=powerflex=${systemID}=myStoragePool=100GB # Helm Deployment karavictl role create --insecure --addr role.csm-authorization.com:30016 --role=FinanceRole=powerflex=${systemID}=myStoragePool=100GB Note:\nThe insecure flag specifies to skip certificate validation when connecting to the role service. Run karavictl role create --help for help. Configuring Role Bindings A role binding binds a role to a tenant. For example, to bind the FinanceRole to the Finance tenant:\n# RPM Deployment karavictl rolebinding create --tenant Finance --role FinanceRole --insecure --addr grpc.\u003cDNS-hostname\u003e:443 # Helm Deployment karavictl rolebinding create --tenant Finance --role FinanceRole --insecure --addr tenant.csm-authorization.com:\u003cingress-nginx-controller-port\u003e Note:\nThe insecure flag specifies to skip certificate validation when connecting to the tenant service. Run karavictl rolebinding create --help for help. Generate a Token RPM Deployment Helm Deployment RPM After creating the role bindings, the next logical step is to generate the access token. The storage admin is responsible for generating and sending the token to the Kubernetes tenant admin.\nNote:\nThe --insecure flag is required if certificates were not provided in $HOME/.karavi/config.json. This sample copies the token directly to the Kubernetes cluster master node. The requirement here is that the token must be copied and/or stored in any location accessible to the Kubernetes tenant admin. echo === Generating token === karavictl generate token --tenant ${tenantName} --insecure --addr grpc.\u003cDNS-hostname\u003e:443 | sed -e 's/\"Token\": //' -e 's/[{}\"]//g' -e 's/\\\\n/\\n/g' \u003e token.yaml echo === Copy token to Driver Host === sshpass -p ${DriverHostPassword} scp token.yaml ${DriverHostVMUser}@{DriverHostVMIP}:/tmp/token.yaml Helm Now that the tenant is bound to a role, a JSON Web Token can be generated for the tenant. For example, to generate a token for the Finance tenant:\nkaravictl generate token --tenant Finance --insecure --addr tenant.csm-authorization.com:\u003cingress-nginx-controller-port\u003e { \"Token\": \"\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n name: proxy-authz-tokens\\ntype: Opaque\\ndata:\\n access: ZXlKaGJHY2lPaUpJVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SmhkV1FpT2lKcllYSmhkbWtpTENKbGVIQWlPakUyTlRNek1qUXhPRFlzSW1keWIzVndJam9pWm05dklpd2lhWE56SWpvaVkyOXRMbVJsYkd3dWEyRnlZWFpwSWl3aWNtOXNaWE1pT2lKaVlYSWlMQ0p6ZFdJaU9pSnJZWEpoZG1rdGRHVnVZVzUwSW4wLmJIODN1TldmaHoxc1FVaDcweVlfMlF3N1NTVnEyRzRKeGlyVHFMWVlEMkU=\\n refresh: ZXlKaGJHY2lPaUpJVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SmhkV1FpT2lKcllYSmhkbWtpTENKbGVIQWlPakUyTlRVNU1UWXhNallzSW1keWIzVndJam9pWm05dklpd2lhWE56SWpvaVkyOXRMbVJsYkd3dWEyRnlZWFpwSWl3aWNtOXNaWE1pT2lKaVlYSWlMQ0p6ZFdJaU9pSnJZWEpoZG1rdGRHVnVZVzUwSW4wLkxNbWVUSkZlX2dveXR0V0lUUDc5QWVaTy1kdmN5SHAwNUwyNXAtUm9ZZnM=\\n\" } Process the above response to filter the secret manifest. For example using sed you can run the following:\nkaravictl generate token --tenant Finance --insecure --addr tenant.csm-authorization.com:\u003cingress-nginx-controller-port\u003e | sed -e 's/\"Token\": //' -e 's/[{}\"]//g' -e 's/\\\\n/\\n/g' apiVersion: v1 kind: Secret metadata: name: proxy-authz-tokens type: Opaque data: access: ZXlKaGJHY2lPaUpJVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SmhkV1FpT2lKcllYSmhkbWtpTENKbGVIQWlPakUyTlRNek1qUTFOekVzSW1keWIzVndJam9pWm05dklpd2lhWE56SWpvaVkyOXRMbVJsYkd3dWEyRnlZWFpwSWl3aWNtOXNaWE1pT2lKaVlYSWlMQ0p6ZFdJaU9pSnJZWEpoZG1rdGRHVnVZVzUwSW4wLk4tNE42Q1pPbUptcVQtRDF5ZkNGdEZqSmRDRjcxNlh1SXlNVFVyckNOS1U= refresh: ZXlKaGJHY2lPaUpJVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SmhkV1FpT2lKcllYSmhkbWtpTENKbGVIQWlPakUyTlRVNU1UWTFNVEVzSW1keWIzVndJam9pWm05dklpd2lhWE56SWpvaVkyOXRMbVJsYkd3dWEyRnlZWFpwSWl3aWNtOXNaWE1pT2lKaVlYSWlMQ0p6ZFdJaU9pSnJZWEpoZG1rdGRHVnVZVzUwSW4wLkVxb3lXNld5ZEFLdU9mSmtkMkZaMk9TVThZMzlKUFc0YmhfNHc5R05ZNmM= This secret must be applied in the driver namespace.\n","categories":"","description":"Configuring the CSM for Authorization Proxy Server\n","excerpt":"Configuring the CSM for Authorization Proxy Server\n","ref":"/csm-docs/v3/authorization/configuration/proxy-server/","tags":"","title":"Proxy Server"},{"body":"Release Notes - CSM Installation Wizard 1.0.1 New Features/Changes Added support for the CSI-PowerStore, CSI-PowerMax, CSI-PowerScale, CSI-PowerFlex, CSI-Unity and the supported modules for helm installation Fixed Issues Known Issues Issue Workaround Tolerations required by the Resiliency module are commented out in the generated values.yaml file when Resiliency module is selected If Resiliency is selected, the commented tolerations in the generated values.yaml file must be manually uncommented. The issue has been created at https://github.com/dell/csm/issues/866 ","categories":"","description":"Release notes for CSM Installation Wizard","excerpt":"Release notes for CSM Installation Wizard","ref":"/csm-docs/v2/deployment/csminstallationwizard/release_notes/","tags":"","title":"Release Notes"},{"body":"The CSM Replication module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. Dell CSM Operator will deploy the CSM Replication sidecar and the CSM Replication Controller Manager.\nPrerequisites To configure Replication prior to installation via CSM Operator, you need:\na source cluster which is the main cluster a target cluster which will serve as the disaster recovery cluster NOTE: If using a single Kubernetes cluster in a stretched configuration, there will be only one cluster. The source cluster is also the target cluster.\n(Optional) If CSM Replication is being deployed using two clusters in an environment where the DNS is not configured, and the cluster API endpoints are FQDNs, it is necessary to add the \u003cFQDN\u003e:\u003cIP\u003e mapping in the /etc/hosts file in order to resolve queries to the remote API server. This change will need to be made to the /etc/hosts file on:\nThe bastion node(s) (or wherever repctl is used). Either the CSM Operator Deployment or ClusterServiceVersion custom resource if using an Operator Lifecycle Manager (such as with an OperatorHub install). Both dell-replication-controller-manager deployments (covered in Configuration Steps below). Update the ClusterServiceVersion before continuing. Execute the command below, replacing the fields for the remote cluster’s FQDN and IP.\nkubectl patch clusterserviceversions.operators.coreos.com -n \u003coperator-namespace\u003e dell-csm-operator-certified.v1.3.0 \\ --type=json -p='[{\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/hostAliases\", \"value\": [{\"ip\":\"\u003cremote-IP\u003e\",\"hostnames\":[\"\u003cremote-FQDN\u003e\"]}]}]' Cloning the GitHub Repository and Building repctl The csm-replication GitHub repository is cloned to your source cluster as part of the installation. On your source cluster run the following to clone and build the repctl tool:\ngit clone -b v1.7.0 https://github.com/dell/csm-replication.git cd csm-replication/repctl make build Alternately, you can download a pre-built repctl binary from our Releases page.\nwget https://github.com/dell/csm-replication/releases/download/v1.7.0/repctl-linux-amd64 mv repctl-linux-amd64 repctl chmod +x repctl The rest of the instructions will assume that your current working directory is the csm-replication/repctl directory.\nConfiguration Steps To configure Replication perform the following steps:\nOn your main cluster collect the cluster admin configurations for each of the clusters. In the following example the source cluster, cluster-1 uses configuration /root/.kube/config-1 and the target cluster, cluster-2 uses the configuration /root/.config/config-2. Use repctl to add the clusters: ./repctl cluster add -f \"/root/.kube/config-1\",\"/root/.kube/config-2\" -n \"cluster-1\",\"cluster-2\" NOTE: If using a single Kubernetes cluster in a stretched configuration there will be only one cluster.\nInstall the replication controller CRDs:\n./repctl create -f ../deploy/replicationcrds.all.yaml Inject the service account’s configuration into the clusters.\n./repctl cluster inject Customize the examples/\u003cstorage\u003e_example_values.yaml sample config. Set the values for sourceClusterID and targetClusterID to the same names used in step 1. For a stretched cluster set both fields to self:\nCreate the replication storage classes using the modified configuration from step 4:\n./repctl create sc --from-config ./examples/\u003cstorage\u003e_example_values.yaml On the target cluster, configure the prerequisites for deploying the driver via Dell CSM Operator.\nInstall the CSI driver for your chosen storage platform on the source cluster according to the instructions for installing the drivers using CSM Operator.\n(Optional) If CSM Replication is deployed using two clusters in an environment where the DNS is not configured, it is necessary to update the dell-replication-controller-manager Kubernetes deployment to map the API endpoint FQDN to an IP address by adding the hostAliases field and associated FQDN:IP mappings.\nTo update the dell-replication-controller-manager deployment, execute the command below, replacing the fields for the remote cluster’s FQDN and IP. Make sure to update the deployment on both the primary and disaster recovery clusters.\nkubectl patch deployment -n dell-replication-controller dell-replication-controller-manager \\ -p '{\"spec\":{\"template\":{\"spec\":{\"hostAliases\":[{\"hostnames\":[\"\u003cremote-FQDN\u003e\"],\"ip\":\"\u003cremote-IP\u003e\"}]}}}}' ","categories":"","description":"Installing Replication via Dell CSM Operator\n","excerpt":"Installing Replication via Dell CSM Operator\n","ref":"/csm-docs/docs/deployment/csmoperator/modules/replication/","tags":"","title":"Replication"},{"body":"The CSM Replication module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. Dell CSM Operator will deploy the CSM Replication sidecar and the CSM Replication Controller Manager.\nPrerequisites To configure Replication prior to installation via CSM Operator, you need:\na source cluster which is the main cluster a target cluster which will serve as the disaster recovery cluster NOTE: If using a single Kubernetes cluster in a stretched configuration, there will be only one cluster. The source cluster is also the target cluster.\n(Optional) If CSM Replication is being deployed using two clusters in an environment where the DNS is not configured, and the cluster API endpoints are FQDNs, it is necessary to add the \u003cFQDN\u003e:\u003cIP\u003e mapping in the /etc/hosts file in order to resolve queries to the remote API server. This change will need to be made to the /etc/hosts file on:\nThe bastion node(s) (or wherever repctl is used). Either the CSM Operator Deployment or ClusterServiceVersion custom resource if using an Operator Lifecycle Manager (such as with an OperatorHub install). Both dell-replication-controller-manager deployments (covered in Configuration Steps below). Update the ClusterServiceVersion before continuing. Execute the command below, replacing the fields for the remote cluster’s FQDN and IP.\nkubectl patch clusterserviceversions.operators.coreos.com -n \u003coperator-namespace\u003e dell-csm-operator-certified.v1.3.0 \\ --type=json -p='[{\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/hostAliases\", \"value\": [{\"ip\":\"\u003cremote-IP\u003e\",\"hostnames\":[\"\u003cremote-FQDN\u003e\"]}]}]' Cloning the GitHub Repository and Building repctl The csm-replication GitHub repository is cloned to your source cluster as part of the installation. On your source cluster run the following to clone and build the repctl tool:\ngit clone -b v1.6.0 https://github.com/dell/csm-replication.git cd csm-replication/repctl make build Alternately, you can download a pre-built repctl binary from our Releases page.\nwget https://github.com/dell/csm-replication/releases/download/v1.6.0/repctl-linux-amd64 mv repctl-linux-amd64 repctl chmod +x repctl The rest of the instructions will assume that your current working directory is the csm-replication/repctl directory.\nConfiguration Steps To configure Replication perform the following steps:\nOn your main cluster collect the cluster admin configurations for each of the clusters. In the following example the source cluster, cluster-1 uses configuration /root/.kube/config-1 and the target cluster, cluster-2 uses the configuration /root/.config/config-2. Use repctl to add the clusters: ./repctl cluster add -f \"/root/.kube/config-1\",\"/root/.kube/config-2\" -n \"cluster-1\",\"cluster-2\" NOTE: If using a single Kubernetes cluster in a stretched configuration there will be only one cluster.\nInstall the replication controller CRDs:\n./repctl create -f ../deploy/replicationcrds.all.yaml Inject the service account’s configuration into the clusters.\n./repctl cluster inject Customize the examples/\u003cstorage\u003e_example_values.yaml sample config. Set the values for sourceClusterID and targetClusterID to the same names used in step 1. For a stretched cluster set both fields to self:\nCreate the replication storage classes using the modified configuration from step 4:\n./repctl create sc --from-config ./examples/\u003cstorage\u003e_example_values.yaml On the target cluster, configure the prerequisites for deploying the driver via Dell CSM Operator.\nInstall the CSI driver for your chosen storage platform on the source cluster according to the instructions for installing the drivers using CSM Operator.\n(Optional) If CSM Replication is deployed using two clusters in an environment where the DNS is not configured, it is necessary to update the dell-replication-controller-manager Kubernetes deployment to map the API endpoint FQDN to an IP address by adding the hostAliases field and associated FQDN:IP mappings.\nTo update the dell-replication-controller-manager deployment, execute the command below, replacing the fields for the remote cluster’s FQDN and IP. Make sure to update the deployment on both the primary and disaster recovery clusters.\nkubectl patch deployment -n dell-replication-controller dell-replication-controller-manager \\ -p '{\"spec\":{\"template\":{\"spec\":{\"hostAliases\":[{\"hostnames\":[\"\u003cremote-FQDN\u003e\"],\"ip\":\"\u003cremote-IP\u003e\"}]}}}}' ","categories":"","description":"Installing Replication via Dell CSM Operator\n","excerpt":"Installing Replication via Dell CSM Operator\n","ref":"/csm-docs/v1/deployment/csmoperator/modules/replication/","tags":"","title":"Replication"},{"body":"The CSM Replication module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. Dell CSM Operator will deploy the CSM Replication sidecar and the CSM Replication Controller Manager.\nPrerequisites To configure Replication prior to installation via CSM Operator, you need:\na source cluster which is the main cluster a target cluster which will serve as the disaster recovery cluster NOTE: If using a single Kubernetes cluster in a stretched configuration, there will be only one cluster. The source cluster is also the target cluster.\nCloning the GitHub Repository and Building repctl The csm-replication GitHub repository is cloned to your source cluster as part of the installation. On your source cluster run the following to clone and build the repctl tool:\ngit clone -b v1.4.0 https://github.com/dell/csm-replication.git cd csm-replication/repctl make build The rest of the instructions will assume that your current working directory is the csm-replication/repctl directory.\nConfiguration Steps To configure Replication perform the following steps:\nOn your main cluster collect the cluster admin configurations for each of the clusters. In the following example the source cluster, cluster-1 uses configuration /root/.kube/config-1 and the target cluster, cluster-2 uses the configuration /root/.config/config-2. Use repctl to add the clusters: ./repctl cluster add -f \"/root/.kube/config-1\",\"/root/.kube/config-2\" -n \"cluster-1\",\"cluster-2\" NOTE: If using a single Kubernetes cluster in a stretched configuration there will be only one cluster.\nInstall the replication controller CRDs:\n./repctl create -f ../deploy/replicationcrds.all.yaml Inject the service account’s configuration into the clusters.\n./repctl cluster inject Customize the examples/\u003cstorage\u003e_example_values.yaml sample config. Set the values for sourceClusterID and targetClusterID to the same names used in step 1. For a stretched cluster set both fields to self:\nCreate the replication storage classes using the modified configuration from step 4:\n./repctl create sc --from-config ./examples/\u003cstorage\u003e_example_values.yaml On the target cluster, configure the prerequisites for deploying the driver via Dell CSM Operator.\nInstall the CSI driver for your chosen storage platform on the source cluster according to the instructions for installing the drivers using CSM Operator.\n","categories":"","description":"Installing Replication via Dell CSM Operator\n","excerpt":"Installing Replication via Dell CSM Operator\n","ref":"/csm-docs/v2/deployment/csmoperator/modules/replication/","tags":"","title":"Replication"},{"body":"The CSM Replication module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. Dell CSM Operator will deploy the CSM Replication sidecar and the CSM Replication Controller Manager.\nPrerequisites To configure Replication prior to installation via CSM Operator, you need:\na source cluster which is the main cluster a target cluster which will serve as the disaster recovery cluster NOTE: If using a single Kubernetes cluster in a stretched configuration, there will be only one cluster. The source cluster is also the target cluster.\nCloning the GitHub Repository and Building repctl The csm-replication GitHub repository is cloned to your source cluster as part of the installation. On your source cluster run the following to clone and build the repctl tool:\ngit clone -b v1.4.0 https://github.com/dell/csm-replication.git cd csm-replication/repctl make build The rest of the instructions will assume that your current working directory is the csm-replication/repctl directory.\nConfiguration Steps To configure Replication perform the following steps:\nOn your main cluster collect the cluster admin configurations for each of the clusters. In the following example the source cluster, cluster-1 uses configuration /root/.kube/config-1 and the target cluster, cluster-2 uses the configuration /root/.config/config-2. Use repctl to add the clusters: ./repctl cluster add -f \"/root/.kube/config-1\",\"/root/.kube/config-2\" -n \"cluster-1\",\"cluster-2\" NOTE: If using a single Kubernetes cluster in a stretched configuration there will be only one cluster.\nInstall the replication controller CRDs:\n./repctl create -f ../deploy/replicationcrds.all.yaml Inject the service account’s configuration into the clusters.\n./repctl cluster inject Customize the examples/\u003cstorage\u003e_example_values.yaml sample config. Set the values for sourceClusterID and targetClusterID to the same names used in step 1. For a stretched cluster set both fields to self:\nCreate the replication storage classes using the modified configuration from step 4:\n./repctl create sc --from-config ./examples/\u003cstorage\u003e_example_values.yaml On the target cluster, configure the prerequisites for deploying the driver via Dell CSM Operator.\nInstall the CSI driver for your chosen storage platform on the source cluster according to the instructions for installing the drivers using CSM Operator.\n","categories":"","description":"Pre-requisite for Installing Replication via Dell CSM Operator\n","excerpt":"Pre-requisite for Installing Replication via Dell CSM Operator\n","ref":"/csm-docs/v3/deployment/csmoperator/modules/replication/","tags":"","title":"Replication"},{"body":"The CSM Resiliency module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. Dell CSM Operator will deploy CSM Resiliency sidecar.\nPrerequisite When utilizing CSM for Resiliency module, it is crucial to note that it will solely act upon pods that have been assigned a designated label. This label must have both a key and a value that match what has been set in the resiliency module configuration. Upon startup, CSM for Resiliency generates a log message that displays the label key and value being used to monitor pods.:\nlabelSelector: {map[podmon.dellemc.com/driver:csi-vxflexos] The above message indicates the key is: podmon.dellemc.com/driver and the label value is csi-vxflexos. To search for the pods that would be monitored, try this:\nkubectl get pods -A -l podmon.dellemc.com/driver=csi-vxflexos NAMESPACE NAME READY STATUS RESTARTS AGE pmtu1 podmontest-0 1/1 Running 0 3m7s pmtu2 podmontest-0 1/1 Running 0 3m8s pmtu3 podmontest-0 1/1 Running 0 3m6s User must follow all the prerequisites of the respective drivers before enabling this module.\nHow to enable this module To enable this module, user should choose the sample file for the respective driver for specific version. By default, the module is disabled but this can be enabled by setting the enabled flag to true in the sample file.\nmodules: - name: resiliency # enabled: Enable/Disable Resiliency feature # Allowed values: # true: enable Resiliency feature(deploy podmon sidecar) # false: disable Resiliency feature(do not deploy podmon sidecar) # Default value: false enabled: true configVersion: v1.8.0 components: - name: podmon-controller args: - \"--labelvalue=csi-powerstore\" - \"--arrayConnectivityPollRate=60\" - \"--skipArrayConnectionValidation=false\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" # Below 4 args should not be modified. - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--mode=controller\" - \"--driver-config-params=/powerstore-config-params/driver-config-params.yaml\" - \"--driverPath=csi-powerstore.dellemc.com\" - name: podmon-node envs: # podmonAPIPort: Defines the port to be used within the kubernetes cluster # Allowed values: Any valid and free port (string) # Default value: 8083 - name: \"X_CSI_PODMON_API_PORT\" value: \"8083\" args: - \"--labelvalue=csi-powerstore\" - \"--arrayConnectivityPollRate=60\" - \"--leaderelection=false\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" # Below 4 args should not be modified. - \"--csisock=unix:/var/lib/kubelet/plugins/csi-powerstore.dellemc.com/csi_sock\" - \"--mode=node\" - \"--driver-config-params=/powerstore-config-params/driver-config-params.yaml\" - \"--driverPath=csi-powerstore.dellemc.com\" ","categories":"","description":"Installing Resiliency via Dell CSM Operator\n","excerpt":"Installing Resiliency via Dell CSM Operator\n","ref":"/csm-docs/docs/deployment/csmoperator/modules/resiliency/","tags":"","title":"Resiliency"},{"body":"The CSM Resiliency module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. Dell CSM Operator will deploy CSM Resiliency sidecar.\nPrerequisite When utilizing CSM for Resiliency module, it is crucial to note that it will solely act upon pods that have been assigned a designated label. This label must have both a key and a value that match what has been set in the resiliency module configuration. Upon startup, CSM for Resiliency generates a log message that displays the label key and value being used to monitor pods.:\nlabelSelector: {map[podmon.dellemc.com/driver:csi-vxflexos] The above message indicates the key is: podmon.dellemc.com/driver and the label value is csi-vxflexos. To search for the pods that would be monitored, try this:\nkubectl get pods -A -l podmon.dellemc.com/driver=csi-vxflexos NAMESPACE NAME READY STATUS RESTARTS AGE pmtu1 podmontest-0 1/1 Running 0 3m7s pmtu2 podmontest-0 1/1 Running 0 3m8s pmtu3 podmontest-0 1/1 Running 0 3m6s User must follow all the prerequisites of the respective drivers before enabling this module.\nHow to enable this module To enable this module, user should choose the sample file for the respective driver for specific version. By default, the module is disabled but this can be enabled by setting the enabled flag to true in the sample file.\nmodules: - name: resiliency # enabled: Enable/Disable Resiliency feature # Allowed values: # true: enable Resiliency feature(deploy podmon sidecar) # false: disable Resiliency feature(do not deploy podmon sidecar) # Default value: false enabled: true configVersion: v1.6.0 components: - name: podmon-controller args: - \"--labelvalue=csi-powerstore\" - \"--arrayConnectivityPollRate=60\" - \"--skipArrayConnectionValidation=false\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" # Below 4 args should not be modified. - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--mode=controller\" - \"--driver-config-params=/powerstore-config-params/driver-config-params.yaml\" - \"--driverPath=csi-powerstore.dellemc.com\" - name: podmon-node envs: # podmonAPIPort: Defines the port to be used within the kubernetes cluster # Allowed values: Any valid and free port (string) # Default value: 8083 - name: \"X_CSI_PODMON_API_PORT\" value: \"8083\" args: - \"--labelvalue=csi-powerstore\" - \"--arrayConnectivityPollRate=60\" - \"--leaderelection=false\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" # Below 4 args should not be modified. - \"--csisock=unix:/var/lib/kubelet/plugins/csi-powerstore.dellemc.com/csi_sock\" - \"--mode=node\" - \"--driver-config-params=/powerstore-config-params/driver-config-params.yaml\" - \"--driverPath=csi-powerstore.dellemc.com\" ","categories":"","description":"Installing Resiliency via Dell CSM Operator\n","excerpt":"Installing Resiliency via Dell CSM Operator\n","ref":"/csm-docs/v1/deployment/csmoperator/modules/resiliency/","tags":"","title":"Resiliency"},{"body":"The CSM Resiliency module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. Dell CSM Operator will deploy CSM Resiliency sidecar.\nPrerequisite When utilizing CSM for Resiliency module, it is crucial to note that it will solely act upon pods that have been assigned a designated label. This label must have both a key and a value that match what has been set in the resiliency module configuration. Upon startup, CSM for Resiliency generates a log message that displays the label key and value being used to monitor pods.:\nlabelSelector: {map[podmon.dellemc.com/driver:csi-vxflexos] The above message indicates the key is: podmon.dellemc.com/driver and the label value is csi-vxflexos. To search for the pods that would be monitored, try this:\nkubectl get pods -A -l podmon.dellemc.com/driver=csi-vxflexos NAMESPACE NAME READY STATUS RESTARTS AGE pmtu1 podmontest-0 1/1 Running 0 3m7s pmtu2 podmontest-0 1/1 Running 0 3m8s pmtu3 podmontest-0 1/1 Running 0 3m6s User must follow all the prerequisites of the respective drivers before enabling this module.\nHow to enable this module To enable this module, user should choose the sample file for the respective driver for specific version. By default, the module is disabled but this can be enabled by setting the enabled flag to true in the sample file.\nmodules: - name: resiliency # enabled: Enable/Disable Resiliency feature # Allowed values: # true: enable Resiliency feature(deploy podmon sidecar) # false: disable Resiliency feature(do not deploy podmon sidecar) # Default value: false enabled: true configVersion: v1.6.0 components: - name: podmon-controller args: - \"--labelvalue=csi-powerstore\" - \"--arrayConnectivityPollRate=60\" - \"--skipArrayConnectionValidation=false\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" # Below 4 args should not be modified. - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--mode=controller\" - \"--driver-config-params=/powerstore-config-params/driver-config-params.yaml\" - \"--driverPath=csi-powerstore.dellemc.com\" - name: podmon-node envs: # podmonAPIPort: Defines the port to be used within the kubernetes cluster # Allowed values: Any valid and free port (string) # Default value: 8083 - name: \"X_CSI_PODMON_API_PORT\" value: \"8083\" args: - \"--labelvalue=csi-powerstore\" - \"--arrayConnectivityPollRate=60\" - \"--leaderelection=false\" - \"--driverPodLabelValue=dell-storage\" - \"--ignoreVolumelessPods=false\" # Below 4 args should not be modified. - \"--csisock=unix:/var/lib/kubelet/plugins/csi-powerstore.dellemc.com/csi_sock\" - \"--mode=node\" - \"--driver-config-params=/powerstore-config-params/driver-config-params.yaml\" - \"--driverPath=csi-powerstore.dellemc.com\" ","categories":"","description":"Installing Resiliency via Dell CSM Operator\n","excerpt":"Installing Resiliency via Dell CSM Operator\n","ref":"/csm-docs/v2/deployment/csmoperator/modules/resiliency/","tags":"","title":"Resiliency"},{"body":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE) v1.4.1.\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters. Installation on this cluster is done using helm and via Operator has not been qualified.\nRKE Examples ","categories":"","description":"About Rancher Kubernetes Engine","excerpt":"About Rancher Kubernetes Engine","ref":"/csm-docs/v1/csidriver/partners/rancher/","tags":"","title":"RKE"},{"body":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE) v1.4.1.\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters. Installation on this cluster is done using helm and via Operator has not been qualified.\nRKE Examples ","categories":"","description":"About Rancher Kubernetes Engine","excerpt":"About Rancher Kubernetes Engine","ref":"/csm-docs/v2/csidriver/partners/rancher/","tags":"","title":"RKE"},{"body":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE) v1.4.1.\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters. Installation on this cluster is done using helm and via Operator has not been qualified.\nRKE Examples ","categories":"","description":"About Rancher Kubernetes Engine","excerpt":"About Rancher Kubernetes Engine","ref":"/csm-docs/v3/csidriver/partners/rancher/","tags":"","title":"RKE"},{"body":" The CSM Authorization RPM is no longer actively maintained or supported. It will be deprecated in CSM 2.0. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nRoles Role data is stored in the common Config Map in the underlying k3s deployment.\nSteps to execute in the existing Authorization deployment Save the role data by saving the common configMap to a file. k3s kubectl -n karavi get configMap common -o yaml \u003e roles.yaml Steps to execute in the Authorization deployment to restore Delete the existing common configMap. k3s kubectl -n karavi delete configMap common Apply the file containing the role data created in step 1. k3s kubectl apply -f roles.yaml Restart the proxy-server deployment. k3s kubectl -n karavi rollout restart deploy/proxy-server deployment.apps/proxy-server restarted Storage Storage data is stored in the karavi-storage-secret Secret in the underlying k3s deployment.\nSteps to execute in the existing Authorization deployment Save the storage data by saving the karavi-storage-secret secret to a file. k3s kubectl -n karavi get secret karavi-storage-secret -o yaml \u003e storage.yaml Steps to execute in the Authorization deployment to restore Delete the existing karavi-storage-secret secret. k3s kubectl -n karavi delete secret karavi-storage-secret Apply the file containing the storage data created in step 1. k3s kubectl apply -f storage.yaml Restart the proxy-server deployment. k3s kubectl -n karavi rollout restart deploy/proxy-server deployment.apps/proxy-server restarted Tenants, Quota, and Volume ownership Redis is used to store application data regarding tenants, quota, and volume ownership. This data is stored on the system under /var/lib/rancher/k3s/storage/\u003credis-primary-pv-claim-volume-name\u003e/appendonly.aof.\nappendonly.aof can be copied and used to restore this appliation data in Authorization deployments. See the example.\nSteps to execute in the existing Authorization deployment Determine the Persistent Volume related to the redis-primary-pv-claim Persistent Volume Claim. k3s kubectl -n karavi get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE redis-primary-pv-claim Bound pvc-12d8cc05-910d-45bd-9f30-f6807b287a69 8Gi RWO local-path 65m The Persistent Volume related to the redis-primary-pv-claim Persistent Volume Claim is pvc-12d8cc05-910d-45bd-9f30-f6807b287a69.\nCopy appendonly.aof from the appropriate path to another location. cp /var/lib/rancher/k3s/storage/pvc-12d8cc05-910d-45bd-9f30-f6807b287a69/appendonly.aof /path/to/copy/appendonly.aof Steps to execute in the Authorization deployment to restore Determine the Persistent Volume related to the redis-primary-pv-claim Persistent Volume Claim. k3s kubectl -n karavi get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE redis-primary-pv-claim Bound pvc-e7ea31bf-3d79-41fc-88d8-50ba356a298b 8Gi RWO local-path 65m The Persistent Volume related to the redis-primary-pv-claim Persistent Volume Claim is pvc-e7ea31bf-3d79-41fc-88d8-50ba356a298b.\nCopy/Overwrite the appendonly.aof in the appropriate path using the file copied in step 2. cp /path/to/copy/appendonly.aof /var/lib/rancher/k3s/storage/pvc-e7ea31bf-3d79-41fc-88d8-50ba356a298b/appendonly.aof Restart the redis-primary deployment. k3s kubectl -n karavi rollout restart deploy/redis-primary deployment.apps/redis-primary restarted ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization RPM backup and restore\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/authorization/backup-and-restore/rpm/","tags":"","title":"RPM"},{"body":" The CSM Authorization RPM is no longer actively maintained or supported. It will be deprecated in CSM 2.0. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nRoles Role data is stored in the common Config Map in the underlying k3s deployment.\nSteps to execute in the existing Authorization deployment Save the role data by saving the common configMap to a file. k3s kubectl -n karavi get configMap common -o yaml \u003e roles.yaml Steps to execute in the Authorization deployment to restore Delete the existing common configMap. k3s kubectl -n karavi delete configMap common Apply the file containing the role data created in step 1. k3s kubectl apply -f roles.yaml Restart the proxy-server deployment. k3s kubectl -n karavi rollout restart deploy/proxy-server deployment.apps/proxy-server restarted Storage Storage data is stored in the karavi-storage-secret Secret in the underlying k3s deployment.\nSteps to execute in the existing Authorization deployment Save the storage data by saving the karavi-storage-secret secret to a file. k3s kubectl -n karavi get secret karavi-storage-secret -o yaml \u003e storage.yaml Steps to execute in the Authorization deployment to restore Delete the existing karavi-storage-secret secret. k3s kubectl -n karavi delete secret karavi-storage-secret Apply the file containing the storage data created in step 1. k3s kubectl apply -f storage.yaml Restart the proxy-server deployment. k3s kubectl -n karavi rollout restart deploy/proxy-server deployment.apps/proxy-server restarted Tenants, Quota, and Volume ownership Redis is used to store application data regarding tenants, quota, and volume ownership. This data is stored on the system under /var/lib/rancher/k3s/storage/\u003credis-primary-pv-claim-volume-name\u003e/appendonly.aof.\nappendonly.aof can be copied and used to restore this appliation data in Authorization deployments. See the example.\nSteps to execute in the existing Authorization deployment Determine the Persistent Volume related to the redis-primary-pv-claim Persistent Volume Claim. k3s kubectl -n karavi get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE redis-primary-pv-claim Bound pvc-12d8cc05-910d-45bd-9f30-f6807b287a69 8Gi RWO local-path 65m The Persistent Volume related to the redis-primary-pv-claim Persistent Volume Claim is pvc-12d8cc05-910d-45bd-9f30-f6807b287a69.\nCopy appendonly.aof from the appropriate path to another location. cp /var/lib/rancher/k3s/storage/pvc-12d8cc05-910d-45bd-9f30-f6807b287a69/appendonly.aof /path/to/copy/appendonly.aof Steps to execute in the Authorization deployment to restore Determine the Persistent Volume related to the redis-primary-pv-claim Persistent Volume Claim. k3s kubectl -n karavi get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE redis-primary-pv-claim Bound pvc-e7ea31bf-3d79-41fc-88d8-50ba356a298b 8Gi RWO local-path 65m The Persistent Volume related to the redis-primary-pv-claim Persistent Volume Claim is pvc-e7ea31bf-3d79-41fc-88d8-50ba356a298b.\nCopy/Overwrite the appendonly.aof in the appropriate path using the file copied in step 2. cp /path/to/copy/appendonly.aof /var/lib/rancher/k3s/storage/pvc-e7ea31bf-3d79-41fc-88d8-50ba356a298b/appendonly.aof Restart the redis-primary deployment. k3s kubectl -n karavi rollout restart deploy/redis-primary deployment.apps/redis-primary restarted ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization RPM backup and restore\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v1/authorization/backup-and-restore/rpm/","tags":"","title":"RPM"},{"body":"Roles Role data is stored in the common Config Map in the underlying k3s deployment.\nSteps to execute in the existing Authorization deployment Save the role data by saving the common configMap to a file. k3s kubectl -n karavi get configMap common -o yaml \u003e roles.yaml Steps to execute in the Authorization deployment to restore Delete the existing common configMap. k3s kubectl -n karavi delete configMap common Apply the file containing the role data created in step 1. k3s kubectl apply -f roles.yaml Restart the proxy-server deployment. k3s kubectl -n karavi rollout restart deploy/proxy-server deployment.apps/proxy-server restarted Storage Storage data is stored in the karavi-storage-secret Secret in the underlying k3s deployment.\nSteps to execute in the existing Authorization deployment Save the storage data by saving the karavi-storage-secret secret to a file. k3s kubectl -n karavi get secret karavi-storage-secret -o yaml \u003e storage.yaml Steps to execute in the Authorization deployment to restore Delete the existing karavi-storage-secret secret. k3s kubectl -n karavi delete secret karavi-storage-secret Apply the file containing the storage data created in step 1. k3s kubectl apply -f storage.yaml Restart the proxy-server deployment. k3s kubectl -n karavi rollout restart deploy/proxy-server deployment.apps/proxy-server restarted Tenants, Quota, and Volume ownership Redis is used to store application data regarding tenants, quota, and volume ownership. This data is stored on the system under /var/lib/rancher/k3s/storage/\u003credis-primary-pv-claim-volume-name\u003e/appendonly.aof.\nappendonly.aof can be copied and used to restore this appliation data in Authorization deployments. See the example.\nSteps to execute in the existing Authorization deployment Determine the Persistent Volume related to the redis-primary-pv-claim Persistent Volume Claim. k3s kubectl -n karavi get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE redis-primary-pv-claim Bound pvc-12d8cc05-910d-45bd-9f30-f6807b287a69 8Gi RWO local-path 65m The Persistent Volume related to the redis-primary-pv-claim Persistent Volume Claim is pvc-12d8cc05-910d-45bd-9f30-f6807b287a69.\nCopy appendonly.aof from the appropriate path to another location. cp /var/lib/rancher/k3s/storage/pvc-12d8cc05-910d-45bd-9f30-f6807b287a69/appendonly.aof /path/to/copy/appendonly.aof Steps to execute in the Authorization deployment to restore Determine the Persistent Volume related to the redis-primary-pv-claim Persistent Volume Claim. k3s kubectl -n karavi get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE redis-primary-pv-claim Bound pvc-e7ea31bf-3d79-41fc-88d8-50ba356a298b 8Gi RWO local-path 65m The Persistent Volume related to the redis-primary-pv-claim Persistent Volume Claim is pvc-e7ea31bf-3d79-41fc-88d8-50ba356a298b.\nCopy/Overwrite the appendonly.aof in the appropriate path using the file copied in step 2. cp /path/to/copy/appendonly.aof /var/lib/rancher/k3s/storage/pvc-e7ea31bf-3d79-41fc-88d8-50ba356a298b/appendonly.aof Restart the redis-primary deployment. k3s kubectl -n karavi rollout restart deploy/redis-primary deployment.apps/redis-primary restarted ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization RPM backup and restore\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v2/authorization/backup-and-restore/rpm/","tags":"","title":"RPM"},{"body":" The CSM Authorization RPM is no longer actively maintained or supported. It will be deprecated in CSM 2.0. It is highly recommended that you use CSM Authorization Helm deployment or CSM Operator going forward.\nRoles Role data is stored in the common Config Map in the underlying k3s deployment.\nSteps to execute in the existing Authorization deployment Save the role data by saving the common configMap to a file. k3s kubectl -n karavi get configMap common -o yaml \u003e roles.yaml Steps to execute in the Authorization deployment to restore Delete the existing common configMap. k3s kubectl -n karavi delete configMap common Apply the file containing the role data created in step 1. k3s kubectl apply -f roles.yaml Restart the proxy-server deployment. k3s kubectl -n karavi rollout restart deploy/proxy-server deployment.apps/proxy-server restarted Storage Storage data is stored in the karavi-storage-secret Secret in the underlying k3s deployment.\nSteps to execute in the existing Authorization deployment Save the storage data by saving the karavi-storage-secret secret to a file. k3s kubectl -n karavi get secret karavi-storage-secret -o yaml \u003e storage.yaml Steps to execute in the Authorization deployment to restore Delete the existing karavi-storage-secret secret. k3s kubectl -n karavi delete secret karavi-storage-secret Apply the file containing the storage data created in step 1. k3s kubectl apply -f storage.yaml Restart the proxy-server deployment. k3s kubectl -n karavi rollout restart deploy/proxy-server deployment.apps/proxy-server restarted Tenants, Quota, and Volume ownership Redis is used to store application data regarding tenants, quota, and volume ownership. This data is stored on the system under /var/lib/rancher/k3s/storage/\u003credis-primary-pv-claim-volume-name\u003e/appendonly.aof.\nappendonly.aof can be copied and used to restore this appliation data in Authorization deployments. See the example.\nSteps to execute in the existing Authorization deployment Determine the Persistent Volume related to the redis-primary-pv-claim Persistent Volume Claim. k3s kubectl -n karavi get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE redis-primary-pv-claim Bound pvc-12d8cc05-910d-45bd-9f30-f6807b287a69 8Gi RWO local-path 65m The Persistent Volume related to the redis-primary-pv-claim Persistent Volume Claim is pvc-12d8cc05-910d-45bd-9f30-f6807b287a69.\nCopy appendonly.aof from the appropriate path to another location. cp /var/lib/rancher/k3s/storage/pvc-12d8cc05-910d-45bd-9f30-f6807b287a69/appendonly.aof /path/to/copy/appendonly.aof Steps to execute in the Authorization deployment to restore Determine the Persistent Volume related to the redis-primary-pv-claim Persistent Volume Claim. k3s kubectl -n karavi get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE redis-primary-pv-claim Bound pvc-e7ea31bf-3d79-41fc-88d8-50ba356a298b 8Gi RWO local-path 65m The Persistent Volume related to the redis-primary-pv-claim Persistent Volume Claim is pvc-e7ea31bf-3d79-41fc-88d8-50ba356a298b.\nCopy/Overwrite the appendonly.aof in the appropriate path using the file copied in step 2. cp /path/to/copy/appendonly.aof /var/lib/rancher/k3s/storage/pvc-e7ea31bf-3d79-41fc-88d8-50ba356a298b/appendonly.aof Restart the redis-primary deployment. k3s kubectl -n karavi rollout restart deploy/redis-primary deployment.apps/redis-primary restarted ","categories":"","description":"Dell Technologies (Dell) Container Storage Modules (CSM) for Authorization RPM backup and restore\n","excerpt":"Dell Technologies (Dell) Container Storage Modules (CSM) for …","ref":"/csm-docs/v3/authorization/backup-and-restore/rpm/","tags":"","title":"RPM"},{"body":"","categories":"","description":"","excerpt":"","ref":"/csm-docs/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"Tests to validate CSI Driver Installation","excerpt":"Tests to validate CSI Driver Installation","ref":"/csm-docs/docs/csidriver/installation/test/","tags":"","title":"Testing Drivers"},{"body":"","categories":"","description":"Tests to validate CSI Driver Installation","excerpt":"Tests to validate CSI Driver Installation","ref":"/csm-docs/v1/csidriver/installation/test/","tags":"","title":"Testing Drivers"},{"body":"","categories":"","description":"Tests to validate CSI Driver Installation","excerpt":"Tests to validate CSI Driver Installation","ref":"/csm-docs/v2/csidriver/installation/test/","tags":"","title":"Testing Drivers"},{"body":"","categories":"","description":"Tests to validate CSI Driver Installation","excerpt":"Tests to validate CSI Driver Installation","ref":"/csm-docs/v3/csidriver/installation/test/","tags":"","title":"Testing Drivers"},{"body":"The CSI Driver for Dell Unity XT can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nPrerequisites Before you install CSI Driver for Unity XT, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements Install Kubernetes or OpenShift (see supported versions) Install Helm v3 To use FC protocol, the host must be zoned with Unity XT array and Multipath needs to be configured To use iSCSI protocol, iSCSI initiator utils packages needs to be installed and Multipath needs to be configured To use NFS protocol, NFS utility packages needs to be installed Mount propagation is enabled on container runtime that is being used Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell Unity XT.\nSteps\nRun the command to install Helm 3.0.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Fibre Channel requirements Dell Unity XT supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell Unity XT:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done. Set up the iSCSI Initiator The CSI Driver for Dell Unity XT supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\nEnsure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell Unity XT array that has IP interfaces. Manually create IP routes for each node that connects to the Dell Unity XT. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi. For more information about configuring iSCSI, see Dell Host Connectivity guide.\nLinux multipathing requirements Dell Unity XT supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell Unity XT.\nSet up Linux multipathing as follows:\nEnsure that all nodes have the Device Mapper Multipathing package installed. You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\nEnable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes. As a best practice, use the following options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 Install CSI Driver Install CSI Driver for Unity XT using this procedure.\nBefore you begin\nAs a pre-requisite for running this procedure, you must have the downloaded files, including the Helm chart from the source git repository with the command git clone -b v2.9.1 https://github.com/dell/csi-unity.git In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. Ensure unity namespace exists in Kubernetes cluster. Use the kubectl create namespace unity command to create the namespace if the namespace is not present. Procedure\nCollect information from the Unity XT Systems like unique ArrayId, IP address, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml and myvalues.yaml file.\nNote:\nArrayId corresponds to the serial number of Unity XT array. Unity XT Array username must have role as Storage Administrator to be able to perform CRUD operations. If the user is using a complex K8s version like “v1.24.6-mirantis-1”, use this kubeVersion check in helm/csi-unity/Chart.yaml file. kubeVersion: “\u003e= 1.24.0-0 \u003c 1.29.0-0” Get the required values.yaml using the command below:\ncd dell-csi-helm-installer \u0026\u0026 wget -O my-unity-settings.yaml https://github.com/dell/helm-charts/raw/csi-unity-2.9.1/charts/csi-unity/values.yaml Edit values.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity XT driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\nParameter Description Required Default images List all the images used by the CSI driver and CSM. If you use a private repository, change the registries accordingly. Yes \"\" logLevel LogLevel is used to set the logging level of the driver No info allowRWOMultiPodAccess Flag to enable multiple pods to use the same PVC on the same node with RWO access mode. No false kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet syncNodeInfoInterval Time interval to add node info to the array. Default 15 minutes. The minimum value should be 1 minute. No 15 maxUnityVolumesPerNode Maximum number of volumes that controller can publish to the node. No 0 certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. (unity-cert-0..unity-cert-n). The minimum value should be 1. No 1 imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent podmon.enabled service to monitor failing jobs and notify No false tenantName Tenant name added while adding host entry to the array No fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity.enabled Enable/Disable storage capacity tracking No true storageCapacity.pollInterval Configure how often the driver checks for changed capacity No 5m controller Allows configuration of the controller-specific parameters. - - controllerCount Defines the number of csi-unity controller pods to deploy to the Kubernetes release Yes 2 volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s” snapshot.enabled Enable/Disable volume snapshot feature Yes true snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot” resizer.enabled Enable/Disable volume expansion feature Yes true nodeSelector Define node selection constraints for pods of controller deployment No tolerations Define tolerations for the controller deployment, if required No healthMonitor.enabled Enable/Disable deployment of external health monitor sidecar for controller side volume health monitoring. No false healthMonitor.interval Interval of monitoring volume health condition. Allowed values: Number followed by unit (s,m,h) No 60s node Allows configuration of the node-specific parameters. - - dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition No false nodeSelector Define node selection constraints for pods of node deployment No tolerations Define tolerations for the node deployment, if required No Note:\nUser should provide all boolean values with double-quotes. This applies only for myvalues.yaml. Example: “true”/“false”\ncontrollerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails.\nUser can a create separate StorageClass (with topology-related keys) by referring to existing default storage classes.\nHost IO Limit must have a minimum bandwidth of 1 MBPS to discover the volumes on node successfully.\nUser must not change the value of allowRWOMultiPodAccess to true unless intended to use the feature and is aware of the consequences. Enabling multiple pods to access the same PVC with RWO access mode on the same node might cause data to be overwritten and therefore leading to data loss in some cases.\nExample myvalues.yaml\nlogLevel: \"info\" imagePullPolicy: Always certSecretCount: 1 kubeletConfigDir: /var/lib/kubelet controller: controllerCount: 2 volumeNamePrefix : csivol snapshot: enabled: true snapNamePrefix: csi-snap resizer: enabled: false allowRWOMultiPodAccess: false syncNodeInfoInterval: 5 maxUnityVolumesPerNode: 0 fsGroupPolicy: ReadWriteOneFSType For certificate validation of Unisphere REST API calls refer here. Otherwise, create an empty secret with file csi-unity/samples/secret/emptysecret.yaml file by running the kubectl create -f csi-unity/samples/secret/emptysecret.yaml command.\nPrepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\nParameter Description Required Default storageArrayList.username Username for accessing Unity XT system Yes - storageArrayList.password Password for accessing Unity XT system Yes - storageArrayList.endpoint REST API gateway HTTPS endpoint Unity XT system Yes - storageArrayList.arrayId ArrayID for Unity XT system Yes - storageArrayList.skipCertificateValidation “skipCertificateValidation \" determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface. If it is set to false, then a secret unity-certs has to be created with an X.509 certificate of CA which signed the Unisphere certificate. Yes true storageArrayList.isDefault An array having isDefault=true or isDefault=true will be considered as the default array when arrayId is not specified in the storage class. This parameter should occur only once in the list. Yes - Example: secret.yaml\nstorageArrayList: - arrayId: \"APM00******1\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.1/\" skipCertificateValidation: true isDefault: true - arrayId: \"APM00******2\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.2/\" skipCertificateValidation: true isDefault: false Use the following command to create a new secret unity-creds from secret.yaml file.\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml Use the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f - Note: The user needs to validate the yaml syntax and array-related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the yaml file.\nAlternatively, users can configure and use secret.yaml for driver configuration. The parameters remain the same as in the above table and below is a sample of secret.yaml. Samples of secret.yaml is available in the directory csi-unity/samples/secret/ .\nExample: secret.yaml\nstorageArrayList: - arrayId: \"APM00******1\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.1/\" skipCertificateValidation: true isDefault: true - arrayId: \"APM00******2\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.2/\" skipCertificateValidation: true isDefault: false Note: Parameters “allowRWOMultiPodAccess” and “syncNodeInfoInterval” have been enabled for configuration in values.yaml and this helps users to dynamically change these values without the need for driver re-installation.\nIf you want to leverage snapshotting feature, the pre-requisite is to install external-snapshotter. Installation of external-snapshotter is required only for Kubernetes and is available by default with OpenShift installations. Click here to follow the procedure to install external-snapshotter.\nRun the command to proceed with the installation using bash script.\n./csi-install.sh --namespace unity --values ./myvalues.yaml A successful installation must display messages that look similar to the following samples:\n------------------------------------------------------ \u003e Installing CSI Driver: csi-unity on 1.27 ------------------------------------------------------ ------------------------------------------------------ \u003e Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u003e Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.27 | |- Driver: csi-unity | |- Verifying Kubernetes version | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that optional secrets have been created Success | |- Verifying alpha snapshot resources | |--\u003e Verifying that alpha snapshot CRDs are not installed Success | |- Verifying sshpass installation.. | |- Verifying iSCSI installation Enter the root password of 10.**.**.**: Enter the root password of 10.**.**.**: Success | |- Verifying snapshot support | |--\u003e Verifying that snapshot CRDs are available Success | |--\u003e Verifying that the snapshot controller is available Success | |- Verifying helm version Success | |- Verifying helm values version Success ------------------------------------------------------ \u003e Verification Complete - Success ------------------------------------------------------ | |- Installing Driver Success | |--\u003e Waiting for Deployment unity-controller to be ready Success | |--\u003e Waiting for DaemonSet unity-node to be ready Success ------------------------------------------------------ \u003e Operation complete ------------------------------------------------------ Results:\nAt the end of the script unity-controller Deployment and DaemonSet unity-node will be ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\nOne or more Unity XT Controllers (based on controllerCount) with 5/5 containers ready, and status displayed as Running. Agent pods with 2/2 containers and the status displayed as Running. Note: To install nightly or latest csi driver build using bash script use this command:\n/csi-install.sh --namespace unity --values ./myvalues.yaml --version latest --helm-charts-version \u003cversion\u003e You can also install the driver using standalone helm chart by cloning the centralised helm charts and running the helm install command as shown.\nSyntax:\ngit clone -b csi-unity-2.9.1 https://github.com/dell/helm-charts helm install \u003crelease-name\u003e dell/container-storage-modules -n \u003cnamespace\u003e --version \u003ccontainer-storage-module chart-version\u003e -f \u003cvalues.yaml location\u003e Example: helm install unity dell/container-storage-modules -n csi-unity --version 1.0.1 -f values.yaml Certificate validation for Unisphere REST API calls This topic provides details about setting up the Dell Unity XT certificate validation for the CSI Driver.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on the “.Values.certSecretCount” parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\nTo fetch the certificate, run the following command.\nopenssl s_client -showcerts -connect \u003cUnisphere IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example:\nopenssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Run the following command to create the cert secret with index ‘0’:\nkubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret:\nkubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)\nNote:\n“unity” is the namespace for helm-based installation but namespace can be user-defined in operator-based installation.\nUser can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation.\nWhenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\nVolume Snapshot Class A wide set of annotated storage class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nStorage Classes Storage Classes are an essential Kubernetes construct for Storage provisioning. To know more about Storage Classes, refer to https://kubernetes.io/docs/concepts/storage/storage-classes/\nA wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nFor the Unity XT CSI Driver, a wide set of annotated storage class manifests have been provided in the csi-unity/samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class: There are samples storage class yaml files available under csi-unity/samples/storageclass. These can be copied and modified as needed.\nPick any of unity-fc.yaml, unity-iscsi.yaml or unity-nfs.yaml Copy the file as unity-\u003cARRAY_ID\u003e-fc.yaml, unity-\u003cARRAY_ID\u003e-iscsi.yaml or unity-\u003cARRAY_ID\u003e-nfs.yaml Replace \u003cARRAY_ID\u003e with the Array Id of the Unity Array to be used Replace \u003cSTORAGE_POOL\u003e with the storage pool you have Replace \u003cTIERING_POLICY\u003e with the Tiering policy that is to be used for provisioning Replace \u003cHOST_IO_LIMIT_NAME\u003e with the Host IO Limit Name that is to be used for provisioning Replace \u003cmountOption1\u003e with the necessary mount options. If not required, this can be removed from the storage class Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f unity-\u003cARRAY_ID\u003e-fc.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-iscsi.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-nfs.yaml Note:\nAt least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es): Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nDynamically update the unity-creds secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Note: Updating unity-certs-x secrets is a manual process, unlike unity-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nDynamic Logging Configuration Helm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\n","categories":"","description":"Installing CSI Driver for Unity XT via Helm\n","excerpt":"Installing CSI Driver for Unity XT via Helm\n","ref":"/csm-docs/docs/csidriver/installation/helm/unity/","tags":"","title":"Unity XT"},{"body":"Test deploying a simple Pod and PVC with Unity XT storage In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes and automatically mounts them to the pod.\nSteps\nTo run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./test/sample.yaml You can find all the created resources in unity namespace.\nCheck if the pod is created and Ready and Running by running:\nkubectl get all -n unity If it is in CrashLoopback state then the driver installation was not successful. Check the logs of the node and the controller.\nGo into the created container and verify that everything is mounted correctly.\nAfter verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./test/sample.yaml Support for SLES 15 The CSI Driver for Dell Unity XT requires these of packages installed on all worker nodes that run on SLES 15.\nopen-iscsi open-iscsi is required in order to make use of iSCSI protocol for provisioning nfs-utils nfs-utils is required in order to make use of NFS protocol for provisioning multipath-tools multipath-tools is required in order to make use of FC and iSCSI protocols for provisioning After installing open-iscsi, ensure “iscsi” and “iscsid” services have been started and /etc/isci/initiatorname.iscsi is created and has the host initiator id. The pre-requisites are mandatory for provisioning with the iSCSI protocol to work.\n","categories":"","description":"Tests to validate Unity XT CSI Driver installation","excerpt":"Tests to validate Unity XT CSI Driver installation","ref":"/csm-docs/docs/csidriver/installation/test/unity/","tags":"","title":"Test Unity XT CSI Driver"},{"body":"Release Notes - CSI Unity XT v2.9.1 New Features/Changes #947 - [FEATURE]: Support for Kubernetes 1.28 #1066 - [FEATURE]: Support for Openshift 4.14 #851 - [FEATURE]: Helm Chart Enhancement - Container Images Configurable in values.yaml #905 - [FEATURE]: Add support for CSI Spec 1.6 #996 - [FEATURE]: Dell CSI to Dell CSM Operator Migration Process Fixed Issues #1014 - [BUG]: Missing error check for os.Stat call during volume publish #1110 - [BUG]: Multi Controller defect - sidecars timeout #1103 - [BUG]: CSM Operator doesn’t apply fSGroupPolicy value to CSIDriver Object Known Issues Issue Workaround Topology-related node labels are not removed automatically. Currently, when the driver is uninstalled, topology-related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released, remove the labels manually after the driver un-installation using command kubectl label node \u003cnode_name\u003e - - … Example: kubectl label node csi-unity.dellemc.com/array123-iscsi- Note: there must be - at the end of each label to remove it. NFS Clone - Resize of the snapshot is not supported by Unity XT Platform, however the user should never try to resize the cloned NFS volume. Currently, when the driver takes a clone of NFS volume, it succeeds but if the user tries to resize the NFS volumesnapshot, the driver will throw an error. Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100 When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the VolumeAttachment to the node that went down. Now the volume can be attached to the new node. A CSI ephemeral pod may not get created in OpenShift 4.13 and fail with the error \"error when creating pod: the pod uses an inline volume provided by CSIDriver csi-unity.dellemc.com, and the namespace has a pod security enforcement level that is lower than privileged.\" This issue occurs because OpenShift 4.13 introduced the CSI Volume Admission plugin to restrict the use of a CSI driver capable of provisioning CSI ephemeral volumes during pod admission. Therefore, an additional label security.openshift.io/csi-ephemeral-volume-profile in csidriver.yaml file with the required security profile value should be provided. Follow OpenShift 4.13 documentation for CSI Ephemeral Volumes for more information. If the volume limit is exhausted and there are pending pods and PVCs due to exceed max volume count, the pending PVCs will be bound to PVs and the pending pods will be scheduled to nodes when the driver pods are restarted. It is advised not to have any pending pods or PVCs once the volume limit per node is exhausted on a CSI Driver. There is an open issue reported with kubenetes at https://github.com/kubernetes/kubernetes/issues/95911 with the same behavior. Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for Unity XT CSI driver","excerpt":"Release notes for Unity XT CSI driver","ref":"/csm-docs/docs/csidriver/release/unity/","tags":"","title":"Unity XT"},{"body":" Symptoms Prevention, Resolution or Workaround When you run the command kubectl describe pods unity-controller-\u003csuffix\u003e –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry The kubectl logs -n unity unity-node-\u003csuffix\u003e driver logs show that the driver can’t connect to Unity XT - Authentication failure. Check if you have created a secret with correct credentials fsGroup specified in pod spec is not reflected in files or directories at mounted path of volume. fsType of PVC must be set for fsGroup to work. fsType can be specified while creating a storage class. For NFS protocol, fsType can be specified as nfs. fsGroup doesn’t work for ephemeral inline volumes. Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver controller and node pod should be restarted with command kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}’| xargs kubectl delete -n unity pod when topology-based storage classes are used. For dynamic array addition without topology, the driver will detect the newly added or removed arrays automatically If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in the cluster but on array, it will still be present and marked for deletion. All the cloned PVC should be deleted in order to delete the source PVC from the array. PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass “unity-iscsi”: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}’| xargs kubectl delete -n unity pod Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.24.0 \u003c 1.29.0 which is incompatible with Kubernetes 1.24.6-mirantis-1 If you are using an extended Kubernetes version, see the helm Chart at helm/csi-unity/Chart.yaml and use the alternate kubeVersion check that is provided in the comments. Note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. When a node goes down, the block volumes attached to the node cannot be attached to another node 1. Force delete the pod running on the node that went down 2. Delete the VolumeAttachment to the node that went down. Now the volume can be attached to the new node. Standby controller pod is in crashloopbackoff state Scale down the replica count of the controller pod’s deployment to 1 using kubectl scale deployment \u003cdeployment_name\u003e --replicas=1 -n \u003cdriver_namespace\u003e ","categories":"","description":"Troubleshooting Unity XT Driver","excerpt":"Troubleshooting Unity XT Driver","ref":"/csm-docs/docs/csidriver/troubleshooting/unity/","tags":"","title":"Unity XT"},{"body":"Installing CSI Driver for Unity XT via Dell CSM Operator The CSI Driver for Dell Unity XT can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using the following command:\nkubectl get csm --all-namespaces Prerequisite Create namespace. Execute kubectl create namespace unity to create the unity namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘unity’.\nCreate a file called secret.yaml that has Unity XT array connection details with the following content\nstorageArrayList: - arrayId: \"APM00******1\" # unique array id of the Unisphere array username: \"user\" # username for connecting to API password: \"password\" # password for connecting to API endpoint: \"https://10.1.1.1/\" # full URL path to the Unity XT API skipCertificateValidation: true # indicates if client side validation of (management)server's certificate can be skipped isDefault: true # treat current array as a default (would be used by storage classes without arrayID parameter) Change the parameters with relevant values for your Unity XT array. Add more blocks similar to above for each Unity XT array if necessary.\nUse the following command to create a new secret unity-creds from secret.yaml file.\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nInstall Driver Follow all the prerequisites above\nCreate a CR (Custom Resource) for Unity XT using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity XT driver and their default values:\nParameter Description Required Default replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be in pending state until new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2 namespace Specifies namespace where the driver will be installed Yes “unity” fsGroupPolicy Defines which FS Group policy mode to be used. Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity.enabled Enable/Disable storage capacity tracking No true storageCapacity.pollInterval Configure how often the driver checks for changed capacity No 5m Common parameters for node and controller X_CSI_UNITY_ALLOW_MULTI_POD_ACCESS To enable sharing of volumes across multiple pods within the same node in RWO access mode No false X_CSI_UNITY_SYNC_NODEINFO_INTERVAL Time interval to add node info to array. Default 15 minutes. Minimum value should be 1 No 15 CSI_LOG_LEVEL Sets the logging level of the driver true info TENANT_NAME Tenant name added while adding host entry to the array No CERT_SECRET_COUNT Represents the number of certificate secrets, which the user is going to create for SSL authentication. (unity-cert-0..unity-cert-n). The minimum value should be 1. false 1 X_CSI_UNITY_SKIP_CERTIFICATE_VALIDATION Specifies if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface.If it is set to false, then a secret unity-certs has to be created with an X.509 certificate of CA which signed the Unisphere certificate No true Controller parameters X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition No false Node parameters X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition No false Execute the following command to create Unity XT custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e This command will deploy the CSI Unity XT driver in the namespace specified in the input YAML file.\nNext, the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e Verify the CSI Driver installation Note :\n“Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Snapshotter and resizer sidecars are not optional. They are defaults with Driver installation. ","categories":"","description":"Installing Dell CSI Driver for Unity XT via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for Unity XT via Dell CSM Operator\n","ref":"/csm-docs/docs/deployment/csmoperator/drivers/unity/","tags":"","title":"Unity XT"},{"body":"The CSI Driver for Dell Unity XT can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\nCSI Driver for Unity XT Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume Kubernetes External Health Monitor, which provides volume health status The node section of the Helm chart installs the following component in a DaemonSet:\nCSI Driver for Unity XT Kubernetes Node Registrar, which handles the driver registration Prerequisites Before you install CSI Driver for Unity XT, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements Install Kubernetes or OpenShift (see supported versions) Install Helm v3 To use FC protocol, the host must be zoned with Unity XT array and Multipath needs to be configured To use iSCSI protocol, iSCSI initiator utils packages needs to be installed and Multipath needs to be configured To use NFS protocol, NFS utility packages needs to be installed Mount propagation is enabled on container runtime that is being used Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell Unity XT.\nSteps\nRun the command to install Helm 3.0.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Fibre Channel requirements Dell Unity XT supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell Unity XT:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done. Set up the iSCSI Initiator The CSI Driver for Dell Unity XT supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\nEnsure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell Unity XT array that has IP interfaces. Manually create IP routes for each node that connects to the Dell Unity XT. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi. For more information about configuring iSCSI, see Dell Host Connectivity guide.\nLinux multipathing requirements Dell Unity XT supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell Unity XT.\nSet up Linux multipathing as follows:\nEnsure that all nodes have the Device Mapper Multipathing package installed. You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\nEnable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes. As a best practice, use the following options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 Install CSI Driver Install CSI Driver for Unity XT using this procedure.\nBefore you begin\nAs a pre-requisite for running this procedure, you must have the downloaded files, including the Helm chart from the source git repository with the command git clone -b v2.8.0 https://github.com/dell/csi-unity.git In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. Ensure unity namespace exists in Kubernetes cluster. Use the kubectl create namespace unity command to create the namespace if the namespace is not present. Procedure\nCollect information from the Unity XT Systems like unique ArrayId, IP address, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml and myvalues.yaml file.\nNote:\nArrayId corresponds to the serial number of Unity XT array. Unity XT Array username must have role as Storage Administrator to be able to perform CRUD operations. If the user is using a complex K8s version like “v1.24.6-mirantis-1”, use this kubeVersion check in helm/csi-unity/Chart.yaml file. kubeVersion: “\u003e= 1.24.0-0 \u003c 1.29.0-0” Get the required values.yaml using the command below:\ncd dell-csi-helm-installer \u0026\u0026 wget -O my-unity-settings.yaml https://github.com/dell/helm-charts/raw/csi-unity-2.8.0/charts/csi-unity/values.yaml Edit values.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity XT driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\nParameter Description Required Default logLevel LogLevel is used to set the logging level of the driver No info allowRWOMultiPodAccess Flag to enable multiple pods to use the same PVC on the same node with RWO access mode. No false kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet syncNodeInfoInterval Time interval to add node info to the array. Default 15 minutes. The minimum value should be 1 minute. No 15 maxUnityVolumesPerNode Maximum number of volumes that controller can publish to the node. No 0 certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. (unity-cert-0..unity-cert-n). The minimum value should be 1. No 1 imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent podmon.enabled service to monitor failing jobs and notify No false podmon.image pod man image name No - tenantName Tenant name added while adding host entry to the array No fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity.enabled Enable/Disable storage capacity tracking No true storageCapacity.pollInterval Configure how often the driver checks for changed capacity No 5m controller Allows configuration of the controller-specific parameters. - - controllerCount Defines the number of csi-unity controller pods to deploy to the Kubernetes release Yes 2 volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s” snapshot.enabled Enable/Disable volume snapshot feature Yes true snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot” resizer.enabled Enable/Disable volume expansion feature Yes true nodeSelector Define node selection constraints for pods of controller deployment No tolerations Define tolerations for the controller deployment, if required No healthMonitor.enabled Enable/Disable deployment of external health monitor sidecar for controller side volume health monitoring. No false healthMonitor.interval Interval of monitoring volume health condition. Allowed values: Number followed by unit (s,m,h) No 60s node Allows configuration of the node-specific parameters. - - dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition No false nodeSelector Define node selection constraints for pods of node deployment No tolerations Define tolerations for the node deployment, if required No Note:\nUser should provide all boolean values with double-quotes. This applies only for myvalues.yaml. Example: “true”/“false”\ncontrollerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails.\nUser can a create separate StorageClass (with topology-related keys) by referring to existing default storage classes.\nHost IO Limit must have a minimum bandwidth of 1 MBPS to discover the volumes on node successfully.\nUser must not change the value of allowRWOMultiPodAccess to true unless intended to use the feature and is aware of the consequences. Enabling multiple pods to access the same PVC with RWO access mode on the same node might cause data to be overwritten and therefore leading to data loss in some cases.\nExample myvalues.yaml\nlogLevel: \"info\" imagePullPolicy: Always certSecretCount: 1 kubeletConfigDir: /var/lib/kubelet controller: controllerCount: 2 volumeNamePrefix : csivol snapshot: enabled: true snapNamePrefix: csi-snap resizer: enabled: false allowRWOMultiPodAccess: false syncNodeInfoInterval: 5 maxUnityVolumesPerNode: 0 fsGroupPolicy: ReadWriteOneFSType For certificate validation of Unisphere REST API calls refer here. Otherwise, create an empty secret with file csi-unity/samples/secret/emptysecret.yaml file by running the kubectl create -f csi-unity/samples/secret/emptysecret.yaml command.\nPrepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\nParameter Description Required Default storageArrayList.username Username for accessing Unity XT system Yes - storageArrayList.password Password for accessing Unity XT system Yes - storageArrayList.endpoint REST API gateway HTTPS endpoint Unity XT system Yes - storageArrayList.arrayId ArrayID for Unity XT system Yes - storageArrayList.skipCertificateValidation “skipCertificateValidation \" determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface. If it is set to false, then a secret unity-certs has to be created with an X.509 certificate of CA which signed the Unisphere certificate. Yes true storageArrayList.isDefault An array having isDefault=true or isDefault=true will be considered as the default array when arrayId is not specified in the storage class. This parameter should occur only once in the list. Yes - Example: secret.yaml\nstorageArrayList: - arrayId: \"APM00******1\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.1/\" skipCertificateValidation: true isDefault: true - arrayId: \"APM00******2\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.2/\" skipCertificateValidation: true isDefault: false Use the following command to create a new secret unity-creds from secret.yaml file.\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml Use the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f - Note: The user needs to validate the yaml syntax and array-related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the yaml file.\nAlternatively, users can configure and use secret.yaml for driver configuration. The parameters remain the same as in the above table and below is a sample of secret.yaml. Samples of secret.yaml is available in the directory csi-unity/samples/secret/ .\nExample: secret.yaml\nstorageArrayList: - arrayId: \"APM00******1\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.1/\" skipCertificateValidation: true isDefault: true - arrayId: \"APM00******2\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.2/\" skipCertificateValidation: true isDefault: false Note: Parameters “allowRWOMultiPodAccess” and “syncNodeInfoInterval” have been enabled for configuration in values.yaml and this helps users to dynamically change these values without the need for driver re-installation.\nIf you want to leverage snapshotting feature, the pre-requisite is to install external-snapshotter. Installation of external-snapshotter is required only for Kubernetes and is available by default with OpenShift installations. Click here to follow the procedure to install external-snapshotter.\nRun the command to proceed with the installation using bash script.\n./csi-install.sh --namespace unity --values ./myvalues.yaml A successful installation must display messages that look similar to the following samples:\n------------------------------------------------------ \u003e Installing CSI Driver: csi-unity on 1.27 ------------------------------------------------------ ------------------------------------------------------ \u003e Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u003e Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.27 | |- Driver: csi-unity | |- Verifying Kubernetes version | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that optional secrets have been created Success | |- Verifying alpha snapshot resources | |--\u003e Verifying that alpha snapshot CRDs are not installed Success | |- Verifying sshpass installation.. | |- Verifying iSCSI installation Enter the root password of 10.**.**.**: Enter the root password of 10.**.**.**: Success | |- Verifying snapshot support | |--\u003e Verifying that snapshot CRDs are available Success | |--\u003e Verifying that the snapshot controller is available Success | |- Verifying helm version Success | |- Verifying helm values version Success ------------------------------------------------------ \u003e Verification Complete - Success ------------------------------------------------------ | |- Installing Driver Success | |--\u003e Waiting for Deployment unity-controller to be ready Success | |--\u003e Waiting for DaemonSet unity-node to be ready Success ------------------------------------------------------ \u003e Operation complete ------------------------------------------------------ Results:\nAt the end of the script unity-controller Deployment and DaemonSet unity-node will be ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\nOne or more Unity XT Controllers (based on controllerCount) with 5/5 containers ready, and status displayed as Running. Agent pods with 2/2 containers and the status displayed as Running. Note: To install nightly or latest csi driver build using bash script use this command:\n/csi-install.sh --namespace unity --values ./myvalues.yaml --version latest --helm-charts-version \u003cversion\u003e You can also install the driver using standalone helm chart by cloning the centralised helm charts and running the helm install command as shown.\nSyntax:\ngit clone -b csi-unity-2.8.0 https://github.com/dell/helm-charts helm install \u003crelease-name\u003e dell/container-storage-modules -n \u003cnamespace\u003e --version \u003ccontainer-storage-module chart-version\u003e -f \u003cvalues.yaml location\u003e Example: helm install unity dell/container-storage-modules -n csi-unity --version 1.0.1 -f values.yaml Certificate validation for Unisphere REST API calls This topic provides details about setting up the Dell Unity XT certificate validation for the CSI Driver.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on the “.Values.certSecretCount” parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\nTo fetch the certificate, run the following command.\nopenssl s_client -showcerts -connect \u003cUnisphere IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example:\nopenssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Run the following command to create the cert secret with index ‘0’:\nkubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret:\nkubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)\nNote:\n“unity” is the namespace for helm-based installation but namespace can be user-defined in operator-based installation.\nUser can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation.\nWhenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\nVolume Snapshot Class A wide set of annotated storage class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nStorage Classes Storage Classes are an essential Kubernetes construct for Storage provisioning. To know more about Storage Classes, refer to https://kubernetes.io/docs/concepts/storage/storage-classes/\nA wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nFor the Unity XT CSI Driver, a wide set of annotated storage class manifests have been provided in the csi-unity/samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class: There are samples storage class yaml files available under csi-unity/samples/storageclass. These can be copied and modified as needed.\nPick any of unity-fc.yaml, unity-iscsi.yaml or unity-nfs.yaml Copy the file as unity-\u003cARRAY_ID\u003e-fc.yaml, unity-\u003cARRAY_ID\u003e-iscsi.yaml or unity-\u003cARRAY_ID\u003e-nfs.yaml Replace \u003cARRAY_ID\u003e with the Array Id of the Unity Array to be used Replace \u003cSTORAGE_POOL\u003e with the storage pool you have Replace \u003cTIERING_POLICY\u003e with the Tiering policy that is to be used for provisioning Replace \u003cHOST_IO_LIMIT_NAME\u003e with the Host IO Limit Name that is to be used for provisioning Replace \u003cmountOption1\u003e with the necessary mount options. If not required, this can be removed from the storage class Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f unity-\u003cARRAY_ID\u003e-fc.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-iscsi.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-nfs.yaml Note:\nAt least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es): Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nDynamically update the unity-creds secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Note: Updating unity-certs-x secrets is a manual process, unlike unity-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nDynamic Logging Configuration Helm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\n","categories":"","description":"Installing CSI Driver for Unity XT via Helm\n","excerpt":"Installing CSI Driver for Unity XT via Helm\n","ref":"/csm-docs/v1/csidriver/installation/helm/unity/","tags":"","title":"Unity XT"},{"body":" The Dell CSI Operator is no longer actively maintained or supported. Dell CSI Operator has been replaced with Dell CSM Operator. If you are currently using Dell CSI Operator, refer to the operator migration documentation to migrate from Dell CSI Operator to Dell CSM Operator.\nCSI Driver for Unity XT Pre-requisites Create secret to store Unity XT credentials Create a namespace called unity (it can be any user-defined name; But commands in this section assumes that the namespace is unity) Prepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\nParameter Description Required Default username Username for accessing Unity XT system true - password Password for accessing Unity XT system true - endpoint REST API gateway HTTPS endpoint Unity XT system true - arrayId ArrayID for Unity XT system true - isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list. true - skipCertificateValidation Determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface true true Ex: secret.yaml\nstorageArrayList: - arrayId: \"APM00******1\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.1/\" skipCertificateValidation: true isDefault: true - arrayId: \"APM00******2\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.2/\" skipCertificateValidation: true kubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml Use the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f - Note: The user needs to validate the YAML syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\nCreate secret for client side TLS verification Please refer detailed documentation on how to create this secret here\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion: v1 kind: Secret metadata: name: unity-certs-0 namespace: unity type: Opaque data: cert-0: \"\" Execute command: kubectl create -f empty-secret.yaml\nModify/Set the following optional environment variables Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\nParameter Description Required Default Common parameters for node and controller CSI_ENDPOINT Specifies the HTTP endpoint for Unity XT. No /var/run/csi/csi.sock X_CSI_UNITY_ALLOW_MULTI_POD_ACCESS Flag to enable multiple pods use same pvc on same node with RWO access mode No false Controller parameters X_CSI_MODE Driver starting mode No controller X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin No Node parameters X_CSI_MODE Driver starting mode No node X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands No /noderoot X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Node plugin No Example CR for Unity XT Refer samples from here. Below is an example CR:\napiVersion: storage.dell.com/v1 kind: CSIUnity metadata: name: unity namespace: unity spec: driver: configVersion: v2.7.0 replicas: 2 dnsPolicy: ClusterFirstWithHostNet forceUpdate: false common: image: \"dellemc/csi-unity:v2.7.0\" imagePullPolicy: IfNotPresent sideCars: - name: provisioner args: [\"--volume-name-prefix=csiunity\",\"--default-fstype=ext4\"] - name: snapshotter args: [\"--snapshot-name-prefix=csiunitysnap\"] # Enable/Disable health monitor of CSI volumes from node plugin. Provides details of volume usage. # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" # nodeSelector: Define node selection constraints for controller pods. # For the pod to be eligible to run on a node, the node must have each # of the indicated key-value pairs as labels. # Leave as blank to consider all nodes # Allowed values: map of key-value pairs # Default value: None nodeSelector: # Uncomment if nodes you wish to use have the node-role.kubernetes. io/control-plane taint # node-role.kubernetes.io/control-plane: \"\" # tolerations: Define tolerations for the controllers, if required. # Leave as blank to install controller on worker nodes # Default value: None tolerations: # Uncomment if nodes you wish to use have the node-role.kubernetes.io/control-plane taint # - key: \"node-role.kubernetes.io/control-plane\" # operator: \"Exists\" # effect: \"NoSchedule\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" # nodeSelector: Define node selection constraints for node pods. # For the pod to be eligible to run on a node, the node must have each # of the indicated key-value pairs as labels. # Leave as blank to consider all nodes # Allowed values: map of key-value pairs # Default value: None nodeSelector: # Uncomment if nodes you wish to use have the node-role.kubernetes.io/control-plane taint # node-role.kubernetes.io/control-plane: \"\" # tolerations: Define tolerations for the node daemonset, if required. # Default value: None tolerations: # Uncomment if nodes you wish to use have the node-role.kubernetes.io/control-plane taint # - key: \"node-role.kubernetes.io/control-plane\" # operator: \"Exists\" # effect: \"NoSchedule\" # - key: \"node.kubernetes.io/memory-pressure\" # operator: \"Exists\" # effect: \"NoExecute\" # - key: \"node.kubernetes.io/disk-pressure\" # operator: \"Exists\" # effect: \"NoExecute\" # - key: \"node.kubernetes.io/network-unavailable\" # operator: \"Exists\" # effect: \"NoExecute\" --- apiVersion: v1 kind: ConfigMap metadata: name: unity-config-params namespace: unity data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"info\" ALLOW_RWO_MULTIPOD_ACCESS: \"false\" MAX_UNITY_VOLUMES_PER_NODE: \"0\" SYNC_NODE_INFO_TIME_INTERVAL: \"15\" TENANT_NAME: \"\" Dynamic Logging Configuration Operator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params Note :\nThe log level is not allowed to be updated dynamically through logLevel attribute in the secret object. “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. Volume Health Monitoring Operator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_ENABLE_VOL_HEALTH_MONITOR to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin- volume status, volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" ","categories":"","description":"Installing CSI Driver for Unity XT via Operator\n","excerpt":"Installing CSI Driver for Unity XT via Operator\n","ref":"/csm-docs/v1/csidriver/installation/operator/unity/","tags":"","title":"Unity XT"},{"body":"Test deploying a simple Pod and PVC with Unity XT storage In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes and automatically mounts them to the pod.\nSteps\nTo run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./test/sample.yaml You can find all the created resources in unity namespace.\nCheck if the pod is created and Ready and Running by running:\nkubectl get all -n unity If it is in CrashLoopback state then the driver installation was not successful. Check the logs of the node and the controller.\nGo into the created container and verify that everything is mounted correctly.\nAfter verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./test/sample.yaml Support for SLES 15 The CSI Driver for Dell Unity XT requires these of packages installed on all worker nodes that run on SLES 15.\nopen-iscsi open-iscsi is required in order to make use of iSCSI protocol for provisioning nfs-utils nfs-utils is required in order to make use of NFS protocol for provisioning multipath-tools multipath-tools is required in order to make use of FC and iSCSI protocols for provisioning After installing open-iscsi, ensure “iscsi” and “iscsid” services have been started and /etc/isci/initiatorname.iscsi is created and has the host initiator id. The pre-requisites are mandatory for provisioning with the iSCSI protocol to work.\n","categories":"","description":"Tests to validate Unity XT CSI Driver installation","excerpt":"Tests to validate Unity XT CSI Driver installation","ref":"/csm-docs/v1/csidriver/installation/test/unity/","tags":"","title":"Test Unity XT CSI Driver"},{"body":"Release Notes - CSI Unity XT v2.8.0 New Features/Changes #724 - [FEATURE]: CSM support for Openshift 4.13 #876 - [FEATURE]: CSI 1.5 spec support -StorageCapacityTracking #877 - [FEATURE]: Make standalone helm chart available from helm repository : https://dell.github.io/dell/helm-charts #891 - [FEATURE]: Enhancing Unity XT driver to handle API requests after the sessionIdleTimeOut in STIG mode Fixed Issues #849 - [BUG]: CSI driver does not verify iSCSI initiators on the array correctly #916 - [BUG]: Remove references to deprecated io/ioutil package Known Issues Issue Workaround Topology-related node labels are not removed automatically. Currently, when the driver is uninstalled, topology-related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released, remove the labels manually after the driver un-installation using command kubectl label node \u003cnode_name\u003e - - … Example: kubectl label node csi-unity.dellemc.com/array123-iscsi- Note: there must be - at the end of each label to remove it. NFS Clone - Resize of the snapshot is not supported by Unity XT Platform, however the user should never try to resize the cloned NFS volume. Currently, when the driver takes a clone of NFS volume, it succeeds but if the user tries to resize the NFS volumesnapshot, the driver will throw an error. Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100 When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the VolumeAttachment to the node that went down. Now the volume can be attached to the new node. A CSI ephemeral pod may not get created in OpenShift 4.13 and fail with the error \"error when creating pod: the pod uses an inline volume provided by CSIDriver csi-unity.dellemc.com, and the namespace has a pod security enforcement level that is lower than privileged.\" This issue occurs because OpenShift 4.13 introduced the CSI Volume Admission plugin to restrict the use of a CSI driver capable of provisioning CSI ephemeral volumes during pod admission. Therefore, an additional label security.openshift.io/csi-ephemeral-volume-profile in csidriver.yaml file with the required security profile value should be provided. Follow OpenShift 4.13 documentation for CSI Ephemeral Volumes for more information. If the volume limit is exhausted and there are pending pods and PVCs due to exceed max volume count, the pending PVCs will be bound to PVs and the pending pods will be scheduled to nodes when the driver pods are restarted. It is advised not to have any pending pods or PVCs once the volume limit per node is exhausted on a CSI Driver. There is an open issue reported with kubenetes at https://github.com/kubernetes/kubernetes/issues/95911 with the same behavior. Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for Unity XT CSI driver","excerpt":"Release notes for Unity XT CSI driver","ref":"/csm-docs/v1/csidriver/release/unity/","tags":"","title":"Unity XT"},{"body":" Symptoms Prevention, Resolution or Workaround When you run the command kubectl describe pods unity-controller-\u003csuffix\u003e –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry The kubectl logs -n unity unity-node-\u003csuffix\u003e driver logs show that the driver can’t connect to Unity XT - Authentication failure. Check if you have created a secret with correct credentials fsGroup specified in pod spec is not reflected in files or directories at mounted path of volume. fsType of PVC must be set for fsGroup to work. fsType can be specified while creating a storage class. For NFS protocol, fsType can be specified as nfs. fsGroup doesn’t work for ephemeral inline volumes. Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver controller and node pod should be restarted with command kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}’| xargs kubectl delete -n unity pod when topology-based storage classes are used. For dynamic array addition without topology, the driver will detect the newly added or removed arrays automatically If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in the cluster but on array, it will still be present and marked for deletion. All the cloned PVC should be deleted in order to delete the source PVC from the array. PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass “unity-iscsi”: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}’| xargs kubectl delete -n unity pod Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.24.0 \u003c 1.29.0 which is incompatible with Kubernetes 1.24.6-mirantis-1 If you are using an extended Kubernetes version, see the helm Chart at helm/csi-unity/Chart.yaml and use the alternate kubeVersion check that is provided in the comments. Note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. When a node goes down, the block volumes attached to the node cannot be attached to another node 1. Force delete the pod running on the node that went down 2. Delete the VolumeAttachment to the node that went down. Now the volume can be attached to the new node. ","categories":"","description":"Troubleshooting Unity XT Driver","excerpt":"Troubleshooting Unity XT Driver","ref":"/csm-docs/v1/csidriver/troubleshooting/unity/","tags":"","title":"Unity XT"},{"body":"Installing CSI Driver for Unity XT via Dell CSM Operator The CSI Driver for Dell Unity XT can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using the following command:\nkubectl get csm --all-namespaces Prerequisite Create namespace. Execute kubectl create namespace unity to create the unity namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘unity’.\nCreate a file called secret.yaml that has Unity XT array connection details with the following content\nstorageArrayList: - arrayId: \"APM00******1\" # unique array id of the Unisphere array username: \"user\" # username for connecting to API password: \"password\" # password for connecting to API endpoint: \"https://10.1.1.1/\" # full URL path to the Unity XT API skipCertificateValidation: true # indicates if client side validation of (management)server's certificate can be skipped isDefault: true # treat current array as a default (would be used by storage classes without arrayID parameter) Change the parameters with relevant values for your Unity XT array. Add more blocks similar to above for each Unity XT array if necessary.\nUse the following command to create a new secret unity-creds from secret.yaml file.\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nInstall Driver Follow all the prerequisites above\nCreate a CR (Custom Resource) for Unity XT using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity XT driver and their default values:\nParameter Description Required Default replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be in pending state until new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2 namespace Specifies namespace where the driver will be installed Yes “unity” fsGroupPolicy Defines which FS Group policy mode to be used. Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” storageCapacity.enabled Enable/Disable storage capacity tracking No true storageCapacity.pollInterval Configure how often the driver checks for changed capacity No 5m Common parameters for node and controller X_CSI_UNITY_ALLOW_MULTI_POD_ACCESS To enable sharing of volumes across multiple pods within the same node in RWO access mode No false X_CSI_UNITY_SYNC_NODEINFO_INTERVAL Time interval to add node info to array. Default 15 minutes. Minimum value should be 1 No 15 CSI_LOG_LEVEL Sets the logging level of the driver true info TENANT_NAME Tenant name added while adding host entry to the array No CERT_SECRET_COUNT Represents the number of certificate secrets, which the user is going to create for SSL authentication. (unity-cert-0..unity-cert-n). The minimum value should be 1. false 1 X_CSI_UNITY_SKIP_CERTIFICATE_VALIDATION Specifies if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface.If it is set to false, then a secret unity-certs has to be created with an X.509 certificate of CA which signed the Unisphere certificate No true Controller parameters X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition No false Node parameters X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition No false Execute the following command to create Unity XT custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e This command will deploy the CSI Unity XT driver in the namespace specified in the input YAML file.\nNext, the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e Verify the CSI Driver installation Note :\n“Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Snapshotter and resizer sidecars are not optional. They are defaults with Driver installation. ","categories":"","description":"Installing Dell CSI Driver for Unity XT via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for Unity XT via Dell CSM Operator\n","ref":"/csm-docs/v1/deployment/csmoperator/drivers/unity/","tags":"","title":"Unity XT"},{"body":"The CSI Driver for Dell Unity XT can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\nCSI Driver for Unity XT Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume Kubernetes External Health Monitor, which provides volume health status The node section of the Helm chart installs the following component in a DaemonSet:\nCSI Driver for Unity XT Kubernetes Node Registrar, which handles the driver registration Prerequisites Before you install CSI Driver for Unity XT, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements Install Kubernetes or OpenShift (see supported versions) Install Helm v3 To use FC protocol, the host must be zoned with Unity XT array and Multipath needs to be configured To use iSCSI protocol, iSCSI initiator utils packages needs to be installed and Multipath needs to be configured To use NFS protocol, NFS utility packages needs to be installed Mount propagation is enabled on container runtime that is being used Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell Unity XT.\nSteps\nRun the command to install Helm 3.0.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Fibre Channel requirements Dell Unity XT supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell Unity XT:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done. Set up the iSCSI Initiator The CSI Driver for Dell Unity XT supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\nEnsure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell Unity XT array that has IP interfaces. Manually create IP routes for each node that connects to the Dell Unity XT. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi. For more information about configuring iSCSI, see Dell Host Connectivity guide.\nLinux multipathing requirements Dell Unity XT supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell Unity XT.\nSet up Linux multipathing as follows:\nEnsure that all nodes have the Device Mapper Multipathing package installed. You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\nEnable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes. As a best practice, use the following options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 Install CSI Driver Install CSI Driver for Unity XT using this procedure.\nBefore you begin\nAs a pre-requisite for running this procedure, you must have the downloaded files, including the Helm chart from the source git repository with the command git clone -b v2.7.0 https://github.com/dell/csi-unity.git In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. Ensure unity namespace exists in Kubernetes cluster. Use the kubectl create namespace unity command to create the namespace if the namespace is not present. Procedure\nCollect information from the Unity XT Systems like Unique ArrayId, IP address, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml and myvalues.yaml file.\nNote:\nArrayId corresponds to the serial number of Unity XT array. Unity XT Array username must have role as Storage Administrator to be able to perform CRUD operations. If the user is using a complex K8s version like “v1.24.6-mirantis-1”, use this kubeVersion check in helm/csi-unity/Chart.yaml file. kubeVersion: “\u003e= 1.24.0-0 \u003c 1.28.0-0” Copy the helm/csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.\nEdit myvalues.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity XT driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\nParameter Description Required Default version helm version true - logLevel LogLevel is used to set the logging level of the driver true info allowRWOMultiPodAccess Flag to enable multiple pods to use the same PVC on the same node with RWO access mode. false false kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet syncNodeInfoInterval Time interval to add node info to the array. Default 15 minutes. The minimum value should be 1 minute. false 15 maxUnityVolumesPerNode Maximum number of volumes that controller can publish to the node. false 0 certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. (unity-cert-0..unity-cert-n). The minimum value should be 1. false 1 imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent podmon.enabled service to monitor failing jobs and notify false - podmon.image pod man image name false - tenantName Tenant name added while adding host entry to the array No fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” controller Allows configuration of the controller-specific parameters. - - controllerCount Defines the number of csi-unity controller pods to deploy to the Kubernetes release Yes 2 volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s” snapshot.enabled Enable/Disable volume snapshot feature Yes true snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot” resizer.enabled Enable/Disable volume expansion feature Yes true nodeSelector Define node selection constraints for pods of controller deployment No tolerations Define tolerations for the controller deployment, if required No healthMonitor.enabled Enable/Disable deployment of external health monitor sidecar for controller side volume health monitoring. No false healthMonitor.interval Interval of monitoring volume health condition. Allowed values: Number followed by unit (s,m,h) No 60s node Allows configuration of the node-specific parameters. - - dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition No false nodeSelector Define node selection constraints for pods of node deployment No tolerations Define tolerations for the node deployment, if required No Note:\nUser should provide all boolean values with double-quotes. This applies only for myvalues.yaml. Example: “true”/“false”\ncontrollerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails.\nUser can a create separate StorageClass (with topology-related keys) by referring to existing default storage classes.\nHost IO Limit must have a minimum bandwidth of 1 MBPS to discover the volumes on node successfully.\nUser must not change the value of allowRWOMultiPodAccess to true unless intended to use the feature and is aware of the consequences. Enabling multiple pods to access the same PVC with RWO access mode on the same node might cause data to be overwritten and therefore leading to data loss in some cases.\nExample myvalues.yaml\nlogLevel: \"info\" imagePullPolicy: Always certSecretCount: 1 kubeletConfigDir: /var/lib/kubelet controller: controllerCount: 2 volumeNamePrefix : csivol snapshot: enabled: true snapNamePrefix: csi-snap resizer: enabled: false allowRWOMultiPodAccess: false syncNodeInfoInterval: 5 maxUnityVolumesPerNode: 0 fsGroupPolicy: ReadWriteOneFSType For certificate validation of Unisphere REST API calls refer here. Otherwise, create an empty secret with file csi-unity/samples/secret/emptysecret.yaml file by running the kubectl create -f csi-unity/samples/secret/emptysecret.yaml command.\nPrepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\nParameter Description Required Default storageArrayList.username Username for accessing Unity XT system true - storageArrayList.password Password for accessing Unity XT system true - storageArrayList.endpoint REST API gateway HTTPS endpoint Unity XT system true - storageArrayList.arrayId ArrayID for Unity XT system true - storageArrayList.skipCertificateValidation “skipCertificateValidation \" determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface. If it is set to false, then a secret unity-certs has to be created with an X.509 certificate of CA which signed the Unisphere certificate. true true storageArrayList.isDefault An array having isDefault=true or isDefault=true will be considered as the default array when arrayId is not specified in the storage class. This parameter should occur only once in the list. true - Example: secret.yaml\nstorageArrayList: - arrayId: \"APM00******1\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.1/\" skipCertificateValidation: true isDefault: true - arrayId: \"APM00******2\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.2/\" skipCertificateValidation: true Use the following command to create a new secret unity-creds from secret.yaml file.\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml Use the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f - Note: The user needs to validate the yaml syntax and array-related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the yaml file.\nAlternatively, users can configure and use secret.yaml for driver configuration. The parameters remain the same as in the above table and below is a sample of secret.yaml. Samples of secret.yaml is available in the directory csi-unity/samples/secret/ .\nExample: secret.yaml\nstorageArrayList: - arrayId: \"APM00******1\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.1/\" skipCertificateValidation: true isDefault: true - arrayId: \"APM00******2\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.2/\" skipCertificateValidation: true Note: Parameters “allowRWOMultiPodAccess” and “syncNodeInfoInterval” have been enabled for configuration in values.yaml and this helps users to dynamically change these values without the need for driver re-installation.\nIf you want to leverage snapshotting feature, the pre-requisite is to install external-snapshotter. Installation of external-snapshotter is required only for Kubernetes and is available by default with OpenShift installations. Click here to follow the procedure to install external-snapshotter.\nRun the command to proceed with the installation using bash script.\n./csi-install.sh --namespace unity --values ./myvalues.yaml A successful installation must display messages that look similar to the following samples:\n------------------------------------------------------ \u003e Installing CSI Driver: csi-unity on 1.27 ------------------------------------------------------ ------------------------------------------------------ \u003e Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u003e Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.27 | |- Driver: csi-unity | |- Verifying Kubernetes version | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that optional secrets have been created Success | |- Verifying alpha snapshot resources | |--\u003e Verifying that alpha snapshot CRDs are not installed Success | |- Verifying sshpass installation.. | |- Verifying iSCSI installation Enter the root password of 10.**.**.**: Enter the root password of 10.**.**.**: Success | |- Verifying snapshot support | |--\u003e Verifying that snapshot CRDs are available Success | |--\u003e Verifying that the snapshot controller is available Success | |- Verifying helm version Success | |- Verifying helm values version Success ------------------------------------------------------ \u003e Verification Complete - Success ------------------------------------------------------ | |- Installing Driver Success | |--\u003e Waiting for Deployment unity-controller to be ready Success | |--\u003e Waiting for DaemonSet unity-node to be ready Success ------------------------------------------------------ \u003e Operation complete ------------------------------------------------------ Results:\nAt the end of the script unity-controller Deployment and DaemonSet unity-node will be ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\nOne or more Unity XT Controllers (based on controllerCount) with 5/5 containers ready, and status displayed as Running. Agent pods with 2/2 containers and the status displayed as Running. Note: To install nightly or latest csi driver build using bash script use this command:\n/csi-install.sh --namespace unity --values ./myvalues.yaml --version latest You can also install the driver using standalone helm chart by running helm install command, first using the –dry-run flag to confirm various parameters are as desired. Once the parameters are validated, run the command without the –dry-run flag. Note: This example assumes that the user is at repo root helm folder i.e csi-unity/helm.\nSyntax:\nhelm install --dry-run --values \u003cmyvalues.yaml location\u003e --namespace \u003cnamespace\u003e \u003cname of secret\u003e \u003chelmPath\u003e \u003cnamespace\u003e - namespace of the driver installation. \u003cname of secret\u003e - unity in case of unity-creds and unity-certs-0 secrets. \u003chelmPath\u003e - Path of the helm directory. e.g:\nhelm install --dry-run --values ./csi-unity/myvalues.yaml --namespace unity unity ./csi-unity Certificate validation for Unisphere REST API calls This topic provides details about setting up the Dell Unity XT certificate validation for the CSI Driver.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on the “.Values.certSecretCount” parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\nTo fetch the certificate, run the following command.\nopenssl s_client -showcerts -connect \u003cUnisphere IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example:\nopenssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Run the following command to create the cert secret with index ‘0’:\nkubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret:\nkubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)\nNote:\n“unity” is the namespace for helm-based installation but namespace can be user-defined in operator-based installation.\nUser can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation.\nWhenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\nVolume Snapshot Class A wide set of annotated storage class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nStorage Classes Storage Classes are an essential Kubernetes construct for Storage provisioning. To know more about Storage Classes, refer to https://kubernetes.io/docs/concepts/storage/storage-classes/\nA wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nFor the Unity XT CSI Driver, a wide set of annotated storage class manifests have been provided in the csi-unity/samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class: There are samples storage class yaml files available under csi-unity/samples/storageclass. These can be copied and modified as needed.\nPick any of unity-fc.yaml, unity-iscsi.yaml or unity-nfs.yaml Copy the file as unity-\u003cARRAY_ID\u003e-fc.yaml, unity-\u003cARRAY_ID\u003e-iscsi.yaml or unity-\u003cARRAY_ID\u003e-nfs.yaml Replace \u003cARRAY_ID\u003e with the Array Id of the Unity Array to be used Replace \u003cSTORAGE_POOL\u003e with the storage pool you have Replace \u003cTIERING_POLICY\u003e with the Tiering policy that is to be used for provisioning Replace \u003cHOST_IO_LIMIT_NAME\u003e with the Host IO Limit Name that is to be used for provisioning Replace \u003cmountOption1\u003e with the necessary mount options. If not required, this can be removed from the storage class Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f unity-\u003cARRAY_ID\u003e-fc.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-iscsi.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-nfs.yaml Note:\nAt least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es): Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nDynamically update the unity-creds secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Note: Updating unity-certs-x secrets is a manual process, unlike unity-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nDynamic Logging Configuration Helm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\n","categories":"","description":"Installing CSI Driver for Unity XT via Helm\n","excerpt":"Installing CSI Driver for Unity XT via Helm\n","ref":"/csm-docs/v2/csidriver/installation/helm/unity/","tags":"","title":"Unity XT"},{"body":" The Dell CSI Operator is no longer actively maintained or supported. Dell CSI Operator has been replaced with Dell CSM Operator. If you are currently using Dell CSI Operator, refer to the operator migration documentation to migrate from Dell CSI Operator to Dell CSM Operator.\nCSI Driver for Unity XT Pre-requisites Create secret to store Unity XT credentials Create a namespace called unity (it can be any user-defined name; But commands in this section assumes that the namespace is unity) Prepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\nParameter Description Required Default username Username for accessing Unity XT system true - password Password for accessing Unity XT system true - endpoint REST API gateway HTTPS endpoint Unity XT system true - arrayId ArrayID for Unity XT system true - isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list. true - skipCertificateValidation Determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface true true Ex: secret.yaml\nstorageArrayList: - arrayId: \"APM00******1\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.1/\" skipCertificateValidation: true isDefault: true - arrayId: \"APM00******2\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.2/\" skipCertificateValidation: true kubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml Use the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f - Note: The user needs to validate the YAML syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\nCreate secret for client side TLS verification Please refer detailed documentation on how to create this secret here\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion: v1 kind: Secret metadata: name: unity-certs-0 namespace: unity type: Opaque data: cert-0: \"\" Execute command: kubectl create -f empty-secret.yaml\nModify/Set the following optional environment variables Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\nParameter Description Required Default Common parameters for node and controller CSI_ENDPOINT Specifies the HTTP endpoint for Unity XT. No /var/run/csi/csi.sock X_CSI_UNITY_ALLOW_MULTI_POD_ACCESS Flag to enable multiple pods use same pvc on same node with RWO access mode No false Controller parameters X_CSI_MODE Driver starting mode No controller X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin No Node parameters X_CSI_MODE Driver starting mode No node X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands No /noderoot X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Node plugin No Example CR for Unity XT Refer samples from here. Below is an example CR:\napiVersion: storage.dell.com/v1 kind: CSIUnity metadata: name: unity namespace: unity spec: driver: configVersion: v2.7.0 replicas: 2 dnsPolicy: ClusterFirstWithHostNet forceUpdate: false common: image: \"dellemc/csi-unity:v2.7.0\" imagePullPolicy: IfNotPresent sideCars: - name: provisioner args: [\"--volume-name-prefix=csiunity\",\"--default-fstype=ext4\"] - name: snapshotter args: [\"--snapshot-name-prefix=csiunitysnap\"] # Enable/Disable health monitor of CSI volumes from node plugin. Provides details of volume usage. # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" # nodeSelector: Define node selection constraints for controller pods. # For the pod to be eligible to run on a node, the node must have each # of the indicated key-value pairs as labels. # Leave as blank to consider all nodes # Allowed values: map of key-value pairs # Default value: None nodeSelector: # Uncomment if nodes you wish to use have the node-role.kubernetes. io/control-plane taint # node-role.kubernetes.io/control-plane: \"\" # tolerations: Define tolerations for the controllers, if required. # Leave as blank to install controller on worker nodes # Default value: None tolerations: # Uncomment if nodes you wish to use have the node-role.kubernetes.io/control-plane taint # - key: \"node-role.kubernetes.io/control-plane\" # operator: \"Exists\" # effect: \"NoSchedule\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" # nodeSelector: Define node selection constraints for node pods. # For the pod to be eligible to run on a node, the node must have each # of the indicated key-value pairs as labels. # Leave as blank to consider all nodes # Allowed values: map of key-value pairs # Default value: None nodeSelector: # Uncomment if nodes you wish to use have the node-role.kubernetes.io/control-plane taint # node-role.kubernetes.io/control-plane: \"\" # tolerations: Define tolerations for the node daemonset, if required. # Default value: None tolerations: # Uncomment if nodes you wish to use have the node-role.kubernetes.io/control-plane taint # - key: \"node-role.kubernetes.io/control-plane\" # operator: \"Exists\" # effect: \"NoSchedule\" # - key: \"node.kubernetes.io/memory-pressure\" # operator: \"Exists\" # effect: \"NoExecute\" # - key: \"node.kubernetes.io/disk-pressure\" # operator: \"Exists\" # effect: \"NoExecute\" # - key: \"node.kubernetes.io/network-unavailable\" # operator: \"Exists\" # effect: \"NoExecute\" --- apiVersion: v1 kind: ConfigMap metadata: name: unity-config-params namespace: unity data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"info\" ALLOW_RWO_MULTIPOD_ACCESS: \"false\" MAX_UNITY_VOLUMES_PER_NODE: \"0\" SYNC_NODE_INFO_TIME_INTERVAL: \"15\" TENANT_NAME: \"\" Dynamic Logging Configuration Operator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params Note :\nThe log level is not allowed to be updated dynamically through logLevel attribute in the secret object. “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. Volume Health Monitoring Operator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_ENABLE_VOL_HEALTH_MONITOR to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin- volume status, volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" ","categories":"","description":"Installing CSI Driver for Unity XT via Operator\n","excerpt":"Installing CSI Driver for Unity XT via Operator\n","ref":"/csm-docs/v2/csidriver/installation/operator/unity/","tags":"","title":"Unity XT"},{"body":"Test deploying a simple Pod and PVC with Unity XT storage In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes and automatically mounts them to the pod.\nSteps\nTo run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./test/sample.yaml You can find all the created resources in unity namespace.\nCheck if the pod is created and Ready and Running by running:\nkubectl get all -n unity If it is in CrashLoopback state then the driver installation was not successful. Check the logs of the node and the controller.\nGo into the created container and verify that everything is mounted correctly.\nAfter verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./test/sample.yaml Support for SLES 15 The CSI Driver for Dell Unity XT requires these of packages installed on all worker nodes that run on SLES 15.\nopen-iscsi open-iscsi is required in order to make use of iSCSI protocol for provisioning nfs-utils nfs-utils is required in order to make use of NFS protocol for provisioning multipath-tools multipath-tools is required in order to make use of FC and iSCSI protocols for provisioning After installing open-iscsi, ensure “iscsi” and “iscsid” services have been started and /etc/isci/initiatorname.iscsi is created and has the host initiator id. The pre-requisites are mandatory for provisioning with the iSCSI protocol to work.\n","categories":"","description":"Tests to validate Unity XT CSI Driver installation","excerpt":"Tests to validate Unity XT CSI Driver installation","ref":"/csm-docs/v2/csidriver/installation/test/unity/","tags":"","title":"Test Unity XT CSI Driver"},{"body":"Release Notes - CSI Unity XT v2.7.0 New Features/Changes Migrated image registry from k8s.gcr.io to registry.k8s.io Added support for OpenShift 4.12 Added support for Kubernetes 1.27 Added support for K3s on Debian OS Added support for Unisphere 5.3.0 array Fixed Issues There are no fixed issues in this release.\nKnown Issues Issue Workaround Topology-related node labels are not removed automatically. Currently, when the driver is uninstalled, topology-related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released, remove the labels manually after the driver un-installation using command kubectl label node \u003cnode_name\u003e - - … Example: kubectl label node csi-unity.dellemc.com/array123-iscsi- Note: there must be - at the end of each label to remove it. NFS Clone - Resize of the snapshot is not supported by Unity XT Platform, however the user should never try to resize the cloned NFS volume. Currently, when the driver takes a clone of NFS volume, it succeeds but if the user tries to resize the NFS volumesnapshot, the driver will throw an error. Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100 When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the VolumeAttachment to the node that went down. Now the volume can be attached to the new node. CSI driver does not verify iSCSI initiators on the array correctly when iSCSI initiator names are not in lowercase - After any node reboot, the driver pod on that rebooted node goes into a failed state, failing to find the iSCSI initiator on the array Work around is to rename host iSCSI initiators to lowercase and reboot the respective worker node. The CSI driver pod will spin off successfully. Example: Rename “iqn.2000-11.com.DEMOWORKERNODE01:1a234b56cd78” to “iqn.2000-11.com.demoworkernode01:1a234b56cd78” in lowercase. Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for Unity XT CSI driver","excerpt":"Release notes for Unity XT CSI driver","ref":"/csm-docs/v2/csidriver/release/unity/","tags":"","title":"Unity XT"},{"body":" Symptoms Prevention, Resolution or Workaround When you run the command kubectl describe pods unity-controller-\u003csuffix\u003e –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry The kubectl logs -n unity unity-node-\u003csuffix\u003e driver logs show that the driver can’t connect to Unity XT - Authentication failure. Check if you have created a secret with correct credentials fsGroup specified in pod spec is not reflected in files or directories at mounted path of volume. fsType of PVC must be set for fsGroup to work. fsType can be specified while creating a storage class. For NFS protocol, fsType can be specified as nfs. fsGroup doesn’t work for ephemeral inline volumes. Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver controller and node pod should be restarted with command kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}’| xargs kubectl delete -n unity pod when topology-based storage classes are used. For dynamic array addition without topology, the driver will detect the newly added or removed arrays automatically If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in the cluster but on array, it will still be present and marked for deletion. All the cloned PVC should be deleted in order to delete the source PVC from the array. PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass “unity-iscsi”: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}’| xargs kubectl delete -n unity pod Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.24.0 \u003c 1.28.0 which is incompatible with Kubernetes 1.24.6-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart at helm/csi-unity/Chart.yaml and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. When a node goes down, the block volumes attached to the node cannot be attached to another node 1. Force delete the pod running on the node that went down 2. Delete the VolumeAttachment to the node that went down. Now the volume can be attached to the new node. ","categories":"","description":"Troubleshooting Unity XT Driver","excerpt":"Troubleshooting Unity XT Driver","ref":"/csm-docs/v2/csidriver/troubleshooting/unity/","tags":"","title":"Unity XT"},{"body":"Installing CSI Driver for Unity XT via Dell CSM Operator The CSI Driver for Dell Unity XT can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using the following command:\nkubectl get csm --all-namespaces Prerequisite Create namespace. Execute kubectl create namespace unity to create the unity namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘unity’.\nCreate a file called secret.yaml that has Unity XT array connection details with the following content\nstorageArrayList: - arrayId: \"APM00******1\" # unique array id of the Unisphere array username: \"user\" # username for connecting to API password: \"password\" # password for connecting to API endpoint: \"https://10.1.1.1/\" # full URL path to the Unity XT API skipCertificateValidation: true # indicates if client side validation of (management)server's certificate can be skipped isDefault: true # treat current array as a default (would be used by storage classes without arrayID parameter) Change the parameters with relevant values for your Unity XT array. Add more blocks similar to above for each Unity XT array if necessary.\nUse the following command to create a new secret unity-creds from secret.yaml file.\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nInstall Driver Follow all the prerequisites above\nCreate a CR (Custom Resource) for Unity XT using the sample files provided here. This file can be modified to use custom parameters if needed.\nUsers should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity XT driver and their default values:\nParameter Description Required Default replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be in pending state until new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2 namespace Specifies namespace where the driver will be installed Yes “unity” fsGroupPolicy Defines which FS Group policy mode to be used. Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” Common parameters for node and controller X_CSI_UNITY_ALLOW_MULTI_POD_ACCESS To enable sharing of volumes across multiple pods within the same node in RWO access mode No false X_CSI_UNITY_SYNC_NODEINFO_INTERVAL Time interval to add node info to array. Default 15 minutes. Minimum value should be 1 No 15 CSI_LOG_LEVEL Sets the logging level of the driver true info TENANT_NAME Tenant name added while adding host entry to the array No CERT_SECRET_COUNT Represents the number of certificate secrets, which the user is going to create for SSL authentication. (unity-cert-0..unity-cert-n). The minimum value should be 1. false 1 X_CSI_UNITY_SKIP_CERTIFICATE_VALIDATION Specifies if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface.If it is set to false, then a secret unity-certs has to be created with an X.509 certificate of CA which signed the Unisphere certificate No true Controller parameters X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition No false Node parameters X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition No false Execute the following command to create Unity XT custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e This command will deploy the CSI Unity XT driver in the namespace specified in the input YAML file.\nNext, the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e Verify the CSI Driver installation Note :\n“Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Snapshotter and resizer sidecars are not optional. They are defaults with Driver installation. ","categories":"","description":"Installing Dell CSI Driver for Unity XT via Dell CSM Operator\n","excerpt":"Installing Dell CSI Driver for Unity XT via Dell CSM Operator\n","ref":"/csm-docs/v2/deployment/csmoperator/drivers/unity/","tags":"","title":"Unity XT"},{"body":"The CSI Driver for Dell Unity XT can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\nCSI Driver for Unity XT Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume Kubernetes External Health Monitor, which provides volume health status The node section of the Helm chart installs the following component in a DaemonSet:\nCSI Driver for Unity XT Kubernetes Node Registrar, which handles the driver registration Prerequisites Before you install CSI Driver for Unity XT, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements Install Kubernetes or OpenShift (see supported versions) Install Helm v3 To use FC protocol, the host must be zoned with Unity XT array and Multipath needs to be configured To use iSCSI protocol, iSCSI initiator utils packages needs to be installed and Multipath needs to be configured To use NFS protocol, NFS utility packages needs to be installed Mount propagation is enabled on container runtime that is being used Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell Unity XT.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell Unity XT supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell Unity XT:\nZoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done. Set up the iSCSI Initiator The CSI Driver for Dell Unity XT supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\nEnsure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell Unity XT array that has IP interfaces. Manually create IP routes for each node that connects to the Dell Unity XT. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi. For more information about configuring iSCSI, see Dell Host Connectivity guide.\nLinux multipathing requirements Dell Unity XT supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell Unity XT.\nSet up Linux multipathing as follows:\nEnsure that all nodes have the Device Mapper Multipathing package installed. You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\nEnable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes. As a best practice, use the following options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 Install CSI Driver Install CSI Driver for Unity XT using this procedure.\nBefore you begin\nAs a pre-requisite for running this procedure, you must have the downloaded files, including the Helm chart from the source git repository with the command git clone -b v2.6.0 https://github.com/dell/csi-unity.git. In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. Ensure unity namespace exists in Kubernetes cluster. Use the kubectl create namespace unity command to create the namespace if the namespace is not present. Procedure\nCollect information from the Unity XT Systems like Unique ArrayId, IP address, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml and myvalues.yaml file.\nNote:\nArrayId corresponds to the serial number of Unity XT array. Unity XT Array username must have role as Storage Administrator to be able to perform CRUD operations. If the user is using a complex K8s version like “v1.24.6-mirantis-1”, use this kubeVersion check in helm/csi-unity/Chart.yaml file. kubeVersion: “\u003e= 1.24.0-0 \u003c 1.27.0-0” Copy the helm/csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.\nEdit myvalues.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity XT driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\nParameter Description Required Default version helm version true - logLevel LogLevel is used to set the logging level of the driver true info allowRWOMultiPodAccess Flag to enable multiple pods to use the same PVC on the same node with RWO access mode. false false kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet syncNodeInfoInterval Time interval to add node info to the array. Default 15 minutes. The minimum value should be 1 minute. false 15 maxUnityVolumesPerNode Maximum number of volumes that controller can publish to the node. false 0 certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. (unity-cert-0..unity-cert-n). The minimum value should be 1. false 1 imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent podmon.enabled service to monitor failing jobs and notify false - podmon.image pod man image name false - tenantName Tenant name added while adding host entry to the array No fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType” controller Allows configuration of the controller-specific parameters. - - controllerCount Defines the number of csi-unity controller pods to deploy to the Kubernetes release Yes 2 volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s” snapshot.enabled Enable/Disable volume snapshot feature Yes true snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot” resizer.enabled Enable/Disable volume expansion feature Yes true nodeSelector Define node selection constraints for pods of controller deployment No tolerations Define tolerations for the controller deployment, if required No healthMonitor.enabled Enable/Disable deployment of external health monitor sidecar for controller side volume health monitoring. No false healthMonitor.interval Interval of monitoring volume health condition. Allowed values: Number followed by unit (s,m,h) No 60s node Allows configuration of the node-specific parameters. - - dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition No false nodeSelector Define node selection constraints for pods of node deployment No tolerations Define tolerations for the node deployment, if required No Note:\nUser should provide all boolean values with double-quotes. This applies only for myvalues.yaml. Example: “true”/“false”\ncontrollerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails.\nUser can a create separate StorageClass (with topology-related keys) by referring to existing default storage classes.\nHost IO Limit must have a minimum bandwidth of 1 MBPS to discover the volumes on node successfully.\nUser must not change the value of allowRWOMultiPodAccess to true unless intended to use the feature and is aware of the consequences. Enabling multiple pods to access the same PVC with RWO access mode on the same node might cause data to be overwritten and therefore leading to data loss in some cases.\nExample myvalues.yaml\nlogLevel: \"info\" imagePullPolicy: Always certSecretCount: 1 kubeletConfigDir: /var/lib/kubelet controller: controllerCount: 2 volumeNamePrefix : csivol snapshot: enabled: true snapNamePrefix: csi-snap resizer: enabled: false allowRWOMultiPodAccess: false syncNodeInfoInterval: 5 maxUnityVolumesPerNode: 0 fsGroupPolicy: ReadWriteOneFSType For certificate validation of Unisphere REST API calls refer here. Otherwise, create an empty secret with file csi-unity/samples/secret/emptysecret.yaml file by running the kubectl create -f csi-unity/samples/secret/emptysecret.yaml command.\nPrepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\nParameter Description Required Default storageArrayList.username Username for accessing Unity XT system true - storageArrayList.password Password for accessing Unity XT system true - storageArrayList.endpoint REST API gateway HTTPS endpoint Unity XT system true - storageArrayList.arrayId ArrayID for Unity XT system true - storageArrayList.skipCertificateValidation “skipCertificateValidation \" determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface. If it is set to false, then a secret unity-certs has to be created with an X.509 certificate of CA which signed the Unisphere certificate. true true storageArrayList.isDefault An array having isDefault=true or isDefault=true will be considered as the default array when arrayId is not specified in the storage class. This parameter should occur only once in the list. true - Example: secret.yaml\nstorageArrayList: - arrayId: \"APM00******1\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.1/\" skipCertificateValidation: true isDefault: true - arrayId: \"APM00******2\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.2/\" skipCertificateValidation: true Use the following command to create a new secret unity-creds from secret.yaml file.\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the yaml syntax and array-related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the yaml file.\nAlternatively, users can configure and use secret.yaml for driver configuration. The parameters remain the same as in the above table and below is a sample of secret.yaml. Samples of secret.yaml is available in the directory csi-unity/samples/secret/ .\nExample: secret.yaml\nstorageArrayList: - arrayId: \"APM00******1\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.1/\" skipCertificateValidation: true isDefault: true - arrayId: \"APM00******2\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.2/\" skipCertificateValidation: true ``` **Note:** * Parameters \"allowRWOMultiPodAccess\" and \"syncNodeInfoInterval\" have been enabled for configuration in values.yaml and this helps users to dynamically change these values without the need for driver re-installation. For detailed snapshot setup procedure, click here.\nRun the ./csi-install.sh --namespace unity --values ./myvalues.yaml command to proceed with the installation using bash script.\nA successful installation must display messages that look similar to the following samples:\n------------------------------------------------------ \u003e Installing CSI Driver: csi-unity on 1.25 ------------------------------------------------------ ------------------------------------------------------ \u003e Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u003e Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.25 | |- Driver: csi-unity | |- Verifying Kubernetes version | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that optional secrets have been created Success | |- Verifying alpha snapshot resources | |--\u003e Verifying that alpha snapshot CRDs are not installed Success | |- Verifying sshpass installation.. | |- Verifying iSCSI installation Enter the root password of 10.**.**.**: Enter the root password of 10.**.**.**: Success | |- Verifying snapshot support | |--\u003e Verifying that snapshot CRDs are available Success | |--\u003e Verifying that the snapshot controller is available Success | |- Verifying helm version Success | |- Verifying helm values version Success ------------------------------------------------------ \u003e Verification Complete - Success ------------------------------------------------------ | |- Installing Driver Success | |--\u003e Waiting for Deployment unity-controller to be ready Success | |--\u003e Waiting for DaemonSet unity-node to be ready Success ------------------------------------------------------ \u003e Operation complete ------------------------------------------------------ Results:\nAt the end of the script unity-controller Deployment and DaemonSet unity-node will be ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\nOne or more Unity XT Controllers (based on controllerCount) with 5/5 containers ready, and status displayed as Running. Agent pods with 2/2 containers and the status displayed as Running. Note: To install nightly or latest csi driver build using bash script use this command: /csi-install.sh --namespace unity --values ./myvalues.yaml --version nightly/latest\nYou can also install the driver using standalone helm chart by running helm install command, first using the –dry-run flag to confirm various parameters are as desired. Once the parameters are validated, run the command without the –dry-run flag. Note: This example assumes that the user is at repo root helm folder i.e csi-unity/helm.\nSyntax:helm install --dry-run --values \u003cmyvalues.yaml location\u003e --namespace \u003cnamespace\u003e \u003cname of secret\u003e \u003chelmPath\u003e \u003cnamespace\u003e - namespace of the driver installation. \u003cname of secret\u003e - unity in case of unity-creds and unity-certs-0 secrets. \u003chelmPath\u003e - Path of the helm directory. e.g: helm install –dry-run –values ./csi-unity/myvalues.yaml –namespace unity unity ./csi-unity\nCertificate validation for Unisphere REST API calls This topic provides details about setting up the Dell Unity XT certificate validation for the CSI Driver.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on the “.Values.certSecretCount” parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\nTo fetch the certificate, run the following command. openssl s_client -showcerts -connect \u003cUnisphere IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem\nRun the following command to create the cert secret with index ‘0’: kubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret: kubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -\nRepeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)\nNote:\n“unity” is the namespace for helm-based installation but namespace can be user-defined in operator-based installation.\nUser can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation.\nWhenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\nVolume Snapshot Class A wide set of annotated storage class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nStorage Classes Storage Classes are an essential Kubernetes construct for Storage provisioning. To know more about Storage Classes, refer to https://kubernetes.io/docs/concepts/storage/storage-classes/\nA wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nFor the Unity XT CSI Driver, a wide set of annotated storage class manifests have been provided in the csi-unity/samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class: There are samples storage class yaml files available under csi-unity/samples/storageclass. These can be copied and modified as needed.\nPick any of unity-fc.yaml, unity-iscsi.yaml or unity-nfs.yaml Copy the file as unity-\u003cARRAY_ID\u003e-fc.yaml, unity-\u003cARRAY_ID\u003e-iscsi.yaml or unity-\u003cARRAY_ID\u003e-nfs.yaml Replace \u003cARRAY_ID\u003e with the Array Id of the Unity Array to be used Replace \u003cSTORAGE_POOL\u003e with the storage pool you have Replace \u003cTIERING_POLICY\u003e with the Tiering policy that is to be used for provisioning Replace \u003cHOST_IO_LIMIT_NAME\u003e with the Host IO Limit Name that is to be used for provisioning Replace \u003cmountOption1\u003e with the necessary mount options. If not required, this can be removed from the storage class Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f unity-\u003cARRAY_ID\u003e-fc.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-iscsi.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-nfs.yaml Note:\nAt least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es): Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nDynamically update the unity-creds secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Note: Updating unity-certs-x secrets is a manual process, unlike unity-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nDynamic Logging Configuration Helm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\n","categories":"","description":"Installing CSI Driver for Unity XT via Helm\n","excerpt":"Installing CSI Driver for Unity XT via Helm\n","ref":"/csm-docs/v3/csidriver/installation/helm/unity/","tags":"","title":"Unity XT"},{"body":"CSI Driver for Unity XT Pre-requisites Create secret to store Unity XT credentials Create a namespace called unity (it can be any user-defined name; But commands in this section assumes that the namespace is unity) Prepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\nParameter Description Required Default username Username for accessing Unity XT system true - password Password for accessing Unity XT system true - endpoint REST API gateway HTTPS endpoint Unity XT system true - arrayId ArrayID for Unity XT system true - isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list. true - skipCertificateValidation Determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface true true Ex: secret.yaml\nstorageArrayList: - arrayId: \"APM00******1\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.1/\" skipCertificateValidation: true isDefault: true - arrayId: \"APM00******2\" username: \"user\" password: \"password\" endpoint: \"https://10.1.1.2/\" skipCertificateValidation: true kubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml\nUse the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the YAML syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\nCreate secret for client side TLS verification Please refer detailed documentation on how to create this secret here\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion: v1 kind: Secret metadata: name: unity-certs-0 namespace: unity type: Opaque data: cert-0: \"\" Execute command: kubectl create -f empty-secret.yaml\nModify/Set the following optional environment variables Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\nParameter Description Required Default Common parameters for node and controller CSI_ENDPOINT Specifies the HTTP endpoint for Unity XT. No /var/run/csi/csi.sock X_CSI_UNITY_ALLOW_MULTI_POD_ACCESS Flag to enable multiple pods use same pvc on same node with RWO access mode No false Controller parameters X_CSI_MODE Driver starting mode No controller X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin No Node parameters X_CSI_MODE Driver starting mode No node X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands No /noderoot X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Node plugin No Example CR for Unity XT Refer samples from here. Below is an example CR:\napiVersion: storage.dell.com/v1 kind: CSIUnity metadata: name: test-unity namespace: test-unity spec: driver: configVersion: v2.6.0 replicas: 2 dnsPolicy: ClusterFirstWithHostNet forceUpdate: false common: image: \"dellemc/csi-unity:v2.6.0\" imagePullPolicy: IfNotPresent sideCars: - name: provisioner args: [\"--volume-name-prefix=csiunity\",\"--default-fstype=ext4\"] - name: snapshotter args: [\"--snapshot-name-prefix=csiunitysnap\"] # Enable/Disable health monitor of CSI volumes from node plugin. Provides details of volume usage. # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" # nodeSelector: Define node selection constraints for controller pods. # For the pod to be eligible to run on a node, the node must have each # of the indicated key-value pairs as labels. # Leave as blank to consider all nodes # Allowed values: map of key-value pairs # Default value: None nodeSelector: # Uncomment if nodes you wish to use have the node-role.kubernetes. io/control-plane taint # node-role.kubernetes.io/control-plane: \"\" # tolerations: Define tolerations for the controllers, if required. # Leave as blank to install controller on worker nodes # Default value: None tolerations: # Uncomment if nodes you wish to use have the node-role.kubernetes.io/control-plane taint # - key: \"node-role.kubernetes.io/control-plane\" # operator: \"Exists\" # effect: \"NoSchedule\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" # nodeSelector: Define node selection constraints for node pods. # For the pod to be eligible to run on a node, the node must have each # of the indicated key-value pairs as labels. # Leave as blank to consider all nodes # Allowed values: map of key-value pairs # Default value: None nodeSelector: # Uncomment if nodes you wish to use have the node-role.kubernetes.io/control-plane taint # node-role.kubernetes.io/control-plane: \"\" # tolerations: Define tolerations for the node daemonset, if required. # Default value: None tolerations: # Uncomment if nodes you wish to use have the node-role.kubernetes.io/control-plane taint # - key: \"node-role.kubernetes.io/control-plane\" # operator: \"Exists\" # effect: \"NoSchedule\" # - key: \"node.kubernetes.io/memory-pressure\" # operator: \"Exists\" # effect: \"NoExecute\" # - key: \"node.kubernetes.io/disk-pressure\" # operator: \"Exists\" # effect: \"NoExecute\" # - key: \"node.kubernetes.io/network-unavailable\" # operator: \"Exists\" # effect: \"NoExecute\" --- apiVersion: v1 kind: ConfigMap metadata: name: unity-config-params namespace: test-unity data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"info\" ALLOW_RWO_MULTIPOD_ACCESS: \"false\" MAX_UNITY_VOLUMES_PER_NODE: \"0\" SYNC_NODE_INFO_TIME_INTERVAL: \"15\" TENANT_NAME: \"\" Dynamic Logging Configuration Operator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params Note :\nThe log level is not allowed to be updated dynamically through logLevel attribute in the secret object. “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation. Volume Health Monitoring Operator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_ENABLE_VOL_HEALTH_MONITOR to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin- volume status, volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" ","categories":"","description":"Installing CSI Driver for Unity XT via Operator\n","excerpt":"Installing CSI Driver for Unity XT via Operator\n","ref":"/csm-docs/v3/csidriver/installation/operator/unity/","tags":"","title":"Unity XT"},{"body":"Test deploying a simple Pod and Pvc with Unity XT storage In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes and automatically mounts them to the pod.\nSteps\nTo run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./test/sample.yaml You can find all the created resources in test-unity namespace.\nCheck if the pod is created and Ready and Running by running:\nkubectl get all -n test-unity If it is in CrashLoopback state then the driver installation was not successful. Check the logs of the node and the controller.\nGo into the created container and verify that everything is mounted correctly.\nAfter verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./test/sample.yaml Support for SLES 15 The CSI Driver for Dell Unity XT requires these of packages installed on all worker nodes that run on SLES 15.\nopen-iscsi open-iscsi is required in order to make use of iSCSI protocol for provisioning nfs-utils nfs-utils is required in order to make use of NFS protocol for provisioning multipath-tools multipath-tools is required in order to make use of FC and iSCSI protocols for provisioning After installing open-iscsi, ensure “iscsi” and “iscsid” services have been started and /etc/isci/initiatorname.iscsi is created and has the host initiator id. The pre-requisites are mandatory for provisioning with the iSCSI protocol to work.\n","categories":"","description":"Tests to validate Unity XT CSI Driver installation","excerpt":"Tests to validate Unity XT CSI Driver installation","ref":"/csm-docs/v3/csidriver/installation/test/unity/","tags":"","title":"Test Unity XT CSI Driver"},{"body":"Release Notes - CSI Unity XT v2.6.0 New Features/Changes Added support to Kubernetes 1.26 Added support for MKE 3.6 Added support for RKE 1.4.1 Added support for SLES SP4 Fixed Issues PVC fails to resize with message “Invalid value 0; must be greater than zero” Known Issues Issue Workaround Topology-related node labels are not removed automatically. Currently, when the driver is uninstalled, topology-related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released, remove the labels manually after the driver un-installation using command kubectl label node \u003cnode_name\u003e - - … Example: kubectl label node csi-unity.dellemc.com/array123-iscsi- Note: there must be - at the end of each label to remove it. NFS Clone - Resize of the snapshot is not supported by Unity XT Platform, however the user should never try to resize the cloned NFS volume. Currently, when the driver takes a clone of NFS volume, it succeeds but if the user tries to resize the NFS volumesnapshot, the driver will throw an error. Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100 When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the VolumeAttachment to the node that went down. Now the volume can be attached to the new node. Note: Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. ","categories":"","description":"Release notes for Unity XT CSI driver","excerpt":"Release notes for Unity XT CSI driver","ref":"/csm-docs/v3/csidriver/release/unity/","tags":"","title":"Unity XT"},{"body":" Symptoms Prevention, Resolution or Workaround When you run the command kubectl describe pods unity-controller-\u003csuffix\u003e –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry The kubectl logs -n unity unity-node-\u003csuffix\u003e driver logs show that the driver can’t connect to Unity XT - Authentication failure. Check if you have created a secret with correct credentials fsGroup specified in pod spec is not reflected in files or directories at mounted path of volume. fsType of PVC must be set for fsGroup to work. fsType can be specified while creating a storage class. For NFS protocol, fsType can be specified as nfs. fsGroup doesn’t work for ephemeral inline volumes. Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver controller and node pod should be restarted with command kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}’| xargs kubectl delete -n unity pod when topology-based storage classes are used. For dynamic array addition without topology, the driver will detect the newly added or removed arrays automatically If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in the cluster but on array, it will still be present and marked for deletion. All the cloned PVC should be deleted in order to delete the source PVC from the array. PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass “unity-iscsi”: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}’| xargs kubectl delete -n unity pod Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.24.0 \u003c 1.27.0 which is incompatible with Kubernetes 1.24.6-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart at helm/csi-unity/Chart.yaml and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported. When a node goes down, the block volumes attached to the node cannot be attached to another node 1. Force delete the pod running on the node that went down 2. Delete the VolumeAttachment to the node that went down. Now the volume can be attached to the new node. ","categories":"","description":"Troubleshooting Unity XT Driver","excerpt":"Troubleshooting Unity XT Driver","ref":"/csm-docs/v3/csidriver/troubleshooting/unity/","tags":"","title":"Unity XT"},{"body":"The CSI Driver for Dell Unity XT, PowerScale and PowerStore supports VMware Tanzu. The deployment of these Tanzu clusters is done using the VMware Tanzu supervisor cluster and the supervisor namespace.\nCurrently, VMware Tanzu 7.0 with normal configuration(without NAT) supports Kubernetes 1.22. The CSI driver can be installed on this cluster using Helm. Installation of CSI drivers in Tanzu via Operator has not been qualified.\nTo login to the Tanzu cluster, download kubectl and kubectl vsphere binaries to any of the system\nRefer: https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-0F6E45C4-3CB1-4562-9370-686668519FCA.html\nConnect to the VCenter using kubectl vSphere commands as shown below.\nkubectl vsphere login --insecure-skip-tls-verify --vsphere-username vSphere username --server=https://\u003ctanzu-server-ip\u003e/ -v 5 Once login is done to the Tanzu cluster, the installation of CSI driver is done using kubectl binary similar to how we do for other systems.\nTanzu example ","categories":"","description":"About VMware Tanzu basic","excerpt":"About VMware Tanzu basic","ref":"/csm-docs/v1/csidriver/partners/tanzu/","tags":"","title":"VMware Tanzu"},{"body":"The CSI Driver for Dell Unity XT, PowerScale and PowerStore supports VMware Tanzu. The deployment of these Tanzu clusters is done using the VMware Tanzu supervisor cluster and the supervisor namespace.\nCurrently, VMware Tanzu 7.0 with normal configuration(without NAT) supports Kubernetes 1.22. The CSI driver can be installed on this cluster using Helm. Installation of CSI drivers in Tanzu via Operator has not been qualified.\nTo login to the Tanzu cluster, download kubectl and kubectl vsphere binaries to any of the system\nRefer: https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-0F6E45C4-3CB1-4562-9370-686668519FCA.html\nConnect to the VCenter using kubectl vSphere commands as shown below.\nkubectl vsphere login --insecure-skip-tls-verify --vsphere-username vSphere username --server=https://\u003ctanzu-server-ip\u003e/ -v 5 Once login is done to the Tanzu cluster, the installation of CSI driver is done using kubectl binary similar to how we do for other systems.\nTanzu example ","categories":"","description":"About VMware Tanzu basic","excerpt":"About VMware Tanzu basic","ref":"/csm-docs/v2/csidriver/partners/tanzu/","tags":"","title":"VMware Tanzu"},{"body":"The CSI Driver for Dell Unity XT, PowerScale and PowerStore supports VMware Tanzu. The deployment of these Tanzu clusters is done using the VMware Tanzu supervisor cluster and the supervisor namespace.\nCurrently, VMware Tanzu 7.0 with normal configuration(without NAT) supports Kubernetes 1.22. The CSI driver can be installed on this cluster using Helm. Installation of CSI drivers in Tanzu via Operator has not been qualified.\nTo login to the Tanzu cluster, download kubectl and kubectl vsphere binaries to any of the system\nRefer: https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-0F6E45C4-3CB1-4562-9370-686668519FCA.html\nConnect to the VCenter using kubectl vSphere commands as shown below.\nkubectl vsphere login --insecure-skip-tls-verify --vsphere-username vSphere username --server=https://\u003ctanzu-server-ip\u003e/ -v 5 Once login is done to the Tanzu cluster, the installation of CSI driver is done using kubectl binary similar to how we do for other systems.\nTanzu example ","categories":"","description":"About VMware Tanzu basic","excerpt":"About VMware Tanzu basic","ref":"/csm-docs/v3/csidriver/partners/tanzu/","tags":"","title":"VMware Tanzu"}]