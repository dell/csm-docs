<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dell Technologies – Troubleshooting</title>
    <link>https://dell.github.io/csm-docs/v3/csidriver/troubleshooting/</link>
    <description>Recent content in Troubleshooting on Dell Technologies</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    
	  <atom:link href="https://dell.github.io/csm-docs/v3/csidriver/troubleshooting/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>V3: Dell CSI Operator</title>
      <link>https://dell.github.io/csm-docs/v3/csidriver/troubleshooting/operator/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/csm-docs/v3/csidriver/troubleshooting/operator/</guid>
      <description>
        
        
        &lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Before installing the drivers, Dell CSI Operator tries to validate the Custom Resource being created. If some mandatory environment variables are missing or there is a type mismatch, then the Operator will report an error during the reconciliation attempts.&lt;/p&gt;
&lt;p&gt;Because of this, the status of the Custom Resource will change to &amp;ldquo;Failed&amp;rdquo; and the error captured in the &amp;ldquo;ErrorMessage&amp;rdquo; field in the status.&lt;/p&gt;
&lt;p&gt;For example - If the PowerMax driver was installed in the namespace test-powermax and has the name powermax, then run the command &lt;code&gt;kubectl get csipowermax/powermax -n test-powermax -o yaml&lt;/code&gt; to get the Custom Resource details.&lt;/p&gt;
&lt;p&gt;If there was an error while installing the driver, then you would see a status like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;status&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;status&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;errorMessage&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;mandatory Env - X_CSI_K8S_CLUSTER_PREFIX not specified in user spec&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;state&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;Failed&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The state of the Custom Resource can also change to &lt;code&gt;Failed&lt;/code&gt; because of any other prohibited updates or any failure while installing the driver. In order to recover from this failure, fix the error in the manifest and update/patch the Custom Resource&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After an update to the driver, the controller pod may not have the latest desired specification.&lt;/p&gt;
&lt;p&gt;This happens when the controller pod was in a failed state before applying the update. Even though the Dell CSI Operator updates the pod template specification for the StatefulSet, the StatefulSet controller does not apply the update to the pod. This happens because of the unique nature of StatefulSets where the controller tries to retain the last known working state.&lt;/p&gt;
&lt;p&gt;To get around this problem, the Dell CSI Operator forces an update of the pod specification by deleting the older pod. In case the Dell CSI Operator fails to do so, delete the controller pod to force an update of the controller pod specification&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Status of the CSI Driver Custom Resource shows the state of the driver pods after installation. This state will not be updated automatically if there are any changes to the driver pods outside any Operator operations.&lt;/p&gt;
&lt;p&gt;At times because of inconsistencies in fetching data from the Kubernetes cache, the state of some driver pods may not be updated correctly in the status. To force an update of the state, you can update the Custom Resource forcefully by setting forceUpdate to true. If all the driver pods are in the &lt;code&gt;Available&lt;/code&gt; State, then the state of the Custom Resource will be updated as &lt;code&gt;Running&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>V3: PowerFlex</title>
      <link>https://dell.github.io/csm-docs/v3/csidriver/troubleshooting/powerflex/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/csm-docs/v3/csidriver/troubleshooting/powerflex/</guid>
      <description>
        
        
        &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Symptoms&lt;/th&gt;
&lt;th&gt;Prevention, Resolution or Workaround&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;The installation fails with the following error message: &lt;br /&gt;&lt;code&gt;Node xxx does not have the SDC installed&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that need to pull an image of the driver.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;When you run the command &lt;code&gt;kubectl describe pods vxflexos-controller-* –n vxflexos&lt;/code&gt;, the system indicates that the driver image could not be loaded.&lt;/td&gt;
&lt;td&gt;- If on Kubernetes, edit the &lt;code&gt;daemon.json&lt;/code&gt; file found in the registry location and add &lt;br /&gt;&lt;code&gt;{ &amp;quot;insecure-registries&amp;quot; :[ &amp;quot;hostname.cloudapp.net:5000&amp;quot; ] }&lt;/code&gt;&lt;br /&gt;- If on OpenShift, run the command &lt;code&gt;oc edit image.config.openshift.io/cluster&lt;/code&gt; and add registries to yaml file that is displayed when you run the command.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs -n vxflexos vxflexos-controller-* driver&lt;/code&gt; logs show that the driver is not authenticated.&lt;/td&gt;
&lt;td&gt;Check the username, password, and the gateway IP address for the PowerFlex system.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs vxflexos-controller-* -n vxflexos driver&lt;/code&gt; logs show that the system ID is incorrect.&lt;/td&gt;
&lt;td&gt;Use the &lt;code&gt;get_vxflexos_info.sh&lt;/code&gt; to find the correct system ID.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs vxflexos-controller-* -n vxflexos driver&lt;/code&gt; logs show that the system ID is incorrect.&lt;/td&gt;
&lt;td&gt;Use the &lt;code&gt;get_vxflexos_info.sh&lt;/code&gt; to find the correct system ID. Add the system ID to &lt;code&gt;myvalues.yaml&lt;/code&gt; script.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CreateVolume error System &lt;Name&gt; is not configured in the driver&lt;/td&gt;
&lt;td&gt;Powerflex name if used for systemID in StorageClass ensure same name is also used in array config systemID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Defcontext mount option seems to be ignored, volumes still are not being labeled correctly.&lt;/td&gt;
&lt;td&gt;Ensure SElinux is enabled on a worker node, and ensure your container run time manager is properly configured to be utilized with SElinux.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mount options that interact with SElinux are not working (like defcontext).&lt;/td&gt;
&lt;td&gt;Check that your container orchestrator is properly configured to work with SElinux.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Installation of the driver on Kubernetes v1.24/v1.25/v1.26 fails with the following error: &lt;br /&gt;&lt;code&gt;Error: unable to build kubernetes objects from release manifest: unable to recognize &amp;quot;&amp;quot;: no matches for kind &amp;quot;VolumeSnapshotClass&amp;quot; in version &amp;quot;snapshot.storage.k8s.io/v1&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes v1.23/v1.24/v1.25 requires v1 version of snapshot CRDs to be created in cluster, see the &lt;a href=&#34;../../installation/helm/powerflex/#optional-volume-snapshot-requirements&#34;&gt;Volume Snapshot Requirements&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs -n vxflexos vxflexos-controller-* driver&lt;/code&gt; logs show &lt;code&gt;x509: certificate signed by unknown authority&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A self assigned certificate is used for PowerFlex array. See &lt;a href=&#34;../../installation/helm/powerflex/#certificate-validation-for-powerflex-gateway-rest-api-calls&#34;&gt;certificate validation for PowerFlex Gateway&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;When you run the command &lt;code&gt;kubectl apply -f snapclass-v1.yaml&lt;/code&gt;, you get the error &lt;code&gt;error: unable to recognize &amp;quot;snapclass-v1.yaml&amp;quot;: no matches for kind &amp;quot;VolumeSnapshotClass&amp;quot; in version &amp;quot;snapshot.storage.k8s.io/v1&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Check to make sure that the v1 snapshotter CRDs are installed, and not the v1beta1 CRDs, which are no longer supported.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The controller pod is stuck and producing errors such as&amp;quot; &lt;code&gt;Failed to watch *v1.VolumeSnapshotContent: failed to list *v1.VolumeSnapshotContent: the server could not find the requested resource (get volumesnapshotcontents.snapshot.storage.k8s.io)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Make sure that v1 snapshotter CRDs and v1 snapclass are installed, and not v1beta1, which is no longer supported.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: &lt;code&gt;Error: UPGRADE FAILED: chart requires kubeVersion: &amp;gt;= 1.21.0 &amp;lt;= 1.26.0 which is incompatible with Kubernetes V1.21.11-mirantis-1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If you are using an extended Kubernetes version, please see the helm Chart at &lt;code&gt;helm/csi-vxflexos/Chart.yaml&lt;/code&gt; and use the alternate &lt;code&gt;kubeVersion&lt;/code&gt; check that is provided in the comments. &lt;em&gt;Please note&lt;/em&gt; that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Volume metrics are missing&lt;/td&gt;
&lt;td&gt;Enable &lt;a href=&#34;../../features/powerflex#volume-health-monitoring&#34;&gt;Volume Health Monitoring&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;When a node goes down, the block volumes attached to the node cannot be attached to another node&lt;/td&gt;
&lt;td&gt;This is a known issue and has been reported at &lt;a href=&#34;https://github.com/kubernetes-csi/external-attacher/issues/215&#34;&gt;https://github.com/kubernetes-csi/external-attacher/issues/215&lt;/a&gt;. Workaround: &lt;br /&gt; 1. Force delete the pod running on the node that went down &lt;br /&gt; 2. Delete the volumeattachment to the node that went down. &lt;br /&gt; Now the volume can be attached to the new node.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CSI-PowerFlex volumes cannot mount; are being recognized as multipath devices&lt;/td&gt;
&lt;td&gt;CSI-PowerFlex does not support multipath; to fix: &lt;br/&gt; 1. Remove any multipath mapping involving a powerflex volume with &lt;code&gt;multipath -f &amp;lt;powerflex volume&amp;gt;&lt;/code&gt; &lt;br/&gt; 2. Blacklist CSI-PowerFlex volumes in multipath config file&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;When attempting a driver upgrade, you see: &lt;code&gt;spec.fsGroupPolicy: Invalid value: &amp;quot;xxx&amp;quot;: field is immutable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;You cannot upgrade between drivers with different fsGroupPolicies. See &lt;a href=&#34;../../upgradation/drivers/powerflex&#34;&gt;upgrade documentation&lt;/a&gt; for more details&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: &lt;code&gt;vxflexos-controller-*&lt;/code&gt; is the controller pod that acquires leader lease&lt;/p&gt;
&lt;/blockquote&gt;

      </description>
    </item>
    
    <item>
      <title>V3: PowerMax</title>
      <link>https://dell.github.io/csm-docs/v3/csidriver/troubleshooting/powermax/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/csm-docs/v3/csidriver/troubleshooting/powermax/</guid>
      <description>
        
        
        &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Symptoms&lt;/th&gt;
&lt;th&gt;Prevention, Resolution or Workaround&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubectl describe pod powermax-controller-&amp;lt;xyz&amp;gt; –n &amp;lt;namespace&amp;gt;&lt;/code&gt; indicates that the driver image could not be loaded&lt;/td&gt;
&lt;td&gt;You may need to put an insecure-registries entry in &lt;code&gt;/etc/docker/daemon.json&lt;/code&gt; or log in to the docker registry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubectl logs powermax-controller-&amp;lt;xyz&amp;gt; –n &amp;lt;namespace&amp;gt; driver&lt;/code&gt; logs show that the driver cannot authenticate&lt;/td&gt;
&lt;td&gt;Check your secret’s username and password&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubectl logs powermax-controller-&amp;lt;xyz&amp;gt; –n &amp;lt;namespace&amp;gt; driver&lt;/code&gt; logs show that the driver failed to connect to the U4P because it could not verify the certificates&lt;/td&gt;
&lt;td&gt;Check the powermax-certs secret and ensure it is not empty or it has the valid certificates&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: &amp;gt;= 1.22.0 &amp;lt; 1.25.0 which is incompatible with Kubernetes V1.22.11-mirantis-1&lt;/td&gt;
&lt;td&gt;If you are using an extended Kubernetes version, please see the &lt;a href=&#34;https://github.com/dell/helm-charts/blob/main/charts/csi-powermax/Chart.yaml&#34;&gt;helm Chart&lt;/a&gt; and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;When a node goes down, the block volumes attached to the node cannot be attached to another node&lt;/td&gt;
&lt;td&gt;1. Force delete the pod running on the node that went down &lt;br /&gt; 2. Delete the volumeattachment to the node that went down. &lt;br /&gt; Now the volume can be attached to the new node.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;When attempting a driver upgrade, you see: &lt;code&gt;spec.fsGroupPolicy: Invalid value: &amp;quot;xxx&amp;quot;: field is immutable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;You cannot upgrade between drivers with different fsGroupPolicies. See &lt;a href=&#34;../../upgradation/drivers/powermax&#34;&gt;upgrade documentation&lt;/a&gt; for more details&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ater the migration group is in “migrated” state but unable to move to “commit ready” state because the new paths are not being discovered on the cluster nodes.&lt;/td&gt;
&lt;td&gt;Run the following commands manually on the cluster nodes &lt;code&gt;rescan-scsi-bus.sh  -i&lt;/code&gt;  &lt;code&gt;rescan-scsi-bus.sh  -a&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;Failed to fetch details for array: 000000000000. [Unauthorized]&lt;/code&gt;&amp;quot;&lt;/td&gt;
&lt;td&gt;Please make sure that correct encrypted username and password in secret files are used, also ensure whether the RBAC is enabled for the user&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;Error looking up volume for idempotence check: Not Found&lt;/code&gt; or &lt;code&gt;Get Volume step fails for: (000000000000) symID with error (Invalid Response from API)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Make sure that Unisphere endpoint doesn&amp;rsquo;t end with front slash&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;FailedPrecondition desc = no topology keys could be generate&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Make sure that FC or iSCSI connectivity to the arrays are proper&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CreateHost failed with error &lt;code&gt;initiator is already part of different host.&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Update modifyHostName to true in values.yaml Or Remove the initiator from existing host &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubectl logs powermax-controller-&amp;lt;xyz&amp;gt; –n &amp;lt;namespace&amp;gt;&lt;/code&gt; driver logs says connection refused and the reverseproxy logs says &amp;ldquo;Failed to setup server.(secrets &amp;quot;secret-name&amp;quot; not found)&amp;rdquo;&lt;/td&gt;
&lt;td&gt;Make sure the given secret &lt;secret-name&gt; exist on the cluster &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nodestage is failing with error &lt;code&gt;Error invalid IQN Target iqn.EMC.0648.SE1F&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;1. Update initiator name to full default name , ex: iqn.1993-08.org.debian:01:e9afae962192 &lt;br&gt; 2.Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed and it should be full default name.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Volume mount is failing on few OS(ex:VMware Virtual Platform) during node publish with error &lt;code&gt;wrong fs type, bad option, bad superblock&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;1. Check the multipath configuration(if enabled) 2. Edit Vm Advanced settings-&amp;gt;hardware and add the param &lt;code&gt;disk.enableUUID=true&lt;/code&gt; and reboot the node&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>V3: PowerScale</title>
      <link>https://dell.github.io/csm-docs/v3/csidriver/troubleshooting/powerscale/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/csm-docs/v3/csidriver/troubleshooting/powerscale/</guid>
      <description>
        
        
        &lt;p&gt;Here are some installation failures that might be encountered and how to mitigate them.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Symptoms&lt;/th&gt;
&lt;th&gt;Prevention, Resolution or Workaround&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs isilon-controller-0 -n isilon -c driver&lt;/code&gt; logs shows the driver &lt;strong&gt;cannot authenticate&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Check your secret&amp;rsquo;s username and password for corresponding cluster&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs isilon-controller-0 -n isilon -c driver&lt;/code&gt; logs shows the driver failed to connect to the Isilon because it &lt;strong&gt;couldn&amp;rsquo;t verify the certificates&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Check the isilon-certs-&lt;n&gt; secret and ensure it is not empty and it has the valid certificates. Set &lt;code&gt;isiInsecure: &amp;quot;true&amp;quot;&lt;/code&gt; for insecure connection. SSL validation is recommended in the production environment.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs isilon-controller-0 -n isilon -c driver&lt;/code&gt; logs shows the driver error: &lt;strong&gt;create volume failed, Access denied. create directory as requested&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;This situation can happen when the user who created the base path is different from the user configured for the driver. Make sure the user used to deploy CSI-Driver must have enough rights on the base path (i.e. isiPath) to perform all operations.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Volume/filesystem is allowed to mount by any host in the network, though that host is not a part of the export of that particular volume under /ifs directory&lt;/td&gt;
&lt;td&gt;&amp;ldquo;Dell PowerScale: OneFS NFS Design Considerations and Best Practices&amp;rdquo;: &lt;br&gt; There is a default shared directory (ifs) of OneFS, which lets clients running Windows, UNIX, Linux, or Mac OS X access the same directories and files. It is recommended to disable the ifs shared directory in a production environment and create dedicated NFS exports and SMB shares for your workload.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class is not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency.&lt;/td&gt;
&lt;td&gt;Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control.&lt;/td&gt;
&lt;td&gt;To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:&lt;br&gt; * ISI_PRIV_LOGIN_PAPI&lt;br&gt; * ISI_PRIV_NFS&lt;br&gt; * ISI_PRIV_QUOTA&lt;br&gt; * ISI_PRIV_SNAPSHOT&lt;br&gt; * ISI_PRIV_IFS_RESTORE&lt;br&gt; * ISI_PRIV_NS_IFS_ACCESS&lt;br&gt; In some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;If the hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to driver version 1.4.0 or later there is a possibility of &amp;ldquo;localhost&amp;rdquo; as a stale entry in export&lt;/td&gt;
&lt;td&gt;Recommended setup: User should not map a hostname to loopback IP in /etc/hosts file&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Driver node pod is in &amp;ldquo;CrashLoopBackOff&amp;rdquo; as &amp;ldquo;Node ID&amp;rdquo; generated is not with proper FQDN.&lt;/td&gt;
&lt;td&gt;This might be due to &amp;ldquo;dnsPolicy&amp;rdquo; implemented on the driver node pod which may differ with different networks. &lt;br&gt;&lt;br&gt; This parameter is configurable in both helm and Operator installer and the user can try with different &amp;ldquo;dnsPolicy&amp;rdquo; according to the environment.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs isilon-controller-0 -n isilon -c driver&lt;/code&gt; logs shows the driver &lt;strong&gt;Authentication failed. Trying to re-authenticate&lt;/strong&gt; when using Session-based authentication&lt;/td&gt;
&lt;td&gt;The issue has been resolved from OneFS 9.3 onwards, for OneFS versions prior to 9.3 for session-based authentication either smart connect can be created against a single node of Isilon or CSI Driver can be installed/pointed to a particular node of the Isilon else basic authentication can be used by setting isiAuthType in &lt;code&gt;values.yaml&lt;/code&gt; to 0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;When an attempt is made to create more than one ReadOnly PVC from the same volume snapshot, the second and subsequent requests result in PVCs in state &lt;code&gt;Pending&lt;/code&gt;, with a warning &lt;code&gt;another RO volume from this snapshot is already present&lt;/code&gt;. This is because the driver allows only one RO volume from a specific snapshot at any point in time. This is to allow faster creation(within a few seconds) of a RO PVC from a volume snapshot irrespective of the size of the volume snapshot.&lt;/td&gt;
&lt;td&gt;Wait for the deletion of the first RO PVC created from the same volume snapshot.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;While attaching a ReadOnly PVC from a volume snapshot to a pod, the mount operation will fail with error &lt;code&gt;mounting ... failed, reason given by server: No such file or directory&lt;/code&gt;, if RO volume&amp;rsquo;s access zone(non System access zone) on Isilon is configured with a dedicated service IP(which is same as &lt;code&gt;AzServiceIP&lt;/code&gt; storage class parameter). This operation results in accessing the snapshot base directory(&lt;code&gt;/ifs&lt;/code&gt;) and results in overstepping the RO volume&amp;rsquo;s access zone&amp;rsquo;s base directory, which the OneFS doesn&amp;rsquo;t allow.&lt;/td&gt;
&lt;td&gt;Provide a service ip that belongs to RO volume&amp;rsquo;s access zone which set the highest level &lt;code&gt;/ifs&lt;/code&gt; as its zone base directory.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: &amp;gt;= 1.22.0 &amp;lt; 1.25.0 which is incompatible with Kubernetes V1.22.11-mirantis-1&lt;/td&gt;
&lt;td&gt;If you are using an extended Kubernetes version, please see the &lt;a href=&#34;https://github.com/dell/helm-charts/blob/main/charts/csi-isilon/Chart.yaml&#34;&gt;helm Chart&lt;/a&gt; and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>V3: PowerStore</title>
      <link>https://dell.github.io/csm-docs/v3/csidriver/troubleshooting/powerstore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/csm-docs/v3/csidriver/troubleshooting/powerstore/</guid>
      <description>
        
        
        &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Symptoms&lt;/th&gt;
&lt;th&gt;Prevention, Resolution or Workaround&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;When you run the command &lt;code&gt;kubectl describe pods powerstore-controller-&amp;lt;suffix&amp;gt; –n csi-powerstore&lt;/code&gt;, the system indicates that the driver image could not be loaded.&lt;/td&gt;
&lt;td&gt;- If on Kubernetes, edit the daemon.json file found in the registry location and add &lt;code&gt;{ &amp;quot;insecure-registries&amp;quot; :[ &amp;quot;hostname.cloudapp.net:5000&amp;quot; ] }&lt;/code&gt; &lt;br&gt; - If on OpenShift, run the command &lt;code&gt;oc edit image.config.openshift.io/cluster&lt;/code&gt; and add registries to yaml file that is displayed when you run the command.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs -n csi-powerstore powerstore-node-&amp;lt;suffix&amp;gt;&lt;/code&gt; driver logs show that the driver can&amp;rsquo;t connect to PowerStore API.&lt;/td&gt;
&lt;td&gt;Check if you&amp;rsquo;ve created a secret with correct credentials&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Installation of the driver on Kubernetes supported versions fails with the following error: &lt;br /&gt;&lt;code&gt;Error: unable to build kubernetes objects from release manifest: unable to recognize &amp;quot;&amp;quot;: no matches for kind &amp;quot;VolumeSnapshotClass&amp;quot; in version &amp;quot;snapshot.storage.k8s.io/v1&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes v1.21/v1.22/v1.23 requires v1 version of snapshot CRDs to be created in cluster, see the &lt;a href=&#34;../../installation/helm/powerstore/#optional-volume-snapshot-requirements&#34;&gt;Volume Snapshot Requirements&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;If PVC is not getting created and getting the following error in PVC description: &lt;br /&gt;&lt;code&gt;failed to provision volume with StorageClass &amp;quot;powerstore-iscsi&amp;quot;: rpc error: code = Internal desc = : Unknown error:&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Check if you&amp;rsquo;ve created a secret with correct credentials&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;If the NVMeFC pod is not getting created and the host looses the ssh connection, causing the driver pods to go to error state&lt;/td&gt;
&lt;td&gt;remove the nvme_tcp module from the host incase of NVMeFC connection&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;When a node goes down, the block volumes attached to the node cannot be attached to another node&lt;/td&gt;
&lt;td&gt;1. Force delete the pod running on the node that went down &lt;br /&gt; 2. Delete the volumeattachment to the node that went down. &lt;br /&gt; Now the volume can be attached to the new node.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;If the pod creation for NVMe takes time when the connections between the host and the array are more than 2 and considerable volumes are mounted on the host&lt;/td&gt;
&lt;td&gt;Reduce the number of connections between the host and the array to 2.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: &amp;gt;= 1.22.0 &amp;lt; 1.25.0 which is incompatible with Kubernetes V1.22.11-mirantis-1&lt;/td&gt;
&lt;td&gt;If you are using an extended Kubernetes version, please see the &lt;a href=&#34;https://github.com/dell/helm-charts/blob/main/charts/csi-powerstore/Chart.yaml&#34;&gt;helm Chart&lt;/a&gt; and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>V3: Unity XT</title>
      <link>https://dell.github.io/csm-docs/v3/csidriver/troubleshooting/unity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/csm-docs/v3/csidriver/troubleshooting/unity/</guid>
      <description>
        
        
        &lt;hr&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Symptoms&lt;/th&gt;
&lt;th&gt;Prevention, Resolution or Workaround&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;When you run the command &lt;code&gt;kubectl describe pods unity-controller-&amp;lt;suffix&amp;gt; –n unity&lt;/code&gt;, the system indicates that the driver image could not be loaded.&lt;/td&gt;
&lt;td&gt;You may need to put an insecure-registries entry in &lt;code&gt;/etc/docker/daemon.json&lt;/code&gt; or login to the docker registry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs -n unity unity-node-&amp;lt;suffix&amp;gt;&lt;/code&gt; driver logs show that the driver can&amp;rsquo;t connect to Unity XT - Authentication failure.&lt;/td&gt;
&lt;td&gt;Check if you have created a secret with correct credentials&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fsGroup&lt;/code&gt; specified in pod spec is not reflected in files or directories at mounted path of volume.&lt;/td&gt;
&lt;td&gt;fsType of PVC must be set for fsGroup to work. fsType can be specified while creating a storage class. For NFS protocol, fsType can be specified as &lt;code&gt;nfs&lt;/code&gt;. fsGroup doesn&amp;rsquo;t work for ephemeral inline volumes.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dynamic array detection will not work in Topology based environment&lt;/td&gt;
&lt;td&gt;Whenever a new array is added or removed, then the driver controller and node pod should be restarted with command &lt;strong&gt;kubectl get pods -n unity &amp;ndash;no-headers=true | awk &amp;lsquo;/unity-/{print $1}&amp;rsquo;| xargs kubectl delete -n unity pod&lt;/strong&gt; when &lt;strong&gt;topology-based storage classes are used&lt;/strong&gt;. For dynamic array addition without topology, the driver will detect the newly added or removed arrays automatically&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in the cluster but on array, it will still be present and marked for deletion.&lt;/td&gt;
&lt;td&gt;All the cloned PVC should be deleted in order to delete the source PVC from the array.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PVC creation fails on a fresh cluster with &lt;strong&gt;iSCSI&lt;/strong&gt; and &lt;strong&gt;NFS&lt;/strong&gt; protocols alone enabled with error &lt;strong&gt;failed to provision volume with StorageClass &amp;ldquo;unity-iscsi&amp;rdquo;: error generating accessibility requirements: no available topology found&lt;/strong&gt;.&lt;/td&gt;
&lt;td&gt;This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with &lt;strong&gt;kubectl get pods -n unity &amp;ndash;no-headers=true | awk &amp;lsquo;/unity-/{print $1}&amp;rsquo;| xargs kubectl delete -n unity pod&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: &lt;code&gt;Error: UPGRADE FAILED: chart requires kubeVersion: &amp;gt;= 1.24.0 &amp;lt; 1.27.0 which is incompatible with Kubernetes 1.24.6-mirantis-1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If you are using an extended Kubernetes version, please see the helm Chart at &lt;code&gt;helm/csi-unity/Chart.yaml&lt;/code&gt; and use the alternate &lt;code&gt;kubeVersion&lt;/code&gt; check that is provided in the comments. &lt;em&gt;Please note&lt;/em&gt; that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;When a node goes down, the block volumes attached to the node cannot be attached to another node&lt;/td&gt;
&lt;td&gt;1. Force delete the pod running on the node that went down &lt;br /&gt; 2. Delete the VolumeAttachment to the node that went down. &lt;br /&gt; Now the volume can be attached to the new node.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
  </channel>
</rss>
