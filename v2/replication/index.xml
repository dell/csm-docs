<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Replication on </title>
    <link>https://dell.github.io/csm-docs/v2/replication/</link>
    <description>Recent content in Replication on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <atom:link href="https://dell.github.io/csm-docs/v2/replication/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Cluster Topologies</title>
      <link>https://dell.github.io/csm-docs/v2/replication/cluster-topologies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dell.github.io/csm-docs/v2/replication/cluster-topologies/</guid>
      <description>Replication Cluster Topologies Container Storage Modules (CSM) for Replication project supports the replication of volumes within a single Kubernetes cluster or between two different Kubernetes clusters. The replication controller can support multiple clusters at once, but a single volume can be replicated to a maximum of two clusters.
Each cluster should be assigned the unique identifier clusterId. The rules for naming are as follows:
must be 63 characters or fewer (cannot be empty) must begin and end with an alphanumeric character ([a-z, 0-9, A-Z]) could contain dashes (-), underscores (_), dots (.</description>
    </item>
    <item>
      <title>Disaster Recovery</title>
      <link>https://dell.github.io/csm-docs/v2/replication/disaster-recovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dell.github.io/csm-docs/v2/replication/disaster-recovery/</guid>
      <description>Disaster Recovery Workflows Once the DellCSIReplicationGroup &amp;amp; PersistentVolume objects have been replicated across clusters (or within the same cluster), users can exercise the general Disaster Recovery workflows.
Planned Migration to the target cluster/array This scenario is the typical choice when you want to try your disaster recovery plan or you need to switch activities from one site to another:
a. Execute &amp;ldquo;failover&amp;rdquo; action on selected ReplicationGroup using the cluster name</description>
    </item>
    <item>
      <title>High Availability</title>
      <link>https://dell.github.io/csm-docs/v2/replication/high-availability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dell.github.io/csm-docs/v2/replication/high-availability/</guid>
      <description>One of the goals of high availability is to eliminate single points of failure in a storage system. In Kubernetes, this can mean that a single PV represents multiple read/write enabled volumes on different arrays, located at reasonable distances with both the volumes in sync with each other. If one of the volumes goes down, there will still be another volume available for read and write. This kind of high availability can be achieved by using SRDF Metro replication mode, supported only by PowerMax arrays.</description>
    </item>
    <item>
      <title>Monitoring</title>
      <link>https://dell.github.io/csm-docs/v2/replication/monitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dell.github.io/csm-docs/v2/replication/monitoring/</guid>
      <description>The dell-csm-replicator supports monitoring of DellCSIReplicationGroup Custom Resources (CRs).
Each RG is polled at a pre-defined interval and for each RG, a gRPC call is made to the driver which returns the status of the protection group on the array.
If an RG doesn&amp;rsquo;t have any PVs associated with it, the driver will not receive any monitoring request for that RG.
This status can be obtained from the RG using a standard kubectl get call on the resource name:</description>
    </item>
    <item>
      <title>Replication Actions</title>
      <link>https://dell.github.io/csm-docs/v2/replication/replication-actions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dell.github.io/csm-docs/v2/replication/replication-actions/</guid>
      <description>You can exercise native replication control operations from Dell storage arrays by performing &amp;ldquo;Actions&amp;rdquo; on the replicated group of volumes using the DellCSIReplicationGroup (RG) object.
You can patch the DellCSIReplicationGroup Custom Resource (CR) and set the action field in the spec to one of the allowed values (refer to tables in this document).
When you set the action field in the Custom Resource object, the following happens:
State of the RG CR is set to action_in_progress.</description>
    </item>
    <item>
      <title>Volume Expansion</title>
      <link>https://dell.github.io/csm-docs/v2/replication/volume_expansion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dell.github.io/csm-docs/v2/replication/volume_expansion/</guid>
      <description>Starting in v2.4.0, the CSI PowerMax driver supports the expansion of Replicated Persistent Volumes (PVs). This expansion is done online, which is when the PVC is attached to any node.
Prerequisites To use this feature, enable resizer in values.yaml: resizer: enabled: true To use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. Basic Usage To resize a PVC, edit the existing PVC spec and set spec.</description>
    </item>
    <item>
      <title>Tools</title>
      <link>https://dell.github.io/csm-docs/v2/replication/tools/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dell.github.io/csm-docs/v2/replication/tools/</guid>
      <description>repctlrepctl is a command-line client for configuring replication and managing replicated resources between multiple Kubernetes clusters.
Usage Managing Clusters To begin managing replication with repctl you need to add your Kubernetes clusters, you can do that using cluster add command:
./repctl cluster add -f &amp;lt;config-file&amp;gt; -n &amp;lt;name&amp;gt; You can view clusters that are currently being managed by repctl by running cluster get command:
./repctl cluster get Or, alternatively, using get cluster command:</description>
    </item>
    <item>
      <title>Troubleshooting</title>
      <link>https://dell.github.io/csm-docs/v2/replication/troubleshooting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dell.github.io/csm-docs/v2/replication/troubleshooting/</guid>
      <description>Symptoms Prevention, Resolution or Workaround Persistent volumes don&amp;rsquo;t get created on the target cluster. Run kubectl describe on one of the pods of replication controller and see if event says Config update won&#39;t be applied because of invalid configmap/secrets. Please fix the invalid configuration. If it does, then ensure you correctly populated replication ConfigMap. You can check the current status by running kubectl describe cm -n dell-replication-controller dell-replication-controller-config. If ConfigMap is empty, please edit it yourself or use repctl cluster inject command.</description>
    </item>
    <item>
      <title>Upgrade</title>
      <link>https://dell.github.io/csm-docs/v2/replication/upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dell.github.io/csm-docs/v2/replication/upgrade/</guid>
      <description>CSM Replication module consists of two components:
CSM Replication sidecar (installed along with the driver) CSM Replication controller Those two components should be upgraded separately. When upgrading them ensure that you use the same versions for both sidecar and controller, because different versions could be incompatible with each other.
Note: While upgrading the module via helm, the replicas variable in myvalues.yaml can be at most one less than the number of worker nodes.</description>
    </item>
    <item>
      <title>Uninstall</title>
      <link>https://dell.github.io/csm-docs/v2/replication/uninstall/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dell.github.io/csm-docs/v2/replication/uninstall/</guid>
      <description>This section outlines the uninstallation steps for Container Storage Modules (CSM) for Replication.
Uninstalling replication controller To uninstall the replication controller, you can use the script uninstall.sh located in the scripts folder:
./uninstall.sh This script will automatically detect how the current version was installed (repctl or Helm) and use the correct method to delete it.
You can also manually uninstall the replication controller using a method that depends on how you installed it.</description>
    </item>
  </channel>
</rss>
