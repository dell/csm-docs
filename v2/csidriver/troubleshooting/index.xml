<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dell Technologies – Troubleshooting</title>
    <link>https://dell.github.io/csm-docs/v2/csidriver/troubleshooting/</link>
    <description>Recent content in Troubleshooting on Dell Technologies</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="https://dell.github.io/csm-docs/v2/csidriver/troubleshooting/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>V2: Dell CSI Operator</title>
      <link>https://dell.github.io/csm-docs/v2/csidriver/troubleshooting/operator/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/csm-docs/v2/csidriver/troubleshooting/operator/</guid>
      <description>
        
        
        &lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Before installing the drivers, Dell CSI Operator tries to validate the Custom Resource being created. If some mandatory environment variables are missing or there is a type mismatch, then the Operator will report an error during the reconciliation attempts.&lt;/p&gt;
&lt;p&gt;Because of this, the status of the Custom Resource will change to &amp;ldquo;Failed&amp;rdquo; and the error captured in the &amp;ldquo;ErrorMessage&amp;rdquo; field in the status.&lt;/p&gt;
&lt;p&gt;For example - If the PowerMax driver was installed in the namespace test-powermax and has the name powermax, then run the command &lt;code&gt;kubectl get csipowermax/powermax -n test-powermax -o yaml&lt;/code&gt; to get the Custom Resource details.&lt;/p&gt;
&lt;p&gt;If there was an error while installing the driver, then you would see a status like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;status&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;status&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;errorMessage&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;mandatory&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;Env&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;- X_CSI_K8S_CLUSTER_PREFIX&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;not&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;specified&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;in&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;user&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;spec&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;state&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;Failed&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The state of the Custom Resource can also change to &lt;code&gt;Failed&lt;/code&gt; because of any other prohibited updates or any failure while installing the driver. In order to recover from this failure, fix the error in the manifest and update/patch the Custom Resource&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After an update to the driver, the controller pod may not have the latest desired specification.&lt;/p&gt;
&lt;p&gt;This happens when the controller pod was in a failed state before applying the update. Even though the Dell CSI Operator updates the pod template specification for the StatefulSet, the StatefulSet controller does not apply the update to the pod. This happens because of the unique nature of StatefulSets where the controller tries to retain the last known working state.&lt;/p&gt;
&lt;p&gt;To get around this problem, the Dell CSI Operator forces an update of the pod specification by deleting the older pod. In case the Dell CSI Operator fails to do so, delete the controller pod to force an update of the controller pod specification&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Status of the CSI Driver Custom Resource shows the state of the driver pods after installation. This state will not be updated automatically if there are any changes to the driver pods outside any Operator operations.&lt;/p&gt;
&lt;p&gt;At times because of inconsistencies in fetching data from the Kubernetes cache, the state of some driver pods may not be updated correctly in the status. To force an update of the state, you can update the Custom Resource forcefully by setting forceUpdate to true. If all the driver pods are in the &lt;code&gt;Available&lt;/code&gt; State, then the state of the Custom Resource will be updated as &lt;code&gt;Running&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>V2: PowerFlex</title>
      <link>https://dell.github.io/csm-docs/v2/csidriver/troubleshooting/powerflex/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/csm-docs/v2/csidriver/troubleshooting/powerflex/</guid>
      <description>
        
        
        &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Symptoms&lt;/th&gt;
&lt;th&gt;Prevention, Resolution or Workaround&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;The installation fails with the following error message: &lt;br /&gt;&lt;code&gt;Node xxx does not have the SDC installed&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that need to pull an image of the driver.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;When you run the command &lt;code&gt;kubectl describe pods vxflexos-controller-* –n vxflexos&lt;/code&gt;, the system indicates that the driver image could not be loaded.&lt;/td&gt;
&lt;td&gt;- If on Kubernetes, edit the &lt;code&gt;daemon.json&lt;/code&gt; file found in the registry location and add &lt;br /&gt;&lt;code&gt;{ &amp;quot;insecure-registries&amp;quot; :[ &amp;quot;hostname.cloudapp.net:5000&amp;quot; ] }&lt;/code&gt;&lt;br /&gt;- If on OpenShift, run the command &lt;code&gt;oc edit image.config.openshift.io/cluster&lt;/code&gt; and add registries to yaml file that is displayed when you run the command.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs -n vxflexos vxflexos-controller-* driver&lt;/code&gt; logs show that the driver is not authenticated.&lt;/td&gt;
&lt;td&gt;Check the username, password, and the gateway IP address for the PowerFlex system.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs vxflexos-controller-* -n vxflexos driver&lt;/code&gt; logs show that the system ID is incorrect.&lt;/td&gt;
&lt;td&gt;Use the &lt;code&gt;get_vxflexos_info.sh&lt;/code&gt; to find the correct system ID.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs vxflexos-controller-* -n vxflexos driver&lt;/code&gt; logs show that the system ID is incorrect.&lt;/td&gt;
&lt;td&gt;Use the &lt;code&gt;get_vxflexos_info.sh&lt;/code&gt; to find the correct system ID. Add the system ID to &lt;code&gt;myvalues.yaml&lt;/code&gt; script.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CreateVolume error System &lt;Name&gt; is not configured in the driver&lt;/td&gt;
&lt;td&gt;Powerflex name if used for systemID in StorageClass ensure same name is also used in array config systemID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Defcontext mount option seems to be ignored, volumes still are not being labeled correctly.&lt;/td&gt;
&lt;td&gt;Ensure SElinux is enabled on a worker node, and ensure your container run time manager is properly configured to be utilized with SElinux.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mount options that interact with SElinux are not working (like defcontext).&lt;/td&gt;
&lt;td&gt;Check that your container orchestrator is properly configured to work with SElinux.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Installation of the driver on Kubernetes v1.20/v1.21/v1.22 fails with the following error: &lt;br /&gt;&lt;code&gt;Error: unable to build kubernetes objects from release manifest: unable to recognize &amp;quot;&amp;quot;: no matches for kind &amp;quot;VolumeSnapshotClass&amp;quot; in version &amp;quot;snapshot.storage.k8s.io/v1&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes v1.20/1.21/v1.22 requires v1 version of snapshot CRDs to be created in cluster, see the &lt;a href=&#34;../../installation/helm/powerflex/#optional-volume-snapshot-requirements&#34;&gt;Volume Snapshot Requirements&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs -n vxflexos vxflexos-controller-* driver&lt;/code&gt; logs show &lt;code&gt;x509: certificate signed by unknown authority&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A self assigned certificate is used for PowerFlex array. See &lt;a href=&#34;../../installation/helm/powerflex/#certificate-validation-for-powerflex-gateway-rest-api-calls&#34;&gt;certificate validation for PowerFlex Gateway&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;When you run the command &lt;code&gt;kubectl apply -f snapclass-v1.yaml&lt;/code&gt;, you get the error &lt;code&gt;error: unable to recognize &amp;quot;snapclass-v1.yaml&amp;quot;: no matches for kind &amp;quot;VolumeSnapshotClass&amp;quot; in version &amp;quot;snapshot.storage.k8s.io/v1&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Check to make sure that the v1 snapshotter CRDs are installed, and not the v1beta1 CRDs, which are no longer supported.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The controller pod is stuck and producing errors such as&amp;rdquo; &lt;code&gt;Failed to watch *v1.VolumeSnapshotContent: failed to list *v1.VolumeSnapshotContent: the server could not find the requested resource (get volumesnapshotcontents.snapshot.storage.k8s.io)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Make sure that v1 snapshotter CRDs and v1 snapclass are installed, and not v1beta1, which is no longer supported.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: &lt;code&gt;vxflexos-controller-*&lt;/code&gt; is the controller pod that acquires leader lease&lt;/p&gt;
&lt;/blockquote&gt;

      </description>
    </item>
    
    <item>
      <title>V2: PowerMax</title>
      <link>https://dell.github.io/csm-docs/v2/csidriver/troubleshooting/powermax/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/csm-docs/v2/csidriver/troubleshooting/powermax/</guid>
      <description>
        
        
        &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Symptoms&lt;/th&gt;
&lt;th&gt;Prevention, Resolution or Workaround&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Warning about feature gates&lt;/td&gt;
&lt;td&gt;Double check that you have applied all the features to the indicated processes. Restart kubelet when remediated.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubectl describe pod powermax-controller-&amp;lt;xyz&amp;gt; –n &amp;lt;namespace&amp;gt;&lt;/code&gt; indicates that the driver image could not be loaded&lt;/td&gt;
&lt;td&gt;You may need to put an insecure-registries entry in &lt;code&gt;/etc/docker/daemon.json&lt;/code&gt; or log in to the docker registry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubectl logs powermax-controller-&amp;lt;xyz&amp;gt; –n &amp;lt;namespace&amp;gt; driver&lt;/code&gt; logs show that the driver cannot authenticate&lt;/td&gt;
&lt;td&gt;Check your secret’s username and password&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubectl logs powermax-controller-&amp;lt;xyz&amp;gt; –n &amp;lt;namespace&amp;gt; driver&lt;/code&gt; logs show that the driver failed to connect to the U4P because it could not verify the certificates&lt;/td&gt;
&lt;td&gt;Check the powermax-certs secret and ensure it is not empty or it has the valid certificates&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>V2: PowerScale</title>
      <link>https://dell.github.io/csm-docs/v2/csidriver/troubleshooting/powerscale/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/csm-docs/v2/csidriver/troubleshooting/powerscale/</guid>
      <description>
        
        
        &lt;p&gt;Here are some installation failures that might be encountered and how to mitigate them.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Symptoms&lt;/th&gt;
&lt;th&gt;Prevention, Resolution or Workaround&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs isilon-controller-0 -n isilon -c driver&lt;/code&gt; logs shows the driver &lt;strong&gt;cannot authenticate&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Check your secret&amp;rsquo;s username and password for corresponding cluster&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs isilon-controller-0 -n isilon -c driver&lt;/code&gt; logs shows the driver failed to connect to the Isilon because it &lt;strong&gt;couldn&amp;rsquo;t verify the certificates&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Check the isilon-certs-&lt;n&gt; secret and ensure it is not empty and it has the valid certificates. Set &lt;code&gt;isiInsecure: &amp;quot;true&amp;quot;&lt;/code&gt; for insecure connection. SSL validation is recommended in the production environment.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs isilon-controller-0 -n isilon -c driver&lt;/code&gt; logs shows the driver error: &lt;strong&gt;create volume failed, Access denied. create directory as requested&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;This situation can happen when the user who created the base path is different from the user configured for the driver. Make sure the user used to deploy CSI-Driver must have enough rights on the base path (i.e. isiPath) to perform all operations.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Volume/filesystem is allowed to mount by any host in the network, though that host is not a part of the export of that particular volume under /ifs directory&lt;/td&gt;
&lt;td&gt;&amp;ldquo;Dell EMC PowerScale: OneFS NFS Design Considerations and Best Practices&amp;rdquo;: &lt;br&gt; There is a default shared directory (ifs) of OneFS, which lets clients running Windows, UNIX, Linux, or Mac OS X access the same directories and files. It is recommended to disable the ifs shared directory in a production environment and create dedicated NFS exports and SMB shares for your workload.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class is not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency.&lt;/td&gt;
&lt;td&gt;Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control.&lt;/td&gt;
&lt;td&gt;To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:&lt;br&gt; * ISI_PRIV_LOGIN_PAPI&lt;br&gt; * ISI_PRIV_NFS&lt;br&gt; * ISI_PRIV_QUOTA&lt;br&gt; * ISI_PRIV_SNAPSHOT&lt;br&gt; * ISI_PRIV_IFS_RESTORE&lt;br&gt; * ISI_PRIV_NS_IFS_ACCESS&lt;br&gt; In some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;If the hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to driver version 1.4.0 or later there is a possibility of &amp;ldquo;localhost&amp;rdquo; as a stale entry in export&lt;/td&gt;
&lt;td&gt;Recommended setup: User should not map a hostname to loopback IP in /etc/hosts file&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CSI Driver installation fails with the error message &amp;ldquo;error getting FQDN&amp;rdquo;.&lt;/td&gt;
&lt;td&gt;Map IP address of host with its FQDN in /etc/hosts file.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Driver node pod is in &amp;ldquo;CrashLoopBackOff&amp;rdquo; as &amp;ldquo;Node ID&amp;rdquo; generated is not with proper FQDN.&lt;/td&gt;
&lt;td&gt;This might be due to &amp;ldquo;dnsPolicy&amp;rdquo; implemented on the driver node pod which may differ with different networks. &lt;br&gt;&lt;br&gt; This parameter is configurable in both helm and Operator installer and the user can try with different &amp;ldquo;dnsPolicy&amp;rdquo; according to the environment.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>V2: PowerStore</title>
      <link>https://dell.github.io/csm-docs/v2/csidriver/troubleshooting/powerstore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/csm-docs/v2/csidriver/troubleshooting/powerstore/</guid>
      <description>
        
        
        &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Symptoms&lt;/th&gt;
&lt;th&gt;Prevention, Resolution or Workaround&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;When you run the command &lt;code&gt;kubectl describe pods powerstore-controller-&amp;lt;suffix&amp;gt; –n csi-powerstore&lt;/code&gt;, the system indicates that the driver image could not be loaded.&lt;/td&gt;
&lt;td&gt;- If on Kubernetes, edit the daemon.json file found in the registry location and add &lt;code&gt;{ &amp;quot;insecure-registries&amp;quot; :[ &amp;quot;hostname.cloudapp.net:5000&amp;quot; ] }&lt;/code&gt; &lt;br&gt; - If on OpenShift, run the command &lt;code&gt;oc edit image.config.openshift.io/cluster&lt;/code&gt; and add registries to yaml file that is displayed when you run the command.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs -n csi-powerstore powerstore-node-&amp;lt;suffix&amp;gt;&lt;/code&gt; driver logs show that the driver can&amp;rsquo;t connect to PowerStore API.&lt;/td&gt;
&lt;td&gt;Check if you&amp;rsquo;ve created a secret with correct credentials&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Installation of the driver on Kubernetes supported versions fails with the following error: &lt;br /&gt;&lt;code&gt;Error: unable to build kubernetes objects from release manifest: unable to recognize &amp;quot;&amp;quot;: no matches for kind &amp;quot;VolumeSnapshotClass&amp;quot; in version &amp;quot;snapshot.storage.k8s.io/v1&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes v1.20/1.21/v1.22 requires v1 version of snapshot CRDs to be created in cluster, see the &lt;a href=&#34;../../installation/helm/powerstore/#optional-volume-snapshot-requirements&#34;&gt;Volume Snapshot Requirements&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;If PVC is not getting created and getting the following error in PVC description: &lt;br /&gt;&lt;code&gt;failed to provision volume with StorageClass &amp;quot;powerstore-iscsi&amp;quot;: rpc error: code = Internal desc = : Unknown error:&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Check if you&amp;rsquo;ve created a secret with correct credentials&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>V2: Unity</title>
      <link>https://dell.github.io/csm-docs/v2/csidriver/troubleshooting/unity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/csm-docs/v2/csidriver/troubleshooting/unity/</guid>
      <description>
        
        
        &lt;hr&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Symptoms&lt;/th&gt;
&lt;th&gt;Prevention, Resolution or Workaround&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;When you run the command &lt;code&gt;kubectl describe pods unity-controller-&amp;lt;suffix&amp;gt; –n unity&lt;/code&gt;, the system indicates that the driver image could not be loaded.&lt;/td&gt;
&lt;td&gt;You may need to put an insecure-registries entry in &lt;code&gt;/etc/docker/daemon.json&lt;/code&gt; or login to the docker registry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The &lt;code&gt;kubectl logs -n unity unity-node-&amp;lt;suffix&amp;gt;&lt;/code&gt; driver logs show that the driver can&amp;rsquo;t connect to Unity - Authentication failure.&lt;/td&gt;
&lt;td&gt;Check if you have created a secret with correct credentials&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fsGroup&lt;/code&gt; specified in pod spec is not reflected in files or directories at mounted path of volume.&lt;/td&gt;
&lt;td&gt;fsType of PVC must be set for fsGroup to work. fsType can be specified while creating a storage class. For NFS protocol, fsType can be specified as &lt;code&gt;nfs&lt;/code&gt;. fsGroup doesn&amp;rsquo;t work for ephemeral inline volumes.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dynamic array detection will not work in Topology based environment&lt;/td&gt;
&lt;td&gt;Whenever a new array is added or removed, then the driver controller and node pod should be restarted with command &lt;strong&gt;kubectl get pods -n unity &amp;ndash;no-headers=true | awk &amp;lsquo;/unity-/{print $1}&#39;| xargs kubectl delete -n unity pod&lt;/strong&gt; when &lt;strong&gt;topology-based storage classes are used&lt;/strong&gt;. For dynamic array addition without topology, the driver will detect the newly added or removed arrays automatically&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in the cluster but on array, it will still be present and marked for deletion.&lt;/td&gt;
&lt;td&gt;All the cloned PVC should be deleted in order to delete the source PVC from the array.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PVC creation fails on a fresh cluster with &lt;strong&gt;iSCSI&lt;/strong&gt; and &lt;strong&gt;NFS&lt;/strong&gt; protocols alone enabled with error &lt;strong&gt;failed to provision volume with StorageClass &amp;ldquo;unity-iscsi&amp;rdquo;: error generating accessibility requirements: no available topology found&lt;/strong&gt;.&lt;/td&gt;
&lt;td&gt;This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with &lt;strong&gt;kubectl get pods -n unity &amp;ndash;no-headers=true | awk &amp;lsquo;/unity-/{print $1}&#39;| xargs kubectl delete -n unity pod&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
  </channel>
</rss>
