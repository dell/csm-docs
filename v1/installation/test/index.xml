<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dell Technologies – Testing Drivers</title>
    <link>https://dell.github.io/storage-plugin-docs/v1/installation/test/</link>
    <description>Recent content in Testing Drivers on Dell Technologies</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="https://dell.github.io/storage-plugin-docs/v1/installation/test/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>V1: Test PowerFlex CSI Driver</title>
      <link>https://dell.github.io/storage-plugin-docs/v1/installation/test/powerflex/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/storage-plugin-docs/v1/installation/test/powerflex/</guid>
      <description>
        
        
        &lt;p&gt;This section provides multiple methods to test driver functionality in your environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: To run the test for CSI Driver for Dell EMC PowerFlex, install Helm 3.&lt;/p&gt;
&lt;h2 id=&#34;test-deploying-a-simple-pod-with-powerflex-storage&#34;&gt;Test deploying a simple pod with PowerFlex storage&lt;/h2&gt;
&lt;p&gt;Test the deployment workflow of a simple pod on PowerFlex storage.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a &lt;em&gt;helmtest-vxflexos&lt;/em&gt; namespace, using &lt;code&gt;kubectl create namespace helmtest-vxflexos&lt;/code&gt;, before you can start testing. HELM 3 must be installed to perform the tests.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;starttest.sh&lt;/code&gt; script is located in the &lt;code&gt;csi-vxflexos/test/helm&lt;/code&gt; directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Navigate to the test/helm directory, which contains the &lt;code&gt;starttest.sh&lt;/code&gt; and the &lt;em&gt;2vols&lt;/em&gt; directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes.
&lt;em&gt;NOTE:&lt;/em&gt; Helm tests are designed assuming users are using the default &lt;em&gt;storageclass&lt;/em&gt; names (&lt;em&gt;vxflexos&lt;/em&gt; and &lt;em&gt;vxflexos-xfs&lt;/em&gt;). If your &lt;em&gt;storageclass&lt;/em&gt; names differ from the default values, such as when deploying with the Dell CSI Operator, please update the templates in 2vols accordingly (located in &lt;code&gt;test/helm/2vols/templates&lt;/code&gt; directory). You can use &lt;code&gt;kubectl get sc&lt;/code&gt; to check for the &lt;em&gt;storageclass&lt;/em&gt; names.&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;sh starttest.sh 2vols&lt;/code&gt; to deploy the pod. You should see the following:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;Normal Pulled  38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image &amp;quot;docker.io/centos:latest&amp;quot;
Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container
Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container
/dev/scinib 8125880 36852 7653216 1% /data
/dev/scinia 16766976 32944 16734032 1% /data
/dev/scinib on /data0 type ext4 (rw,relatime,data=ordered)
/dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota)
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;To stop the test, run &lt;code&gt;sh stoptest.sh 2vols&lt;/code&gt;. This script deletes the pods and the volumes depending on the retention setting you have configured.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;An outline of this workflow is described below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;em&gt;2vols&lt;/em&gt; helm chart contains two PersistentVolumeClaim definitions, one in &lt;code&gt;pvc0.yaml&lt;/code&gt; , and the other in &lt;code&gt;pvc1.yaml&lt;/code&gt;. They are referenced by the &lt;code&gt;test.yaml&lt;/code&gt; which creates the pod. The contents of the &lt;code&gt;Pvc0.yaml&lt;/code&gt; file are described below:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvol
  namespace: helmtest-vxflexos
spec:
  accessModes:
  - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
  storageClassName: vxflexos
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;The &lt;em&gt;volumeMode: Filesystem&lt;/em&gt; requires a mounted file system, and the &lt;em&gt;resources.requests.storage&lt;/em&gt; of 8Gi requires an 8 GB file. In this case, the &lt;em&gt;storageClassName: vxflexos&lt;/em&gt; directs the system to use one of the pre-defined storage classes created by the CSI Driver for Dell EMC PowerFlex installation process. This step yields a mounted &lt;em&gt;ext4&lt;/em&gt; file system. You can see the storage class definitions in the PowerFlex installation helm chart files &lt;em&gt;storageclass.yaml&lt;/em&gt; and &lt;em&gt;storageclass-xfs.yaml&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;If you compare &lt;em&gt;pvol0.yaml&lt;/em&gt; and &lt;em&gt;pvol1.yaml&lt;/em&gt; , you will find that the latter uses a different storage class; &lt;em&gt;vxflexos-xfs&lt;/em&gt;. This class gives you an &lt;em&gt;xfs&lt;/em&gt; file system.&lt;/li&gt;
&lt;li&gt;To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos.
&lt;em&gt;NOTE:&lt;/em&gt; For more information about Kubernetes objects like &lt;em&gt;StatefulSet&lt;/em&gt; and &lt;em&gt;PersistentVolumeClaim&lt;/em&gt; see &lt;a href=&#34;https://kubernetes.io/docs/concepts/&#34;&gt;Kubernetes documentation: Concepts&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;test-creating-snapshots&#34;&gt;Test creating snapshots&lt;/h2&gt;
&lt;p&gt;Test the workflow for snapshot creation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Start the &lt;em&gt;2vols&lt;/em&gt; container and leave it running.
&lt;ul&gt;
&lt;li&gt;Helm tests are designed assuming users are using the default &lt;em&gt;storageclass&lt;/em&gt; names (&lt;em&gt;vxflexos&lt;/em&gt; and &lt;em&gt;vxflexos-xfs&lt;/em&gt;). If your &lt;em&gt;storageclass&lt;/em&gt; names differ from the default values, such as when deploying with the Operator, update the templates in 2vols accordingly (located in &lt;code&gt;test/helm/2vols/templates&lt;/code&gt; directory). You can use &lt;code&gt;kubectl get sc&lt;/code&gt; to check for the &lt;em&gt;storageclass&lt;/em&gt; names.&lt;/li&gt;
&lt;li&gt;Helm tests are designed assuming users are using the default &lt;em&gt;snapshotclass&lt;/em&gt; name. If your &lt;em&gt;snapshotclass&lt;/em&gt; names differ from the default values, update &lt;code&gt;snap1.yaml&lt;/code&gt; and &lt;code&gt;snap2.yaml&lt;/code&gt; accordingly.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;sh snaptest.sh&lt;/code&gt; to start the test.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This will create a snapshot of each of the volumes in the container using &lt;em&gt;VolumeSnapshot&lt;/em&gt; objects defined in &lt;code&gt;snap1.yaml&lt;/code&gt; and &lt;code&gt;snap2.yaml&lt;/code&gt;. The following are the contents of &lt;code&gt;snap1.yaml&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshot
metadata:
  name: pvol0-snap
  namespace: helmtest-vxflexos
spec:
  snapshotClassName: vxflexos-snapclass
  source:
    name: pvol
    kind: PersistentVolumeClaim
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;snaptest.sh&lt;/code&gt; script will create a snapshot using the definitions in the &lt;code&gt;snap1.yaml&lt;/code&gt; file. The &lt;em&gt;spec.source&lt;/em&gt; section contains the volume that will be snapped. For example, if the volume to be snapped is &lt;em&gt;pvol0&lt;/em&gt; , then the created snapshot is named &lt;em&gt;pvol0-snap&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NOTE:&lt;/em&gt; The &lt;code&gt;snaptest.sh&lt;/code&gt; shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using &lt;code&gt;kubectl get volumesnapshot -n test&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Notice that this &lt;em&gt;VolumeSnapshot&lt;/em&gt; class has a reference to a &lt;em&gt;snapshotClassName: vxflexos-snapclass&lt;/em&gt;. The CSI Driver for Dell EMC PowerFlex installation creates this class as its default snapshot class. You can see its definition in the installation directory file &lt;code&gt;volumesnapshotclass.yaml&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;test-restoring-from-a-snapshot&#34;&gt;Test restoring from a snapshot&lt;/h2&gt;
&lt;p&gt;Test the restore operation workflow to restore from a snapshot.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Ensure that you have stopped any previous test instance before performing this procedure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run &lt;code&gt;sh snaprestoretest.sh&lt;/code&gt; to start the test.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This script deploys the &lt;em&gt;2vols&lt;/em&gt; example, creates a snap of &lt;em&gt;pvol0&lt;/em&gt;, and then updates the deployed helm chart from the updateddirectory &lt;em&gt;2vols+restore&lt;/em&gt;. This then adds an additional volume that is created from the snapshot.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Helm tests are designed assuming users are using the default &lt;em&gt;storageclass&lt;/em&gt; names (&lt;em&gt;vxflexos&lt;/em&gt; and &lt;em&gt;vxflexos-xfs&lt;/em&gt;). If your &lt;em&gt;storageclass&lt;/em&gt; names differ from the default values, such as when deploying with the Dell CSI Operator, update the templates for snap restore tests accordingly (located in &lt;code&gt;test/helm/2vols+restore/template&lt;/code&gt; directory). You can use &lt;code&gt;kubectl get sc&lt;/code&gt; to check for the &lt;em&gt;storageclass&lt;/em&gt; names.&lt;/li&gt;
&lt;li&gt;Helm tests are designed assuming users are using the default &lt;em&gt;snapshotclass&lt;/em&gt; name. If your &lt;em&gt;snapshotclass&lt;/em&gt; names differ from the default values, update &lt;code&gt;snap1.yaml&lt;/code&gt; and &lt;code&gt;snap2.yaml&lt;/code&gt; accordingly.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;An outline of this workflow is described below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The snapshot is taken using &lt;code&gt;snap1.yaml&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Helm&lt;/em&gt; is called to upgrade the deployment with a new definition, which is found in the &lt;em&gt;2vols+restore&lt;/em&gt; directory. The &lt;code&gt;csi-vxflexos/test/helm/2vols+restore/templates&lt;/code&gt; directory contains the newly created &lt;code&gt;createFromSnap.yaml&lt;/code&gt; file. The script then creates a &lt;em&gt;PersistentVolumeClaim&lt;/em&gt; , which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the &lt;code&gt;snaprestoretest.sh&lt;/code&gt; creates a new volume with data from the snapshot, the restore operation is tested. The contents of the &lt;code&gt;createFromSnap.yaml&lt;/code&gt; are described below:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: restorepvc
  namespace: helmtest-vxflexos
spec:
  storageClassName: vxflexos
  dataSource:
    name: pvol0-snap
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 8Gi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;NOTE:&lt;/em&gt; The &lt;em&gt;spec.dataSource&lt;/em&gt; clause, specifies a source &lt;em&gt;VolumeSnapshot&lt;/em&gt; named &lt;em&gt;pvol0-snap1&lt;/em&gt; which matches the snapshot&amp;rsquo;s name in &lt;code&gt;snap1.yaml&lt;/code&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>V1: Test PowerMax CSI Driver</title>
      <link>https://dell.github.io/storage-plugin-docs/v1/installation/test/powermax/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/storage-plugin-docs/v1/installation/test/powermax/</guid>
      <description>
        
        
        &lt;p&gt;This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: To run the test for CSI Driver for Dell EMC PowerMax, install Helm 3.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;csi-powermax&lt;/em&gt; repository includes examples of how you can use the CSI Driver for Dell EMC PowerMax. These examples automate the creation of Pods using the default storage classes that were created during installation. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with different number of volumes. To test the installation of the CSI driver, perform these tests:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Volume clone test&lt;/li&gt;
&lt;li&gt;Volume test&lt;/li&gt;
&lt;li&gt;Snapshot test&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;volume-test&#34;&gt;Volume test&lt;/h4&gt;
&lt;p&gt;Use this procedure to perform a volume test.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a namespace with the name &lt;em&gt;test&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the &lt;code&gt;cd csi-powermax/test/helm&lt;/code&gt; command to go to the &lt;code&gt;csi-powermax/test/helm&lt;/code&gt; directory, which contains the &lt;code&gt;starttest.sh&lt;/code&gt; script and the &lt;em&gt;2vols&lt;/em&gt; directories.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the starttest.sh script and provide it with a test name. The following is a sample command that can be used to run the &lt;em&gt;2vols&lt;/em&gt; test: &lt;code&gt;./starttest.sh -t 2vols -n test&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the &lt;code&gt;./stoptest.sh -t 2vols -n test&lt;/code&gt; script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: Helm tests have been designed assuming that users are using the default storageclass names (powermax and powermax-xfs). If your storageclass names differ from the default values, such as when deploying with the Operator, update the templates in &lt;em&gt;2vols&lt;/em&gt; accordingly (located in &lt;code&gt;test/helm/2vols/templates/&lt;/code&gt; directory). You can use &lt;code&gt;kubectl get sc&lt;/code&gt; to check for the storageclass names.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;volume-clone-test&#34;&gt;Volume clone test&lt;/h4&gt;
&lt;p&gt;Use this procedure to perform a volume clone test.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a namespace with the name &lt;em&gt;test&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Run the &lt;code&gt;cd csi-powermax/test/helm&lt;/code&gt; command to go to the &lt;code&gt;csi-powermax/test/helm&lt;/code&gt; directory, which contains the &lt;code&gt;volumeclonetest.sh&lt;/code&gt; script.&lt;/li&gt;
&lt;li&gt;Run the &lt;code&gt;volumeclonetest.sh&lt;/code&gt; script using the following command: &lt;code&gt;bash volumeclonetest.sh&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This script does the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container.&lt;/li&gt;
&lt;li&gt;Then it creates a file on one of the PVCs and calculates its checksum.&lt;/li&gt;
&lt;li&gt;After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum and compares it to the checksum previously calculated.&lt;/li&gt;
&lt;li&gt;Finally, it cleans up all the resources that are created as part of the test.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;snapshot-test&#34;&gt;Snapshot test&lt;/h4&gt;
&lt;p&gt;Use this procedure to perform a snapshot test.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a namespace with the name &lt;em&gt;test&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Run the &lt;code&gt;cd csi-powermax/test/helm&lt;/code&gt; command to go to the &lt;code&gt;csi-powermax/test/helm&lt;/code&gt; directory, which contains the &lt;code&gt;snaprestoretest.sh&lt;/code&gt;script.&lt;/li&gt;
&lt;li&gt;Run the &lt;code&gt;snaprestoretest.sh&lt;/code&gt; script by running the command : &lt;code&gt;bash snaprestoretest.sh&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This script does the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container.&lt;/li&gt;
&lt;li&gt;Writes some data to one of the PVCs.&lt;/li&gt;
&lt;li&gt;After that, it creates a snapshot on that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs.&lt;/li&gt;
&lt;li&gt;Cleans up all the resources that were created as part of the test.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: This test has been designed assuming that users are using the snapshot class name &lt;code&gt;powermax-snapclass&lt;/code&gt; which is created by the Helm-based installer. If you have an operator-based deployment, the name of the snapshot class will differ. You must update the snapshot class name in the file &lt;code&gt;betaSnap1.yaml&lt;/code&gt; present in the &lt;code&gt;test/helm&lt;/code&gt; folder based on your method of deployment. To get a list of volume snapshot classes, run the command - &lt;code&gt;kubectl get volumesnapshotclass&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;volume-expansion-test&#34;&gt;Volume Expansion test&lt;/h4&gt;
&lt;p&gt;Use this procedure to perform a volume expansion test.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a namespace with the name &lt;em&gt;test&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Run the &lt;code&gt;cd csi-powermax/test/helm&lt;/code&gt; command to go to the &lt;code&gt;csi-powermax/test/helm&lt;/code&gt; directory, which contains the &lt;code&gt;volumeexpansiontest.sh&lt;/code&gt;script.&lt;/li&gt;
&lt;li&gt;Run the &lt;code&gt;volumeexpansiontest.sh&lt;/code&gt; script by running the command : &lt;code&gt;bash volumeexpansiontest.sh&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This script does the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Installs a helm chart that creates a Pod with a container, creates one PVC and mounts it into the created container&lt;/li&gt;
&lt;li&gt;Writes some data to the PVC&lt;/li&gt;
&lt;li&gt;After that, it calculates the checksum of the written data, expands the PVC and then recalculates the checksum&lt;/li&gt;
&lt;li&gt;Cleans up all the resources that were created as part of the test&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>V1: Test PowerScale CSI Driver</title>
      <link>https://dell.github.io/storage-plugin-docs/v1/installation/test/powerscale/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/storage-plugin-docs/v1/installation/test/powerscale/</guid>
      <description>
        
        
        &lt;p&gt;This section provides multiple methods to test driver functionality in your environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: To run the test for CSI Driver for Dell EMC PowerScale, install Helm 3.&lt;/p&gt;
&lt;h2 id=&#34;test-deploying-a-simple-pod-with-powerscale-storage&#34;&gt;Test deploying a simple pod with PowerScale storage&lt;/h2&gt;
&lt;p&gt;Test the deployment workflow of a simple pod on PowerScale storage.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Creating a volume:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Create a file &lt;code&gt;pvc.yaml&lt;/code&gt; using sample yaml files located at test/sample_files/&lt;/p&gt;
&lt;p&gt;Execute the following command to create volume&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create -f $PWD/pvc.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Result: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing &lt;code&gt;kubectl get pvc&lt;/code&gt;.
Note: Verify system for the new volume&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Attach the volume to Host&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create &lt;code&gt;nginx.yaml&lt;/code&gt;
using sample yaml files located at test/sample_files/.&lt;/p&gt;
&lt;p&gt;Execute the following command to mount the volume to Kubernetes node&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create -f $PWD/nginx.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Result: After executing the above command, new nginx pod will be successfully created and started in the default namespace.
Note: Verify PowerScale system for host to be part of clients/rootclients field of export created for volume and used by nginx application.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Create Snapshot&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snap.yaml. The sample file for snapshot creation is located
at test/sample_files/&lt;/p&gt;
&lt;p&gt;Execute the following command to create snapshot&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create -f $PWD/snap.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is testvolclaim1, then the created snapshot is named testvolclaim1-snap1. Verify the PowerScale system for newly created snapshot.&lt;/p&gt;
&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User can see the snapshots using &lt;code&gt;kubectl get volumesnapshot&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. The CSI Driver for PowerScale installation creates this class
as its default snapshot class.&lt;/li&gt;
&lt;li&gt;You can see its definition using &lt;code&gt;kubectl get volumesnapshotclasses isilon-snapclass -o yaml&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Create Volume from Snapshot&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The following procedure will create a new volume from a given snapshot which is specified in spec dataSource field.&lt;/p&gt;
&lt;p&gt;The sample file for volume creation from snapshot is located under test/sample_files/&lt;/p&gt;
&lt;p&gt;Execute the following command to create snapshot&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create -f $PWD/volume_from_snap.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Verify the PowerScale system for newly created volume from snapshot.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Delete Snapshot&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Execute the following commands to delete the snapshot&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get volumesnapshot
kubectl delete volumesnapshot testvolclaim1-snap1
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Create new volume from existing volume(volume clone)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The following procedure will create a new volume from another existing volume which is specified in spec dataSource field.&lt;/p&gt;
&lt;p&gt;The sample file for volume creation from volume is located at test/sample_files/&lt;/p&gt;
&lt;p&gt;Execute the following command to create snapshot&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create -f $PWD/volume_from_volume.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Verify the PowerScale system for new created volume from volume.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;To Unattach the volume from Host&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Delete the nginx application to unattach the volume from host&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete -f nginx.yaml&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;To delete the volume&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get pvc
kubectl delete pvc testvolclaim1
kubectl get pvc
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>V1: Test PowerStore CSI Driver</title>
      <link>https://dell.github.io/storage-plugin-docs/v1/installation/test/powerstore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/storage-plugin-docs/v1/installation/test/powerstore/</guid>
      <description>
        
        
        &lt;p&gt;In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs and nfs
storage classes, and automatically mounts them to the pod. Note that nfs storage class is optional and will not be created if you haven&amp;rsquo;t turned it on in &lt;code&gt;myvalues.yaml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To run this test, run the kubectl command from the root directory of the repository:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl create -f ./tests/simple/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can find all the created resources in &lt;code&gt;testpowerstore&lt;/code&gt; namespace.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;Check if the pod is created and Ready and Running by running:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get all -n testpowerstore
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If it&amp;rsquo;s in CrashLoopback state then the driver installation wasn&amp;rsquo;t successful. Check the logs of the node and the controller.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go into the created container and verify that everything is mounted correctly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After verifying, you can uninstall the testing PVCs and StatefulSet.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl delete -f ./tests/simple/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>V1: Test Unity CSI Driver</title>
      <link>https://dell.github.io/storage-plugin-docs/v1/installation/test/unity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dell.github.io/storage-plugin-docs/v1/installation/test/unity/</guid>
      <description>
        
        
        &lt;p&gt;In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes, and automatically mounts them to the pod.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To run this test, run the kubectl command from the root directory of the repository:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl create -f ./tests/sample.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can find all the created resources in &lt;code&gt;test-unity&lt;/code&gt; namespace.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;Check if the pod is created and Ready and Running by running:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get all -n test-unity
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If it&amp;rsquo;s in CrashLoopback state then the driver installation wasn&amp;rsquo;t successful. Check the logs of the node and the controller.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go into the created container and verify that everything is mounted correctly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After verifying, you can uninstall the testing PVCs and StatefulSet.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl delete -f ./tests/sample.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
  </channel>
</rss>
