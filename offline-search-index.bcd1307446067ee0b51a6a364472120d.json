[{"body":" \t window.onload = function() { const ui = SwaggerUIBundle({ url: \"../swagger.yaml\", dom_id: '#ohpen_swagger_ui', presets: [ SwaggerUIBundle.presets.apis, SwaggerUIStandalonePreset ] }) window.ui = ui } \t","excerpt":" \t window.onload = function() { const ui = SwaggerUIBundle({ url: …","ref":"/csm-docs/docs/deployment/csmapi/","title":"CSM REST API"},{"body":"To upgrade Dell CSI Operator from v1.2.0/v1.3.0 to v1.4.0/v1.5.0, perform the following steps.\nUsing Installation Script Run the following command to upgrade the operator\n$ bash scripts/install.sh --upgrade Using OLM The upgrade of the Dell CSI Operator is done via Operator Lifecycle Manager. If the InstallPlan for the Operator subscription is set to Automatic, the operator will be automatically upgraded to the new version. If the InstallPlan is set to Manual, then a Cluster Administrator would need to approve the upgrade.\nNOTE: The recommended version of OLM for Upstream Kubernetes is v0.18.3 when upgrading operator to v1.5.0.\n","excerpt":"To upgrade Dell CSI Operator from v1.2.0/v1.3.0 to v1.4.0/v1.5.0, …","ref":"/csm-docs/docs/csidriver/upgradation/drivers/operator/","title":"Dell CSI Operator"},{"body":"To upgrade Dell CSI Operator from v1.2.0 or v1.3.0 to v1.4.0, perform the following steps.\nUsing Installation Script Run the following command to upgrade the operator from the v1.2.0 release\n$ bash scripts/install.sh --upgrade Using OLM The upgrade of the Dell CSI Operator is done via Operator Lifecycle Manager. If the InstallPlan for the Operator subscription is set to Automatic, the operator will be automatically upgraded to the new version. If the InstallPlan is set to Manual, then a Cluster Administrator would need to approve the upgrade.\nNOTE: The recommended version of OLM for Upstream Kubernetes is v0.17.0 when upgrading operator to v1.4.0.\n","excerpt":"To upgrade Dell CSI Operator from v1.2.0 or v1.3.0 to v1.4.0, perform …","ref":"/csm-docs/v1/upgradation/drivers/operator/","title":"Dell CSI Operator"},{"body":"If you are upgrading the Dell CSI Operator from v1.1.0 or v1.2.0 to v1.3.0, then follow the instructions below. If you are trying to upgrade the Operator from an older version, please refer the instructions here\nUsing Installation Script Run the following command to upgrade the operator from v1.2.0 release\n$ bash scripts/install.sh --upgrade Using OLM The upgrade of the Dell CSI Operator is done via Operator Lifecycle Manager. If the InstallPlan for the Operator subscription is set to Automatic, the operator will be automatically upgraded to the new version. If the InstallPlan is set to Manual, then a Cluster Administrator would need to approve the upgrade.\nUpgrade Operator from version older than v1.1.0 to v1.3.0  Uninstall the old version of the Operator If required, upgrade your cluster to a supported version Follow the installation instructions to install the v1.3.0 of the Operator here  ","excerpt":"If you are upgrading the Dell CSI Operator from v1.1.0 or v1.2.0 to …","ref":"/csm-docs/v2/upgradation/drivers/operator/","title":"Dell CSI Operator"},{"body":"","excerpt":"","ref":"/csm-docs/docs/replication/deployment/","title":"Deployment"},{"body":"Container Storage Modules (CSM) for Authorization is designed as a service mesh solution and consists of many internal components that work together in concert to achieve its overall functionality.\nThis document provides an overview of the major components, including how they fit together and pointers to implementation details.\nIf you are a developer who is new to CSM for Authorization and want to build a mental map of how it works, you’re in the right place.\nTerminology  Service Mesh - An infrastructure layer consisting of proxies that intercept and route requests between existing services. CSI - Acronym for the Container Storage Interface. Proxy (L7) - A gateway between networked services that inspects request traffic. Sidecar Proxy - A service mesh proxy that runs alongside existing services, rather than within them. Pod - A Kubernetes abstraction for a set of related containers that are to be considered as one unit. Tenant - A named persona who owns a Kubernetes cluster and is considered the “client-side” user. Storage Administrator - A named persona who owns a storage array and is considered the admin user.  Bird’s Eye View +-----------------------------------+ | Kubernetes | | | | +---------+ +---------+ | +---------------+ | | CSI | | Sidecar | | | CSM | +---------+ | | Driver |---------\u003e Proxy |---------------\u003e Authorization |--------------\u003e Storage | | +---------+ +---------+ | | Server | | Array | | | +---------------+ +---------+ +-----------------------------------+ ^ | | | +------------+ | karavictl | | CLI | +------------+ NOTE: Arrows indicate request or connection initiation, not necessarily data flow direction.\nThe sections below explain each component in the diagram.\nKubernetes The architecture assumes a Kubernetes cluster that intends to offer external storage to applications hosted therein. The mechanism for managing this storage would utilize a CSI Driver.\nArchitecture Invariant: We assume there may be many Kubernetes clusters, potentially containing multiple CSI Drivers each with their own Sidecar Proxy.\nCSI Driver A CSI Driver supports the Container Service Interface (CSI) specification. Dell EMC provides customers with CSI Drivers for its various storage arrays. CSM for Authorization intends to support a majority, if not all, of these drivers.\nA CSI Driver will typically be configured to communicate directly to its intended storage array and as such will be limited in using only the authentication methods supported by the Storage Array itself, e.g. Basic authentication over TLS.\nArchitecture Invariant: We try to avoid having to make any code changes to the CSI Driver when adding support for it. Any CSI Driver should ideally not be aware that it is communicating to the Sidecar Proxy.\nSidecar Proxy The CSM for Authorization Sidecar Proxy is a sidecar container that gets “injected” into the CSI Driver’s Pod. It acts as a proxy and forwards all requests to a CSM Authorization Server.\nThe CSI Driver section noted the limitation of a CSI Driver using Storage Array supported authentication methods only. By nature of being a proxy, the CSM for Authorization Sidecar Proxy is able to override the Authorization HTTP header for outbound requests to use Bearer tokens. Such tokens are managed by CSM for Authorization as will be described later in this document.\nCSM for Authorization Server The CSM for Authorization Server is, at its core, a Layer 7 proxy for intercepting traffic between a CSI Driver and a Storage Array.\nInbound requests are expected to originate from the CSM for Authorization Sidecar Proxy, for the following reasons:\n Processing a set of agreed upon HTTP headers (added by the CSM for Authorization Sidecar Proxy) to assist in routing traffic to the intended Storage Array. Inspection of CSM-specific Authorization Bearer tokens.  CSM for Authorization CLI The karavictl CLI (Command Line Interface) application allows Storage Admins to manage and interact with a running CSM for Authorization Server.\nAdditionally, karavictl provides functionality for supporting the sidecar proxy injection mechanism mentioned above. Injection is discussed in more detail later on in this document.\nStorage Array A Storage Array is typically considered to be one of the various Dell EMC storage offerings, e.g. Dell EMC PowerFlex which is supported by CSM for Authorization today. Support for more Storage Arrays will come in the future.\nHow it Works CSM for Authorization intends to override the existing authorization methods between a CSI Driver and its Storage Array. This may be desirable for several reasons, if:\n The CSI Driver requires privileged login credentials (e.g. “root”) in order to function. The Storage Array does not natively support the concept of RBAC and/or multi-tenancy.  This section of of the document describes how CSM for Authorization provides a solution to these problems.\nBearer Tokens CSM for Authorization overrides any existing authorization mechanism between a CSI Driver and its corresponding Storage Array with the use of JSON Web Tokens (JWTs). The CSI Driver and Storage Array will not be aware of this taking place.\nIn the context of RFC-6749 there are two such JWTs that are used:\n Access token: a single token valid for a short period of time. Refresh token: a single token used to obtain access tokens. Typically valid for a longer period of time.  Both tokens are opaque to the client, yet provide meaningful information to the server, specifically:\n The Tenant for whom the token is associated with. The Roles that are bound to the Tenant.  Tokens encode the following set of claims:\n{ \"aud\": \"karavi\", \"exp\": 1915585883, \"iss\": \"com.dell.karavi\", \"sub\": \"karavi-tenant\", \"roles\": \"role-a,role-b,role-c\", \"group\": \"Tenant-1\" } Both tokens are signed using a server-side secret preventing the risk of tampering by any client. For example, a bad-actor is unable to modify a token to give themselves a role that they should not have, at least without knowing the server-side secret.\nThe refresh approach is beneficial for the following reasons:\n Accidental exposure of an access token poses a lesser security concern, given the set expiration time is short (e.g. 30 seconds). The CSM for Authorization Server can fully trust the access token without having to perform a database check on each request (doing so would nullify the benefits of using tokens in the first place). The CSM for Authorization Server can defer Tenant checks at refresh time only, e.g. do not allow refresh if the Tenant’s access has been revoked by a Storage Admin. There may be a short time window in between revocation and enforcement, depending on the access token’s expiration time.  The following diagram shows the access and refresh tokens in play and how a valid access token is required for a request to be proxied to the intended Storage Array.\n +---------+ +---------------+ | | | | | | | | +----------+ | |--(A)------------ Access Token -----------\u003e| |------\u003e| | | | | CSM | | | | |\u003c-(B)---------- Protected Resource --------| Authorization |\u003c------| Storage | | Sidecar | | Server | | Array | | Proxy |--(C)------------ Access Token -----------\u003e| | | | | | | | | | | |\u003c-(D)------ Invalid Token Error -----------| | | | | | | | +----------+ | | | | | |--(E)----------- Refresh Token -----------\u003e| | | | \u0026 Expired Access Token | | | |\u003c-(F)----------- Access Token -------------| | +---------+ +---------------+  A) CSI Driver makes a request to the Storage Array:  request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token valid. The CSM for Authorization Server permits the request to be proxied to the intended Storage Array.   B) Storage Array response is sent back as expected. C) CSI Driver makes a request to the Storage Array:  request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token is invalid; it has since expired.   D) The CSM for Authorization Server responds with HTTP 401 Unauthorized. E) Sidecar Proxy requests a new access token by passing both refresh token and expired token. F) The CSM for Authorization Server processes the request:  is the refresh token valid? is the access token expired? has the Tenant had access revoked? a new access token is sent in response if the checks pass.    Roles So we know a token encodes both the identification of a Tenant and their Roles, but what’s in a Role?\nA role can be defined as follows:\n It has a name, e.g. “role-a”. It can be bound to a Tenant It can be unbound from a Tenant. It determines access to zero or more storage pools and assigns a storage quota for each.  Quota represents the upper-limit of the total aggregation of used storage capacity for a Tenant’s resources in a storage pool.   It prevents ambiguity by identifying each storage pool in the form of system-type:system-id:pool-name.  Below is an example of how roles are represented internally in JSON:\n{ \"Developer\": { \"system_types\": { \"powerflex\": { \"system_ids\": { \"542a2d5f5122210f\": { \"pool_quotas\": { \"bronze\": 99000000 } } } } } } } This role says Allow Tenants with the Developer role access to the bronze pool on PowerFlex system 542a2d5f5122210f, and cap their total capacity usage at 99000000Kb (99Gb).\nPolicy CSM for Authorization leverages the Open Policy Agent to use a policy-as-code approach to policy management. It stores a collection of policy files written in Rego language. Each policy file defines a set of policy rules that form the basis of a policy decision. A policy decision is made by processing the inputs provided. For CSM for Authorization, the inputs are:\n The set of roles defined by the Storage Admin. The claims section of a validated JWT. The JSON payload of the storage request.  Given these inputs, many decisions can be made to answer questions like “Can Tenant X, with these roles provision this volume of size Y?”. The result of the policy decision will determine whether or not the request is proxied.\n +----------------+ | Open Policy | | Agent | | | JWT | +--------+ | Claims ------\\ | | Policy | ----------\u003e Allow/Deny -----\u003e | (Rego) | | Storage -----\u003e +--------+ | Request -----/ +-------^--------+ | | | Role Data Quota \u0026 Volume Ownership Policy decisions based on the current request and set of roles alone are not enough. CSM for Authorization must maintain a cache of volumes approved for creation and deletion in order to know if a Tenant has already consumed their quota on a given storage pool.\nA Redis database is used to store this volume data and their relationship with a Tenant, Storage Array and Pool. The use of composite keys provide fast, constant time look up of volumes, e.g. quota:powerflex:542a2d5f5122210f:bronze:Tenant-1:data is a Redis hash with volume data as its values.\nCross-Cutting Concerns This section documents the pieces of code that are general in nature and shared across multiple packages.\nLogging CSM for Authorization uses the Logrus package when logging messages.\nObservability Both the CSM for Authorization Server and Sidecar Proxy are long-running processes, so it’s important to understand what’s going on inside. We use OpenTelemetry (otel) to help with that.\nThe following otel exporters are used:\n go.opentelemetry.io/otel/exporters/metric/prometheus go.opentelemetry.io/otel/exporters/trace/zipkin go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp  ","excerpt":"Container Storage Modules (CSM) for Authorization is designed as a …","ref":"/csm-docs/docs/authorization/design/","title":"Design"},{"body":"The solution takes the approach that each storage system that Container Storage Modules (CSM) for Observability supports will have their own metrics deployments in the Kubernetes cluster.\n Metrics Deployment: Queries the Kubernetes API to gather information about storage resources and then queries the storage system’s REST API to gather specific metrics. These metrics are then exported to the OTEL collector. Each supported storage system will have their own Deployment for metrics. They will each follow a similar pattern of querying the Kubernetes and StorageSystem APIs to gather information about storage resources (ex: volumes, storage pools, etc) and their metrics. Metrics will be exported directly to the OTEL collector.  A single topology deployment will query the Kubernetes API to gather mapping information between Persistent Volumes and storage resources located on multiple storage systems. This information is queried directly from Grafana and displayed in a custom dashboard.\nRequired Components The following prerequisites must be deployed into the namespace where CSM for Observability is located to support the storage system metrics and topology deployments:\n Prometheus for scraping the metrics from the OTEL collector. Grafana for visualizing the metrics from Prometheus and Topology services using custom dashboards. CSM for Observability will use secrets to get details about the storage systems used by the CSI drivers. These secrets should be copied from the namespaces where the drivers are deployed. CSI Powerflex driver uses the ‘vxflexos-config’ secret and CSI PowerStore uses the ‘powerstore-config’ secret.  Deployment Architectures CSM for Observability can be deployed to either direct storage system requests directly to the storage system or through the CSM for Authorization proxy. The CSI driver must be configured to route storage system requests through the CSM for Authorization proxy in order for CSM for Observability to do the same.\nDefault Deployment of CSM for Observability Deployment of CSM for Observability with CSM for Authorization ","excerpt":"The solution takes the approach that each storage system that …","ref":"/csm-docs/docs/observability/design/","title":"Design"},{"body":"This section covers CSM for Resiliency’s design. The detail is sufficient that you should be able to understand what CSM for Resiliency is designed to do in various situations and how it works. CSM for Resiliency is deployed as a sidecar named podmon with a CSI driver in both the controller pods and node pods. These are referred to as controller-podmon and node-podmon respectively.\nGenerally controller-podmon and the driver controller pods are deployed using a Deployment. The Deployments support one or multiple replicas for High Availability and use a standard K8S leader election protocol so that only one controller is active at a time (as does the driver and all the controller sidecars.) The controller deployment also supports a Node Selector that allows the controllers to be placed on K8S Manager (non Worker) nodes.\nNode-podmon and the driver node pods are deployed in a DaemonSet, with a Pod deployed on every K8S Worker Node.\nController-Podmon Controller-podmon is responsible for:\n  Setting up a Watch for CSM for Resiliency labeled pods, and if a Pod is Initialized but Not Ready and resident on a Node with a NoSchedule or NoExecute taint, calling controllerCleanupPod to cleanup the pod so that a replacement pod can be scheduled.\n  Periodically polling the arrays to see if it has connectivity to the nodes that are hosting CSM for Resiliency labeled pods (if enabled.) If an array has lost connectivity to a node hosting CSM for Resiliency labeled pods using that array, controllerCleanupPod is invoked to cleanup the pods that have lost I/O connectivity.\n  Tainting nodes that have failed so that a) no further pods will get scheduled to them until they are returned to service, and b) podmon-node upon seeing the taint will invoke the cleanup operations to make sure any zombie pods (pods that have been replaced) cannot write to the volumes they were using.\n  If a CSM for Resiliency labeled pod enters a CrashLoopBackOff state, deleting that pod so it can be replaced.\n  ControllerCleanupPod cleans up the pod by taking the following actions:\n The VolumeAttachments (VAs) are loaded, and all VAs belonging to the pod being cleaned up are identified. The PVs for each VolumeAttachment are identified and used to get the Volume Handle (array identifier for the volume.) If enabled, the array is queried if any of the volumes to the pod are still doing I/O. If so, cleanup is aborted. The pod’s volumes are “fenced” from the node the pod resides on to prevent any potential I/O from a zombie pod. This is done by calling the CSI ControllerUnpublishVolume call for each of the volumes. A taint is applied to the node to keep any new pods from being scheduled to the node. If the replacement pod were to get scheduled to the same node as a zombie pod, they might both gain access to the volume concurrently causing corruption. The VolumeAttachments for the pod is deleted. This is necessary so the replacement pod to be created can attach the volumes. The pod is forcibly deleted so that a StatefulSet controller which created the pod is free to create a replacement pod.  Node-Podmon Node-podmon has the following responsibilities:\n Establishing a pod watch which is used to maintain a list of pods executing on this node that may need to be cleaned up. The list includes information about each Mount volume or Block volume used by the pod including the volume handle, volume name, private mount path, and mount path in the pod. Periodically (every 30 seconds) polling to see if controller-podmon has applied a taint to the node. If so, node-podmon calls nodeModeCleanupPod for each pod to clean up any remnants of the pod (which is potentially a zombie pod.) If all pods have been successfully cleaned up, and there are no labeled pods on this node still existing, only then will node-podmon remove the taint placed on the node by controller-podmon.  NodeModeCleanupPod cleans up the pod remnants by taking the following actions for each volume used by the pod:\n Calling NodeUnpublishVolume to unpublish the volume from the pod. Unmounting and deleting the target path for the volume. Calling NodeUnstageVolume to unpublish the volume from the node. Unmounting and deleting the staging path for the volume.  Design Limitations There are some limitations with the current design. Some might be able to be addressed in the future- others are inherent in the approach.\n The design relies on the array’s ability to revoke access to a volume for a particular node for the fencing operation. The granularity of access control for a volume is per node. Consequently, it isn’t possible to revoke access from one pod on a node while retaining access to another pod on the same node if we cannot communicate with the node. The implications of this are that if more than one pod on a node is sharing the same volume(s), they all must be protected by CSM for Resiliency, and they all must be cleaned up by controller-podmon if the node fails. If only some of the pods are cleaned up, the other pods will lose access to the volumes shared with pods that have been cleaned, so those pods should also fail. The node-podmon cleanup algorithm purposefully will not remove the node taint until all the protected volumes have been cleaned up from the node. This works well if the node fault lasts long enough that controller-podmon can evacuate all the protected pods from the node. However, if the failure is short-lived, and controller-podmon does not clean up all the protected pods on the node, or if for some reason node-podmon cannot clean a pod completely, the taint is left on the node, and manual intervention is required. The required intervention is for the operator to reboot the node, which will ensure that no zombie pods survive. Upon seeing the reboot, node-podmon will then remove the taint. If the node failure is short-lived and controller-podmon has not evacuated some of the protected pods on the node, they may try and restart on the same pod. This has been observed to cause such pods to go into CrashLoopBackoff. We are currently considering solutions to this problem.  ","excerpt":"This section covers CSM for Resiliency’s design. The detail is …","ref":"/csm-docs/docs/resiliency/design/","title":"Design"},{"body":"What is a container: https://www.docker.com/resources/what-container\nWhat is container orchestration: https://www.infoworld.com/article/3268073/what-is-kubernetes-container-orchestration-explained.html\nKubernetes (k8s): https://kubernetes.io/\nDocker: https://www.docker.com/\nUnderstanding CSI: https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b\nHow to write a CSI plugin: https://arslan.io/2018/06/21/how-to-write-a-container-storage-interface-csi-plugin/\n","excerpt":"What is a container: https://www.docker.com/resources/what-container …","ref":"/csm-docs/docs/grasp/start/","title":"Getting Started"},{"body":"What is a container: https://www.docker.com/resources/what-container\nWhat is container orchestration: https://www.infoworld.com/article/3268073/what-is-kubernetes-container-orchestration-explained.html\nKubernetes (k8s): https://kubernetes.io/\nDocker: https://www.docker.com/\nUnderstanding CSI: https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b\nHow to write a CSI plugin: https://arslan.io/2018/06/21/how-to-write-a-container-storage-interface-csi-plugin/\n","excerpt":"What is a container: https://www.docker.com/resources/what-container …","ref":"/csm-docs/v1/grasp/start/","title":"Getting Started"},{"body":"What is a container : https://www.docker.com/resources/what-container\nWhat is container orchestration : https://www.infoworld.com/article/3268073/what-is-kubernetes-container-orchestration-explained.html\nKubernetes (k8s) : https://kubernetes.io/\nDocker : https://www.docker.com/\nUnderstanding CSI : https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b\nHow to write a CSI plugin: https://arslan.io/2018/06/21/how-to-write-a-container-storage-interface-csi-plugin/\n","excerpt":"What is a container : https://www.docker.com/resources/what-container …","ref":"/csm-docs/v2/grasp/start/","title":"Getting Started"},{"body":"What is a container : https://www.docker.com/resources/what-container\nWhat is container orchestration : https://www.infoworld.com/article/3268073/what-is-kubernetes-container-orchestration-explained.html\nKubernetes (k8s) : https://kubernetes.io/\nDocker : https://www.docker.com/\nUnderstanding CSI : https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b\nHow to write a CSI plugin: https://arslan.io/2018/06/21/how-to-write-a-container-storage-interface-csi-plugin/\n","excerpt":"What is a container : https://www.docker.com/resources/what-container …","ref":"/csm-docs/v3/grasp/start/","title":"Getting Started"},{"body":"The CSI Drivers by Dell EMC implement an interface between CSI (CSI spec v1.3) enabled Container Orchestrator (CO) and Dell EMC Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using Dell storage system.\nThe following are the drivers provided for the Dell storage family:\n   Release/Drivers PowerScale/Isilon Unity PowerStore PowerFlex/VxFlex OS PowerMax     Current v1.6 v1.6 v1.4 v1.5 v1.7   Previous v1.5 v1.5 v1.3 v1.4 v1.6   Older v1.4 v1.4 v1.2 v1.3 v1.5   Archives v1.3 v1.3 v1.1 v1.2 v1.4    Architecture Features and capabilities Supported Platforms    PowerMax PowerFlex Unity PowerScale/Isilon PowerStore     Storage Array 5978.479.479, 5978.669.669, Unisphere 9.2 3.5.x, 3.6.x 5.0.3, 5.0.4, 5.0.5, 5.0.6, 5.0.7 OneFS 8.1, 8.2, 9.0, 9.1 1.0.x, 2.0.x   Kubernetes 1.19, 1.20, 1.21 1.19, 1.20, 1.21 1.19, 1.20, 1.21 1.19, 1.20, 1.21 1.19, 1.20, 1.21   RHEL 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x   Ubuntu 20.04 20.04 18.04, 20.04 18.04, 20.04 20.04   CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9   SLES no 15SP2 15SP2 15SP2 15SP2   Fedora Core OS no 5.x no no no   Red Hat OpenShift 4.6, 4.6 EUS, 4.7 4.6, 4.6 EUS, 4.7 4.6, 4.6 EUS, 4.7 4.6, 4.6 EUS, 4.7 4.6, 4.6 EUS, 4.7   Mirantis Kubernetes Engine 3.4.0 3.4.0 3.4.0 3.4.0 3.4.0   Google Anthos 1.6 1.6 no no 1.7   VMware Tanzu no no yes yes no   Rancher Kubernetes Engine no yes yes no no    CSI Driver Capabilities   Features PowerMax PowerFlex/VxFlexOS Unity PowerScale/Isilon PowerStore     Static Provisioning yes yes yes yes yes   Dynamic Provisioning yes yes yes yes yes   Expand Persistent Volume yes yes yes yes yes   Create VolumeSnapshot yes yes yes yes yes   Create Volume from Snapshot yes yes yes yes yes   Delete Snapshot yes yes yes yes yes   Access Mode RWO(FC/iSCSI)\nRWO/RWX/ROX(Raw block) RWO\nRWO/RWX/ROX(Raw block) RWO(FC/iSCSI)\nRWO/RWX(RawBlock)\nRWO/RWX/ROX(NFS) RWO/RWX/ROX RWO(FC/iSCSI)\nRWO/RWX/ROX(RawBlock, NFS)   CSI Volume Cloning yes yes yes yes yes   CSI Raw Block Volume yes yes yes no yes   CSI Ephemeral Volume no yes yes yes yes   Topology yes yes yes yes yes   Multi-array yes yes yes yes yes    Backend Storage Details   Features PowerMax VxFlexOS/PowerFlex Unity Isilon/PowerScale PowerStore     Fibre Channel yes N/A yes N/A yes   iSCSI yes N/A yes N/A yes   NFS N/A N/A yes yes yes   Other N/A ScaleIO protocol N/A N/A N/A   Supported FS ext4 / xfs ext4 / xfs ext3 / ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS   Thin / Thick provisioning yes yes yes N/A yes   Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering Policy\nNFS Host IO size\nSnapshot Retention duration Access Zone\nNFS version (3 or 4);Configurable Export IPs iSCSI CHAP    ","excerpt":"The CSI Drivers by Dell EMC implement an interface between CSI (CSI …","ref":"/csm-docs/v1/dell-csi-driver/","title":"Introduction"},{"body":"The CSI Drivers by Dell EMC implement an interface between CSI (CSI spec v1.2) enabled Container Orchestrator (CO) and Dell EMC Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using Dell storage system.\nThe following are the drivers provided for the Dell storage family:\n   Release/Drivers PowerScale/Isilon Unity PowerStore PowerFlex/VxFlex OS PowerMax     Current v1.5 v1.5 v1.3 v1.4 v1.6   Previous v1.4 v1.4 v1.2 v1.3 v1.5   Older v1.3 v1.3 v1.1 v1.2 v1.4    NOTE: This doc version is no longer supported by us. You can check our latest version\nArchitecture Features and capabilities Supported Platforms    PowerMax PowerFlex/VxFlex OS Unity PowerScale/Isilon PowerStore     Storage Array 5978.479.479, 5978.669.669, Unisphere 9.1, 9.2 3.5.x OE 5.0.2, 5.0.3, 5.0.4, 5.0.5, 5.0.6 OneFS 8.1, 8.2, 9.0, 9.1 1.0.x   Kubernetes 1.18, 1.19, 1.20 1.18, 1.19, 1.20 1.18, 1.19, 1.20 1.18, 1.19, 1.20 1.18, 1.19, 1.20   RHEL 7.8, 7.9, 8.3 7.8, 7.9, 8.3 7.8, 7.9, 8.3 7.8, 7.9, 8.3 7.8, 7.9, 8.3   Ubuntu 20.04 20.04 20.04 20.04 20.04   CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9   SLES no 15SP2 15SP2 no no   Fedora Core OS no 5.10 no no no   OpenShift 4.6, 4.7 4.6, 4.7 4.6, 4.7 4.6, 4.7 4.6, 4.7   Docker EE 3.1 3.1 3.1 3.1 3.1   Google Anthos 1.5 1.6 no no 1.5    CSI Driver Capabilities   Features PowerMax PowerFlex/VxFlexOS Unity PowerScale/Isilon PowerStore     Static Provisioning yes yes yes yes yes   Dynamic Provisioning yes yes yes yes yes   Expand Persistent Volume yes yes yes yes yes   Create VolumeSnapshot yes yes yes yes yes   Create Volume from Snapshot yes yes yes yes yes   Delete Snapshot yes yes yes yes yes   Access Mode RWO(FC/iSCSI)\nRWO/RWX/ROX(Raw block) RWO\nRWO/RWX/ROX(Raw block) RWO(FC/iSCSI)\nRWO/RWX(RawBlock)\nRWO/RWX/ROX(NFS) RWO/RWX/ROX RWO(FC/iSCSI)\nRWO/RWX/ROX(RawBlock, NFS)   CSI Volume Cloning yes yes yes yes yes   CSI Raw Block Volume yes yes yes no yes   CSI Ephemeral Volume no yes yes yes yes   Topology yes yes yes yes yes   Multi-array yes (via Unisphere) yes yes yes yes    Backend Storage Details   Features PowerMax VxFlexOS/PowerFlex Unity Isilon/PowerScale PowerStore     Fibre Channel yes N/A yes N/A yes   iSCSI yes N/A yes N/A yes   NFS N/A N/A yes yes yes   Other N/A ScaleIO protocol N/A N/A N/A   Supported FS ext4 / xfs ext4 / xfs ext3 / ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS   Thin / Thick provisioning yes yes yes N/A yes   Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering Policy\nNFS Host IO size\nSnapshot Retention duration Access Zone\nNFS version (3 or 4);Configurable Export IPs iSCSI CHAP    ","excerpt":"The CSI Drivers by Dell EMC implement an interface between CSI (CSI …","ref":"/csm-docs/v2/dell-csi-driver/","title":"Introduction"},{"body":"The CSI Drivers by Dell EMC implement an interface between CSI enabled Container Orchestrator (CO) and Dell EMC Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using Dell storage system.\nThe following are the drivers provided for the Dell storage family:\n   Driver PowerScale/Isilon Unity PowerStore PowerFlex/VxFlex OS PowerMax     Current version v1.4 v1.4 v1.2 v1.3 v1.5   Older Versions v1.3 v1.3 v1.1 v1.2 v1.4    NOTE: This doc version is no longer supported by us. You can check our latest version\nArchitecture Features and capabilities Supported Platforms   Features PowerMax PowerFlex/VxFlex OS Unity PowerScale/Isilon PowerStore     Storage Array 5978.479.479, 5978.669.669 3.0.x, 3.5.x 5.0.0, 5.0.1, 5.0.2, 5.0.3 OneFS 8.1, 8.2, 9.0, 9.1 1.0.x   Kubernetes 1.17, 1.18, 1.19 1.17, 1.18, 1.19 1.17, 1.18, 1.19 1.17, 1.18, 1.19 1.17, 1.18, 1.19   RHEL 7.7, 7.8, 7.9 7.7, 7.8, 7.9 7.7, 7.8, 7.9 7.7, 7.8, 7.9 7.7, 7.8, 7.9   Ubuntu 20.04 20.04 20.04 20.04 20.04   CentOS 7.6, 7.7, 7.8 7.6, 7.7, 7.8 7.6, 7.7, 7.8 7.6, 7.7, 7.8 7.6, 7.7, 7.8   SLES no 15SP2 no no no   OpenShift 4.5, 4.6 4.5, 4.6 4.5, 4.6 4.5, 4.6 4.5, 4.6   Docker EE 3.1 3.1 3.1 3.1 3.1   Google Anthos 1.5 no no no 1.5    CSI Driver Capabilities   Features PowerMax PowerFlex/VxFlexOS Unity PowerScale/Isilon PowerStore     Static Provisioning yes yes yes yes yes   Dynamic Provisioning yes yes yes yes yes   Expand Persistent Volume yes yes yes yes yes   Create VolumeSnapshot yes yes yes yes yes   Create Volume from Snapshot yes yes yes yes yes   Delete Snapshot yes yes yes yes yes   Access Mode RWO/RWX/ROX RWO RWO(FC/iSCSI)\nRWO/RWX/ROX(NFS)\nRWO/RWX/ROX(Raw block FC and iSCSI) RWO/RWX/ROX RWO(FC/iSCSI)\nRWO/RWX/ROX(RawBlock, NFS)   CSI Volume Cloning yes yes yes yes yes   CSI Raw Block Volume yes yes yes no yes   CSI Ephemeral Volume no no yes yes yes   Topology yes yes yes yes yes   Multi-array yes (via Unisphere) no yes (with single driver) no no    Backend Storage Details   Features PowerMax VxFlexOS/PowerFlex Unity Isilon/PowerScale PowerStore     Fibre Channel yes N/A yes N/A yes   iSCSI yes N/A yes N/A yes   NFS N/A N/A yes yes yes   Other N/A ScaleIO protocol N/A N/A N/A   Supported FS ext4 / xfs ext4 / xfs ext3 / ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS   Thin / Thick provisioning yes yes yes N/A yes   Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering\nNFS host IO size\nSnapshot retention duration Access Zone\nNFS version (3 or 4) iSCSI CHAP    ","excerpt":"The CSI Drivers by Dell EMC implement an interface between CSI enabled …","ref":"/csm-docs/v3/dell-csi-driver/","title":"Introduction"},{"body":"Volume Snapshot Feature The CSI PowerFlex driver version 2.0 supports v1 snapshots on Kubernetes 1.20/1.21/1.22.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  Volume Snapshot Class Installation of PowerFlex driver v1.5 and later does not create VolumeSnapshotClass. You can find a sample of a default v1 VolumeSnapshotClass instance in samples/volumesnapshotclass directory. If needed, you can install the default sample. Following is the default sample for v1:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com # Configure what happens to a VolumeSnapshotContent when the VolumeSnapshot object # it is bound to is to be deleted # Allowed values: # Delete: the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. # Retain: both the underlying snapshot and VolumeSnapshotContent remain. deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Create Consistent Snapshot of Group of Volumes This feature extends CSI specification to add the capability to create crash-consistent snapshots of a group of volumes. This feature is available as a technical preview. To use this feature, users have to deploy the csi-volumegroupsnapshotter side-car as part of the PowerFlex driver. Once the sidecar has been deployed, users can make snapshots by using yaml files such as this one:\napiVersion: volumegroup.storage.dell.com/v1alpha2 kind: DellCsiVolumeGroupSnapshot metadata: # Name must be 13 characters or less in length name: \"vg-snaprun1\" namespace: \"helmtest-vxflexos\" spec: # Add fields here driverName: \"csi-vxflexos.dellemc.com\" # defines how to process VolumeSnapshot members when volume group snapshot is deleted # \"retain\" - keep VolumeSnapshot instances # \"delete\" - delete VolumeSnapshot instances memberReclaimPolicy: \"retain\" volumesnapshotclass: \"vxflexos-snapclass\" pvcLabel: \"vgs-snap-label\" # pvcList: # - \"pvcName1\" # - \"pvcName2\" In the metadata section, the name is limited to 13 characters because the snapshotter will append a timestamp to it. Additionally, the pvcLabel field specifies a label that must be present in PVCs that are to be snapshotted. Here is a sample of that portion of a .yaml for a PVC:\nmetadata: name: pvol0 namespace: helmtest-vxflexos labels: volume-group: vgs-snap-label More details about the installation and use of the VolumeGroup Snapshotter can be found here: dell-csi-volumegroup-snapshotter.\nVolume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, which is when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true.\nFollowing is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound  NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the next highest multiple of 8.\n Volume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nCustom File System Format Options The CSI PowerFlex driver version 1.5 and later support additional mkfs format options. A user is able to specify additional format options as needed for the driver. Format options are specified in storageclass yaml under mkfsFormatOption as in the following example:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \u003cSTORAGE_POOL\u003e # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID mkfsFormatOption: \"\u003cmkfs_format_option\u003e\" # Insert file system format option volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e # Insert System ID values: - csi-vxflexos.dellemc.com  WARNING: Before utilizing format options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.  Topology Support The CSI PowerFlex driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\n The PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed.  This Topology support does not include customer-defined topology, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that the pod schedule takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.beta.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2020-05-27T13:24:55Z\" labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \"170198\" selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\n NOTE: In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\n Controller HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\n NOTE: If the controller count is greater than the number of available nodes, excess controller pods will be stuck in a pending state.\n If you are using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file at csi-vxflexos/helm/csi-vxflexos/values.yaml:\nTo assign controller pods to worker nodes only (Default):\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: # - key: \"node-role.kubernetes.io/master\" # operator: \"Exists\" # effect: \"NoSchedule\" To assign controller pods to master and worker nodes:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" To assign controller pods to master nodes only:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\"  NOTE: Tolerations/selectors work the same way for node pods.\n For configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nSDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Kubernetes nodes which run the node portion of the CSI driver. The deployment of the SDC kernel module occurs on these nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\n On Kubernetes nodes which run the node portion of the CSI driver, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the nodes with OS version which supports automatic SDC deployment. If there is an SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On nodes that do not support automatic SDC deployment by SDC init container, manual installation steps must be followed. The SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions. There is no automated uninstallation of the SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from the node.  Multiarray Support The CSI PowerFlex driver version 1.4 added support for managing multiple PowerFlex arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration There is a sample yaml file in the samples folder under the top-level directory called config.yaml with the following content:\n# Username for accessing PowerFlex system.\t- username:\"admin\"# Password for accessing PowerFlex system.\tpassword:\"password\"# System name/ID of PowerFlex system.\tsystemID:\"ID1\"# REST API gateway HTTPS endpoint for PowerFlex system.endpoint:\"https://127.0.0.1\"# Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface.# Allowed values: true or false# Default value: trueskipCertificateValidation:true# indicates if this array is the default array# needed for backwards compatibility# only one array is allowed to have this set to true # Default value: falseisDefault:true# defines the MDM(s) that SDC should register with on start.# Allowed values: a list of IP addresses or hostnames separated by comma.# Default value: none mdm:\"10.0.0.1,10.0.0.2\"- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"Here we specify that we want the CSI driver to manage two arrays: one with an IP 127.0.0.1 and the other with an IP 127.0.0.2.\nTo use this config we need to create a Kubernetes secret from it. To do so, run the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml\nDynamic Array Configuration To update or change any array configuration property, edit the secret. The driver will detect the change automatically and use the new values based on the Kubernetes watcher file change detection time. You can use kubectl command to delete the current secret and create a new secret with changes. For example, refer yaml above and change only the password.\n- username:\"admin\"password:\"Password123\"to\n- username:\"admin\"password:\"Password456\"Below are sample command lines to delete a secret and create modified properties from file secret.yaml.\nkubectl delete secret vxflexos-config -n vxflexos kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=./secret.yaml Dynamic array configuration change detection is only used for properties of an existing array, like username or password. To add a new array to the secret, or to alter an array’s mdm field, you must run csi-install.sh with --upgrade option to update the MDM key in secret and restart the node pods.\ncd \u003cDRIVER-HOME\u003e/dell-csi-helm-installer ./csi-install.sh --upgrade --namespace vxflexos --values ../helm/csi-vxflexos/values.yaml kubectl delete pods --all -n vxflexos Creating storage classes To be able to provision Kubernetes volumes using a specific array, we need to create corresponding storage classes.\nFind the sample yaml files under samples/storageclass. Edit storageclass.yaml if you want ext4 filesystem, and use storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have, and replace \u003cSYSTEM_ID\u003e with the system ID or system name for the array you’d like to use.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl apply -f storageclass.yaml After that, you can use the storage class for the corresponding array.\nEphemeral Inline Volume Starting from version 1.4, CSI PowerFlex driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest (found in csi-vxflexos/test/helm/ephemeral) for creating ephemeral volume in pod manifest with CSI PowerFlex driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumesspec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data0\"name:my-csi-volume- mountPath:\"/data1\"name:my-csi-volume-xfsvolumes:- name:my-csi-volumecsi:driver:csi-vxflexos.dellemc.comfsType:\"ext4\"volumeAttributes:volumeName:\"my-csi-volume\"size:\"8Gi\"storagepool:samplesystemID:sample- name:my-csi-volume-xfscsi:driver:csi-vxflexos.dellemc.comfsType:\"xfs\"volumeAttributes:volumeName:\"my-csi-volume-xfs\"size:\"10Gi\"storagepool:samplesystemID:sampleThis manifest creates a pod and attach two newly created ephemeral inline csi volumes to it, one ext4 and the other xfs.\nTo run the corresponding helm test, go to csi-vxflexos/test/helm/ephemeral and fill in the values for storagepool and systemID in sample.yaml.\nThen run:\n./testEphemeral.sh this test deploys the pod with two ephemeral volumes, and write some data to them before deleting the pod.\nWhen creating ephemeral volumes, it is important to specify the following within the volumeAttributes section: volumeName, size, storagepool, and if you want to use a non-default array, systemID.\nDynamic Logging Configuration The dynamic logging configuration that was introduced in v1.5 of the driver was revamped for v2.0; v1.5 logging configuration is not compatible with v2.0.\nTwo fields in values.yaml (located at helm/csi-vxflexos/values.yaml) are used to configure the dynamic logging: logLevel and logFormat.\n# CSI driver log level # Allowed values: \"error\", \"warn\"/\"warning\", \"info\", \"debug\" # Default value: \"debug\" logLevel: \"debug\" # CSI driver log format # Allowed values: \"TEXT\" or \"JSON\" # Default value: \"TEXT\" logFormat: \"TEXT\" To change the logging fields after the driver is deployed, you can use this command to edit the configmap:\nkubectl edit configmap -n vxflexos vxflexos-config-params\nand then make the necessary adjustments for CSI_LOG_LEVEL and CSI_LOG_FORMAT.\nIf either option is set to a value outside of what is supported, the driver will use the default values of “debug” and “text” .\nSingle Pod Access Mode for PersistentVolumes Kubernetes v1.22 introduced a new ReadWriteOncePod access mode for PersistentVolumes and PersistentVolumeClaims. With this alpha feature, Kubernetes allows you to restrict volume access to a single pod in the cluster.\nTo use this feature you need to enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet by setting command line arguments:\n--feature-gates=\"...,ReadWriteOncePod=true\" Then you can create a new PVC with the access mode. This allows only a single pod to access single-writer-only:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:single-writer-onlyspec:accessModes:- ReadWriteOncePod# Allow only a single pod to access single-writer-only.resources:requests:storage:1Gi","excerpt":"Volume Snapshot Feature The CSI PowerFlex driver version 2.0 supports …","ref":"/csm-docs/docs/csidriver/features/powerflex/","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v1.4/v1.5 to v2.0 using Helm Steps\n Run git clone -b v2.0.0 https://github.com/dell/csi-powerflex.git to clone the git repository and get the v2.0 driver. You need to create config.yaml with the configuration of your system. Check this section in installation documentation: Install the Driver You must set the only system managed in v1.4/v1.5 driver as default in config.json in v2.0 so that the driver knows the existing volumes belong to that system. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  NOTE:\n If you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerFlex version 2.0 driver is not supported on Kubernetes upstream clusters running version 1.18 and 1.19. You must upgrade your cluster to 1.20, 1.21 or 1.22 before attempting to install the new version of the driver.(k8s-1.19 is still supported on openshift-4.6) To update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade. The logging configuration from v1.5 will not work in v2.0, since the log configuration parameters are now set in the values.yaml file located at helm/csi-vxflexos/values.yaml. Please set the logging configuration parameters in the values.yaml file.  Upgrade using Dell CSI Operator:   Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or …","ref":"/csm-docs/docs/csidriver/upgradation/drivers/powerflex/","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v1.3/v1.4 to v1.5 using Helm Steps\n Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository and get the v1.5 driver. You need to create config.json with the configuration of your system. Check this section in installation documentation: Install the Driver You must set the only system managed in v1.4/v1.3 driver as default in config.json in v1.5 so that the driver knows the existing volumes belong to that system. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  NOTE:\n If you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerFlex version 1.5 driver is not supported on Kubernetes upstream clusters running version 1.17. You must upgrade your cluster to 1.19, 1.20, or 1.21 before attempting to install the new version of the driver. To update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  Upgrade using Dell CSI Operator:   Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or …","ref":"/csm-docs/v1/upgradation/drivers/powerflex/","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v1.2/v1.3 to v1.4 using Helm Steps\n Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository and get the v1.4 driver. You need to create config.json with configuration of your system. Check this section in installation documentation: Install the Driver You must set the only system managed in v1.3/v1.2 driver as default in config.json in v1.4 so that the driver know the existing volumes belong to that system. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  Update Driver from pre-v1.2 to v1.4 using Helm A direct upgrade of the driver from an older version pre-v1.2 to version 1.4 is not supported because of breaking changes in Kubernetes APIs in the migration from alpha snapshots to beta snapshots. In order to update the driver in this situation you need to remove alpha snapshot related artifacts.\nSteps\n Before deleting the alpha snapshot CRDs, ensure that their version is v1alpha1 by examining the output of the kubectl get crd command. Delete any VolumeSnapshotClass present in the cluster. Delete all the alpha snapshot CRDs from the cluster by running the following commands: kubectl delete crd volumesnapshotclasses.snapshot.storage.k8s.io kubectl delete crd volumesnapshotcontents.snapshot.storage.k8s.io kubectl delete crd volumesnapshots.snapshot.storage.k8s.io  Uninstall the driver using the csi-uninstall.sh script by running the command: ./csi-uninstall.sh --namespace vxflexos. Install the driver using the steps described in the Installation Using Helm section for the CSI PowerFlex driver.  NOTE:\n If you are upgrading from a driver version which was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerFlex version 1.4 driver is not supported on Kubernetes upstream clusters running version 1.17. You must upgrade your cluster to 1.18, 1.19, or 1.20 before attempting to install the new version of the driver. To update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or …","ref":"/csm-docs/v2/upgradation/drivers/powerflex/","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v1.2 to v1.3 using Helm Steps\n Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository and get the v1.3 driver. Update values file as needed. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  Update Driver from pre-v1.2 to v1.3 using Helm A direct upgrade of the driver from an older version pre-v1.2 to version 1.3 is not supported because of breaking changes in Kubernetes APIs in the migration from alpha snapshots to beta snapshots. In order to update the driver in this situation you need to remove alpha snapshot related artifacts.\nSteps\n Before deleting the alpha snapshot CRDs, ensure that their version is v1alpha1 by examining the output of the kubectl get crd command. Delete any VolumeSnapshotClass present in the cluster. Delete all the alpha snapshot CRDs from the cluster by running the following commands: kubectl delete crd volumesnapshotclasses.snapshot.storage.k8s.io kubectl delete crd volumesnapshotcontents.snapshot.storage.k8s.io kubectl delete crd volumesnapshots.snapshot.storage.k8s.io  Uninstall the driver using the csi-uninstall.sh script by running the command: ./csi-uninstall.sh --namespace vxflexos. Install the driver using the steps described in the Installation Using Helm section for the CSI PowerFlex driver.  NOTE:\n If you are upgrading from a driver version which was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerFlex version 1.3 driver is not supported on Kubernetes upstream clusters running version 1.16. You must upgrade your cluster to 1.17, 1.18, or 1.19 before attempting to install the new version of the driver. To update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option –upgrade, for example: ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or …","ref":"/csm-docs/v3/upgradation/drivers/powerflex/","title":"PowerFlex"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerFlex. The Grafana reference dashboards for PowerFlex metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the sdc_metrics_enabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerflex_export_node_read_bw_megabytes_per_second The export node read bandwidth (MB/s) within PowerFlex system   powerflex_export_node_write_bw_megabytes_per_second The export node write bandwidth (MB/s)   powerflex_export_node_read_latency_milliseconds The time (in ms) to complete read operations within PowerFlex system by the export node   powerflex_export_node_write_latency_milliseconds The time (in ms) to complete write operations within PowerFlex system by the export host   powerflex_export_node_read_iops_per_second The number of read operations performed by an export node (per second)   powerflex_export_node_write_iops_per_second The number of write operations performed by an export node (per second)   powerflex_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s)   powerflex_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s)   powerflex_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume   powerflex_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume   powerflex_volume_read_iops_per_second The number of read operations performed against a volume (per second)   powerflex_volume_write_iops_per_second The number of write operations performed against a volume (per second)    Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the storage_class_pool_metrics_enabled field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerflex_storage_pool_total_logical_capacity_gigabytes The logical capacity (size) of a storage pool (GB)   powerflex_storage_pool_logical_capacity_available_gigabytes The capacity available for use (GB)   powerflex_storage_pool_logical_capacity_in_use_gigabytes The logical capacity of a storage pool in use (GB)   powerflex_storage_pool_logical_provisioned_gigabytes The total size of volumes (thick and thin) provisioned in a storage pool (GB)    ","excerpt":"This section outlines the metrics collected by the Container Storage …","ref":"/csm-docs/docs/observability/metrics/powerflex/","title":"PowerFlex Metrics"},{"body":"Multi Unisphere Support Starting with v1.7, the CSI PowerMax driver can communicate with multiple Unisphere for PowerMax servers to manage multiple PowerMax arrays. In order to use this feature, you must install CSI PowerMax ReverseProxy in StandAlone mode with the driver. For more details on how to configure the driver and ReverseProxy, see the relevant section here\nVolume Snapshot Feature The CSI PowerMax driver version 1.7 and later supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class  To use this feature, enable it in values.yaml\nsnapshot:enabled:true Note: From v1.7, the CSI PowerMax driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the csi-powermax/samples/volumesnapshotclass folder\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powermax-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-restore-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-snapshot-demokind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiCreating PVCs with PVCs as source This is a sample manifest for creating a PVC with another PVC as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-clone-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-pvc-demokind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP With version 1.3.0, support has been added for the unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI. To enable CHAP authentication:\n Create secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u003csymid\u003e -iscsi \u003chost iqn\u003e set chap -cred \u003chost IQN\u003e -secret \u003cCHAP secret\u003e\nWhere is the name of the iSCSI initiator of a host IQN, and is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which is stored in the host initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name With version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell EMC PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this feature, set the following values under customDriverName in my-powermax-settings.yaml.\n Value: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template.  If enabled, the driver name is in the following format: \u003cnamespace\u003e.\u003cdriver name\u003e.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax, then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers To install multiple CSI Drivers for Dell EMC PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions that should be strictly adhered to:\n Only one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations  To install multiple CSI drivers, follow these steps:\n Create (or use) a new namespace. Ensure that all the pre-requisites are met:  powermax-creds secret is created in this namespace (Optional) powermax-certs secret is created in this namespace   Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver.  Volume expansion Starting in v1.4, the CSI PowerMax driver supports the expansion of Persistent Volumes (PVs). This expansion is done online, which is when the PVC is attached to any node.\nTo use this feature, enable in values.yaml\nresizer:enabled:trueTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThis is a sample manifest for a storage class that allows for Volume Expansion.\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true#Set this attribute to true if you plan to expand any PVCscreatedusingthisstorageclassparameters:SYMID:\"000000000001\"SRP:\"DEFAULT_SRP\"ServiceLevel:\"Bronze\"To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC.\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pmax-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:10Gi#Updated size from 5Gi to 10GistorageClassName:powermax-expand-scNOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of the volume, it cannot be used to shrink a volume.\nRaw block support Starting in v1.4, the CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind:StatefulSetapiVersion:apps/v1metadata:name:powermaxtestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powermaxresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy To get the maximum performance out of the CSI driver for PowerMax and Unisphere for PowerMax REST APIs, starting with v1.4 of the driver, you can deploy the optional CSI PowerMax Reverse Proxy application.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server that acts as a reverse proxy for the Unisphere for PowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy application helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and the performance of the CSI PowerMax driver.\nOptionally, you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation The CSI PowerMax Reverse Proxy can be installed in two ways:\n It can be installed as a Kubernetes deployment in the same namespace as the driver. It can be installed as a sidecar to the driver’s controller Pod.  It is also configured as a Kubernetes “NodePort” service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is an HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be an X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nHere is an example showing how to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Using Helm installer In the my-powermax-settings.yaml file, the csireverseproxy section can be used to deploy and configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. If it is enabled (using the enabled parameter in the csireverseproxy section of the my-powermax-settings.yaml file), the install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name “powermax-reverseproxy”. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation for PowerMax.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two ‘%’ characters. String prefixing first ‘%’ and string suffixing second ‘%’ is used as is before and after every node identifier.\nAlso, a new setting, modifyHostName, can be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u003cClusterPrefix\u003e-\u003cHostName\u003e*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1, then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], ‘-’ and ‘_’, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of the controller Pod. At any time, only one controller Pod is active(leader), and the rest are on standby. In case of a failure, one of the standby Pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two-controller Pods are ever scheduled on the same node.\nTo increase or decrease the number of controller Pods, edit the following value in values.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. We recommend not changing this unless it is really necessary. Also, if the controller count is greater than the number of available nodes (where the Pods can be scheduled), some controller Pods will remain in the Pending state\n If you are using dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, see the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment and driver node Daemonset. There are two new sections in the values file - controller and node - where you can specify these values separately for the controller and node Pods.\ncontroller If you want to apply nodeSelectors and tolerations for the controller Pods, edit the controller section in the values file.\nHere are some examples:\n To schedule controller Pods to worker nodes only (Default):  controller:nodeSelector:tolerations: Set the following values for controller Pods to tolerate the taint NoSchedule on master nodes:  controller:nodeSelector:tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\" Set the following values for controller Pods to be scheduled only on nodes labelled master (node-role.kubernetes.io/master):  controller:nodeSelector:node-role.kubernetes.io/master:\"\"tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"node If you want to apply nodeSelectors and tolerations for the node Pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add and remove tolerations to this list\n# \"node\" allows to configure node specific parametersnode:# \"node.nodeSelector\" defines what nodes would be selected for Pods of node daemonset# Leave as blank to use all nodesnodeSelector:# node-role.kubernetes.io/master: \"\"# \"node.tolerations\" defines tolerations that would be applied to node daemonset# Add/Remove tolerations as per requirement# Leave as blank if you wish to not apply any tolerationstolerations:- key:\"node.kubernetes.io/memory-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/disk-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/network-unavailable\"operator:\"Exists\"effect:\"NoExecute\"Topology Support Starting from version 1.5, the CSI PowerMax driver supports topology-aware volume provisioning which helps the Kubernetes scheduler place PVCs on worker nodes that have access to the backend storage. When used with nodeSelectors which can be specified for the driver node Pods, it provides an effective way to provision applications on nodes that have access to the PowerMax array.\nAfter a successful installation of the driver, if a node Pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\n csi-powermax.dellemc.com/\\\u003carray-id\\\u003e If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.iscsi  The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\n NOTE: The Topology support does not include any customer-defined topology, that is, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\n Topology Usage To use the Topology feature, the storage classes must be modified as follows:\n volumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section  For example, a PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-fcparameters:SRP:\"SRP_1\"SYMID:\"000000000001\"ServiceLevel:\u003cServiceLevel\u003e#Insert Service Level Nameprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:WaitForFirstConsumerallowVolumeExpansion:trueallowedTopologies:- matchLabelExpressions:- key:csi-powermax.dellemc.com/000000000001values:- csi-powermax.dellemc.com- key:csi-powermax.dellemc.com/000000000001.fcvalues:- csi-powermax.dellemc.comIn the above example, if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\n A set of sample storage class definitions to enable topology-aware volume provisioning has been provided in the csi-powermax/samples/storageclass folder\n For additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\nDynamic Logging Configuration This feature is introduced in CSI Driver for PowerMax version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in my-powermax-settings.yaml during driver installation.\nTo change the log level dynamically to a different value, the user can edit the same my-powermax-settings.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade Note: my-powermax-settings.yaml is a values.yaml file which the user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level the user can set this field during driver installation.\nTo update the log level dynamically, the user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params ","excerpt":"Multi Unisphere Support Starting with v1.7, the CSI PowerMax driver …","ref":"/csm-docs/docs/csidriver/features/powermax/","title":"PowerMax"},{"body":"You can upgrade CSI Driver for Dell EMC PowerMax using Helm or Dell CSI Operator.\nUpdate Driver from v1.7 to v2.0 using Helm Steps\n Run git clone -b v2.0.0 https://github.com/dell/csi-powermax.git to clone the git repository and get the v2.0 driver. Update the values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade.  NOTE:\n If you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver. To update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade.  Upgrade using Dell CSI Operator:   Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade This command installs the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and later installs to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, see here.  ","excerpt":"You can upgrade CSI Driver for Dell EMC PowerMax using Helm or Dell …","ref":"/csm-docs/docs/csidriver/upgradation/drivers/powermax/","title":"PowerMax"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or Dell CSI Operator.\nUpdate Driver from v1.6 to v1.7 using Helm Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository and get the v1.7 driver. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade.  NOTE:\n If you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver. To update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade.  Upgrade using Dell CSI Operator:   Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or …","ref":"/csm-docs/v1/upgradation/drivers/powermax/","title":"PowerMax"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or Dell CSI Operator.\nUpdate Driver from v1.5 to v1.6 using Helm Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository and get the v1.6 driver. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade.  Update Driver from pre-v1.4 to v1.6 using Helm A rolling upgrade of the driver from an older version to v1.4 is not supported because of breaking changes in Kubernetes APIs in the migration from alpha snapshots to beta snapshots. In order to update the driver in this situation you need to remove alpha snapshot related artifacts.\nSteps\n  Delete any alpha VolumeSnapshot or VolumeSnapshotContent in the cluster.\n  Before deleting the alpha snapshot CRDs, ensure that their version is v1alpha1 by examining the output of the kubectl get crd command.\n  Delete any VolumeSnapshotClass present in the cluster.\n  Delete all the alpha snapshot CRDs from the cluster by running the following commands:\nkubectl delete crd volumesnapshotclasses.snapshot.storage.k8s.io kubectl delete crd volumesnapshotcontents.snapshot.storage.k8s.io kubectl delete crd volumesnapshots.snapshot.storage.k8s.io   Uninstall the driver using the csi-uninstall.sh script by running the command: ./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e where driver-namespace is the namespace where driver is installed.\n  Install the driver using the steps described in the Installation Using Helm section for the CSI PowerMax driver.\n  NOTE:\n If you are upgrading from a driver version which was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerMax version 1.6 driver is not supported on Kubernetes upstream clusters running Kubernetes version 1.17 or lower. You must upgrade your cluster to 1.18, 1.19, or 1.20 before attempting to install the new version of the driver. To update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or …","ref":"/csm-docs/v2/upgradation/drivers/powermax/","title":"PowerMax"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or Dell CSI Operator.\nUpdate Driver from v1.4 to v1.5 using Helm Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository and get the v1.5 driver. Update values file as needed. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade.  Update Driver from pre-v1.4 to v1.5 using Helm A rolling upgrade of the driver from an older version to v1.4 is not supported because of breaking changes in Kubernetes APIs in the migration from alpha snapshots to beta snapshots. In order to update the driver in this situation you need to remove alpha snapshot related artifacts.\nSteps\n  Delete any alpha VolumeSnapshot or VolumeSnapshotContent in the cluster.\n  Before deleting the alpha snapshot CRDs, ensure that their version is v1alpha1 by examining the output of the kubectl get crd command.\n  Delete any VolumeSnapshotClass present in the cluster.\n  Delete all the alpha snapshot CRDs from the cluster by running the following commands:\nkubectl delete crd volumesnapshotclasses.snapshot.storage.k8s.io kubectl delete crd volumesnapshotcontents.snapshot.storage.k8s.io kubectl delete crd volumesnapshots.snapshot.storage.k8s.io   Uninstall the driver using the csi-uninstall.sh script by running the command: ./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e where driver-namespace is the namespace where driver is installed.\n  Install the driver using the steps described in the Installation Using Helm section for the CSI PowerMax driver.\n  NOTE:\n If you are upgrading from a driver version which was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerMax version 1.5 driver is not supported on Kubernetes upstream clusters running Kubernetes version 1.16 or lower. You must upgrade your cluster to 1.17, 1.18, or 1.19 before attempting to install the new version of the driver. To update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option –upgrade, for example: ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or …","ref":"/csm-docs/v3/upgradation/drivers/powermax/","title":"PowerMax"},{"body":"Multicluster support You can connect a single CSI-PowerScale driver with multiple PowerScale clusters.\nPre-Requisites:\n Creation of secret.yaml with credentials related to one or more Clusters. Creation of (at least) one Storage class for each cluster. Creation of custom-volumesnapshot classes with proper isiPath matching corresponding storage classes. Inclusion of cluster name in volume handle, if you want to provision existing static volumes.  Consuming existing volumes with static provisioning You can use existent volumes from the PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, the PowerScale cluster accessZone is assumed as ‘System’, storage class as ‘isilon’, cluster name as ‘pscale-cluster’ and volume’s internal name as ‘isilonvol’. The volume-handle should be in the format of \u003cvolume_name\u003e=_=_=\u003cexport_id\u003e=_=_==_=_=\u003ccluster_name\u003e  apiVersion:v1kind:PersistentVolumemetadata:name:isilonstaticpvnamespace:defaultspec:capacity:storage:5GiaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy:RetainstorageClassName:isiloncsi:driver:csi-isilon.dellemc.comvolumeAttributes:Path:\"/ifs/data/csi/isilonvol\"Name:\"isilonvol\"AzServiceIP:'XX.XX.XX.XX'volumeHandle:isilonvol=_=_=652=_=_=System=_=_=pscale-clusterclaimRef:name:isilonstaticpvcnamespace:defaultCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilonstaticpvcnamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:isilonstaticpvstorageClassName:isilonThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:isilonstaticpvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.  PVC Creation Feature Following yaml content can be used to create a PVC without referring any PV.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:testvolumenamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonVolume Snapshot Feature The CSI PowerScale driver version 2.0 and later supports managing v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller   For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerScale driver version 2.0, no default Volume Snapshot Class will get created.\nFollowing are the manifests for the Volume Snapshot Class:\n VolumeSnapshotClass  # For kubernetes version 20 and above (v1 snaps)apiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:\"isilon-snapclass\"driver:csi-isilon.dellemc.com#The deletionPolicy of a volume snapshot class can either be Retain or Delete#If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object.#If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remaindeletionPolicy:Deleteparameters:#IsiPath should match with respective storageClass IsiPathIsiPath:\"/ifs/data/csi\"The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs; The following snippet assumes that the persistent volume claim name is testvolume.\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvcsnapnamespace:defaultspec:volumeSnapshotClassName:isilon-snapclasssource:persistentVolumeClaimName:testvolumeOnce the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:createfromsnapnamespace:defaultspec:storageClassName:isilondataSource:name:pvcsnapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteManyresources:requests:storage:5GiVolume Expansion The CSI PowerScale driver version 1.2 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:\"false\"provisioner:\"csi-isilon.dellemc.com\"reclaimPolicy:Deleteparameters:ClusterName:\u003cclusterNamespecifiedinsecret.yaml\u003e AccessZone: SystemisiPath:\"/ifs/data/csi\"AzServiceIP :'XX.XX.XX.XX'rootClientEnabled:\"true\"allowVolumeExpansion:truevolumeBindingMode:ImmediateTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilon-pvc-expansion-demospec:accessModes:- ReadWriteOnceresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Volume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing PVC:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:existing-pvcspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonThe following is a sample manifest for cloning:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:volume-from-volumenamespace:defaultspec:accessModes:- ReadWriteManyvolumeMode:Filesystemresources:requests:storage:50GistorageClassName:isilondataSource:kind:PersistentVolumeClaimname:existing-pvcapiGroup:\"\"Controller HA The CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two-controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if the controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in a Pending state.\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nEphemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves as use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumespec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data\"name:my-csi-volumevolumes:- name:my-csi-volumecsi:driver:csi-isilon.dellemc.comvolumeAttributes:size:\"2Gi\"ClusterName:\"cluster1\"This manifest creates a pod in a given cluster and attaches a newly created ephemeral inline CSI volume to it.\nTopology Topology Support The CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage, as a result of which the nodes which have access to PowerScale Array are appropriately labeled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, the CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and use them to initialize node pod with custom PowerScale FQDN/IP.\nNote: Only a single cluster can be configured as part of secret.yaml for custom topology.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that the Pod schedule takes advantage of the topology and the selected node has access to provisioned volumes.\nNote: Whenever a new storage cluster is being added in secret, even though it is dynamic, the new storage cluster IP address-related label is not added to worker nodes dynamically. The user has to spin off (bounce) driver-related pods (controller and node pods) in order to apply newly added information to be reflected in worker nodes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options.# PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon# Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server# Provide mount options through \"mountOptions\" attribute # to create PVCs with mount options.apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilonprovisioner:csi-isilon.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:trueparameters:AccessZone:SystemIsiPath:\"/ifs/data/csi\"# AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP.#AzServiceIP : 192.168.2.1# When a PVC is being created, it takes the storage class' value of \"storageclass.rootClientEnabled\", # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \"Root clients\" field (when true) or \"Clients\" field (when false) of the NFS export RootClientEnabled:\"false\"# Name of PowerScale cluster where pv will be provisioned# This name should match with name of one of the cluster configs in isilon-creds secret# If this parameter is not specified, then default cluster config in isilon-creds secret will be considered if available#ClusterName: \"\u003ccluster_name\u003e\"# volumeBindingMode controls when volume binding and dynamic provisioning should occur.# Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created# WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume# until a Pod using the PersistentVolumeClaim is createdvolumeBindingMode:WaitForFirstConsumer# allowedTopologies helps scheduling pod on worker nodes which match all of below expressions# If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologiesallowedTopologies:- matchLabelExpressions:- key:csi-isilon.dellemc.com/\u003cISILON_IP\u003e values:- csi-isilon.dellemc.commountOptions:[\"\u003cmountOption1\u003e\",\"\u003cmountOption2\u003e\",...,\"\u003cmountOptionN\u003e\"]For additional information, see the Kubernetes Topology documentation.\nSupport custom networks for NFS I/O traffic When allowedNetworks is specified for using custom networks to handle NFS traffic, and a user already has workloads scheduled, there is a possibility that it might lead to backward compatibility issues. For example, ControllerUnPublish might not be able to completely remove clients from the NFS exports of previously created pods.\nAlso, the previous workload will still be using the default network and not custom networks. For previous workloads to use custom networks, the recreation of pods is required.\nWhen csi-powerscale driver creates an NFS export, the traffic flows through the client specified in the export. By default, the client is the network interface for Kubernetes communication (same IP/fqdn as k8s node) by default.\nFor a cluster with multiple network interfaces and if a user wants to segregate k8s traffic from NFS traffic; you can use the allowedNetworks option. allowedNetworks takes CIDR addresses as a parameter to match the IPs to be picked up by the driver to allow and route NFS traffic.\nVolume Limit The CSI Driver for Dell EMC PowerScale allows users to specify the maximum number of PowerScale volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-isilon-volumes-per-node and specifying the volume limit for that node.  kubectl label node \u003cnode_name\u003e max-isilon-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxIsilonVolumesPerNode attribute in values.yaml.\n NOTE: The default value of maxIsilonVolumesPerNode is 0. If maxIsilonVolumesPerNode is set to zero, then CO shall decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxIsilonVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-isilon-volumes-per-node is not set.\n Node selector in helm template Now user can define in which worker node, the CSI node pod daemonset can run (just like any other pod in Kubernetes world.)For more information, refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector\nSimilarly, users can define the tolerations based on various conditions like memory pressure, disk pressure and network availability. Refer to https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#taints-and-tolerations for more information.\nUsage of SmartQuotas to Limit Storage Consumption CSI driver for Dell EMC Isilon handles capacity limiting using SmartQuotas feature.\nTo use the SmartQuotas feature user can specify the boolean value ‘enableQuota’ in myvalues.yaml or my-isilon-settings.yaml.\nLet us assume the user creates a PVC with 3 Gi of storage and ‘SmartQuotas’ have already been enabled in PowerScale Cluster.\n  When ‘enableQuota’ is set to ‘true’\n The driver sets the hard limit of the PVC to 3Gi. The user adds data of 2Gi to the above said PVC (by logging into POD). It works as expected. The user tries to add 2Gi more data. Driver doesn’t allow the user to enter more data as total data to be added is 4Gi and PVC limit is 3Gi. The user can expand the volume from 3Gi to 6Gi. The driver allows it and sets the hard limit of PVC to 6Gi. User retries adding 2Gi more data (which has been errored out previously). The driver accepts the data.    When ‘enableQuota’ is set to ‘false’\n Driver doesn’t set any hard limit against the PVC created. The user adds data of 2Gi to the above said PVC, which is having the size 3Gi (by logging into POD). It works as expected. The user tries to add 2Gi more data. Now the total size of data is 4Gi. Driver allows the user to enter more data irrespective of the initial PVC size (since no quota is set against this PVC) The user can expand the volume from an initial size of 3Gi to 4Gi or more. The driver allows it.    Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerScale version 1.6.0 and updated in version 2.0.0\nHelm based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade Note: here my-isilon-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap isilon-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n isilon isilon-config-params  Note: Prior to CSI Driver for PowerScale version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object.\n NAT Support CSI Driver for Dell EMC PowerScale is supported in the NAT environment.\nConfigurable permissions for volume directory This feature is introduced in CSI Driver for PowerScale version 2.0.0\nHelm based installation The permissions for volume directory can now be configured in 3 ways:\n Through values.yaml Through secrets Through storage class   # isiVolumePathPermissions: The permissions for isi volume directory path # This value acts as a default value for isiVolumePathPermissions, if not specified for a cluster config in secret # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" isiVolumePathPermissions: \"0777\" The permissions present in values.yaml are the default for all cluster config.\nIf the volume permission is not present in storage class then secrets are considered and if it is not present even in secrets then values.yaml is considered.\n Note: For volume creation from source (volume from snapshot/volume from volume) permissions are inherited from source. Create myvalues.yaml/my-isilon-settings.yaml and storage class according to csi-powerscale 2.0\n Operator based installation In the case of operator-based installation, default permission for powerscale directory is present in the samples file.\nOther ways of configuring powerscale volume permissions remain the same as helm-based installation.\n","excerpt":"Multicluster support You can connect a single CSI-PowerScale driver …","ref":"/csm-docs/docs/csidriver/features/powerscale/","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerScale using Helm or Dell CSI Operator.\nUpgrade Driver from version 1.6.0 to 2.0.0 Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps\n  Verify that all pre-requisites to install CSI Driver for Dell EMC PowerScale version 2.0.0 are fulfilled. Note that change in secret format should be taken care.\n Delete the existing secret (isilon-creds and isilon-certs-0) Create new secrets (isilon-creds and isilon-certs-0) Refer Installation section here.    Clone the repository using git clone -b v2.0.0 https://github.com/dell/csi-powerscale.git, copy the helm/csi-isilon/values.yaml into a new location with a custom name say my-isilon-settings.yaml, to customize settings for installation. Edit my-isilon-settings.yaml as per the requirements.\n  Change to directory dell-csi-helm-installer to install the Dell EMC PowerScale cd dell-csi-helm-installer\n  Upgrade the CSI Driver for Dell EMC PowerScale version 2.0.0 using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade\n  Upgrade using Dell CSI Operator: Note: While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes.\nTo upgrade the driver from version 1.6.0 to 2.0.0:\nNote: It is highly recommended to take Backup of existing storage class definition and volumesnapshot class definition, yaml files before the upgrade.\n  Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade . This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerScale using Helm or …","ref":"/csm-docs/docs/csidriver/upgradation/drivers/isilon/","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerScale using Helm or Dell CSI Operator.\nUpgrade Driver from version 1.5.0 to 1.6.0 Steps\n  Verify that all pre-requisites to install CSI Driver for Dell EMC PowerScale version 1.6.0 are fulfilled. Note that change in secret format should be taken care.\n Delete the existing secret (isilon-creds and isilon-certs-0) Create new secrets (isilon-creds and isilon-certs-0) Refer Installation section here.    Clone the repository https://github.com/dell/csi-powerscale , copy the helm/csi-isilon/values.yaml into a new location with a custom name say my-isilon-settings.yaml, to customize settings for installation. Edit my-isilon-settings.yaml as per the requirements.\n  Change to directory dell-csi-helm-installer to install the Dell EMC PowerScale cd dell-csi-helm-installer\n  Upgrade the CSI Driver for Dell EMC PowerScale version 1.6.0 using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade\n  Upgrade using Dell CSI Operator: To upgrade the driver from version 1.5.0 to 1.6.0:\nNote: It is highly recommended to take Backup of existing storage class definition and volumesnapshot class definition, yaml files before the upgrade.\n  Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade . This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerScale using Helm or …","ref":"/csm-docs/v1/upgradation/drivers/isilon/","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUpgrade Driver from version v1.4.0 to v1.5.0 Steps\n  Verify that all pre-requisites to install CSI Driver for Dell EMC PowerScale v1.5.0 are fulfilled (including change in secret formats).\n 1.1 Delete the existing secrets (isilon-creds and isilon-certs) 1.2 Create new secrets (isilon-creds and isilon-certs-0) in the format specified by csi-powerscale 1.5.  Refer Installation section here.\n  Clone the repository https://github.com/dell/csi-powerscale , copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation. Edit my-isilon-settings.yaml as per the requirements.\n  Change to directory dell-csi-helm-installer to install the Dell EMC PowerScale cd dell-csi-helm-installer\n  Upgrade the CSI Driver for Dell EMC PowerScale v1.5.0 using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade\n  Upgrade using Dell CSI Operator: To upgrade the driver from csi-PowerScale v1.4 to csi-PowerScale v1.5 (OpenShift 4.6) :\n Clone operator version 1.3.0 Execute bash scripts/install.sh --upgrade .This command will install latest version of operator. Uninstall the existing driver by executing the command kubectl delete -f \u003cdriver.yaml\u003e with appropriate yaml file used for csi-powerscale 1.4 installation. Delete the existing secrets (both isilon-creds and isilon-certs) Create new isilon-creds secret in the latest csi-PowerScale format. For additional information, refer here Create new isilon-certs secret. Make sure the name of new secret is isilon-certs-0. For additional information, refer here Furnish the sample CR yaml according to your environment. Install csi-PowerScale driver 1.5 by executing the following command: kubectl create -f \u003cfurnished-cr.yaml\u003e  The above said steps are for Operator which was deployed in non-olm way.\nFor additional information, refer Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell …","ref":"/csm-docs/v2/upgradation/drivers/isilon/","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUpgrade Driver from version v1.3.0/v1.3.0.1 to v1.4.0 Steps\n  Verify that all pre-requisites to install CSI Driver for DELL EMC PowerScale v1.4.0 are fulfilled.\n  Clone the repository https://github.com/dell/csi-powerscale , copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation. Edit my-isilon-settings.yaml as per the requirements.\n  Change to directory dell-csi-helm-installer to install the DELL EMC PowerScale cd dell-csi-helm-installer\n  Upgrade the CSI Driver for DELL EMC PowerScale v1.4.0 using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade\n  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell …","ref":"/csm-docs/v3/upgradation/drivers/isilon/","title":"PowerScale"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command creates a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset are created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset’s pods by running kubectl get pods -n testpowerstore\nThe pod must be Ready and Running\n If Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run, use the command:\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u003cpowerstore.api.ip\u003e/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:existingvolspec:accessModes:- ReadWriteOncecapacity:storage:30Gicsi:driver:csi-powerstore.dellemc.comvolumeHandle:0055558c-5ae1-4ed1-b421-6f5a9475c19fpersistentVolumeReclaimPolicy:RetainstorageClassName:powerstorevolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30GistorageClassName:powerstoreThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:quay.io/centos/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:pvolAfter the pod is Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerStore driver version 2.0.0 supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class   Note: From v1.4, the CSI PowerStore driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the samples folder\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powerstore-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueSnapshot feature is optional for the installation CSI PowerStore driver version 1.4 makes the snapshot feature optional for the installation.\nTo enable or disable this feature, change values.snapshot.enable parameter to true or false, specify the following in values.yaml to enable this feature\nsnapshot:enable:trueExternal Snapshotter and its CRDs are not installed even if the Snapshot feature is enabled. These have to be installed manually before the installation.\nDisabling the Snapshot feature will opt out of the snapshotter sidecar from the installation.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP The CSI PowerStore driver Version 1.3.0 and later extends Challenge Handshake Authentication Protocol (CHAP) support by adding automatic credentials generation.\nThis means that you no longer need to provide chapsecret/chapuser credentials, they will be automatically generated by the driver for each host.\nTo enable this feature you need to set connection.enableCHAP to true when installing with helm or set X_CSI_POWERSTORE_ENABLE_CHAP to true in your PowerStore CustomResource when installing using operator.\nThe driver uses the generated chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating a new host on powerstore array driver will populate host chap credentials with generated values. When re-using already existing hosts driver must override existing CHAP credentials with newly generated ones.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pstore-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:powerstoretestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powerstoreresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvol0namespace:testpowerstorespec:storageClassName:powerstoreaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clonedpvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI PowerStore driver version 1.2 and later supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind:PodapiVersion:v1metadata:name:powerstore-inline-volumespec:containers:- name:test-containerimage:quay.io/centos/centoscommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"20Gi\"This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI PowerStore driver version 1.2 and later introduces the controller HA feature. Instead of StatefulSet, controller pods are deployed as a Deployment.\nBy default number of replicas is set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in the cluster, each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods must be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As mentioned earlier, you can configure where node driver pods would be assigned in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they must use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology features user must create their own storage classes similar to those that can be found in samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-fcprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/127.0.0.1-fcvalues:- \"true\"This example matches all nodes where the driver has a connection to PowerStore with an IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS and iSCSI.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels\n Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work.\n For any additional information about the topology, see the Kubernetes Topology documentation.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as a Host on the storage array before. It will check if Host initiators and node initiators (FC or iSCSI) match. If they do, the driver will not create a new host and will take the existing name of the Host as nodeID.\nMultiarray support The CSI PowerStore driver version 1.3.0 and later support managing multiple PowerStore arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration Create a file called config.yaml and populate it with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIglobalID:\"unique\"# global ID to identify arrayusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIskipCertificateValidation:true# use insecure connection or notdefault:true# treat current array as a default (would be used by storage classes without arrayIP parameter)blockProtocol:\"ISCSI\"# what SCSI transport protocol use on node side (FC, ISCSI, None, or auto)nasName:\"nas-server\"# what NAS must be used for NFS volumes- endpoint:\"https://10.0.0.2/api/rest\"globalID:\"unique\"username:\"user\"password:\"password\"skipCertificateValidation:trueblockProtocol:\"FC\"Here we specify that we want to CSI driver to manage two arrays: one with an IP 10.0.0.1 and the other with an IP 10.0.0.2, we want to connect to the first array with iSCSI protocol and with FC to the second array. Also, we want to be able to create NFS-based volume so we provide the name of the NAS to the first array.\nTo use this config we need to create a Kubernetes secret from it, to do so create a file called secret.yaml in the same folder and populate it with the following content:\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLApply the secret by running following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Creating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nCreate file storageclass.yaml and populate it with the following content:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-1provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayID:\"GlobalUniqueID\"FsType:\"ext4\"---apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-2provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayID:\"GlobalUniqueID\"FsType:\"xfs\"Here we specify two storage classes: one of them uses the first array and ext4 filesystem, and the other uses the second array and xfs filesystem.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use powerstore-1 storage class to create volumes on the first array and powerstore-2 storage class to create volumes on the second array.\nDynamic secret change detection CSI PowerStore driver version 1.3.0 and later supports the ability to detect changes to array configuration Kubernetes secret. This essentially means that you can change credentials for your PowerStore arrays in-flight (without restarting the driver).\nTo do so just change your configuration file config.yaml and apply it again using the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - After Kubernetes remounts secret to driver containers (this usually takes around one minute), a driver must detect the change and start using this new configuration information.\nConfiguring custom access to NFS exports CSI PowerStore driver Version 1.3.0 and later supports the ability to configure NFS access to nodes that use dedicated storage networks.\nTo enable this feature you need to specify externalAccess parameter in your helm values.yaml file or X_CSI_POWERSTORE_EXTERNAL_ACCESS variable when creating CustomResource using an operator.\nThe value of that parameter is added as an additional entry to NFS Export host access.\nFor example the following notation:\nexternalAccess:\"10.0.0.0/24\"This means that we allow for NFS Export created by driver to be consumed by address range 10.0.0.0-10.0.0.255.\nArray identification based on GlobalID CSI PowerStore driver version 1.4.0 onwards slightly changes the way arrays are being identified in runtime. In previous versions of the driver, a management IP address was used to identify an array. The address change could lead to an invalid state of PV. From version 1.4.0 a unique GlobalID string is used for an array identification. It has to be specified in config.yaml and in Storage Classes.\nThe change provides backward compatibility with previously created PVs. However, to provision new volumes, make sure to delete old Storage Classes and create new ones with arrayID instead of arrayIP specified.\n NOTE: It is recommended to migrate the PVs to new identifiers before changing management IPs of storage systems. The recommended way to do it is to clone the existing volume and delete the old one. The cloned volume will automatically switch to using globalID instead of management IP.\n Root squashing CSI PowerStore driver version 1.4.0 and later allows users to enable root squashing for NFS volumes provisioned by the driver.\nRoot squashing rule prevents root users on NFS clients from exercising root privileges on the NFS server.\nTo enable this rule, you need to set parameter allowRoot to false in your NFS storage class.\nYour storage class definition must look similar to this:\napiVersion:storage.k8s.io/v1kind:StorageClass...parameters:...allowRoot:\"false\"# enables or disables root squashing The 1.4 version and later of the driver also enables any container user, to have full access to provisioned NFS volume, in earlier versions only root user had access\n Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params NAT Support CSI Driver for Dell EMC Powerstore is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\n","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/csm-docs/docs/csidriver/features/powerstore/","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or Dell CSI Operator.\nUpdate Driver from v1.4 to v2.0 using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps\n  Run git clone -b v2.0.0 https://github.com/dell/csi-powerstore.git to clone the git repository and get the v2.0 driver.\n  Edit helm/config.yaml file and configure connection information for your PowerStore arrays changing the following parameters:\n endpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what SCSI transport protocol we should use (FC, ISCSI, None, or auto). nasName: defines what NAS should be used for NFS volumes.  Add more blocks similar to above for each PowerStore array if necessary.\n  (optional) create new storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n Storage classes created by v1.4 driver will not be deleted, v2.0 driver will use default array to manage volumes provisioned with old storage classes. Thus, if you still have volumes provisioned by v1.4 in your cluster then be sure to include the same array you have used for the v1.4 driver and make it default in the config.yaml file.\n   Create the secret by running kubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml\n  Copy the default values.yaml file cp ./helm/csi-powerstore/values.yaml ./dell-csi-helm-installer/my-powerstore-settings.yaml and update parameters as per the requirement.\n  Run the csi-install script with the option --upgrade by running: ./dell-csi-helm-installer/csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade.\n  Upgrade using Dell CSI Operator: Note: While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes.\n  Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.5.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or …","ref":"/csm-docs/docs/csidriver/upgradation/drivers/powerstore/","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or Dell CSI Operator.\nUpdate Driver from v1.3 to v1.4 using Helm Steps\n  Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository and get the v1.4 driver.\n  Edit helm/config.yaml file and configure connection information for your PowerStore arrays changing the following parameters:\n endpoint: defines the full URL path to the PowerStore API. username, password: defines credentials for connecting to array. insecure: defines if we should use insecure connection or not. default: defines if we should treat the current array as a default. block-protocol: defines what SCSI transport protocol we must use (FC, ISCSI, None, or auto). nas-name: defines what NAS should be used for NFS volumes.  Add more blocks similar to above for each PowerStore array if necessary.\n  (optional) create new storage classes using ones from helm/samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n Storage classes created by v1.3 driver will not be deleted, v1.4 driver will use default array to manage volumes provisioned with old storage classes. Thus, if you still have volumes provisioned by v1.3 in your cluster then be sure to include the same array you have used for the v1.3 driver and make it default in the config.yaml file.\n   Create the secret by running sed \"s/CONFIG_YAML/`cat helm/config.yaml | base64 -w0`/g\" helm/secret.yaml | kubectl apply -f -\n  Update values file as needed.\n  Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade.\n  Upgrade using Dell CSI Operator:   Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the drive, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or …","ref":"/csm-docs/v1/upgradation/drivers/powerstore/","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or Dell CSI Operator.\nUpdate Driver from v1.2 to v1.3 using Helm Steps\n  Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository and get the v1.3 driver.\n  Edit helm/config.yaml file and configure connection information for your PowerStore arrays changing following parameters:\n endpoint: defines the full URL path to the PowerStore API. username, password: defines credentials for connecting to array. insecure: defines if we should use insecure connection or not. default: defines if we should treat the current array as a default. block-protocol: defines what SCSI transport protocol we must use (FC, ISCSI, None, or auto). nas-name: defines what NAS should be used for NFS volumes.  Add more blocks similar to above for each PowerStore array if necessary.\n  (optional) create new storage classes using ones from helm/samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n Storage classes created by v1.2 driver will not be deleted, v1.3 driver will use default array to manage volumes provisioned with old storage classes. Thus, if you still have volumes provisioned by v1.2 in your cluster then be sure to include same array you have used for v1.2 driver and make it default in config.yaml file.\n   Create the secret by running sed \"s/CONFIG_YAML/`cat helm/config.yaml | base64 -w0`/g\" helm/secret.yaml | kubectl apply -f -\n  Update values file as needed.\n  Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade.\n  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or …","ref":"/csm-docs/v2/upgradation/drivers/powerstore/","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or Dell CSI Operator.\nUpdate Driver from v1.1 to v1.2 using Helm Steps\n Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository and get the v1.2 driver. Update values file as needed. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or …","ref":"/csm-docs/v3/upgradation/drivers/powerstore/","title":"PowerStore"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerStore. The Grafana reference dashboards for PowerStore metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the karaviMetricsPowerstore.volumeMetricsEnabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerstore_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s)   powerstore_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s)   powerstore_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume   powerstore_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume   powerstore_volume_read_iops_per_second The number of read operations performed against a volume (per second)   powerstore_volume_write_iops_per_second The number of write operations performed against a volume (per second)   powerstore_filesystem_read_bw_megabytes_per_second The filesystem read bandwidth MB/s   powerstore_filesystem_write_bw_megabytes_per_second The filesystem write bandwidth (MB/s)   powerstore_filesystem_read_iops_per_second The number of read operations performed against a filesystem (per second)   powerstore_filesystem_write_iops_per_second The number of write operations performed against a filesystem (per second)   powerstore_filesystem_read_latency_milliseconds The time (in ms) to complete read operations to a filesystem   powerstore_filesystem_write_latency_milliseconds The time (in ms) to complete write operations to a filesystem    Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the enable_powerstore_metrics field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerstore_array_logical_provisioned_megabytes Total provisioned logical storage on a given array managed by CSI driver   powerstore_array_logical_used_megabytes Total used logical storage on a given array   powerstore_storage_class_logical_provisioned_megabytes Total provisioned logical storage for a given storage class   powerstore_storage_class_logical_used_megabytes Total used logical storage for a given storage class   powerstore_volume_logical_provisioned_megabytes Logical provisioned storage for a volume   powerstore_volume_logical_used_megabytes Logical used storage for a volume   powerstore_filesystem_logical_provisioned_megabytes Logical provisioned storage for a filesystem   powerstore_filesystem_logical_used_megabytes Logical used storage for a filesystem    ","excerpt":"This section outlines the metrics collected by the Container Storage …","ref":"/csm-docs/docs/observability/metrics/powerstore/","title":"PowerStore Metrics"},{"body":"Creating volumes and consuming them Create a file sample.yaml using sample yaml files located at test/sample.yaml\nThe following command creates a statefulset that consumes three volumes of default storage classes:\nkubectl create -f test/sample.yaml After executing this command 3 PVC and statefulset are created in the test-unity namespace. You can check created PVCs by running kubectl get pvc -n test-unity and check statefulset’s pods by running kubectl get pods -n test-unity command. The pod should be Ready and Running.\n If Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f test/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\n Open your volume in Unity Management UI (Unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:static-1annotations:pv.kubernetes.io/provisioned-by:csi-unity.dellemc.comspec:accessModes:- ReadWriteOncecapacity:storage:5Gicsi:driver:csi-unity.dellemc.comvolumeHandle:existingvol-\u003cprotocol\u003e-\u003carray_id\u003e-\u003cvolume-id\u003e persistentVolumeReclaimPolicy: RetainclaimRef:namespace:defaultname:static-pvc1storageClassName:unityvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:static-pvc1spec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:static-1storageClassName:unityThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:static-pvc1After the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of the CSI Unity 2.0 driver, a Volume Snapshot Class is not created and need to create Volume Snapshot Class.\nFollowing is the manifest to create Volume Snapshot Class :\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:unity-snapclassdriver:csi-unity.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:test-unityspec:volumeSnapshotClassName:unity-snapclasssource:persistentVolumeClaimName:pvolOnce the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueNote : For CSI Driver for Unity version 1.6 and later, dell-csi-helm-installer does not create any Volume Snapshot classes as part of the driver installation. A set of annotated volume snapshot class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:test-unityspec:storageClassName:unity-iscsidataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiVolume Expansion The CSI Unity driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:unity-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support The CSI Unity driver version 1.4 and later supports Raw Block Volumes. Raw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. The following is an example configuration:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:rawblockpvcnamespace:defaultspec:accessModes:- ReadWriteOncevolumeMode:Blockresources:requests:storage:5GistorageClassName:unity-iscsiapiVersion:v1kind:Podmetadata:name:rawblockpodnamespace:defaultspec:containers:- name:task-pv-containerimage:nginxports:- containerPort:80name:\"http-server\"volumeDevices:- devicePath:/usr/share/nginx/html/devicename:nov-eleventh-1-pv-storagevolumes:- name:nov-eleventh-1-pv-storagepersistentVolumeClaim:claimName:rawblockpvcAccess modes allowed are ReadWriteOnce and ReadWriteMany. Raw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device.\nRaw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size. Access mode ReadOnlyMany is not supported with raw block since we cannot restrict volumes to be readonly from Unity.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:vol0namespace:test-unityspec:storageClassName:unity-nfsaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:cloned-pvcnamespace:test-unityspec:storageClassName:unity-nfsdataSource:name:vol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI Unity driver version 1.4 and later supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity driver.\nkind:PodapiVersion:v1metadata:name:test-unity-ephemeral-volumespec:containers:- name:test-containerimage:busyboxcommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"10Gi\"arrayId:APM************protocol:iSCSIthinProvisioned:\"true\"isDataReductionEnabled:\"false\"tieringPolicy:\"1\"storagePool:pool_2This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI Unity driver version 1.4 and later supports the controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default, number of replicas is set to 2, you can set the controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator you can change the replicas parameter in the spec.driver section in your Unity Custom Resource.\nWhen multiple replicas of controller pods are in a cluster each sidecar (Attacher, Provisioner, Resizer, and Snapshotter) tries to get a lease so only one instance of each sidecar is active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As said before you can configure where node driver pods would be assigned in a similar way in the node section of myvalues.yaml\nTopology The CSI Unity driver version 1.4 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage User can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-topology-fcprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/\u003carray_id\u003e-fcvalues:- \"true\"This example matches all nodes where the driver has a connection to the Unity array with array ID mentioned via Fiber Channel. Similarly, by replacing fc with iscsi in the key checks for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\n Note that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work properly.\n For any additional information about the topology, see the Kubernetes Topology documentation.\nSupport for SLES 15 SP2 The CSI Driver for Dell EMC Unity requires the following set of packages installed on all worker nodes that run on SLES 15 SP2.\n open-iscsi open-iscsi is required in order to make use of iSCSI protocol for provisioning nfs-utils nfs-utils is required in order to make use of NFS protocol for provisioning multipath-tools multipath-tools is required in order to make use of FC and iSCSI protocols for provisioning  After installing open-iscsi, ensure “iscsi” and “iscsid” services have been started and /etc/isci/initiatorname.iscsi is created and has the host initiator id. The pre-requisites are mandatory for provisioning with the iSCSI protocol to work.\nVolume Limit The CSI Driver for Dell EMC Unity allows users to specify the maximum number of Unity volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-unity-volumes-per-node and specifying the volume limit for that node.  kubectl label node \u003cnode_name\u003e max-unity-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxUnityVolumesPerNode attribute in values.yaml file.\n NOTE: To reflect the changes after setting the value either via node label or in values.yaml file, user has to bounce the driver controller and node pods using the command kubectl get pods -n unity --no-headers=true | awk '/unity-/{print $1}'| xargs kubectl delete -n unity pod. If the value is set both by node label and values.yaml file then node label value will get the precedence and user has to remove the node label in order to reflect the values.yaml value. The default value of maxUnityVolumesPerNode is 0. If maxUnityVolumesPerNode is set to zero, then CO SHALL decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxUnityVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-unity-volumes-per-node is not set.\n NAT Support CSI Driver for Dell EMC Unity is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nDynamic Logging Configuration This feature is introduced in CSI Driver for unity version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params  Note: Prior to CSI Driver for unity version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object.\n ","excerpt":"Creating volumes and consuming them Create a file sample.yaml using …","ref":"/csm-docs/docs/csidriver/features/unity/","title":"Unity"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUsing Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nPreparing myvalues.yaml is the same as explained in the install section.\nTo upgrade the driver from csi-unity v1.6 to csi-unity 2.0 (across K8S 1.20, K8S 1.21, K8S 1.22).\n Get the latest csi-unity 2.0 code from Github. Create myvalues.yaml according to csi-unity 2.0 . Clone the repository using git clone -b v2.0.0 https://github.com/dell/csi-unity.git, copy the helm/csi-unity/values.yaml to the new location csi-unity/dell-csi-helm-installer with name say myvalues.yaml, to customize settings for installation edit myvalues.yaml as per the requirements. Navigate to common-helm-installer folder and execute the following command: ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade  Note:\n User has to re-create existing custom-storage classes (if any) according to the latest (v2.0) format. User has to create Volumesnapshotclass after upgrade for taking Snapshots. Secret.yaml files can be updated according to Multiarray Normalization parameters only after upgrading the driver.  Using Operator Note: While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes.\nTo upgrade the driver from csi-unity v1.6 to csi-unity v2.0 (OpenShift 4.6/4.7/4.8) :\n  Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell …","ref":"/csm-docs/docs/csidriver/upgradation/drivers/unity/","title":"Unity"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUsing Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nPreparing myvalues.yaml is the same as explained in the install section.\nTo upgrade the driver from csi-unity v1.5 to csi-unity 1.6 (across K8S 1.19, K8S 1.20, K8S 1.21).\n Get the latest csi-unity 1.6 code from Github. Create myvalues.yaml according to csi-unity 1.6 . Clone the repository https://github.com/dell/csi-unity , copy the helm/csi-unity/values.yaml to the new location csi-unity/dell-csi-helm-installer with name say myvalues.yaml, to customize settings for installation edit myvalues.yaml as per the requirements. Navigate to common-helm-installer folder and execute the following command: ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade  Note:\n User has to re-create existing custom-storage classes (if any) according to the latest (v1.6) format. User has to create Volumesnapshotclass after upgrade for taking Snapshots. Secret.json/Secret.yaml files can be updated according to Multiarray Normalization parameters only after upgrading the driver.  Using Operator Note: While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes.\nTo upgrade the driver from csi-unity v1.5 to csi-unity v1.6 (OpenShift 4.6/4.7) :\n  Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell …","ref":"/csm-docs/v1/upgradation/drivers/unity/","title":"Unity"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUsing Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nPreparing myvalues.yaml is the same as explained in install section.\nTo upgrade the driver from csi-unity v1.4 to csi-unity 1.5 (across K8S 1.18, K8S 1.19, K8S 1.20).\n  Get the latest csi-unity 1.5 code from Github.\n  Create myvalues.yaml according to csi-unity 1.5 .\n  Delete the existing default storage classes of csi-unity 1.4 .\n  Clone the repository https://github.com/dell/csi-unity , copy the helm/csi-unity/values.yaml to the new location csi-unity/dell-csi-helm-installer with name say myvalues.yaml, to customize settings for installation edit myvalues.yaml as per the requirements.\n  Navigate to common-helm-installer folder and execute the following command: ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade\n  If the value of ‘createStorageClassesWithTopology’ is set to “true” in myvalues.yaml , then\n Check the default storage classes, VolumeBindingMode should be ‘WaitForFirstConsumer’.    Note: User has to re-create existing custom-storage classes (if any) according to latest (v1.5) format.\nUsing Operator Note: While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes.\nTo upgrade the driver from csi-unity v1.4 to csi-unity v1.5 (OpenShift 4.6) :\n  Clone operator version 1.3.0\n  Execute bash scripts/install.sh --upgrade This command will install latest version of operator.\n  Furnish the sample CR yaml according to your environment.\n  For upgrading the csi-unity driver execute the following command:\nkubectl apply -f \u003cfurnished-cr.yaml\u003e\n  ","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell …","ref":"/csm-docs/v2/upgradation/drivers/unity/","title":"Unity"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUpdate Driver from v1.3 to v1.4 using Helm Steps\n Run git clone https://github.com/dell/csi-unity.git to clone the git repository and get the v1.3 driver. Update values file as needed. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace unity --values ./my-values.yaml --upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell …","ref":"/csm-docs/v3/upgradation/drivers/unity/","title":"Unity"},{"body":"Replication design and architecture Container Storage Modules (CSM) for Replication project consists of the following components:\n DellCSIReplicationGroup - A Kubernetes Custom Resource CSM Replication controller which replicates the resources across(or within) Kubernetes clusters. CSM Replication sidecar container which is part of the CSI driver controller pod repctl - Multi cluster Kubernetes client for managing replication related objects  DellCSIReplicationGroup DellCSIReplicationGroup (RG) is a cluster scoped Custom Resource that represents a protection group on the backend storage array. It is used to group volumes with the same replication related properties together. DellCSIReplicationGroup's spec contains an action field which can be used to perform replication related operations on the backing protection groups on the storage arrays. This includes operations like Failover, Reprotect, Suspend, Synchronize e.t.c. Any replication related operation is always carried out on all the volumes present in the group.\nSpecification kind:DellCSIReplicationGroupapiVersion:replication.storage.dell.com/v1alpha1metadata:name:rg-e6be24c0-145d-4b62-8674-639282ebdd13spec:driverName:driver.dellemc.com# Name of the CSI driver (same as provisioner name in StorageClass)action:\"\"# Name of the replication action to be performed on the protection groupprotectionGroupAttributes:localAttributeKey:valueprotectionGroupId:protection-group-id# Identifier of the backing protection group on the Storage ArrayremoteClusterId:tgtClusterID# A unique identifier for the remote Kubernetes ClusterremoteProtectionGroupAttributes:remoteAttributeKey:valueremoteProtectionGroupId:csi-rep-sg-test-5-ASYNC# Identifier for the protection group on the remote Storage ArrayStatus The status sub resource of DellCSIReplicationGroup contains information about the state of replication \u0026 any actions which have been performed on the object.\n   Field Description     state State of the Custom Resource   replicationLinkState State of the replication on the storage arrays   lastAction Result of the last performed action   conditions List of recent conditions the CR instance has gone through    status:conditions:- condition:ActionREPROTECT_REMOTEsucceededtime:\"2021-08-11T12:22:05Z\"- condition:ReplicationLinkState:IsSourcechangedfrom(true)to(false)time:\"2021-08-11T12:18:50Z\"- condition:ActionFAILOVER_REMOTEsucceededtime:\"2021-08-11T12:18:50Z\"lastAction:condition:ActionREPROTECT_REMOTEsucceededtime:\"2021-08-11T12:22:05Z\"replicationLinkState:isSource:falselastSuccessfulUpdate:\"2021-08-11T17:18:12Z\"state:Synchronizedstate:ReadyHere is a diagram representing how the state of the CustomResource changes based on actions CSM Replication sidecar CSM Replication sidecar is deployed as sidecar container in the CSI driver controller pod. This container is similar to Kubernetes CSI Sidecar containers and runs a Controller Manager which manages the following controllers -\n PersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller  The PV \u0026 PVC controllers watch for PV/PVC creation events and use dell-csi-extensions APIs to communicate with the CSI Driver controller plugin to discover/create replication enabled volumes and protection groups on the backend storage array. The PersistentVolume controller then uses these details to create DellCSIReplicationGroup objects in the cluster. These controllers are also responsible for associating the PV \u0026 PVC objects with DellCSIReplicationGroup objects. This association is established by applying annotations \u0026 labels on the PV \u0026 PVC objects.\nThe RG controller manages DellCSIReplicationGroup instances and processes any change requests. It is primarily responsible for the following:\n Perform actions on the protection groups Monitor status of replication Updates to the status sub resource  CSM Replication Controller CSM Replication Controller is a Kubernetes application deployed independently of CSI drivers and is responsible for the communication between Kubernetes clusters.\nThe details about the clusters it needs to connect to are provided in the form of a ConfigMap with references to secrets containing the details(KubeConfig/ServiceAccount tokens) required to connect to the respective clusters.\nIt consists of Controller Manager which manages the following controllers:\n PersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller  The PV controller is responsible for creating PV objects (representing the replicated volumes on the backend storage array) in the remote Kubernetes cluster. This controller also enables deletion of the remote PV object in case it is desired by propagating the deletion request across clusters.\nSimilarly, the RG controller is responsible for creating RG objects in the remote Kubernetes cluster. These RG objects represent the remote protection groups on the backend storage array. This controller can also propagate the deletion request of RG objects across clusters.\nBoth the PV \u0026 RG objects in the remote cluster have extra metadata associated with them in form of annotations \u0026 labels. This metadata includes information about the respective objects in the source cluster.\nThe PVC objects are never replicated across the clusters. Instead, the remote PV objects have annotations related to the source PVC objects. This information can be easily used to create the PVCs whenever required using repctl or even kubectl\nSupported Cluster Topologies Click here for details for the various types of supported cluster topologies\n","excerpt":"Replication design and architecture Container Storage Modules (CSM) …","ref":"/csm-docs/docs/replication/architecture/","title":"Architecture"},{"body":"This section outlines the deployment steps for Container Storage Modules (CSM) for Authorization. The deployment of CSM for Authorization is handled in 2 parts:\n Deploying the CSM for Authorization proxy server, to be controlled by storage administrators Configuring one to many supported Dell EMC CSI drivers with CSM for Authorization  Prerequisites The CSM for Authorization proxy server requires a Linux host with the following minimum resource allocations:\n 32 GB of memory 4 CPU 200 GB local storage  Deploying the CSM Authorization Proxy Server The first part deploying CSM for Authorization is installing the proxy server. This activity and the administration of the proxy server will be owned by the storage administrator.\nThe CSM for Authorization proxy server is installed using a single binary installer.\nSingle Binary Installer The easiest way to obtain the single binary installer RPM is directly from the GitHub repository’s releases section.\nThe single binary installer can also be built from source by cloning the GitHub repository and using the following Makefile targets to build the installer:\nmake dist build-installer rpm The build-installer step creates a binary at bin/deploy and embeds all components required for installation. The rpm step generates an RPM package and stores it at deploy/rpm/x86_64/. This allows CSM for Authorization to be installed in network-restricted environments.\nA Storage Administrator can execute the installer or rpm package as a root user or via sudo.\nInstalling the RPM   Before installing the rpm, some network and security configuration inputs need to be provided in json format. The json file should be created in the location $HOME/.karavi/config.json having the following contents:\n{ \"web\": { \"sidecarproxyaddr\": \"docker_registry/sidecar-proxy:latest\", \"jwtsigningsecret\": \"secret\" }, \"proxy\": { \"host\": \":8080\" }, \"zipkin\": { \"collectoruri\": \"http://DNS_host_name:9411/api/v2/spans\", \"probability\": 1 }, \"certificate\": { \"keyFile\": \"path_to_private_key_file\", \"crtFile\": \"path_to_host_cert_file\", \"rootCertificate\": \"path_to_root_CA_file\" }, \"hostName\": \"DNS_host_name\" } In the above template, DNS_host_name refers to the hostname of the system in which the CSM for Authorization server will be installed. This hostname can be found by running the below command on the system:\nnslookup \u003cIP_address\u003e   In order to configure secure grpc connectivity, an additional subdomain in the format grpc.DNS_host_name is also required. All traffic from grpc.DNS_host_name needs to be routed to DNS_host_name address, this can be configured by adding a new DNS entry for grpc.DNS_host_name or providing a temporary path in the /etc/hosts file.\nNOTE: The certificate provided in crtFile should be valid for both the DNS_host_name and the grpc.DNS_host_name address.\nFor example, create the certificate config file with alternate names (to include example.com and grpc.example.com) and then create the .crt file:\n ``` CN = example.com subjectAltName = @alt_names [alt_names] DNS.1 = grpc.example.com openssl x509 -req -in cert_request_file.csr -CA root_CA.pem -CAkey private_key_File.key -CAcreateserial -out example.com.crt -days 365 -sha256 ```    To install the rpm package on the system, run the below command:\nrpm -ivh \u003crpm_file_name\u003e   After installation, application data will be stored on the system under /var/lib/rancher/k3s/storage/.\n  Configuring the CSM for Authorization Proxy Server The storage administrator must first configure the proxy server with the following:\n Storage systems Tenants Roles Bind roles to tenants  Run the following commands on the Authorization proxy server:\n# Specify any desired name export RoleName=\"\" export RoleQuota=\"\" export TenantName=\"\" # Specify info about Array1 export Array1Type=\"\" export Array1SystemID=\"\" export Array1User=\"\" export Array1Password=\"\" export Array1Pool=\"\" export Array1Endpoint=\"\" # Specify info about Array2 export Array2Type=\"\" export Array2SystemID=\"\" export Array2User=\"\" export Array2Password=\"\" export Array2Pool=\"\" export Array2Endpoint=\"\" # Specify IPs export DriverHostVMIP=\"\" export DriverHostVMPassword=\"\" export DriverHostVMUser=\"\" # Specify Authorization host address. NOTE: this is not the same as IP export AuthorizationHost=\"\" echo === Creating Storage(s) === # Add array1 to authorization karavictl storage create \\ --type ${Array1Type} \\ --endpoint ${Array1Endpoint} \\ --system-id ${Array1SystemID} \\ --user ${Array1User} \\ --password ${Array1Password} \\ --insecure # Add array2 to authorization karavictl storage create \\ --type ${Array2Type} \\ --endpoint ${Array2Endpoint} \\ --system-id ${Array2SystemID} \\ --user ${Array2User} \\ --password ${Array2Password} \\ --insecure echo === Creating Tenant === karavictl tenant create -n $TenantName --insecure --addr \"grpc.${AuthorizationHost}\" echo === Creating Role === karavictl role create \\ --role=${RoleName}=${Array1Type}=${Array1SystemID}=${Array1Pool}=${RoleQuota} \\ --role=${RoleName}=${Array2Type}=${Array2SystemID}=${Array2Pool}=${RoleQuota} echo === === Binding Role === karavictl rolebinding create --tenant $TenantName --role $RoleName --insecure --addr \"grpc.${AuthorizationHost}\" Generate a Token After creating the role bindings, the next logical step is to generate the access token. The storage admin is responsible for generating and sending the token to the Kubernetes tenant admin.\necho === Generating token === karavictl generate token --tenant $TenantName --insecure --addr \"grpc.${AuthorizationHost}\" | jq -r '.Token' \u003e token.yaml echo === Copy token to Driver Host === sshpass -p $DriverHostPassword scp token.yaml ${DriverHostVMUser}@{DriverHostVMIP}:/tmp/token.yaml Note: The sample above copies the token directly to the Kubernetes cluster master node. The requirement here is that the token must be copied and/or stored in any location accessible to the Kubernetes tenant admin.\nCopy the karavictl Binary to the Kubernetes Master Node The karavictl binary is available from the CSM for Authorization proxy server. This needs to be copied to the Kubernetes master node where Kubernetes tenant admins so they configure the Dell EMC CSI driver with CSM for Authorization.\nsshpass -p dangerous scp bin/karavictl root@10.247.96.174:/tmp/karavictl Note: The storage admin is responsible for copying the binary to a location accessible by the Kubernetes tenant admin.\nConfiguring a Dell EMC CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the supported. This is controlled by the Kubernetes tenant admin.\nThere are currently 2 ways of doing this:\n Using the CSM Installer (Recommended installation method) Manually by following the steps below  Configuring a Dell EMC CSI Driver Given a setup where Kubernetes, a storage system, CSI driver(s), and CSM for Authorization are deployed, follow the steps below to configure the CSI Drivers to work with the Authorization sidecar:\nRun the following commands on the CSI Driver host\n# Specify Authorization host address. NOTE: this is not the same as IP export AuthorizationHost=\"\" echo === Applying token token === # It is assumed that array type powermax has the namespace \"powermax\" and powerflex has the namepace \"vxflexos\" kubectl apply -f /tmp/token.yaml -n powermax kubectl apply -f /tmp/token.yaml -n vxflexos echo === injecting sidecar in all CSI driver hosts that token has been applied to === sudo curl -k https://${AuthorizationHost}/install | sh # NOTE: you can also query parameters(\"namespace\" and \"proxy-port\") with the curl url if you desire a specific behavior. # 1) For instance, if you want to inject into just powermax, you can run # sudo curl -k https://${AuthorizationHost}/install?namespace=powermax | sh # 2) If you want to specify the proxy-port for powermax to be 900001, you can run # sudo curl -k https://${AuthorizationHost}/install?proxy-port=powermax:900001 | sh # 3) You can mix behaviors # sudo curl -k https://${AuthorizationHost}/install?namespace=powermax\u0026proxy-port=powermax:900001\u0026namespace=vxflexos | sh Updating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically:\n   Parameter Type Default Description     certificate.crtFile String \"” Path to the host certificate file   certificate.keyFile String \"” Path to the host private key file   certificate.rootCertificate String \"” Path to the root CA file   web.sidecarproxyaddr String “127.0.0.1:5000/sidecar-proxy:latest” Docker registry address of the CSM for Authorization sidecar-proxy   web.jwtsigningsecret String “secret” The secret used to sign JWT tokens    Updating configuration parameters can be done by editing the karavi-config-secret on the CSM for the Authorization Server. The secret can be queried using k3s and kubectl like so:\nk3s kubectl -n karavi get secret/karavi-config-secret\nTo update or add parameters, you must edit the base64 encoded data in the secret. The karavi-config-secret data can be decoded like so:\nk3s kubectl -n karavi get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d\nSave the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so:\ncat \u003cfile\u003e | base64\nCopy the new, encoded data and edit the karavi-config-secret with the new data. Run this command to edit the secret:\nk3s kubectl -n karavi edit secret/karavi-config-secret\nReplace the data in config.yaml under the data field with your new, encoded data. Save the changes and CSM for Authorization will read the changed secret.\nNote: If you are updating the signing secret, the tenants need to be updated with new tokens via the karavictl generate token command like so:\nkaravictl generate token --tenant $TenantName --insecure --addr \"grpc.${AuthorizationHost}\" | jq -r '.Token' \u003e kubectl -n $namespace apply -f -\nCSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the karavi-config-secret but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM for Authorization logging settings during runtime, run the below command on the K3s cluster, make your changes, and save the updated configmap data.\nk3s kubectl -n karavi edit configmap/csm-config-params This edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion:\nkubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params Using PowerFlex as an example, kubectl -n vxflexos edit configmap/vxflexos-config-params can be used to update the logging level of the sidecar-proxy and the driver.\n","excerpt":"This section outlines the deployment steps for Container Storage …","ref":"/csm-docs/docs/authorization/deployment/","title":"Deployment"},{"body":"The recommended approach for deploying Container Storage Modules (CSM) is by using the CSM Installer. The CSM Installer simplifies the deployment and management of Dell EMC Container Storage Modules and CSI Drivers to provide persistent storage for your containerized workloads.\nThe CSM Installer must first be deployed in a Kubernetes environment using Helm. After which, the CSM Installer can be used through the following interfaces:\n CSM CLI REST API  Alternatively, the Container Storage Modules and the required CSI Drivers can each be deployed individually:\n Dell EMC CSI Drivers Installation Dell EMC Container Storage Module for Observability Dell EMC Container Storage Module for Authorization Dell EMC Container Storage Module for Resiliency Dell EMC Container Storage Module for Replication   Note: The CSM Installer supports installing Dell EMC Container Storage Modules and CSI Drivers in environments that do not have any existing deployments of CSM or CSI Drivers. The CSM Installer does not support the upgrade of existing CSM or CSI Driver deployments.\n How to Deploy the Container Storage Modules Installer  Add the dell helm repository:  helm repo add dell https://dell.github.io/helm-charts If securing the API service and database, following steps 2 to 4 to generate the certificates, or skip to step 5 to deploy without certificates\nGenerate self-signed certificates using the following commands:  mkdir api-certs openssl req \\ -newkey rsa:4096 -nodes -sha256 -keyout api-certs/ca.key \\ -x509 -days 365 -out api-certs/ca.crt -subj '/' openssl req \\ -newkey rsa:4096 -nodes -sha256 -keyout api-certs/cert.key \\ -out api-certs/cert.csr -subj '/' openssl x509 -req -days 365 -in api-certs/cert.csr -CA api-certs/ca.crt \\ -CAkey api-certs/ca.key -CAcreateserial -out api-certs/cert.crt If required, download the cockroach binary used to generate certificates for the cockroach-db:  curl https://binaries.cockroachdb.com/cockroach-v21.1.8.linux-amd64.tgz | tar -xz \u0026\u0026 sudo cp -i cockroach-v21.1.8.linux-amd64/cockroach /usr/local/bin/ Generate the certificates required for the cockroach-db service:  mkdir db-certs cockroach cert create-ca --certs-dir=db-certs --ca-key=db-certs/ca.key cockroach cert create-node cockroachdb-0.cockroachdb.csm-installer.svc.cluster.local cockroachdb-public cockroachdb-0.cockroachdb --certs-dir=db-certs/ --ca-key=db-certs/ca.key In case multiple instances of cockroachdb are required add all nodes names while creating nodes on the certificates\ncockroach cert create-node cockroachdb-0.cockroachdb.csm-installer.svc.cluster.local cockroachdb-1.cockroachdb.csm-installer.svc.cluster.local cockroachdb-2.cockroachdb.csm-installer.svc.cluster.local cockroachdb-public cockroachdb-0.cockroachdb cockroachdb-1.cockroachdb cockroachdb-2.cockroachdb --certs-dir=db-certs/ --ca-key=db-certs/ca.key cockroach cert create-client root --certs-dir=db-certs/ --ca-key=db-certs/ca.key cockroach cert list --certs-dir=db-certs/ Create a values.yaml file that contains JWT, Cipher key, and Admin username and password of CSM Installer that are required by the installer during helm installation. See the Configuration section for other values that can be set during helm installation.  # string of any length jwtKey: # string of exactly 32 characters cipherKey: \"\" # Admin username of CSM Installer adminUserName: # Admin password of CSM Installer adminPassword: Follow step a if certificates are being used or step b if certificates are not being used:  a) Install the helm chart, specifying the certificates generated in the previous steps:\nhelm install -n csm-installer --create-namespace \\ --set-file serviceCertificate=api-certs/cert.crt \\ --set-file servicePrivateKey=api-certs/cert.key \\ --set-file databaseCertificate=db-certs/node.crt \\ --set-file databasePrivateKey=db-certs/node.key \\ --set-file dbClientCertificate=db-certs/client.root.crt \\ --set-file dbClientPrivateKey=db-certs/client.root.key \\ --set-file caCrt=db-certs/ca.crt \\ -f values.yaml \\ csm-installer dell/csm-installer b) If not deploying with certificates, execute the following command:\nhelm install -n csm-installer --create-namespace \\ --set-string scheme=http \\ --set-string dbSSLEnabled=\"false\" \\ -f values.yaml \\ csm-installer dell/csm-installer  Note: In an OpenShift environment, the cockroachdb StatefulSet will run privileged pods so that it can mount the Persistent Volume used for storage. Follow the documentation for your OpenShift version to enable privileged pods.\n Configuration    Parameter Description Default     csmInstallerCount Number of replicas for the CSM Installer Deployment 1   dbInstanceCount Number of replicas for the CSM Database StatefulSet 2   imagePullPolicy Image pull policy for the CSM Installer images Always   host Host or IP that will be used to bind to the CSM Installer API service 0.0.0.0   port Port that will be used to bind to the CSM Installer API service 8080   scheme Scheme used for the CSM Installer API service. Valid values are https and http https   jwtKey Key used to sign the JWT token    cipherKey Key used to encrypt/decrypt user and storage system credentials. Must be 32 characters in length.    logLevel Log level used for the CSM Installer. Valid values are DEBUG, INFO, WARN, ERROR, and FATAL INFO   dbHost Host name of the Cockroach DB instance cockroachdb-public   dbPort Port number to access the Cockroach DB instance 26257   dbSSLEnabled Enable SSL for the Cockroach DB connectiong true   installerImage Location of the CSM Installer Docker Image dellemc/dell-csm-installer:v1.0.0   dataCollectorImage Location of the CSM Data Collector Docker Image dellemc/csm-data-collector:v1.0.0   adminUserName Username to authenticate with the CSM Installer    adminPassword Password to authenticate with the CSM Installer    dbVolumeDirectory Directory on the worker node to use for the Persistent Volume /var/lib/cockroachdb   api_server_ip If using Swagger, set to public IP or host of the CSM Installer API service localhost    How to Upgrade the Container Storage Modules Installer When a new version of the CSM Installer helm chart is available, the following steps can be used to upgrade to the latest version.\n Note: Upgrading the CSM Installer does not upgrade the Dell EMC CSI Drivers or modules that were previously deployed with the installer. The CSM Installer does not support upgrading of the Dell EMC CSI Drivers or modules. The Dell EMC CSI Drivers and modules must be deleted and re-deployed using the latest CSM Installer in order to get the most recent version of the Dell EMC CSI Driver and modules.\n  Update the helm repository.  helm repo update Follow step a if certificates were used during the initial installation of the helm chart or step b if certificates were not used:  a) Upgrade the helm chart, specifying the certificates used during initial installation:\nhelm upgrade -n csm-installer \\ --set-file serviceCertificate=api-certs/cert.crt \\ --set-file servicePrivateKey=api-certs/cert.key \\ --set-file databaseCertificate=db-certs/node.crt \\ --set-file databasePrivateKey=db-certs/node.key \\ --set-file dbClientCertificate=db-certs/client.root.crt \\ --set-file dbClientPrivateKey=db-certs/client.root.key \\ --set-file caCrt=db-certs/ca.crt \\ -f values.yaml \\ csm-installer dell/csm-installer b) If not deploying with certificates, execute the following command:\nhelm upgrade -n csm-installer \\ --set-string scheme=http \\ --set-string dbSSLEnabled=\"false\" \\ -f values.yaml \\ csm-installer dell/csm-installer ","excerpt":"The recommended approach for deploying Container Storage Modules (CSM) …","ref":"/csm-docs/docs/deployment/","title":"Deployment"},{"body":" What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Where do I start with Dell Container Storage Modules (CSM)? Is the Container Storage Module XYZ available for my array? What are the prerequisites for deploying Container Storage Modules? How do I uninstall or disable a Container Storage Module? How do I troubleshoot Container Storage Modules? Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? Should I install the module in the same namespace as the driver or another? Which Kubernetes distributions are supported? How do I get a list of Container Storage Modules deployed in my cluster with their versions? Does the CSM Installer provide full Container Storage Modules functionality for all products? Do all Container Storage Modules need to be the same version, or can I mix and match? Can I run Container Storage Modules in a production environment? Is Dell Container Storage Modules (CSM) supported by Dell Technologies? Can I modify a module or contribute to the project? What is coming next?  What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Dell Container Storage Modules are a set of modules that aim to extend features beyond what is available in the CSI specification.\nThe main goal with CSM modules is to expose storage array enterprise features directly within Kubernetes so developers are empowered to leverage them for their deployment in a seamless way.\nWhere do I start with Dell Container Storage Modules (CSM)? The umbrella repository for every Dell Container Storage Module is: https://github.com/dell/csm.\nIs the Container Storage Module XYZ available for my array? Please see module and the respectice CSI driver version available for each array:\n   CSM Module CSI PowerFlex v2.0 CSI PowerScale v2.0 CSI PowerStore v2.0 CSI PowerMax v2.0 CSI Unity XT v2.0     Authorization v1.0 ✔️ ✔️ ❌ ✔️ ❌   Observability v1.0 ✔️ ❌ ✔️ ❌ ❌   Replication v1.0 ❌ ❌ ✔️ ✔️ ❌   Resilency v1.0 ✔️ ❌ ❌ ❌ ✔️   CSM Installer v1.0 ✔️ ✔️ ✔️ ✔️ ✔️    What are the prerequisites for deploying Container Storage Modules? Prerequisites can be found on the respective module deployment pages:\n Dell EMC Container Storage Module for Observability Deployment Dell EMC Container Storage Module for Authorization Deployment Dell EMC Container Storage Module for Resiliency Deployment Dell EMC Container Storage Module for Replication Deployment  Prerequisites for deploying the Dell EMC CSI drivers can be found here:\n Dell EMC CSI Drivers Deployment  How do I uninstall or a disable a module?  Dell EMC Container Storage Module for Authorization Dell EMC Container Storage Module for Observability Dell EMC Container Storage Module for Resiliency  How do I troubleshoot Container Storage Modules?  Dell EMC CSI Drivers Dell EMC Container Storage Module for Authorization Dell EMC Container Storage Module for Observability Dell EMC Container Storage Module for Replication Dell EMC Container Storage Module for Resiliency  Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? No, all the modules have been designed to work inside Kubernetes with Dell EMC CSI drivers.\nShould I install the module in the same namespace as the driver or another? It is recommended to install CSM for Observability in a namespace separate from the Dell EMC CSI drivers because it works across multiple drivers. All other modules either run as standalone or are injected into the Dell EMC CSI driver as a sidecar.\nWhich Kubernetes distributions are supported? The supported Kubernetes distributions for Container Storage Modules are documented:\n Dell EMC Container Storage Module for Authorization Dell EMC Container Storage Module for Observability Dell EMC Container Storage Module for Replication Dell EMC Container Storage Module for Resiliency  The supported distros for the Dell EMC CSI Drivers are located here.\nHow do I get a list of Container Storage Modules deployed in my cluster with their versions? The easiest way to find the module version is to check the image tag for the module. For all the namespaces you can execute the following:\nkubectl get pods -A -o jsonpath=\"{..image}\" | tr -s '[[:space:]]' '\\n' | grep 'csm\\|karavi' | sort | uniq -c Or if you know the namespace:\nkubectl get deployment,daemonset -o wide -n {{namespace}} Does the CSM Installer provide full Container Storage Modules functionality for all products? The CSM Installer supports the installation of all the Container Storage Modules and Dell EMC CSI drivers.\nDo all Container Storage Modules need to be the same version, or can I mix and match? It is advised to comply with the support matrices (links below) and not deviate from it with mixed versions.\n Dell EMC Container Storage Module for Authorization Dell EMC Container Storage Module for Observability Dell EMC Container Storage Module for Replication Dell EMC Container Storage Module for Resiliency Dell EMC CSI Drivers.  The CSM installer module will help to stay aligned with compatible versions during the first install and future upgrades.\nCan I run Container Storage Modules in a production environment? As of CSM 1.0, the Container Storage Modules are GA and ready for production systems.\nIs Dell Container Storage Modules (CSM) supported by Dell Technologies? Yes!\nIf you find an issue, please follow our support process\nCan I modify a module or contribute to the project? Yes!\nAll Container Storage Modules are released as open-source projects under Apache-2.0 License. You are free to contribute directly following the contribution guidelines, fork the projects, modify them, and of course share feedback or open tickets ;-)\nWhat is coming next? This is just the beginning of the journey for Dell Container Storage Modules, and there is a full roadmap with more to come, which you can check under the GithHub Milestones page.\n","excerpt":" What are Dell Container Storage Modules (CSM)? How different is it …","ref":"/csm-docs/docs/faq/","title":"CSM FAQ"},{"body":"Installation information for all the drivers/modules can be found on the individual driver’s page in this section\n","excerpt":"Installation information for all the drivers/modules can be found on …","ref":"/csm-docs/docs/csidriver/installation/","title":"Installation"},{"body":"The installation process consists of three steps:\n Install repctl Install Container Storage Modules (CSM) for Replication Controller Install CSI driver after enabling replication  Before you begin Please read this document before proceeding with the installation. It provides detailed steps on how to set up communication between multiple clusters which will be required during or after the installation.\nInstall repctl You can download pre-built repctl binary from our Releases page. Alternately, if you want to build the binary yourself, you can follow these steps:\ngit clone github.com/dell/csm-replication cd csm-replication/repctl make build Installing CSM Replication Controller You can use one of the following methods to install CSM Replication Controller\n Using repctl Installation script  We recommend using repctl for the installation as it simplifies the installation workflow. This process also helps configure repctl for future use during management operations.\nUsing repctl Please follow the steps here to install \u0026 configure Dell Replication Controller\nUsing the installation script Repeat the following steps on all clusters where you want to configure replication\ngit clone github.com/dell/csm-replication cd csm-replication # Modify deploy/config.yaml as per your cluster configuration (optional) bash scripts/install.sh This script will do the following:\n Install DellCSIReplicationGroup CRD in your cluster Create a namespace dell-replication-controller Install dell-replication-controller  During the installation process, you will be prompted to create secrets to connect to other clusters. You can choose to create secrets at this time or even postpone this activity for later.\nIf you choose to update the configuration post installation, then do the following:\n Update the configuration in deploy/config.yaml after going through the guide here Run the following commands to update and complete the installation  cd csm-replication kubectl create configmap dell-replication-controller-config --namespace dell-replication-controller --from-file deploy/config.yaml -o yaml --dry-run | kubectl apply -f - Install CSI driver The following CSI drivers support replication:\n CSI driver for PowerMax CSI driver for PowerStore  Please follow the steps outlined here for enabling replication for PowerMax \u0026 here for PowerStore during the driver installation.\n Note: Please ensure that replication CRDs are installed in the clusters where you are installing the CSI drivers. These CRDs are generally installed as part of the CSM Replication controller installation process.\n Dynamic Log Level Change CSM Replication Controller can dynamically change its logs’ verbosity level. To set log level in runtime you need to edit the controllers ConfigMap:\nkubectl edit cm dell-replication-controller-config -n dell-replication-controller And set the CSI_LOG_LEVEL field to the level of your choosing. CSM Replication controller supports following log levels:\n “PANIC” “FATAL” “ERROR” “WARN” “INFO” “DEBUG” “TRACE”   Note: CSI-Replicator sidecar utilizes the same log level as CSI driver. To change the sidecars log level refer to corresponding csi drivers documentation.\n ","excerpt":"The installation process consists of three steps:\n Install repctl …","ref":"/csm-docs/docs/replication/deployment/installation/","title":"Installation"},{"body":"Installation information for all the drivers can be found on the individual driver’s page in this section\n","excerpt":"Installation information for all the drivers can be found on the …","ref":"/csm-docs/v1/installation/","title":"Installation"},{"body":"Installation information for all the drivers can be found in the individual drivers page in this section\n","excerpt":"Installation information for all the drivers can be found in the …","ref":"/csm-docs/v2/installation/","title":"Installation"},{"body":"Installation information for all the drivers can be found in the individual drivers page in this section\n","excerpt":"Installation information for all the drivers can be found in the …","ref":"/csm-docs/v3/installation/","title":"Installation"},{"body":"This section outlines the metrics collected by Container Storage Modules (CSM) for Observability in the areas of I/O Performance and Storage Capacity. All metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n","excerpt":"This section outlines the metrics collected by Container Storage …","ref":"/csm-docs/docs/observability/metrics/","title":"Metrics"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Authorization.\nUninstalling the RPM To uninstall the rpm package on the system, run the below command:\nrpm -e \u003crpm_file_name\u003e Uninstalling the sidecar-proxy in the CSI Driver To uninstall the sidecar-proxy in the CSI Driver, uninstall the driver and reinstall the driver using the original configuration secret.\n","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/docs/authorization/uninstallation/","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Resiliency.\nUninstalling the sidecar in the CSI Driver To uninstall the sidecar in the CSI Driver, uninstall the driver and reinstall the driver with the podmon feature disabled.\n","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/docs/resiliency/uninstallation/","title":"Uninstallation"},{"body":"CSM for Resiliency is primarily designed to detect pod failures due to some kind of node failure or node communication failure. The diagram below shows the hardware environment that is assumed in the design.\nA Kubernetes Control Plane is assumed to exist that provides the K8S API service used by CSM for Resiliency. There is an arbitrary number of worker nodes (two are shown in the diagram) that are connected to the Control Plane through a K8S Control Plane IP Network.\nThe worker nodes (e.g. Node1 and Node2) can run a mix of CSM for Resiliency monitored Application Pods as well as unmonitored Application Pods. Monitored Pods are designated by a specific label that is applied to each monitored pod. The label key and value are configurable for each driver type when CSM for Resiliency is installed and must be unique for each driver instance.\nThe Worker Nodes are assumed to also have a connection to a Storage System Array (such as PowerFlex.) It is often preferred that a separate network be used for storage access from the network used by the K8S control plane, and CSM for Resiliency takes advantage of the separate networks when available.\nAnti Use-Cases CSM for Resiliency does not generally try to handle any of the following errors:\n  Failure of the Kubernetes control plane, the etcd database used by Kubernetes, or the like. Kubernetes is generally designed to provide a highly available container orchestration system, and it is assumed clients follow the standard and/or best practices in configuring their Kubernetes deployments.\n  CSM for Resiliency is generally not designed to take action upon a failure solely of the Application Pod(s). Applications are still responsible for detecting and providing recovery mechanisms should their application fail. There are some specific recommendations for applications to be monitored by CSM for Resiliency that are described later.\n  Failure Model CSM for Resiliency’s design is focused on detecting the following types of hardware failures, and when they occur, moving protected pods to hardware that is functioning correctly:\n  Node failure. Node failure is defined to be similar to a Power Failure to the node which causes it to cease operation. This is differentiated from Node Communication Failures which require different treatments. Node failures are generally discovered by receipt of a Node event with a NoSchedule or NoExecute taint, or detection of such a taint when retrieving the Node via the K8S API.\nGenerally, it is difficult to distinguish from the outside if a node is truly down (not executing) versus if it has lost connectivity on all its interfaces. (We might add capabilities in the future to query BIOS interfaces such as iDRAC, or perhaps periodically writing to file systems mounted in node-podmon to detect I/O failures, in order to get additional insight as to node status.) However, if the node has simply lost all outside communication paths, the protected pods are possibly still running. We refer to these pods as “zombie pods”. CSM for Resiliency is designed to deal with zombie pods in a way that prevents them from interfering with replacement pods it may have made by fencing the failed nodes and when communication is re-established to the node, going through a cleaning procedure to remove the zombie pod artifacts before allowing the node to go back into service.\n  K8S Control Plane Network Failure. Control Plane Network Failure often has the same K8S failure signature (the node is tainted with NoSchedule or NoExecute). However, if there is a separate Array I/O interface, CSM for Resiliency can often detect that the Array I/O Network may be active even though the Control Plane Network is down.\n  Array I/O Network failure is detected by polling the array to determine if the array has a healthy connection to the node. The capabilities to do this vary greatly by array and communication protocol type (Fibre Channel, iSCSI, NFS, NVMe, or PowerFlex SDC IP protocol). By monitoring the Array I/O Network separately from the Control Plane Network, CSM for Resiliency has two different indicators of whether the node is healthy or not.\n  ","excerpt":"CSM for Resiliency is primarily designed to detect pod failures due to …","ref":"/csm-docs/docs/resiliency/usecases/","title":"Use Cases"},{"body":"This section provides the details and instructions on how to install the CSI Driver components using the provided Helm charts and in the case of the CSI drivers, the Dell CSI Helm Installer.\nDependencies Installing any of the CSI Driver components using Helm requires a few utilities to be installed on the system running the installation.\n   Dependency Usage     kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver.   helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3.   sshpass sshpass is used to check certain pre-requisites in worker nodes (in chosen drivers).    Note: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","excerpt":"This section provides the details and instructions on how to install …","ref":"/csm-docs/docs/csidriver/installation/helm/","title":"CSI Driver installation using Helm"},{"body":"This section provides the details and instructions on how to install the Dell EMC CSI drivers using the provided Helm charts and the Dell CSI Helm Installer.\nDependencies Installing any of the Dell EMC CSI Drivers using Helm requires a few utilities to be installed on the system running the installation.\n   Dependency Usage     kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver.   helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3.   sshpass sshpass is used to check certain pre-requisites in worker nodes (in chosen drivers).    \nNote: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","excerpt":"This section provides the details and instructions on how to install …","ref":"/csm-docs/v1/installation/helm/","title":"CSI Driver installation using Helm"},{"body":"This section provides the details and instructions on how to install the Dell EMC CSI drivers using the provided Helm charts and the Dell CSI Helm Installer.\nDependencies Installing any of the Dell EMC CSI Drivers using Helm requires a few utilities to be installed on the system running the installation.\n   Dependency Usage     kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver.   helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3.   sshpass sshpass is used to check certain pre-requisities in worker nodes (in chosen drivers).    \nNote: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","excerpt":"This section provides the details and instructions on how to install …","ref":"/csm-docs/v2/installation/helm/","title":"CSI Driver installation using Helm"},{"body":"This section provides the details and instructions on how to install the Dell EMC CSI drivers using the provided Helm charts and the Dell CSI Helm Installer.\nDependencies Installing any of the Dell EMC CSI Drivers using Helm requires a few utilities to be installed on the system running the installation.\n   Dependency Usage     kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver.   helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3.   sshpass sshpass is used to check certain pre-requisities in worker nodes (in chosen drivers).    \nNote: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","excerpt":"This section provides the details and instructions on how to install …","ref":"/csm-docs/v3/installation/helm/","title":"CSI Driver installation using Helm"},{"body":"karavictl is a command-line interface (CLI) used to interact with and manage your Container Storage Modules (CSM) Authorization deployment. This document outlines all karavictl commands, their intended use, options that can be provided to alter their execution, and expected output from those commands.\nIf you feel that something is unclear or missing in this document, please open up an issue.\n   Command Description     karavictl karavictl is used to interact with CSM Authorization Server   karavictl cluster-info Display the state of resources within the cluster   karavictl inject Inject the sidecar proxy into a CSI driver pod   karavictl generate Generate resources for use with CSM   karavictl generate token Generate tokens   karavictl role Manage role   karavictl role get Get role   karavictl role list List roles   karavictl role create Create one or more CSM roles   karavictl role update Update one or more CSM roles   karavictl role delete Delete role   karavictl rolebinding Manage role bindings   karavictl rolebinding create Create a rolebinding between role and tenant   karavictl storage Manage storage systems   karavictl storage get Get details on a registered storage system   karavictl storage list List registered storage systems   karavictl storage create Create and register a storage system   karavictl storage update Update a registered storage system   karavictl storage delete Delete a registered storage system   karavictl tenant  Manage tenants   karavictl tenant create Create a tenant resource within CSM   karavictl tenant get Get a tenant resource within CSM   karavictl tenant list Lists tenant resources within CSM   karavictl tenant get Get a tenant resource within CSM   karavictl tenant delete Deletes a tenant resource within CSM    General Commands karavictl karavictl is used to interact with CSM Authorization Server\nSynopsis karavictl provides security, RBAC, and quota limits for accessing Dell EMC storage products from Kubernetes clusters\nOptions  --config string config file (default is $HOME/.karavictl.yaml) -h, --help help for karavictl -t, --toggle Help message for toggle Output Outputs help text\n karavictl cluster-info Display the state of resources within the cluster\nSynopsis Prints table of resources within the cluster, including their readiness\nkaravictl cluster-info [flags] Options  -h, --help help for cluster-info -w, --watch Watch for changes Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl cluster-info NAME READY UP-TO-DATE AVAILABLE AGE github-auth-provider 1/1 1 1 59m tenant-service 1/1 1 1 59m redis-primary 1/1 1 1 59m proxy-server 1/1 1 1 59m redis-commander 1/1 1 1 59m  karavictl inject Inject the sidecar proxy into a CSI driver pod\nSynopsis Injects the sidecar proxy into a CSI driver pod.\nYou can inject resources coming from stdin.\nkaravictl inject [flags] Options  -h, --help help for inject --image-addr string Help message for image-addr --proxy-host string Help message for proxy-host Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Examples: Inject into an existing vxflexos CSI driver\nkubectl get secrets,deployments,daemonsets -n vxflexos -o yaml \\ | karavictl inject --image-addr [IMAGE_REPO]:5000/sidecar-proxy:latest --proxy-host [PROXY_HOST_IP] \\ | kubectl apply -f - Output $ kubectl get secrets,deployments,daemonsets -n vxflexos -o yaml \\ | karavictl inject --image-addr [IMAGE_REPO]:5000/sidecar-proxy:latest --proxy-host [PROXY_HOST_IP] \\ | kubectl apply -f - secret/karavi-authorization-config created deployment.apps/vxflexos-controller configured daemonset.apps/vxflexos-node configured  karavictl generate Generate resources for use with CSM\nSynopsis Generates resources for use with CSM\nkaravictl generate [flags] Options  -h, --help help for generate Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl generate token Generate tokens\nSynopsis Generate tokens for use with the CSI Driver when in proxy mode The tokens are output as a Kubernetes Secret resource, so the results may be piped directly to kubectl:\nExample: karavictl generate token | kubectl apply -f -\nkaravictl generate token [flags] Options  --addr string host:port address (default \"grpc.gatekeeper.cluster:443\") --from-config string File providing self-generated token information -h, --help help for token --tenant Tenant name --shared-secret string Shared secret for token signing Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl generate token --shared-secret supersecret apiVersion: v1 kind: Secret metadata: name: proxy-authz-tokens namespace: vxflexos type: Opaque data: access: \u003cACCESS-TOKEN\u003e refresh: \u003cREFRESH-TOKEN\u003e Usually, you will want to pipe the output to kubectl to apply the secret\n$ karavictl generate token --shared-secret supersecret | kubectl apply -f - Role Commands karavictl role Manage roles\nSynopsis Manage roles\nkaravictl role [flags] Options  -h, --help help for role Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl role get Get role\nSynopsis Get role\nkaravictl role get [flags] Options  -h, --help help for get Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role get CSISilver { \"Name\": \"CSISilver\", \"StorageSystem\": \"3000000000011111\", \"PoolQuotas\": [ { \"Pool\": \"mypool\", \"Quota\": \"16 GB\" } ] }  karavictl role list List roles\nSynopsis List roles\nkaravictl role list [flags] Options  -h, --help help for list Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role list { \"CSIGold\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 32000000 } ] } ], \"CSISilver\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 16000000 } ] } ] }  karavictl role create Create one or more CSM roles\nSynopsis Creates one or more CSM roles\nkaravictl role create [flags] Options  -f, --from-file string role data from a file --role strings role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -h, --help help for create NOTE:\n For PowerScale, set the quota to 0 as CSM for Authorization does not enforce quota limits.  Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role create --from-file roles.json On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the creation occurred.\nAlternatively, you can create a role in-line using:\n$ karavictl role create --role=role-name=system-type=000000000001=mypool=200000000  karavictl role update Update one or more CSM roles\nSynopsis Updates one or more CSM roles\nkaravictl role update [flags] Options  -f, --from-file string role data from a file --role strings role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -h, --help help for update Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role update --from-file roles.json On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the update occurred.\nAlternatively, you can update existing roles in-line using:\n$ karavictl role update --role=role-name=system-type=000000000001=mypool=400000000  karavictl role delete Delete role\nSynopsis Delete role\nkaravictl role delete \u003crole-name\u003e [flags] Options  -h, --help help for delete Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role delete CSISilver On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the deletion occurred.\n karavictl rolebinding Manage role bindings\nSynopsis Management for role bindings\nkaravictl rolebinding [flags] Options  -h, --help help for rolebinding Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl rolebinding create Create a rolebinding between role and tenant\nSynopsis Creates a rolebinding between role and tenant\nkaravictl rolebinding create [flags] Options  -h, --help help for create -r, --role string Role name -t, --tenant string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl rolebinding create --role CSISilver --tenant Alice On success, there will be no output. You may run karavictl tenant get \u003ctenant-name\u003e to confirm the rolebinding creation occurred.\nStorage Commands karavictl storage Manage storage systems\nSynopsis Manages storage systems\nkaravictl storage [flags] Options  -h, --help help for storage Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl storage get Get details on a registered storage system.\nSynopsis Gets details on a registered storage system.\nkaravictl storage get [flags] Options  -h, --help help for get -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage get --type powerflex --system-id 3000000000011111 { \"User\": \"admin\", \"Password\": \"(omitted)\", \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true }  karavictl storage list List registered storage systems.\nSynopsis Lists registered storage systems.\nkaravictl storage list [flags] Options  -h, --help help for list Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage list { \"storage\": { \"powerflex\": { \"3000000000011111\": { \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true, \"Password\": \"(omitted)\", \"User\": \"admin\" } } } }  karavictl storage create Create and register a storage system.\nSynopsis Creates and registers a storage system.\nkaravictl storage create [flags] Options  -e, --endpoint string Endpoint of REST API gateway -h, --help help for create -i, --insecure Insecure skip verify -p, --password string Password (default \"****\") -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") -u, --user string Username (default \"admin\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage create --endpoint https://1.1.1.1 --insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the creation occurred.\n karavictl storage update Update a registered storage system.\nSynopsis Updates a registered storage system.\nkaravictl storage update [flags] Options  -e, --endpoint string Endpoint of REST API gateway -h, --help help for update -i, --insecure Insecure skip verify -p, --pass string Password (default \"****\") -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") -u, --user string Username (default \"admin\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage update --endpoint https://1.1.1.1 --insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the update occurred.\n karavictl storage delete Delete a registered storage system.\nSynopsis Deletes a registered storage system.\nkaravictl storage delete [flags] Options  -h, --help help for delete -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage delete --type powerflex --system-id 3000000000011111 On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the deletion occurred.\nTenant Commands karavictl tenant Manage tenants\nSynopsis Management fortenants\nkaravictl tenant [flags] Options  -h, --help help for tenant Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl tenant create Create a tenant resource within CSM\nSynopsis Creates a tenant resource within CSM\nkaravictl tenant create [flags] Options  -h, --help help for create -n, --name string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant create --name Alice On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the creation occurred.\n karavictl tenant get Get a tenant resource within CSM\nSynopsis Gets a tenant resource within CSM\nkaravictl tenant get [flags] Options  -h, --help help for create -n, --name string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant get --name Alice { \"name\": \"Alice\" }  karavictl tenant list Lists tenant resources within CSM\nSynopsis Lists tenant resources within CSM\nkaravictl tenant list [flags] Options  -h, --help help for create Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant list { \"tenants\": [ { \"name\": \"Alice\" } ] }  karavictl tenant delete Deletes a tenant resource within CSM\nSynopsis Deletes a tenant resource within CSM\nkaravictl tenant delete [flags] Options  -h, --help help for create -n, --name string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant delete --name Alice On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the deletion occurred.\n","excerpt":"karavictl is a command-line interface (CLI) used to interact with and …","ref":"/csm-docs/docs/authorization/cli/","title":"CLI"},{"body":"Replication Cluster Topologies Container Storage Modules (CSM) for Replication project supports the replication of volumes within a single Kubernetes cluster or between two different Kubernetes clusters. The replication controller can support multiple clusters at once, but a single volume can be replicated to a maximum of two clusters.\nEach cluster should be assigned the unique identifier clusterId. The rules for naming are as follows:\n must be 63 characters or fewer (cannot be empty) must begin and end with an alphanumeric character ([a-z, 0-9, A-Z]) could contain dashes (-), underscores (_), dots (.), and alphanumerics between must be unique across clusters ``  Single Cluster Replication Cluster Configuration When configuring replication within a single cluster, you need to create a ConfigMap with at least the clusterId field configured to point to the current cluster:\napiVersion:v1data:config.yaml:| clusterId: cluster-Atargets:[]kind:ConfigMapmetadata:name:dell-replication-controller-confignamespace:dell-replication-controllerNote that the targets parameter is left empty since we don’t require any target clusters to work within a single cluster. This also means that you don’t need to create any Secrets that contain connection information to such clusters, since in this use case, we are limited to a single cluster.\nYou can find more info about configs and secrets for cluster communication in configmaps-secrets\nStorage Class Configuration To create volumes that would be replicated within a single cluster, you need to create a special StorageClass. This StorageClass should contain the usual replication parameter replication.storage.dell.com/remoteClusterID, and it should be set to self to indicate that we want to replicate the volume inside the current cluster.\nAlso, you would need to create another storage class in the same cluster that would serve as a target storage class. This means that all replicated volumes would be derived from it. Its replication.storage.dell.com/remoteClusterID parameter should be also set to self.\nYou can find out more about replication StorageClasses and replication specific parameters in storageclasses\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a single cluster replication, replicated resources (PersistentVolumes, ReplicationGroups) would be created in the same cluster with the replicated- prefix added to them.\nExample:\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s replicated-csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication-tgt 23s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:23:18Z rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z Multiple Cluster Replication Cluster Configuration Similar to a single cluster scenario, you need to create ConfigMap, but this time you need to provide at least one target cluster. You can provide as many as you like but be mindful that a single volume can be replicated to only one of them.\nFor example:\napiVersion:v1data:config.yaml:| clusterId: cluster-Atargets:- clusterId:cluster-Baddress:192.168.111.21secretRef:secretClusterBkind:ConfigMapmetadata:name:dell-replication-controller-confignamespace:dell-replication-controllerNote that target cluster information contains a field called secretRef. This field points to a secret available in the current cluster that contains connection information of cluster-B in the form of a kubeconfig file.\nYou can find more information about how to create such secrets in configmaps-secrets\nStorage Class Configuration To create replicated volumes in the multi-cluster configuration you still need to have a special storage class. Replication parameter replication.storage.dell.com/remoteClusterID should be set to the cluster-id of whatever cluster you want to replicate your volumes.\nFor multi-cluster replication, we can choose one of the target cluster ids we specified in ConfigMap. In our example replication parameter, the target cluster id should be equal to cluster-B.\nYou can find more information about other replication parameters available in storage classes here\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a multi-cluster replication, replicated resources would be created in both source and target clusters under the same names.\nExample:\n[CLUSTER-A] $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z [CLUSTER-B] $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication 18s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 30s Ready SYNCHRONIZED 2021-08-03T11:22:18Z ","excerpt":"Replication Cluster Topologies Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/replication/cluster-topologies/","title":"Cluster Topologies"},{"body":"Communication between clusters Container Storage Modules (CSM) for Replication Controller requires access to remote clusters for replicating various objects. There are two ways to set up this communication -\n Using Normal Kubernetes users Using ServiceAccount token  You need to create secrets (using either of the two methods) in each cluster involved in replication and provide their references in ConfigMap objects which are used to configure the respective CSM Replication Controllers.\n Note: If you are using a single stretched cluster, then you can skip all the following steps\n Inject configuration using repctl This is the simplest way to configure CSM Replication Controller. repctl simplifies the complex configuration process greatly by enabling creation of secrets and updating their references in multiple clusters.\nRecommended method Use repctl to create secrets using service account tokens and update ConfigMaps in multiple clusters in one command. Run the following command -\nrepctl cluster inject --use-sa This will create secrets using the token for the default ServiceAccount and update the ConfigMap in all the clusters which have been configured for repctl\nInject KubeConfigs from repctl configuration repctl is usually configured to communicate with multiple Kubernetes clusters and is provided with a set of KubeConfig files for each cluster. You can use repctl to inject secrets created using these files in each of the configured cluster. Run the following command -\nrepctl cluster inject  Note: For a detailed walkthrough of the simplified installation process using repctl, please refer this link\n Understanding the Config file If you are setting up replication between two clusters - Cluster A \u0026 Cluster B, then the configuration file (deploy/config.yaml) should look like this:\nCluster A clusterId:cluster-A# This cluster's Identifiertargets:- clusterId:cluster-B# Identifier for the remote cluster Baddress:192.168.111.21# Address of the remote clustersecretRef:secretClusterB# Name of the secret required for communication with Cluster BCluster B clusterId:cluster-B# This cluster's Identifiertargets:- clusterId:cluster-A# Identifier for the remote cluster Aaddress:192.168.111.20# Address of the remote clustersecretRef:secretClusterA# Name of the secret required for communication with Cluster AManual configuration Generating KubeConfig We provide a helper script which can help create KubeConfig files for a normal user as well as a Service Account.\n Using a Certificate Signing Request for a user  cd scripts ./gen-kubeconfig.sh -u \u003cCN user\u003e -c \u003cCSR\u003e -k \u003ckey\u003e # where \"CN user\" is the name of the user \u0026 key is the private key of the user  Create kubeconfig file for a Service Account  cd scripts ./gen-kubeconfig.sh -s \u003csa-name\u003e -n \u003cnamespace\u003e Once you have created the KubeConfig file, you can use it to create the secret.\nSecrets using normal Kubernetes users You can create a normal Kubernetes user for your remote Kubernetes cluster and use it for inter cluster communication. The process of creating users is outside the scope of this document. Once you have the user created, you can provide it the RBAC privileges required by the controller.\nExample Continuing from our earlier example with Cluster A \u0026 Cluster B -\n Create a user in Cluster B \u0026 generate a kubeconfig file for it using the helper script Create a ClusterRole in Cluster B using the following command: kubectl apply -f deploy/role.yaml  Create a ClusterRoleBinding in Cluster B for the user: kubectl create clusterrolebinding \u003cname\u003e --clusterrole=dell-replication-manager-role --user=\u003cuser-name\u003e  Create a secret in Cluster A using the kubeconfig file for this user  kubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller Secrets using ServiceAccount tokens You can use service account tokens to establish communication between various clusters. We recommend using the token for the default service account in the dell-replication-controller namespace after the installation as it already has all the required RBAC privileges.\nExample Use the following command to first create a KubeConfig file using the helper script in Cluster B -\n./gen-kubeconfig.sh -s default -n dell-replication-controller Once the KubeConfig file has been generated successfully, use the following command in Cluster A to to create the secret:\nkubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller ","excerpt":"Communication between clusters Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/replication/deployment/configmap-secrets/","title":"ConfigMap \u0026 Secrets"},{"body":"The CSI Drivers by Dell EMC implement an interface between CSI (CSI spec v1.3) enabled Container Orchestrator (CO) and Dell EMC Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using Dell storage system.\nFeatures and capabilities Supported Operating Systems/Container Orchestrator Platforms    PowerMax PowerFlex Unity PowerScale PowerStore     Kubernetes 1.20, 1.21, 1.22 1.20, 1.21, 1.22 1.20, 1.21, 1.22 1.20, 1.21, 1.22 1.20, 1.21, 1.22   RHEL 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x   Ubuntu 20.04 20.04 18.04, 20.04 18.04, 20.04 20.04   CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9   SLES 15SP2 15SP2 15SP2 15SP2 15SP2   Fedora Core OS no 5.x no no no   Red Hat OpenShift 4.6 EUS, 4.7, 4.8 4.6 EUS, 4.7, 4.8 4.6 EUS, 4.7, 4.8 4.6 EUS, 4.7, 4.8 4.6 EUS, 4.7, 4.8   Mirantis Kubernetes Engine 3.4.x 3.4.x 3.4.x 3.4.x 3.4.x   Google Anthos 1.6 1.8 no no 1.7   VMware Tanzu no no NFS NFS NFS   Rancher Kubernetes Engine yes yes yes yes yes    CSI Driver Capabilities   Features PowerMax PowerFlex Unity PowerScale PowerStore     CSI Specification v1.3 v1.3 v1.3 v1.4 v1.3   Static Provisioning yes yes yes yes yes   Dynamic Provisioning yes yes yes yes yes   Expand Persistent Volume yes yes yes yes yes   Create VolumeSnapshot yes yes yes yes yes   Create Volume from Snapshot yes yes yes yes yes   Delete Snapshot yes yes yes yes yes   Access Mode RWO\n(FC/iSCSI)\nRWO/\nRWX/\nROX\n(Raw block) RWO\nRWO/\nRWX/\nROX\n(Raw block) RWO\n(FC/iSCSI)\nRWO/RWX\n(RawBlock)\nRWO/RWX/ROX\n(NFS) RWO/RWX/ROX RWO\n(FC/iSCSI)\nRWO/\nRWX/\nROX\n(RawBlock, NFS)   CSI Volume Cloning yes yes yes yes yes   CSI Raw Block Volume yes yes yes no yes   CSI Ephemeral Volume no yes yes yes yes   Topology yes yes yes yes yes   Multi-array yes yes yes yes yes    Supported Storage Platforms    PowerMax PowerFlex Unity PowerScale PowerStore     Storage Array 5978.479.479, 5978.669.669, 5978.711.711, Unisphere 9.2 3.5.x, 3.6.x 5.0.5, 5.0.6, 5.0.7, 5.1.0 OneFS 8.1, 8.2, 9.0, 9.1, 9.2 1.0.x, 2.0.x    Backend Storage Details   Features PowerMax PowerFlex Unity PowerScale PowerStore     Fibre Channel yes N/A yes N/A yes   iSCSI yes N/A yes N/A yes   NFS N/A N/A yes yes yes   Other N/A ScaleIO protocol N/A N/A N/A   Supported FS ext4 / xfs ext4 / xfs ext3 / ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS   Thin / Thick provisioning Thin Thin Thin/Thick N/A Thin   Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering Policy\nNFS Host IO size\nSnapshot Retention duration Access Zone\nNFS version (3 or 4);Configurable Export IPs iSCSI CHAP    ","excerpt":"The CSI Drivers by Dell EMC implement an interface between CSI (CSI …","ref":"/csm-docs/docs/csidriver/","title":"CSI Drivers"},{"body":"csm is a command-line client for installation of Dell EMC Container Storage Modules and CSI Drivers for Kubernetes clusters.\nPre-requisites  Deploy the Container Storage Modules Installer Download/Install the csm binary from Github: https://github.com/dell/csm. Alternatively, you can build the binary by:  cloning the csm repository changing into csm/cmd/csm directory running make build   create a cli_env.sh file that contains the correct values for the below variables. And export the variables by running source ./cli_env.sh  # Change this to CSM API Server IP export API_SERVER_IP=\"127.0.0.1\" # Change this to CSM API Server Port export API_SERVER_PORT=\"31313\" # CSM API Server protocol - allowed values are https \u0026 http export SCHEME=\"https\" # Path to store JWT \u003ctoken\u003e export AUTH_CONFIG_PATH=\"/home/user/installer-token/\" Usage ~$ ./csm -h csm is command line tool for csm application Usage: csm [flags] csm [command] Available Commands: add add cluster, configuration or storage approve-task approve task for application authenticate authenticate user change change - subcommand is password create create application delete delete storage, cluster, configuration or application get get storage, cluster, application, configuration, supported driver, module, storage type help Help about any command reject-task reject task for an application update update storage, configuration or cluster Flags: -h, --help help for csm-cli Use \"csm [command] --help\" for more information about a command. Authenticate the User To begin with, you need to authenticate the user who will be managing the CSM Installer and its components.\n./csm authenticate --username=\u003cadmin-username\u003e --password=\u003cadmin-password\u003e Or more securely, run the above command without --password to be prompted for one\n./csm authenticate --username=\u003cadmin-username\u003e Enter user's password: Change Password To change password follow below command\n./csm change password --username=\u003cadmin-username\u003e View Supported Platforms You can now view the supported Dell emcCSI Drivers\n./csm get supported-drivers You can also view the supported Modules\n./csm get supported-modules And also view the supported Storage Array Types\n./csm get supported-storage-arrays Add a Cluster You can now add a cluster by providing cluster detail name and Kubeconfig path\n./csm add cluster --clustername \u003cdesire-cluster-name\u003e --configfilepath \u003cpath-to-kubeconfig\u003e Upload Configuration Files You can now add a configuration file that can be used for creating application by providing filename and path\n./csm add configuration --filename \u003cname-of-the-desire-file\u003e --filepath \u003cpath-to-the-desired-file\u003e Add a Storage System You can now add storage endpoints, array type and its unique id\n./csm add storage --endpoint \u003cstorage-array-endpoint\u003e --storage-type \u003cstorage-array-type\u003e --unique-id \u003cstorage-array-unique-id\u003e --username \u003cstorage-array-username\u003e The optional --meta-data flag can be used to provide additional meta-data for the storage system that is used when creating Secrets for the CSI Driver. These fields include:\n isDefault: Set to true if this storage system is used as default for multi-array configuration skipCertificateValidation: Set to true to skip certificate validation mdmId: Comma separated list of MDM IPs for PowerFlex nasName: NAS Name for PowerStore blockProtocol: Block Protocol for PowerStore port: Port for PowerScale portGroups: Comma separated list of port group names for PowerMax  Create an Application You may now create an application depending on the specific use case. Below are the common use cases:\n CSI Driver ./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-type\u003e   CSI Driver with CSM Authorization CSM Authorization requires a token.yaml issued by storage Admin from the CSM Authorization Server, a certificate file, and the  of the authorization server. The token.yaml and cert should be added by following the steps in adding configuration file. CSM Authorization does not yet support all CSI Drivers/platforms(See supported platforms documentation or supported platforms via CLI)). Finally, run the command below:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-type authorization:\u003ctag\u003e \\ --module-configuration \"karaviAuthorizationProxy.proxyAuthzToken.filename=\u003cfilename-for-token\u003e,karaviAuthorizationProxy.rootCertificate.filename=\u003cfilename-for-cert\u003e,karaviAuthorizationProxy.proxyHost=\u003cproxyHost-address\u003e\"   CSM Observability(Standalone) CSM Observability depends on driver config secret(s) corresponding to the metric(s) you want to enable. Please see CSM Observability for all Supported Metrics. For the sake of demonstration, assuming we want to enable CSM Metrics for PowerFlex, the PowerFlex secret yaml should be added by following the steps in adding configuration file. Once this is done, run the command below:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --name \u003cdesired-application-name\u003e \\ --module-type observability:\u003ctag\u003e \\ --module-configuration \"karaviMetricsPowerflex.driverConfig.filename=\u003cfilename-for-powerflex-config\u003e,karaviMetricsPowerflex.enabled=true\"   CSM Observability(Standalone) with CSM Authorization See the individual steps for configuaration file pre-requisites for CSM Observability (Standalone) with CSM Authorization\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --name \u003cdesired-application-name\u003e \\ --module-type \"observability:\u003ctag\u003e,authorization:\u003ctag\u003e\" \\ --module-configuration \"karaviMetricsPowerflex.driverConfig.filename=\u003cfilename-for-powerflex-config\u003e,karaviMetricsPowerflex.enabled=true,karaviAuthorizationProxy.proxyAuthzToken.filename=\u003cfilename-for-token\u003e,karaviAuthorizationProxy.rootCertificate.filename=\u003cfilename-for-cert\u003e,karaviAuthorizationProxy.proxyHost=\u003cproxyHost-address\u003e\"   CSI Driver for Dell EMC PowerMax with reverse proxy module To deploy CSI Driver for Dell EMC PowerMax with reverse proxy module, first upload reverse proxy tls crt and tls key via adding configuration file. Then, use the below command to create application:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powermax:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cpowermax-unique-id\u003e \\ --module-type reverse-proxy:\u003ctag\u003e \\ --module-configuration reverseProxy.tlsSecretKeyFile=\u003crevprotlskey\u003e,reverseProxy.tlsSecretCertFile=\u003crevprotlscert\u003e   CSI Driver with replication module To deploy CSI driver with replication module, first add a target cluster through adding cluster. Then, use the below command(this command is an example to deploy CSI Driver for Dell EMC PowerStore with replication module) to create application::\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerstore:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-configuration target_cluster=\u003ccreated-target-cluster-name\u003e \\ --module-type replication:\u003ctag\u003e   CSI Driver with other module(s) not covered above Assuming you want to deploy a driver with module A and module B. If they have specific configurations of A.image=\"docker:v1\",A.filename=hello, and B.namespace=world.\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-type \"module A:\u003ctag\u003e,module B:\u003ctag\u003e\" \\ --module-configuration \"A.image=docker:v1,A.filename=hello,B.namespace=world\"\t  Note:\n  --driver-type and --module-type flags in create application command MUST match the values from the supported CSM platforms Replication module supports only using a pair of clusters at a time (source and a target/or single cluster) from CSM installer, However repctl can be used if needed to add multiple pairs of target clusters. Using replication module with other modules during application creation is not yet supported.  Approve application/task You may now approve the task so that you can continue to work with the application\n./csm approve-task --applicationname \u003ccreated-application-name\u003e Reject application/task You may want to reject a task or application to discontinue the ongoing process\n./csm reject-task --applicationname \u003ccreated-application-name\u003e Delete application/task If you want to delete an application\n./csm delete application --name \u003ccreated-application-name\u003e  Note: When deleting an application, the namespace and Secrets are not deleted. These resources need to be deleted manually. See more in Troubleshooting.\n  Note: All commands and associated syntax can be displayed with -h or –help\n ","excerpt":"csm is a command-line client for installation of Dell EMC Container …","ref":"/csm-docs/docs/deployment/csmcli/","title":"CSM CLI"},{"body":"CSM for Observability can be deployed in one of three ways:\n CSM Installer (Recommended installation method) Helm CSM for Observability Installer CSM for Observability Offline Installer  Prerequisites  Helm 3.3 The deployment of one or more supported Dell EMC CSI drivers  Post Installation Dependencies The following third-party components are required in the same Kubernetes cluster where CSM for Observability has been deployed:\n Prometheus Grafana  These components must be deployed according to the specifications defined below.\nTip: CSM for Observability must be deployed first. Once the module has been deployed, you can proceed to deploying/configuring Prometheus and Grafana.\nPrometheus The Prometheus service should be running on the same Kubernetes cluster as the CSM for Observability services. As part of the CSM for Observability deployment, the OpenTelemetry Collector gets deployed. The OpenTelemetry Collector is what CSM for Observability pushes metrics so that the metrics can be consumed by Prometheus. This means that Prometheus must be configured to scrape the metrics data from the OpenTelemetry Collector.\n   Supported Version Image Helm Chart     2.22.0 prom/prometheus:v2.22.0 Prometheus Helm chart    Note: It is the user’s responsibility to provide persistent storage for Prometheus if they want to preserve historical data.\nPrometheus Deployment Here is a sample minimal configuration for Prometheus. Please note that the configuration below uses insecure skip verify. If you wish to properly configure TLS, you will need to provide a ca_file in the Prometheus configuration. The certificate provided as part of the CSM for Observability deployment should be signed by this same CA. For more information about Prometheus configuration, see Prometheus configuration.\n  Create a values file named prometheus-values.yaml.\n# prometheus-values.yamlalertmanager:enabled:falsenodeExporter:enabled:falsepushgateway:enabled:falsekubeStateMetrics:enabled:falseconfigmapReload:prometheus:enabled:falseserver:enabled:trueimage:repository:quay.io/prometheus/prometheustag:v2.22.0pullPolicy:IfNotPresentpersistentVolume:enabled:falseservice:type:NodePortservicePort:9090serverFiles:prometheus.yml:scrape_configs:- job_name:'karavi-metrics-powerflex'scrape_interval:5sscheme:httpsstatic_configs:- targets:['otel-collector:8443']tls_config:insecure_skip_verify:true  If using Rancher, create a ServiceMonitor.\napiVersion:monitoring.coreos.com/v1kind:ServiceMonitormetadata:name:otel-collectornamespace:powerflexspec:endpoints:- path:/metricsport:exporter-httpsscheme:httpstlsConfig:insecureSkipVerify:trueselector:matchLabels:app.kubernetes.io/instance:karavi-observabilityapp.kubernetes.io/name:otel-collector  Add the Prometheus Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add stable https://charts.helm.sh/stable helm repo update   Install the Helm chart.\nOn your terminal, run the command below:\nhelm install prometheus prometheus-community/prometheus -n [CSM_NAMESPACE] --create-namespace -f prometheus-values.yaml   Grafana The Grafana dashboards require Grafana to be deployed in the same Kubernetes cluster as CSM for Observability. Below are the configuration details required to properly set up Grafana to work with CSM for Observability.\n   Supported Version Helm Chart     7.3.0-7.3.2 Grafana Helm chart    Grafana must be configured with the following data sources/plugins:\n   Name Additional Information     Prometheus data source Prometheus data source   Data Table plugin Data Table plugin   Pie Chart plugin Pie Chart plugin   SimpleJson data source SimpleJson data source    Settings for the Grafana Prometheus data source:\n   Setting Value Additional Information     Name Prometheus    Type prometheus    URL http://PROMETHEUS_IP:PORT The IP/PORT of your running Prometheus instance   Access Proxy     Settings for the Grafana SimpleJson data source:\n   Setting Value     Name Karavi-Topology   URL Access CSM for Observability Topology service at https://karavi-topology.namespace.svc.cluster.local:8443   Skip TLS Verify Enabled (If not using CA certificate)   With CA Cert Enabled (If using CA certificate)    Grafana Deployment Below are the steps to deploy a new Grafana instance into your Kubernetes cluster:\n  Create a ConfigMap.\nWhen using a network that requires a decryption certificate, the Grafana server MUST be configured with the necessary certificate. If no certificate is required, skip to step 2.\n Create a Config file named grafana-configmap.yaml The file should look like this:  # grafana-configmap.yamlapiVersion:v1kind:ConfigMapmetadata:name:certs-configmapnamespace:[CSM_NAMESPACE]labels:certs-configmap:\"1\"data:ca-certificates.crt:|- -----BEGIN CERTIFICATE-----ReplaceMeWithActualCaCERT=-----ENDCERTIFICATE-----NOTE: you need an actual CA Cert for it to work\nOn your terminal, run the commands below:\nkubectl create -f grafana-configmap.yaml   Create a values file.\nCreate a Config file named grafana-configmap.yaml The file should look like this:\n# grafana-values.yaml image:repository:grafana/grafanatag:7.3.0sha:\"\"pullPolicy:IfNotPresentservice:type:NodePort## Administrator credentials when not using an existing SecretadminUser:adminadminPassword:admin## Pass the plugins you want to be installed as a list.##plugins:- grafana-simple-json-datasource- briangann-datatable-panel- grafana-piechart-panel## Configure grafana datasources## ref: http://docs.grafana.org/administration/provisioning/#datasources##datasources:datasources.yaml:apiVersion:1datasources:- name:Karavi-Topologytype:grafana-simple-json-datasourceaccess:proxyurl:'https://karavi-topology:8443'isDefault:nullversion:1editable:truejsonData:tlsSkipVerify:true- name:Prometheustype:prometheusaccess:proxyurl:'http://prometheus:9090'isDefault:nullversion:1editable:truetestFramework:enabled:falsesidecar:datasources:enabled:truedashboards:enabled:true## Additional grafana server CofigMap mounts## Defines additional mounts with CofigMap. CofigMap must be manually created in the namespace.extraConfigmapMounts:[]# If you created a ConfigMap on the previous step, delete [] and uncomment the lines below # - name: certs-configmap# mountPath: /etc/ssl/certs/ca-certificates.crt# subPath: ca-certificates.crt# configMap: certs-configmap# readOnly: true  Add the Grafana Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add grafana https://grafana.github.io/helm-charts helm repo update   Install the Helm chart.\nOn your terminal, run the commands below:\nhelm install grafana grafana/grafana -n [CSM_NAMESPACE] -f grafana-values.yaml   Importing CSM for Observability Dashboards Once Grafana is properly configured, you can import the pre-built observability dashboards. Log into Grafana and click the + icon in the side menu. Then click Import. From here you can upload the JSON files or paste the JSON text directly into the text area. Below are the locations of the dashboards that can be imported:\n   Dashboard Description     PowerFlex: I/O Performance by Kubernetes Node Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by Kubernetes node   PowerFlex: I/O Performance by Provisioned Volume Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume   PowerFlex: Storage Pool Consumption By CSI Driver Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.   PowerStore: I/O Performance by Provisioned Volume As of Release 0.4.0: Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume   CSI Driver Provisioned Volume Topology Provides visibility into Dell EMC CSI (Container Storage Interface) driver provisioned volume characteristics in Kubernetes correlated with volumes on the storage system.    Dynamic Configuration Some parameters can be configured/updated during runtime without restarting the CSM for Observability services. These parameters will be stored in ConfigMaps that can be updated on the Kubernetes cluster. This will automatically change the settings on the services.\n   ConfigMap Observability Service Parameters     karavi-metrics-powerflex-configmap karavi-metrics-powerflex COLLECTOR_ADDRPROVISIONER_NAMESPOWERFLEX_SDC_METRICS_ENABLEDPOWERFLEX_SDC_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_POLL_FREQUENCYPOWERFLEX_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMAT   karavi-metrics-powerstore-configmap karavi-metrics-powerstore COLLECTOR_ADDRPROVISIONER_NAMESPOWERSTORE_VOLUME_METRICS_ENABLEDPOWERSTORE_VOLUME_IO_POLL_FREQUENCYPOWERSTORE_SPACE_POLL_FREQUENCYPOWERSTORE_ARRAY_POLL_FREQUENCYPOWERSTORE_FILE_SYSTEM_POLL_FREQUENCYPOWERSTORE_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY   karavi-topology-configmap karavi-topology PROVISIONER_NAMESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY    To update any of these settings, run the following command on the Kubernetes cluster then save the updated ConfigMap data.\nkubectl edit configmap [CONFIG_MAP_NAME] -n [CSM_NAMESPACE] Tracing CSM for Observability is instrumented to report trace data to Zipkin. This helps gather timing data needed to troubleshoot latency problems with CSM for Observability. Follow the instructions below to enable the reporting of trace data:\n  Deploy a Zipkin instance in the CSM namespace and expose the service as NodePort for external access.\napiVersion: apps/v1 kind: Deployment metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance template: metadata: labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance spec: containers: - name: zipkin image: \"openzipkin/zipkin\" imagePullPolicy: IfNotPresent env: - name: \"STORAGE_TYPE\" value: \"mem\" - name: \"TRANSPORT_TYPE\" value: \"http\" --- apiVersion: v1 kind: Service metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: ports: - port: 9411 targetPort: 9411 protocol: TCP type: \"NodePort\" selector: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance   Add the Zipkin URI to the CSM for Observability ConfigMaps. Based on the manifest above, Zipkin will be running on port 9411.\nNote: Zipkin tracing is currently not supported for the collection of PowerFlex metrics.\nUpdate the ConfigMaps from the table above. Here is an example updating the karavi-topology-configmap based on the deployment manifest above.\nkubectl edit configmap/karavi-topology-configmap -n [CSM_NAMESPACE] Update the ZIPKIN_URI and ZIPKIN_PROBABILITY values and save the ConfigMap.\nZIPKIN_URI: \"http://zipkin:9411/api/v2/spans\" ZIPKIN_SERVICE_NAME: \"karavi-topology\" ZIPKIN_PROBABILITY: \"1.0\" Once the ConfigMaps are updated, the changes will automatically be applied and tracing can be seen by accessing Zipkin on the exposed port.\n  Updating Storage System Credentials If the storage system credentials have been updated in the relevant CSI Driver, CSM for Observability must be updated with those new credentials as follows:\nWhen CSM for Observability uses the Authorization module In this case, all storage system requests made by CSM for Observability will be routed through the Authorization module. The following must be performed:\nUpdate the Authorization Module Token   Delete the current proxy-authz-tokens Secret from the CSM namespace.\n$ kubectl delete secret proxy-authz-tokens -n [CSM_NAMESPACE]   Copy the proxy-authz-tokens Secret from a CSI Driver to the CSM namespace.\n$ kubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSM_CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   Update Storage Systems If the list of storage systems managed by a Dell EMC CSI Driver have changed, the following steps can be performed to update CSM for Observability to reference the updated systems:\n  Delete the current karavi-authorization-config Secret from the CSM namespace.\n$ kubectl delete secret proxy-authz-tokens -n [CSM_NAMESPACE]   Copy the karavi-authorization-config Secret from the CSI Driver namespace to CSM for Observability namespace.\n$ kubectl get secret karavi-authorization-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSM_CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   When CSM for Observability does not use the Authorization module In this case all storage system requests made by CSM for Observability will not be routed through the Authorization module. The following must be performed:\nCSI Driver for Dell EMC PowerFlex   Delete the current vxflexos-config Secret from the CSM namespace.\n$ kubectl delete secret vxflexos-config -n [CSM_NAMESPACE]   Copy the vxflexos-config Secret from the CSI Driver for Dell EMC PowerFlex namespace to the CSM namespace.\n$ kubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   CSI Driver for Dell EMC PowerStore   Delete the current powerstore-config Secret from the CSM namespace.\n$ kubectl delete secret powerstore-config -n [CSM_NAMESPACE]   Copy the powerstore-config Secret from the CSI Driver for Dell EMC PowerStore namespace to the CSM namespace.\n$ kubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   ","excerpt":"CSM for Observability can be deployed in one of three ways:\n CSM …","ref":"/csm-docs/docs/observability/deployment/","title":"Deployment"},{"body":"CSM for Resiliency is installed as part of the Dell CSI driver installation. The drivers can be installed either by a helm chart or by the Dell CSI Operator. Currently, only Helm chart installation is supported.\nFor information on the PowerFlex CSI driver, see PowerFlex CSI Driver.\nFor information on the Unity CSI driver, see Unity CSI Driver.\nConfigure all the helm chart parameters described below before installing the drivers.\nHelm Chart Installation The drivers that support Helm chart installation allow CSM for Resiliency to be optionally installed by variables in the chart. There is a podmon block specified in the values.yaml file of the chart that will look similar to the text below by default:\n# Podmon is an optional feature under development and tech preview. # Enable this feature only after contact support for additional information podmon: enabled: true image: dellemc/podmon:v1.0.0 controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=controller\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" To install CSM for Resiliency with the driver, the following changes are required:\n Enable CSM for Resiliency by changing the podmon.enabled boolean to true. This will enable both controller-podmon and node-podmon. Specify the podmon image to be used as podmon.image. Specify arguments to controller-podmon in the podmon.controller.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon. Specify arguments to node-podmon in the podmon.node.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon.  Podmon Arguments    Argument Required Description Applicability     enabled Required Boolean “true” enables CSM for Resiliency installation with the driver in a helm installation. top level   image Required Must be set to a repository where the podmon image can be pulled. controller \u0026 node   mode Required Must be set to “controller” for controller-podmon and “node” for node-podmon. controller \u0026 node   csisock Required This should be left as set in the helm template for the driver. For controller: -csisock=unix:/var/run/csi/csi.sock For node it will vary depending on the driver’s identity: -csisock=unix:/var/lib/kubelet/plugins\n/vxflexos.emc.dell.com/csi_sock controller \u0026 node   leaderelection Required Boolean value that should be set true for controller and false for node. The default value is true. controller \u0026 node   skipArrayConnectionValidation Optional Boolean value that if set to true will cause controllerPodCleanup to skip the validation that no I/O is ongoing before cleaning up the pod. controller   labelKey Optional String value that sets the label key used to denote pods to be monitored by CSM for Resiliency. It will make life easier if this key is the same for all driver types, and drivers are differentiated by different labelValues (see below). If the label keys are the same across all drivers you can do kubectl get pods -A -l labelKey to find all the CSM for Resiliency protected pods. labelKey defaults to “podmon.dellemc.com/driver”. controller \u0026 node   labelValue Required String that sets the value that denotes pods to be monitored by CSM for Resiliency. This must be specific for each driver. Defaults to “csi-vxflexos” for CSI Driver for Dell EMC PowerFlex and “csi-unity” for CSI Driver for Dell EMC Unity controller \u0026 node   arrayConnectivityPollRate Optional The minimum polling rate in seconds to determine if the array has connectivity to a node. Should not be set to less than 5 seconds. See the specific section for each array type for additional guidance. controller   arrayConnectivityConnectionLossThreshold Optional Gives the number of failed connection polls that will be deemed to indicate array connectivity loss. Should not be set to less than 3. See the specific section for each array type for additional guidance. controller   driver-config-params Required String that set the path to a file containing configuration parameter(for instance, Log levels) for a driver. controller \u0026 node    PowerFlex Specific Recommendations PowerFlex supports a very robust array connection validation mechanism that can detect changes in connectivity in about two seconds and can detect whether I/O has occurred over a five-second sample. For that reason it is recommended to set “skipArrayConnectionValidation=false” (which is the default) and to set “arrayConnectivityPollRate=5” (5 seconds) and “arrayConnectivityConnectionLossThreshold=3” to 3 or more.\nHere is a typical installation used for testing:\npodmon:image:dellemc/podmonenabled:truecontroller:args:- \"-csisock=unix:/var/run/csi/csi.sock\"- \"-labelvalue=csi-vxflexos\"- \"-mode=controller\"- \"-arrayConnectivityPollRate=5\"- \"-arrayConnectivityConnectionLossThreshold=3\"- \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\"node:args:- \"-csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\"- \"-labelvalue=csi-vxflexos\"- \"-mode=node\"- \"-leaderelection=false\"- \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\"Unity Specific Recommendations Here is a typical installation used for testing:\npodmon:image:dellemc/podmonenabled:truecontroller:args:- \"-csisock=unix:/var/run/csi/csi.sock\"- \"-labelvalue=csi-unity\"- \"-driverPath=csi-unity.dellemc.com\"- \"-mode=controller\"- \"--driver-config-params=/unity-config/driver-config-params.yaml\"node:args:- \"-csisock=unix:/var/lib/kubelet/plugins/unity.emc.dell.com/csi_sock\"- \"-labelvalue=csi-unity\"- \"-driverPath=csi-unity.dellemc.com\"- \"-mode=node\"- \"-leaderelection=false\"- \"--driver-config-params=/unity-config/driver-config-params.yaml\"Dynamic parameters CSM for Resiliency has configuration parameters that can be updated dynamically, such as the logging level and format. This can be done by editing the DellEMC CSI Driver’s parameters ConfigMap. The ConfigMap can be queried using kubectl. For example, the DellEMC Powerflex CSI Driver ConfigMaps can be found using the following command: kubectl get -n vxflexos configmap. The ConfigMap to edit will have this pattern: -config-params (e.g., vxflexos-config-params).\nTo update or add parameters, you can use the kubectl edit command. For example, kubectl edit -n vxflexos configmap vxflexos-config-params.\nThis is a list of parameters that can be adjusted for CSM for Resiliency:\n   Parameter Type Default Description     PODMON_CONTROLLER_LOG_FORMAT String “text” Logging format output for the controller podmon sidecar. Should be “text” or “json”   PODMON_CONTROLLER_LOG_LEVEL String “debug” Logging level for the controller podmon sidecar. Standard values: ‘info’, ‘error’, ‘warning’, ‘debug’, ‘trace’   PODMON_NODE_LOG_FORMAT String “text” Logging format output for the node podmon sidecar. Should be “text” or “json”   PODMON_NODE_LOG_LEVEL String “debug” Logging level for the node podmon sidecar. Standard values: ‘info’, ‘error’, ‘warning’, ‘debug’, ‘trace’   PODMON_ARRAY_CONNECTIVITY_POLL_RATE Integer (\u003e0) 15 An interval in seconds to poll the underlying array   PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD Integer (\u003e0) 3 A value representing the number of failed connection poll intervals before marking the array connectivity as lost   PODMON_SKIP_ARRAY_CONNECTION_VALIDATION Boolean false Flag to disable the array connectivity check    Here is an example of the parameters:\nPODMON_CONTROLLER_LOG_FORMAT:\"text\"PODMON_CONTROLLER_LOG_LEVEL:\"info\"PODMON_NODE_LOG_FORMAT:\"text\"PODMON_NODE_LOG_LEVEL:\"info\"PODMON_ARRAY_CONNECTIVITY_POLL_RATE:20PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD:2PODMON_SKIP_ARRAY_CONNECTION_VALIDATION:true","excerpt":"CSM for Resiliency is installed as part of the Dell CSI driver …","ref":"/csm-docs/docs/resiliency/deployment/","title":"Deployment"},{"body":"The Container Storage Modules (CSM) for Observability Helm chart bootstraps an Observability deployment on a Kubernetes cluster using the Helm package manager.\nPrerequisites   A supported CSI Driver is deployed\n  The cert-manager CustomResourceDefinition resources are created.\n$ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.5.3/cert-manager.crds.yaml   Copy the CSI Driver Secret Copy the config Secret from the Dell CSI Driver namespace into the namespace where CSM for Observability is deployed.\nPowerFlex $ kubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Note: The target namespace must exist before executing this command.\nPowerStore $ kubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Note: The target namespace must exist before executing this command.\nAdd the Repo $ helm repo add dell https://dell.github.io/helm-charts Installing the Chart $ helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] --create-namespace The configuration section below lists all the parameters that can be configured during installation\nConfiguration The following table lists the configurable parameters of the CSM for Observability Helm chart and their default values.\n   Parameter Description Default     karaviTopology.image Location of the csm-topology Docker image dellemc/csm-topology:v1.0   karaviTopology.enabled Enable the CSM for Observability Topology service true   karaviTopology.provisionerNames Provisioner Names used to filter the Persistent Volumes created on the Kubernetes cluster (must be a comma-separated list)  csi-vxflexos.dellemc.com   karaviTopology.service.type Kubernetes service type ClusterIP   karaviTopology.certificateFile Optional valid CA public certificate file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’.    karaviTopology.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’.    karaviTopology.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO   karaviTopology.logFormat Output logs in the specified format (Valid values: text, json) text   otelCollector.certificateFile Optional valid CA public certificate file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’.    otelCollector.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’.    otelCollector.service.type Kubernetes service type ClusterIP   karaviMetricsPowerflex.image CSM Metrics for PowerFlex Service image dellemc/csm-metrics-powerflex:v1.0   karaviMetricsPowerflex.enabled Enable CSM Metrics for PowerFlex service true   karaviMetricsPowerflex.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680   karaviMetricsPowerflex.provisionerNames Provisioner Names used to filter for determining PowerFlex SDC nodes( Must be a Comma-separated list)  csi-vxflexos.dellemc.com   karaviMetricsPowerflex.sdcPollFrequencySeconds The polling frequency (in seconds) to gather SDC metrics 10   karaviMetricsPowerflex.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10   karaviMetricsPowerflex.storageClassPoolPollFrequencySeconds The polling frequency (in seconds) to gather storage class/pool metrics 10   karaviMetricsPowerflex.concurrentPowerflexQueries The number of simultaneous metrics queries to make to Powerflex(MUST be less than 10; otherwise, several request errors from Powerflex will ensue. 10   karaviMetricsPowerflex.sdcMetricsEnabled Enable PowerFlex SDC Metrics Collection true   karaviMetricsPowerflex.volumeMetricsEnabled Enable PowerFlex Volume Metrics Collection true   karaviMetricsPowerflex.storageClassPoolMetricsEnabled Enable PowerFlex Storage Class/Pool Metrics Collection true   karaviMetricsPowerflex.endpoint Endpoint for pod leader election karavi-metrics-powerflex   karaviMetricsPowerflex.service.type Kubernetes service type ClusterIP   karaviMetricsPowerflex.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO   karaviMetricsPowerflex.logFormat Output logs in the specified format (Valid values: text, json) text   karaviMetricsPowerstore.image CSM Metrics for PowerStore Service image dellemc/csm-metrics-powerstore:v1.0   karaviMetricsPowerstore.enabled Enable CSM Metrics for PowerStore service true   karaviMetricsPowerstore.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680   karaviMetricsPowerstore.provisionerNames Provisioner Names used to filter for determining PowerStore volumes (must be a Comma-separated list) csi-powerstore.dellemc.com   karaviMetricsPowerstore.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10   karaviMetricsPowerstore.concurrentPowerflexQueries The number of simultaneous metrics queries to make to PowerStore (must be less than 10; otherwise, several request errors from PowerStore will ensue.) 10   karaviMetricsPowerstore.volumeMetricsEnabled Enable PowerStore Volume Metrics Collection true   karaviMetricsPowerstore.endpoint Endpoint for pod leader election karavi-metrics-powerstore   karaviMetricsPowerstore.service.type Kubernetes service type ClusterIP   karaviMetricsPowerstore.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO   karaviMetricsPowerstore.logFormat Output logs in the specified format (Valid values: text, json) text   karaviMetricsPowerstore.zipkin.uri URI of a Zipkin instance where tracing data can be forwarded    karaviMetricsPowerstore.zipkin.serviceName Service name used for Zipkin tracing data metrics-powerstore   karaviMetricsPowerstore.zipkin.probability Percentage of trace information to send to Zipkin (Valid range: 0.0 to 1.0) 0    Specify each parameter using the ‘–set key=value[,key=value]’ and/or ‘–set-file key=value[,key=value] arguments to ‘helm install’. For example:\n$ helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] --create-namespace \\ --set-file karaviTopology.certificateFile=\u003clocation-of-karavi-topology-certificate-file\u003e \\ --set-file karaviTopology.privateKeyFile=\u003clocation-of-karavi-topology-private-key-file\u003e \\ --set-file otelCollector.certificateFile=\u003clocation-of-otel-collector-certificate-file\u003e \\ --set-file otelCollector.privateKeyFile=\u003clocation-of-otel-collector-private-key-file\u003e Alternatively, a YAML file that specifies the values for the above parameters can be provided while installing the chart. For example:\n$ helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] --create-namespace -f values.yaml Note: You can use the default values.yaml\n","excerpt":"The Container Storage Modules (CSM) for Observability Helm chart …","ref":"/csm-docs/docs/observability/deployment/helm/","title":"Helm"},{"body":"The Container Storage Modules (CSM) for Observability installer bootstraps Helm to create a more simplified and robust deployment option that does the following:\n Verifies CSM for Observability is not yet installed Verifies the Kubernetes/Openshift versions are supported Verifies the Helm version is supported Adds the Dell Helm chart repository Refreshes the Helm chart repositories to download any recent changes Creates the CSM namespace (if not already created) Copies the secrets from the CSI driver namespaces into the CSM namespace (if not already copied) Installs the CertManager CRDs (if not already installed) Installs the CSM for Observability Helm chart Waits for the CSM for Observability pods to become ready  If the Authorization module is enabled for the CSI drivers installed in the same Kubernetes cluster, the installer will perform the current steps to enable CSM for Observability to use the same Authorization instance:\n Verifies the karavictl binary is available. Verifies the appropriate Secret exists in the CSI driver namespace. Queries the CSI driver environment to get references to the Authorization module sidecar-proxy Docker image and URL of the proxy server. Updates the CSM for Observability deployment to use the existing Authorization instance.  Online Installer The following instructions can be followed to install CSM for Observability in an environment that has an internet connection and is capable of downloading the required Helm chart and Docker images.\nDependencies A Linux-based system, with internet access, will be used to execute the script to install CSM for Observability into a Kubernetes/Openshift environment that also has internet access.\n   Dependency Usage     kubectl kubectl will be used to verify the Kubernetes/OpenShift environment   helm helm will be used to install the CSM for Observability helm chart   jq jq will be used to parse the CSM for Authorization configuration file during installation    Installer Usage [user@system /home/user/karavi-observability/installer]# ./karavi-observability-install.sh --help Help for ./karavi-observability-install.sh Usage: ./karavi-observability-install.sh mode options... Mode: install Installs Karavi Observability and enables Karavi Authorization if already installed enable-authorization Updates existing installation of Karavi Observability with Karavi Authorization Options: Required --namespace[=]\u003cnamespace\u003e Namespace where Karavi Observability will be installed Optional --auth-image-addr Docker registry location of the Karavi Authorization sidecar proxy image --auth-proxy-host Host address of the Karavi Authorization proxy server --csi-powerflex-namespace[=]\u003ccsi powerflex namespace\u003e Namespace where CSI PowerFlex is installed, default is 'vxflexos' --set-file Set values from files used during helm installation (can be specified multiple times) --skip-verify Skip verification of the environment --values[=]\u003cvalues.yaml\u003e Values file, which defines configuration values --verbose Display verbose logging --version[=]\u003chelm chart version\u003e Helm chart version to install, default value will be latest --help Help Note: CSM for Authorization currently does not support the Observability module for PowerStore. Therefore setting enable-authorization is not supported in this case.\nExecuting the Installer To perform an online installation of CSM for Observability, the following steps should be performed:\n  Clone the GitHub repository:\n[user@system /home/user]# git clone https://github.com/dell/karavi-observability.git   Change to the installer directory:\n[user@system /home/user]# cd karavi-observability/installer   Execute the installation script. The following example will install CSM for Observability into the CSM namespace.\nA sample values.yaml file is located here. This can be copied into a file named myvalues.yaml and modified accordingly for the installer command below. Configuration options are outlined in the Helm chart deployment section.\n[user@system /home/user/karavi-observability/installer]# ./karavi-observability-install.sh install --namespace [CSM_NAMESPACE] --values myvalues.yaml --------------------------------------------------------------------------------- \u003e Installing Karavi Observability in namespace karavi on 1.19 --------------------------------------------------------------------------------- | |- Karavi Observability is not installed Success | |- Karavi Authorization will be enabled during installation | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying helm version Success | |- Configure helm chart repository | |--\u003e Adding helm repository https://dell.github.io/helm-charts Success | |--\u003e Updating helm repositories Success | |- Creating namespace karavi Success | |- Copying Secret from vxflexos to karavi Success | |- Installing CertManager CRDs Success | |- Installing Karavi Observability helm chart Success | |- Waiting for pods in namespace karavi to be ready Success | |- Copying Secret from vxflexos to karavi Success | |- Enabling Karavi Authorization for Karavi Observability Success | |- Waiting for pods in namespace karavi to be ready Success   ","excerpt":"The Container Storage Modules (CSM) for Observability installer …","ref":"/csm-docs/docs/observability/deployment/online/","title":"Installer"},{"body":"The following instructions can be followed when a Helm chart will be installed in an environment that does not have an internet connection and will be unable to download the Helm chart and related Docker images.\nDependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\n One Linux-based system, with internet access, will be used to create the bundle. This involves the user invoking a script that utilizes docker to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker to restore container images from file and push them to a registry  If one Linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker docker will be used to pull images from public image registries, tag them, and push them to a private registry.\nRequired on both the system building the offline bundle as well as the system preparing for installation. Tested version is docker 18.09    Executing the Installer To perform an offline installation of a Helm chart, the following steps should be performed:\n Build an offline bundle. Unpack the offline bundle and prepare for installation. Perform a Helm installation.  Build the Offline Bundle   Copy the offline-installer.sh script to a local Linux system using curl or wget:\n[user@anothersystem /home/user]# curl https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh --output offline-installer.sh or\n[user@anothersystem /home/user]# wget -O offline-installer.sh https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh   Set the file as executable.\n[user@anothersystem /home/user]# chmod +x offline-installer.sh   Build the bundle by providing the Helm chart name as the argument:\n[user@anothersystem /home/user]# ./offline-installer.sh -c dell/karavi-observability * * Adding Helm repository https://dell.github.io/helm-charts * * Downloading Helm chart dell/karavi-observability to directory /home/user/offline-karavi-observability-bundle/helm-original * * Downloading and saving Docker images dellemc/csm-topology:v0.3.0 dellemc/csm-metrics-powerflex:v0.3.0 otel/opentelemetry-collector:0.9.0 nginxinc/nginx-unprivileged:1.18 * * Compressing offline-karavi-observability-bundle.tar.gz   Unpack the Offline Bundle   Copy the bundle file to another Linux system that has access to the internal Docker registry and that can install the Helm chart. From that Linux system, unpack the bundle.\n[user@anothersystem /home/user]# tar -xzf offline-karavi-observability-bundle.tar.gz   Change directory into the new directory created from unpacking the bundle:\n[user@anothersystem /home/user]# cd offline-karavi-observability-bundle   Prepare the bundle by providing the internal Docker registry URL.\n[user@anothersystem /home/user/offline-karavi-observability-bundle]# ./offline-installer.sh -p \u003cmy-registry\u003e:5000 * * Loading, tagging, and pushing Docker images to registry \u003cmy-registry\u003e:5000/ dellemc/csm-topology:v0.3.0 -\u003e \u003cmy-registry\u003e:5000/csm-topology:v0.3.0 dellemc/csm-metrics-powerflex:v0.3.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerflex:v0.3.0 otel/opentelemetry-collector:0.9.0 -\u003e \u003cmy-registry\u003e:5000/opentelemetry-collector:0.9.0 nginxinc/nginx-unprivileged:1.18 -\u003e \u003cmy-registry\u003e:5000/nginx-unprivileged:1.18   Perform Helm installation   Change directory to helm which contains the updated Helm chart directory:\n[user@anothersystem /home/user/offline-karavi-observability-bundle]# cd helm   Install necessary cert-manager CustomResourceDefinitions provided:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl apply --validate=false -f cert-manager.crds.yaml   Copy the CSI Driver Secret(s)\nCopy the CSI Driver Secret from the namespace where CSI Driver is installed to the namespace where CSM for Observability is to be installed.\nCSI Driver for PowerFlex:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for PowerStore\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   Now that the required images have been made available and the Helm chart’s configuration updated with references to the internal registry location, installation can proceed by following the instructions that are documented within the Helm chart’s repository.\nNote: Optionally, you could provide your own configurations. A sample values.yaml file is located here.\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# helm install -n install-namespace app-name karavi-observability NAME: app-name LAST DEPLOYED: Fri Nov 6 08:48:13 2020 NAMESPACE: install-namespace STATUS: deployed REVISION: 1 TEST SUITE: None   (Optional) The following steps can be performed to enable CSM for Observability to use an existing instance of Authorization for accessing the REST API for the given storage systems.\nNote: CSM for Authorization currently does not support the Observability module for PowerStore.\nCopy the proxy Secret into the CSM for Observability namespace:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Use karavictl to update the Observability module deployment to use the Authorization module. Required parameters are the location of the sidecar-proxy Docker image and the URL of the Authorization module proxy. If the Authorization module was installed using certificates, the flags --insecure=false and --root-certificate \u003clocation-of-root-certificate\u003e must be also be provided. If certificates were not provided during installation, the flag --insecure=true must be provided.\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secrets,deployments -n [CSM_NAMESPACE] -o yaml | karavictl inject --insecure=false --root-certificate \u003clocation-of-root-certificate\u003e --image-addr \u003csidecar-proxy-image-location\u003e --proxy-host \u003cproxy-host\u003e | kubectl apply -f -   ","excerpt":"The following instructions can be followed when a Helm chart will be …","ref":"/csm-docs/docs/observability/deployment/offline/","title":"Offline Installer"},{"body":"Users can install the Dell CSI Operator via Operatorhub.io on Kubernetes. The following outlines the process to do so:\nSteps\n Search dell in the storage category in Operatorhub.io.  Click DellEMC Operator.  Check the desired version is selected and click Install. Follow the provided instructions.  Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","excerpt":"Users can install the Dell CSI Operator via Operatorhub.io on …","ref":"/csm-docs/docs/csidriver/partners/operator/","title":"OperatorHub.io"},{"body":"Users can install the Dell CSI Operator via OperatorHub.io on Kubernetes. The following outlines the process to do so:\nSteps\n Search DellEMC in the storage category in Operatorhub.io.  Click DellEMC Operator.  Check the desired version is selected and click Install. Follow the provided instructions.  Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","excerpt":"Users can install the Dell CSI Operator via OperatorHub.io on …","ref":"/csm-docs/v1/partners/operator/","title":"OperatorHub.io"},{"body":"Users can install the Dell CSI Operator via OperatorHub.io on Kubernetes. The following outlines the process to do so:\nSteps\n  Search DellEMC in storage category in Operatorhub.io.   Click DellEMC Operator.   Check the desired version is selected and click Install. Follow the provided instructions.   Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","excerpt":"Users can install the Dell CSI Operator via OperatorHub.io on …","ref":"/csm-docs/v2/partners/operator/","title":"OperatorHub.io"},{"body":"Users can install the Dell CSI Operator via OperatorHub.io on Kubernetes. The following outlines the process to do so:\nSteps\n  Search DellEMC in storage category in Operatorhub.io.   Click DellEMC Operator.   Check the desired version is selected and click Install. Follow the provided instructions.   Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","excerpt":"Users can install the Dell CSI Operator via OperatorHub.io on …","ref":"/csm-docs/v3/partners/operator/","title":"OperatorHub.io"},{"body":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\n Type “Dell” in the OperatorHub section under Operators, to get the list of available Dell CSI Operators.  Check the version you want to install from the list, you can check the details by clicking it.  Once selected, click “Install” to proceed with the installation process.  You can verify the list of available operators by selecting the “Installed Operator” section.  Select the Dell CSI Operator for further details.  Install CSI Drivers via Operator Steps\n Select the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected.  After clicking the “Create CSIUnity” option in the above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters.  You can check the driver installed and node and controller pods running in the Pods section under Workloads.  ","excerpt":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the …","ref":"/csm-docs/docs/csidriver/partners/redhat/","title":"Red Hat OpenShift"},{"body":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\n Type “Dell” in the OperatorHub section under Operators, to get the list of available Dell CSI Operators.  Check the version you want to install from the list, you can check the details by clicking it.  Once selected, click “Install” to proceed with the installation process.  You can verify the list of available operators by selecting the “Installed Operator” section.  Select the Dell CSI Operator for further details.  Install CSI Drivers via Operator Steps\n Select the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected.  After clicking the “Create CSIUnity” option in the above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters.  You can check the driver installed and node and controller pods running in the Pods section under Workloads.  ","excerpt":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the …","ref":"/csm-docs/v1/partners/redhat/","title":"Red Hat OpenShift"},{"body":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\n  Type “Dell” in the OperatorHub section under Operators, to get the list of available Dell CSI Operators.   Check the version you want to install from the list, you can check the details by clicking it.   Once selected, click “Install” to proceed with installation process.   You can verify the list of available operators by selecting “Installed Operator” section.   Select the Dell CSI Operator to get further description.   Install CSI Drivers via Operator Steps\n  Select the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected.   After clicking “Create CSIUnity” option in above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters.   You can check the driver installed and node and controller pods running in the Pods section under Workloads.   ","excerpt":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the …","ref":"/csm-docs/v2/partners/redhat/","title":"Red Hat OpenShift"},{"body":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\n  Type “Dell” in the OperatorHub section under Operators, to get the list of available Dell CSI Operators.   Check the version you want to install from the list, you can check the details by clicking it.   Once selected, click “Install” to proceed with installation process.   You can verify the list of available operators by selecting “Installed Operator” section.   Select the Dell CSI Operator to get further description.   Install CSI Drivers via Operator Steps\n  Select the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected.   After clicking “Create CSIUnity” option in above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters.   You can check the driver installed and node and controller pods running in the Pods section under Workloads.   ","excerpt":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the …","ref":"/csm-docs/v3/partners/redhat/","title":"Red Hat OpenShift"},{"body":"Uninstall a CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the driver:\n./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e For usage information:\n[dell-csi-helm-installer]# ./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u003cnamespace\u003e Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u003chelm release\u003e Name to register with helm, default value will match the driver name -h Help Uninstall a CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed by the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall the driver installed via the operator, delete the Custom Resource(CR)\n# Replace driver-type, driver-name and driver-namespace with their respective values kubectl delete \u003cdriver-type\u003e/\u003cdriver-name\u003e -n \u003cdriver-namespace\u003e ","excerpt":"Uninstall a CSI driver installed via Helm To uninstall a driver, the …","ref":"/csm-docs/docs/csidriver/uninstall/","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Observability.\nUninstall the CSM for Observability Helm Chart The command below removes all the Kubernetes components associated with the chart.\n$ helm delete karavi-observability --namespace [CSM_NAMESPACE] You may also want to uninstall the CRDs created for cert-manager.\n$ kubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.5.3/cert-manager.crds.yaml ","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/docs/observability/uninstall/","title":"Uninstallation"},{"body":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the driver:\n./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e For usage information:\n[dell-csi-helm-installer]# ./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u003cnamespace\u003e Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u003chelm release\u003e Name to register with helm, default value will match the driver name -h Help Uninstall a Dell CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed by the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall the driver installed via the operator, delete the Custom Resource(CR)\n# Replace driver-type, driver-name and driver-namespace with their respective values kubectl delete \u003cdriver-type\u003e/\u003cdriver-name\u003e -n \u003cdriver-namespace\u003e ","excerpt":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, …","ref":"/csm-docs/v1/uninstall/","title":"Uninstallation"},{"body":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the driver:\n./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e For usage information:\n[dell-csi-helm-installer]# ./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u003cnamespace\u003e Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u003chelm release\u003e Name to register with helm, default value will match the driver name -h Help Uninstall a Dell CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall the driver installed via the operator, delete the Custom Resource(CR)\n# Replace driver-name and driver-namespace with their respective values kubectl delete \u003cdriver-name\u003e -n \u003cdriver-namespace\u003e ","excerpt":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, …","ref":"/csm-docs/v2/uninstall/","title":"Uninstallation"},{"body":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the PowerScale driver:\n./csi-uninstall.sh --namespace isilon/\u003cdriver-namespace\u003e For usage information:\n[dell-csi-helm-installer]# ./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u003cnamespace\u003e Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u003chelm release\u003e Name to register with helm, default value will match the driver name -h Help Uninstall a Dell CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall a PowerFlex driver installed via the operator, delete the Custom Resource(CR)\n# Replace driver-name and driver-namespace with their respective values $ kubectl delete vxflexos/\u003cdriver-name\u003e -n \u003cdriver-namespace\u003e ","excerpt":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, …","ref":"/csm-docs/v3/uninstall/","title":"Uninstallation"},{"body":"","excerpt":"","ref":"/csm-docs/docs/csidriver/upgradation/","title":"Upgrade"},{"body":"CSM for Observability can only be upgraded via the Helm chart following the instructions below.\nCSM for Observability Helm upgrade can be used if the initial deployment was performed using the Helm chart or Online Installer.\n Note: The Offline Installer does not support upgrade.\n Helm Chart Upgrade To upgrade an existing Helm installation of CSM for Observability to the latest release, download the latest Helm charts.\nhelm repo update Check if the latest Helm chart version is available:\nhelm search repo dell NAME CHART VERSION APP VERSION DESCRIPTION dell/karavi-observability 1.0.1 1.0.0 CSM for Observability is part of the [Container...  Note: If using cert-manager CustomResourceDefinitions older than v1.5.3, delete the old CRDs and install v1.5.3 of the CRDs prior to upgrade. See Prerequisites for location of CRDs.\n Upgrade to the latest CSM for Observability release:\n$ helm upgrade --version $latest_chart_version --values values.yaml karavi-observability dell/karavi-observability -n $namespace The configuration section lists all the parameters that can be configured using the values.yaml file.\n","excerpt":"CSM for Observability can only be upgraded via the Helm chart …","ref":"/csm-docs/docs/observability/upgrade/","title":"Upgrade"},{"body":"CSM for Resiliency can be upgraded as part of the Dell CSI driver upgrade process. The drivers can be upgraded either by a helm chart or by the Dell CSI Operator. Currently, only Helm chart upgrade is supported for CSM for Resiliency.\nFor information on the PowerFlex CSI driver upgrade process, see PowerFlex CSI Driver.\nFor information on the Unity CSI driver upgrade process, see Unity CSI Driver.\nHelm Chart Upgrade To upgrade CSM for Resiliency with the driver, the following steps are required.\n Note: These steps refer to the values file and csi-install.sh script that were used during initial installation of the Dell CSI driver.\n Steps\n Update the podmon.image value in the values files to reference the new podmon image. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  ","excerpt":"CSM for Resiliency can be upgraded as part of the Dell CSI driver …","ref":"/csm-docs/docs/resiliency/upgrade/","title":"Upgrade"},{"body":"","excerpt":"","ref":"/csm-docs/v1/upgradation/","title":"Upgrade"},{"body":"","excerpt":"","ref":"/csm-docs/v2/upgradation/","title":"Upgrade"},{"body":"","excerpt":"","ref":"/csm-docs/v3/upgradation/","title":"Upgrade"},{"body":"Container Storage Modules (CSM) for Authorization is part of the open-source suite of Kubernetes storage enablers for Dell EMC products.\nCSM for Authorization provides storage and Kubernetes administrators the ability to apply RBAC for Dell EMC CSI Drivers. It does this by deploying a proxy between the CSI driver and the storage system to enforce role-based access and usage rules.\nStorage administrators of compatible storage platforms will be able to apply quota and RBAC rules that instantly and automatically restrict cluster tenants usage of storage resources. Users of storage through CSM for Authorization do not need to have storage admin root credentials to access the storage system.\nKubernetes administrators will have an interface to create, delete, and manage roles/groups that storage rules may be applied. Administrators and/or users may then generate authentication tokens that may be used by tenants to use storage with proper access policies being automatically enforced.\nThe following diagram shows a high-level overview of CSM for Authorization with a tenant-app that is using a CSI driver to perform storage operations through the CSM for Authorization proxy-server to access the a Dell EMC storage system. All requests from the CSI driver will contain the token for the given tenant that was granted by the Storage Administrator.\nCSM for Authorization Capabilities   Feature PowerFlex PowerMax PowerScale Unity PowerStore     Ability to set storage quota limits to ensure k8s tenants are not overconsuming storage Yes Yes No (natively supported) No No   Ability to create access control policies to ensure k8s tenant clusters are not accessing storage that does not belong to them Yes Yes No (natively supported) No No   Ability to shield storage credentials from Kubernetes administrators ensuring credentials are only handled by storage admins Yes Yes Yes No No    NOTE: PowerScale OneFS implements its own form of Role-Based Access Control (RBAC). CSM for Authorization does not enforce any role-based restrictions for PowerScale. To configure RBAC for PowerScale, refer to the PowerScale OneFS documentation.\nSupported Operating Systems/Container Orchestrator Platforms   COP/OS Supported Versions     Kubernetes 1.20, 1.21, 1.22   Red Hat OpenShift 4.7, 4.8   RHEL 7.x, 8.x   CentOS 7.8, 7.9    Supported Storage Platforms    PowerMax PowerFlex PowerScale     Storage Array 5978.479.479, 5978.669.669, 5978.711.711, Unisphere 9.2 3.5.x, 3.6.x OneFS 8.1, 8.2, 9.0, 9.1, 9.2    Supported CSI Drivers CSM for Authorization supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell EMC PowerFlex csi-powerflex v2.0   CSI Driver for Dell EMC PowerMax csi-powermax v2.0   CSI Driver for Dell EMC PowerScale csi-powerscale v2.0    Note: If the deployed CSI driver has a number of controller pods equal to the number of schedulable nodes in your cluster, CSM for Authorization may not be able to inject properly into the driver’s controller pod. To resolve this, please refer to our troubleshooting guide on the topic.\nRoles and Responsibilities The CSM for Authorization CLI can be executed in the context of the following roles:\n Storage Administrators Kubernetes Tenant Administrators  Storage Administrators Storage Administrators can perform the following operations within CSM for Authorization\n Tenant Management (create, get, list, delete, bind roles, unbind roles) Token Management (generate, revoke) Storage System Management (create, get, list, update, delete) Storage Access Roles Management (assign to a storage system with an optional quota)  Tenant Administrators Tenants of CSM for Authorization can use the token provided by the Storage Administrators in their storage requests.\nWorkflow  Tenant Admin requests storage from a Storage Admin. Storage Admin uses CSM Authorization CLI to:\na) Create a tenant resource.\nb) Create a role permitting desired storage access.\nc) Assign the role to the tenant and generate a token.\n Storage Admin returns a token to the Tenant Admin. Tenant Admin inputs the Token into their Kubernetes cluster as a Secret. Tenant Admin updates CSI driver with CSM Authorization sidecar module.  ","excerpt":"Container Storage Modules (CSM) for Authorization is part of the …","ref":"/csm-docs/docs/authorization/","title":"Authorization"},{"body":"Disaster Recovery Workflows Once the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), users can exercise the general Disaster Recovery workflows.\nPlanned Migration to the target cluster/array This scenario is the choice when you want to try your disaster recovery plan or you need to switch activities from one site to another.\na. Execute \"failover\" action on selected ReplicationGroup using the cluster name ./repctl --rg rg-id failover --to-cluster target-cluster-name b. Execute \"reprotect\" action on selected ReplicationGroup which will resume the replication from new \"source\" ./repctl --rg rg-id reprotect --to-cluster new-source-cluster-name \u003cbr\u003e \u003cbr\u003e ![state_changes1](../state_changes1.png) \u003cbr\u003e \u003cbr\u003e  Unplanned Migration to the target cluster/array This scenario is the choice when you lost a site.\na. Execute \"failover\" action on selected ReplicationGroup using the cluster name ./repctl --rg rg-id failover --to-cluster target-cluster-name --unplanned b. Execute \"swap\" action on selected ReplicationGroup which would swap personalities of R1 and R2 (only applicable for PowerMax driver) ./repctl --rg rg-id swap --to-cluster target-cluster-name **Note:** Unplanned migration usually happens when the original \"source\" cluster is unavailable. The following action makes sense when the cluster is back. c. Execute \"reprotect\" action on selected ReplicationGroup which will resume the replication. ./repctl --rg rg-id reprotect --to-cluster new-source-cluster-name \u003cbr\u003e \u003cbr\u003e ![state_changes2](../state_changes2.png) \u003cbr\u003e \u003cbr\u003e   Note: When users do Failover and Failback, the tests pods on the source cluster may go “CrashLoopOff” state since it will try to remount the same volume which is already mounted. To get around this problem bring down the number of replicas to 0 and then after that is done, bring it up to 1.\n ","excerpt":"Disaster Recovery Workflows Once the DellCSIReplicationGroup \u0026 …","ref":"/csm-docs/docs/replication/disaster-recovery/","title":"Disaster Recovery"},{"body":"","excerpt":"","ref":"/csm-docs/docs/csidriver/features/","title":"Features"},{"body":"","excerpt":"","ref":"/csm-docs/v1/features/","title":"Features"},{"body":"","excerpt":"","ref":"/csm-docs/v2/features/","title":"Features"},{"body":"","excerpt":"","ref":"/csm-docs/v3/features/","title":"Features"},{"body":"Install Replication Walkthrough Here are simple steps on how you can start using Container Storage Modules (CSM) for Replication with help from repctl\n Prepare admin Kubernetes clusters configs Add admin configs as clusters to repctl ./repctl cluster add -f \"/root/.kube/config-1\",\"/root/.kube/config-2\" -n \"cluster-1\",\"cluster-2\"  Install replication controller and CRDs ./repctl create -f ../deploy/replicationcrds.all.yaml ./repctl create -f ../deploy/controller.yaml  NOTE: The controller will report that configmap is invalid. This is expected behavior. The message should disappear once you inject the kubeconfigs (next step).\n  (Choose one)  (More secure) Inject service accounts’ configs into clusters ./repctl cluster inject --use-sa  (Less secure) Inject admin configs into clusters ./repctl cluster inject    Modify examples/\u003cstorage\u003e_example_values.yaml config with replication information  NOTE: clusterID should match names you gave to clusters in step 2\n  Create replication storage classes using config ./repctl create sc --from-config ./examples/\u003cstorage\u003e_example_values.yaml  Install CSI driver for your chosen storage in source cluster and provision replicated volumes (optional) Create PVCs on target cluster from Replication Group ./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false   ","excerpt":"Install Replication Walkthrough Here are simple steps on how you can …","ref":"/csm-docs/docs/replication/deployment/install-repctl/","title":"Installation using repctl"},{"body":" Running karavictl inject leaves the vxflexos-controller in a Pending state Running karavictl inject leaves the powermax-controller in a Pending state Running karavictl inject leaves the isilon-controller in a Pending state   Retrieve CSM Authorization Server Logs To retrieve logs from services on the CSM Authorization Server, run the following command (e.g proxy-server logs):\n$ k3s kubectl logs deploy/proxy-server -n karavi -c proxy-server For OPA related logs, run:\n$ k3s kubectl logs deploy/proxy-server -n karavi -c opa Running “karavictl inject” leaves the vxflexos-controller in a “Pending” state This situation may occur when the number of vxflexos-controller pods that are deployed is equal to the number of schedulable nodes.\n$ kubectl get pods -n vxflexos NAME READY STATUS RESTARTS AGE vxflexos-controller-696cc5945f-4t94d 0/6 Pending 0 3m2s vxflexos-controller-75cdcbc5db-k25zx 5/5 Running 0 3m41s vxflexos-controller-75cdcbc5db-nkxqh 5/5 Running 0 3m42s vxflexos-node-mjc74 3/3 Running 0 2m44s vxflexos-node-zgswp 3/3 Running 0 2m44s Resolution\nTo resolve this issue, we need to temporarily reduce the number of replicas that the driver deployment is using.\n  Edit the deployment\n$ kubectl edit -n vxflexos deploy/vxflexos-controller   Find replicas under the spec section of the deployment manifest.\n  Reduce the number of replicas by 1\n  Save the file\n  Confirm that the updated controller pods have been deployed\n$ kubectl get pods -n vxflexos NAME READY STATUS RESTARTS AGE vxflexos-controller-696cc5945f-4t94d 6/6 Running 0 4m41s vxflexos-node-mjc74 3/3 Running 0 3m44s vxflexos-node-zgswp 3/3 Running 0 3m44s   Edit the deployment again\n  Find replicas under the spec section of the deployment manifest.\n  Increase the number of replicas by 1\n  Save the file\n  Confirm that the updated controller pods have been deployed\n$ kubectl get pods -n vxflexos NAME READY STATUS RESTARTS AGE vxflexos-controller-696cc5945f-4t94d 6/6 Running 0 5m41s vxflexos-controller-696cc5945f-6xxhb 6/6 Running 0 5m41s vxflexos-node-mjc74 3/3 Running 0 4m44s vxflexos-node-zgswp 3/3 Running 0 4m44s   Running “karavictl inject” leaves the powermax-controller in a “Pending” state This situation may occur when the number of powermax-controller pods that are deployed is equal to the number of schedulable nodes.\n$ kubectl get pods -n powermax NAME READY STATUS RESTARTS AGE powermax-controller-58d8779f5d-v7t56 0/6 Pending 0 25s powermax-controller-78f749847-jqphx 5/5 Running 0 10m powermax-controller-78f749847-w6vp5 5/5 Running 0 10m powermax-node-gx5pk 3/3 Running 0 21s powermax-node-k5gwc 3/3 Running 0 17s Resolution\nTo resolve this issue, we need to temporarily reduce the number of replicas that the driver deployment is using.\n  Edit the deployment\n$ kubectl edit -n powermax deploy/powermax-controller   Find replicas under the spec section of the deployment manifest.\n  Reduce the number of replicas by 1\n  Save the file\n  Confirm that the updated controller pods have been deployed\n$ kubectl get pods -n powermax NAME READY STATUS RESTARTS AGE powermax-controller-58d8779f5d-cqx8d 6/6 Running 0 22s powermax-node-gx5pk 3/3 Running 3 8m3s powermax-node-k5gwc 3/3 Running 3 7m59s   Edit the deployment again\n  Find replicas under the spec section of the deployment manifest.\n  Increase the number of replicas by 1\n  Save the file\n  Confirm that the updated controller pods have been deployed\n$ kubectl get pods -n powermax NAME READY STATUS RESTARTS AGE powermax-controller-58d8779f5d-cqx8d 6/6 Running 0 22s powermax-controller-58d8779f5d-v7t56 6/6 Running 22 8m7s powermax-node-gx5pk 3/3 Running 3 8m3s powermax-node-k5gwc 3/3 Running 3 7m59s   Running “karavictl inject” leaves the isilon-controller in a “Pending” state This situation may occur when the number of Isilon controller pods that are deployed is equal to the number of schedulable nodes.\n$ kubectl get pods -n isilon NAME READY STATUS RESTARTS AGE isilon-controller-58d8779f5d-v7t56 0/6 Pending 0 25s isilon-controller-78f749847-jqphx 5/5 Running 0 10m isilon-controller-78f749847-w6vp5 5/5 Running 0 10m isilon-node-gx5pk 3/3 Running 0 21s isilon-node-k5gwc 3/3 Running 0 17s Resolution\nTo resolve this issue, we need to temporarily reduce the number of replicas that the driver deployment is using.\n  Edit the deployment\n$ kubectl edit -n \u003cnamespace\u003e deploy/isilon-controller   Find replicas under the spec section of the deployment manifest.\n  Reduce the number of replicas by 1\n  Save the file\n  Confirm that the updated controller pods have been deployed\n$ kubectl get pods -n isilon NAME READY STATUS RESTARTS AGE isilon-controller-696cc5945f-4t94d 6/6 Running 0 4m41s isilon-node-mjc74 3/3 Running 0 3m44s isilon-node-zgswp 3/3 Running 0 3m44s   Edit the deployment again\n  Find replicas under the spec section of the deployment manifest.\n  Increase the number of replicas by 1\n  Save the file\n  Confirm that the updated controller pods have been deployed\n$ kubectl get pods -n isilon NAME READY STATUS RESTARTS AGE isilon-controller-58d8779f5d-cqx8d 6/6 Running 0 22s isilon-controller-58d8779f5d-v7t56 6/6 Running 22 8m7s isilon-node-gx5pk 3/3 Running 3 8m3s isilon-node-k5gwc 3/3 Running 3 7m59s   ","excerpt":" Running karavictl inject leaves the vxflexos-controller in a Pending …","ref":"/csm-docs/docs/authorization/troubleshooting/","title":"Troubleshooting"},{"body":"Frequently Asked Questions  Why does the installation fail due to an invalid cipherKey value? Why does the cluster-init pod show the error “cluster has already been initialized”? Why does the precheck fail when creating an application? How can I view detailed logs for the CSM Installer? After deleting an application, why can’t I re-create the same application?  Why does the installation fail due to an invalid cipherKey value? The cipherKey value used during deployment of the CSM Installer must be exactly 32 characters in length and contained within quotes.\nWhy does the cluster-init pod show the error “cluster has already been initialized”? During the initial start-up of the CSM Installer, the database will be initialized by the cluster-init job. If the CSM Installer is uninstalled and then re-installed on the same cluster, this error may be shown due to the Persistent Volume for the database already containing an initialized database. The CSM Installer will function as normal and the cluster-init job can be ignored.\nIf a clean installation of the CSM Installer is required, the dbVolumeDirectory (default location /var/lib/cockroachdb) must be deleted from the worker node which is hosting the Persistent Volume. After this directory is deleted, the CSM Installer can be re-installed.\nCaution: Deleting the dbVolumeDirectory location will remove any data persisted by the CSM Installer including clusters, storage systems, and installed applications.\nWhy does the precheck fail when creating an application? Each CSI Driver and CSM Module has required software or CRDs that must be installed before the application can be deployed in the cluster. These prechecks are verified when the csm create application command is executed. If the error message “create application failed” is displayed, review the CSM Installer logs to view details about the failed prechecks.\nIf the precheck fails due to required software (e.g. iSCSI, NFS, SDC) not installed on the cluster nodes, follow these steps to address the issue:\n Delete the cluster from the CSM Installer using the csm delete cluster command. Update the nodes in the cluster by installing required software. Add the cluster to the CSM Installer using the csm add cluster command.  How can I view detailed logs for the CSM Installer? Detailed logs of the CSM Installer can be displayed using the following command:\nkubectl logs -f -n \u003cnamespace\u003e deploy/dell-csm-installer After deleting an application, why can’t I re-create the same application? After deleting an application using the csm delete application command, the namespace and other non-application resources including Secrets are not deleted from the cluster. This is to prevent removing any resources that may not have been created by the CSM Installer. The namespace must be manually deleted before attempting to re-create the same application using the CSM Installer.\n","excerpt":"Frequently Asked Questions  Why does the installation fail due to an …","ref":"/csm-docs/docs/deployment/troubleshooting/","title":"Troubleshooting"},{"body":"Frequently Asked Questions  Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? How can I diagnose an issue with Container Storage Modules (CSM) for Observability? How can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? How can I debug and troubleshoot issues with Kubernetes? How can I troubleshoot latency problems with CSM for Observability? Why does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage?  Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? This issue can arise when the topology service manifest is updated to expose the service as NodePort and a client makes a request to the service. Karavi-toplogy is configured with a self-signed or custom certificate and when a client does not recognize a server’s certificate, it shows an error and pings the server(karavi-topology) with the error. You would see the issue when accessing the service through a browser or curl:\nBrowser experience A user who tries to connect to karavi-topology on any browser may receive an error/warning message about the certificate. The message may vary depending on the browser. For instance, in Internet Explorer, you’ll see:\nThere is a problem with this website's security certificate. The security certificate presented by this website was not issued by a trusted certificate authority While this certificate problem may indicate an attempt to fool you or intercept data you send to the server, see resolution on how to fix it\nCurl experience A user who tries to connect to karavi-topology by using curl may receive the following warning or error message:\n[root@:~]$ curl -v https://\u003ckaravi-topology-cluster-IP\u003e:\u003cport?/query * Trying ***********... * TCP_NODELAY set * Connected to *********** (***********) port 31433 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /etc/ssl/certs/ca-certificates.crt CApath: /etc/ssl/certs * TLSv1.3 (OUT), TLS handshake, Client hello (1): * TLSv1.3 (IN), TLS handshake, Server hello (2): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Unknown (8): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Certificate (11): * TLSv1.3 (OUT), TLS alert, Server hello (2): * SSL certificate problem: unable to get local issuer certificate * stopped the pause stream! * Closing connection 0 curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: https://curl.haxx.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. Kubernetes Admin experience Due to the error above, the client pings the topology server with a TLS handshake error which is logged in karavi-topology pod. For instance,\n[root@:~]$ kubectl logs -n powerflex karavi-topology-5d4669d6dd-trzxw 2021/04/27 09:38:28 Set DriverNames to [csi-vxflexos.dellemc.com] 2021/04/28 07:15:05 http: TLS handshake error from 10.42.0.0:58450: local error: tls: bad record MAC 2021/04/28 07:16:14 http: TLS handshake error from 10.42.0.0:55311: local error: tls: bad record MAC Resolution To resolve this issue, we need to configure the client to be aware of the karavi-topology certificate (this includes all custom SSL certificate that are not issued from a trusted Certificate Authority (CA))\nGet a copy of the certificate used by karavi-topology If we supplied a custom certificate during installing karavi-topology, we can simply open the .crt and copy the text. However, if it was assigned by cert-manager, you can get a copy of the certificate by running the following kubectl command on the clusters.\n[root@:~]$ kubectl -n \u003cnamespace\u003e get secret karavi-topology-tls -o jsonpath='{.data.tls\\.crt}' | base64 -d -----BEGIN CERTIFICATE----- RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe -----END CERTIFICATE----- Configure your client to accept the above certificate A workaround on most browsers is to accept the karavi-topology certificate by clicking Continue to this website (not recommended). This will make all other successive communication to not cause any certificate error. Anyhow, you will need to read the documentation for your specific client to configure the above certificate. For Grafana, here are two ways to configure the karavi-topology datasource to use the above certificate:\n Deploy certificate with new Grafana instance Please follow the steps in Sample Grafana Deployment but attach the certificate to your `grafana-values.yaml` before deploying. The file should look like: # grafana-values.yaml image:repository:grafana/grafanatag:7.3.0sha:\"\"pullPolicy:IfNotPresentservice:type:NodePort## Administrator credentials when not using an existing SecretadminUser:adminadminPassword:admin## Pass the plugins you want installed as a list.##plugins:- grafana-simple-json-datasource- briangann-datatable-panel- grafana-piechart-panel## Configure grafana datasources## ref: http://docs.grafana.org/administration/provisioning/#datasources##datasources:datasources.yaml:apiVersion:1datasources:- name:Karavi-Topologytype:grafana-simple-json-datasourceaccess:proxyurl:'https://karavi-topology:8443'isDefault:nullversion:1editable:truejsonData:tlsAuthWithCACert:truesecureJsonData:tlsCaCert:| -----BEGIN CERTIFICATE-----RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe-----ENDCERTIFICATE----- - name: Prometheustype:prometheusaccess:proxyurl:'http://prometheus:9090'isDefault:nullversion:1editable:truetestFramework:enabled:falsesidecar:datasources:enabled:truedashboards:enabled:true## Additional grafana server ConfigMap mounts## Defines additional mounts with ConfigMap. CofigMap must be manually created in the namespace.extraConfigmapMounts:[]   Add certificate to an existing Grafana instance - This only happens if you configure jsonData to not skip tls verification. If this is the case, you'll need to re-deploy grafana as shown above or, form Grafana UI, edit Karavi-Topology datasource to use the certificate. To do the latter:  Visit your Grafana UI on a browser Navigate to setting and go to Data Sources Click on Karavi-Topology Ensure that Skip TLS Verify is already off Switch on With CA Cert Copy the above certificate into the TLS Auth Details text box that appears Click Save \u0026 Test and validate that everything is working fine when a green bar showing Data source is working appears   How can I diagnose an issue with CSM for Observability? Once you have attempted to install CSM for Observability to your Kubernetes or OpenShift cluster, the first step in troubleshooting is locating the problem.\nGet information on the state of your Pods.\nkubectl get pods -n $namespace Get verbose output of the current state of a Pod.\nkubectl describe pod -n $namespace $pod How can I view logs? View pod container logs. Output logs to a file for further debugging.\nkubectl logs -n $namespace $pod $container kubectl logs -n $namespace $pod $container \u003e $logFileName More information for viewing logs can be found here.\nHow can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? The ServiceMonitor allows us to define how a set of services should be monitored by Prometheus. Please see our prometheus documentation for creating a ServiceMonitor.\nHow can I debug and troubleshoot issues with Kubernetes?   To debug your application that may not be behaving correctly, please reference Kubernetes troubleshooting applications guide.\n  For tips on debugging your cluster, please see this troubleshooting guide.\n  How can I troubleshoot latency problems with CSM for Observability? CSM for Observability is instrumented to report trace data to Zipkin. Please see Tracing for more information on enabling tracing for CSM for Observability.\nWhy does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage? Check the pods in the CSM for Observability namespace. If the pod starting with ‘karavi-observability-cert-manager-cainjector-*’ is in ‘CrashLoopBackOff’ or ‘Error” stage with a number of restarts, check if the logs for that pod show the below error:\nkubectl logs -n $namespace $cert-manager-cainjector-podname error registering secret controller: no matches for kind \"MutatingWebhookConfiguration\" in version \"admissionregistration.k8s.io/v1beta1\" If the Kubernetes cluster version is 1.22.2 (or higher), this error is due to an incompatible cert-manager version. Please upgrade to the latest CSM for Observability release (v1.0.1 or higher).\n","excerpt":"Frequently Asked Questions  Why do I see a certificate problem when …","ref":"/csm-docs/docs/observability/troubleshooting/","title":"Troubleshooting"},{"body":"Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate troubleshooting. If you experience a problem with CSM for Resiliency it is important you provide us with as much information as possible so that we can diagnose the issue and improve CSM for Resiliency. Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate sending us the logs and other information needed to diagnose a problem.\nMonitoring Protected Pods and Node Status There are two tools for monitoring the status of protected pods and nodes.\nThe mon.sh script displays the following information every 5 seconds:\n The date and time. A list of the nodes and their status. A list of the taints applied to each node. A list of the leases in the CSI driver’s namespace. (Edit the script to change the CSI driver namespace if necessary. It defaults to vxflexos as the driver namespace.) A list of the CSI driver pods and their status (defaults to vxflexos namespace.) A list of the protected pods and their status. (Edit the script if you do not use the default podmon label key.)  For systems with many protected pods, the monx.sh may provide a more usable output format. It displays the following fields every 5 seconds:\n The date and time. A list of the nodes and their status. A list of the taints applied to each node. A summary for each node hosting protected pods of the number of pods in various states such as the Running, Creating, and Error states. (Edit the script if you do not use the default podmon label key.) A list of the protected pods not in the Running state.  Collecting Logs If you have a problem with CSM for Resiliency it’s best to collect the logs to help with diagnosis. This tool can also be used to collect logs to submit as part of an issue to help us diagnose. Please use the collect_logs.sh. Type “collect_logs.sh –help” for help on the arguments.\nThe script collects the following information:\n A list of the driver pods. A list of the protected pods. The podmon container logs for each of the driver pods. The driver container logs for each of the driver pods. For each namespace containing protected pods, the recent events logged in that namespace.  After successful execution of the script, it will deposit a file similar to driver.logs.20210319_1407.tgz in the current directory. Please submit that file with any issues.\n","excerpt":"Some tools have been provided in the tools directory that will help …","ref":"/csm-docs/docs/resiliency/troubleshooting/","title":"Troubleshooting"},{"body":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell EMC for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using the OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\nInstallation Dell CSI Operator has been tested and qualified with\n Upstream Kubernetes or OpenShift (see supported versions)  Before you begin If you have installed an old version of the dell-csi-operator which was available with the name CSI Operator, please refer to this section before continuing.\nFull list of CSI Drivers and versions supported by the Dell CSI Operator    CSI Driver Version ConfigVersion Kubernetes Version OpenShift Version     CSI PowerMax 1.6 v5 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerMax 1.7 v6 1.19, 1.20, 1.21 4.6, 4.7   CSI PowerMax 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI PowerFlex 1.4 v4 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerFlex 1.5 v5 1.19, 1.20, 1.21 4.6, 4.7   CSI PowerFlex 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI PowerScale 1.5 v5 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerScale 1.6 v6 1.19, 1.20, 1.21 4.6, 4.7   CSI PowerScale 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI Unity 1.5 v4 1.18, 1.19, 1.20 4.6, 4.7   CSI Unity 1.6 v5 1.19, 1.20, 1.21 4.6, 4.7   CSI Unity 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI PowerStore 1.3 v3 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerStore 1.4 v4 1.19, 1.20, 1.21 4.6, 4.7   CSI PowerStore 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8     Dell CSI Operator can be installed via OLM (Operator Lifecycle Manager) and manual installation.\nInstallation Using Operator Lifecycle Manager dell-csi-operator can be installed using Operator Lifecycle Manager (OLM) on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the Operator to -\n Automatic - If you want the Operator to be automatically installed or upgraded (once an upgrade becomes available) Manual - If you want a Cluster Administrator to manually review and approve the InstallPlan for installation/upgrades  NOTE: The recommended version of OLM for upstream Kubernetes is v0.18.3.\nPre-Requisite for installation with OLM Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM.\n$ git clone https://github.com/dell/dell-csi-operator.git $ cd dell-csi-operator $ tar -czf config.tar.gz driverconfig/ # Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM $ kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u003coperator-namespace\u003e Upstream Kubernetes  For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page.  Red Hat OpenShift Clusters  For installing via OpenShift with the Operator, go to the OpenShift page.  Manual Installation Steps  Skip step 1 for “offline bundle installation” and continue using the workspace created by untar of dell-csi-operator-bundle.tar.gz.\n  Clone the Dell CSI Operator repository. Run bash scripts/install.sh to install the operator.   NOTE: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default. Any existing installations of Dell CSI Operator (v1.2.0 or later) installed using install.sh to the ‘default’ or ‘dell-csi-operator’ namespace can be upgraded to the new version by running install.sh --upgrade.\n  Run the command oc get pods -n dell-csi-operator to validate the install. If completed successfully, you should be able to see the operator-related pod in the ‘dell-csi-operator’ namespace.   Custom Resource Definitions As part of the Dell CSI Operator installation, a CRD representing each driver installation is also installed.\nList of CRDs which are installed in API Group storage.dell.com\n csipowermax csiunity csivxflexos csiisilon csipowerstore csipowermaxrevproxy  For installation of the supported drivers, a CustomResource has to be created in your cluster.\nPre-Requisites for installation of the CSI Drivers Pre-requisites for upstream Kubernetes Clusters On upstream Kubernetes clusters, make sure to install\n VolumeSnapshot CRDs  On clusters running v1.20,v1.21 \u0026 v1.22, make sure to install v1 VolumeSnapshot CRDs On clusters running v1.19, make sure to install v1beta1 VolumeSnapshot CRDs   External Volume Snapshot Controller with the correct version  Pre-requisites for Red Hat OpenShift Clusters iSCSI If you are installing a CSI driver which is going to use iSCSI as the transport protocol, please follow the following instructions.\nIn Red Hat OpenShift clusters, you can create a MachineConfig object using the console or oc to ensure that the iSCSI daemon starts on all the Red Hat CoreOS nodes. Here is an example of a MachineConfig object:\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:99-iscsidlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0systemd:units:- name:\"iscsid.service\"enabled:trueOnce the MachineConfig object has been deployed, CoreOS will ensure that iscsid.service starts automatically.\nAlternatively, you can check the status of the iSCSI service by entering the following command on each worker node in the cluster:\nsudo systemctl status iscsid\nThe service should be up and running (i.e. should be active state).\nIf the iscsid.service is not running, then perform the following steps on each worker node in the cluster\n Login to worker nodes and check if the file /etc/iscsi/initiatorname.iscsi has been created properly If the file doesn’t exist or it doesn’t contain a valid ISCSI IQN, then make sure it exists with valid entries Ensure that iscsid service is running - Enable sudo systemctl enable iscsid \u0026 restart sudo systemctl restart iscsid iscsid if necessary. Note: If your worker nodes are running Red Hat CoreOS, make sure that automatic ISCSI login at boot is configured. Please contact RedHat for more details.  MultiPath If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\necho 'defaults { user_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0\nUse the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:workers-multipath-conf-defaultlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0storage:files:- contents:source:data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0Kverification:{}filesystem:rootmode:400path:/etc/multipath.confAfter deploying thisMachineConfig object, CoreOS will start multipath service automatically.\nAlternatively, you can check the status of the multipath service by entering the following command in each worker nodes.\nsudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally, you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer to official documentation of the multipath configuration.\nReplacing CSI Operator with Dell CSI Operator Dell CSI Operator was previously available, with the name CSI Operator, for both manual and OLM installation.\nCSI Operator has been discontinued and has been renamed to Dell CSI Operator. This is just a name change and as a result, the Kubernetes resources created as part of the Operator deployment will use the name dell-csi-operator instead of csi-operator.\nBefore proceeding with the installation of the new Dell CSI Operator, any existing CSI Operator installation has to be completely removed from the cluster.\nNote - This doesn’t impact any of the CSI Drivers which have been installed in the cluster\nIf the old CSI Operator was installed manually, then run the following command from the root of the repository which was used originally for installation\nbash scripts/undeploy.sh  If you don’t have the original repository available, then run the following commands\ngit clone https://github.com/dell/dell-csi-operator.git cd dell-csi-operator git checkout csi-operator-v1.0.0 bash scripts/undeploy.sh  Note - Once you have removed the old CSI Operator, then for installing the new Dell CSI Operator, you will need to pull/checkout the latest code\nIf you had installed the old CSI Operator using OLM, then please follow the uninstallation instructions provided by OperatorHub. This will mostly involve:\n* Deleting the CSI Operator Subscription * Deleting the CSI Operator CSV  Installing CSI Driver via Operator CSI Drivers can be installed by creating a CustomResource object in your cluster.\nSample manifest files for each driver CustomResourceDefintion have been provided in the samples folder to help with the installation of the drivers. These files follow the naming convention\n{driver name}_{driver version}_k8s_{k8 version}.yaml  Or\n{driver name}_{driver version}_ops_{OpenShift version}.yaml  For e.g.\n sample/powermax_v140_k8s_117.yaml* \u003c- To install CSI PowerMax driver v1.4.0 on a Kubernetes 1.17 cluster sample/powermax_v140_ops_46.yaml* \u003c- To install CSI PowerMax driver v1.4.0 on an OpenShift 4.6 cluster  Copy the correct sample file and edit the mandatory \u0026 any optional parameters specific to your driver installation by following the instructions here\n NOTE: A detailed explanation of the various mandatory and optional fields in the CustomResource is available here. Please make sure to read through and understand the various fields.\n Run the following command to install the CSI driver.\nkubectl create -f \u003cdriver-manifest.yaml\u003e Note: If you are using an OLM based installation, the example manifests are available in the OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the installation Once the driver Custom Resource has been created, you can verify the installation\n  Check if Driver CR got created successfully\nFor e.g. – If you installed the PowerMax driver\n$ kubectl get csipowermax -n \u003cdriver-namespace\u003e   Check the status of the Custom Resource to verify if the driver installation was successful\n  If the driver-namespace was set to test-powermax, and the name of the driver is powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the details of the Custom Resource.\nNote: If the state of the CustomResource is Running then all the driver pods have been successfully installed. If the state is SuccessFul, then it means the driver deployment was successful but some driver pods may not be in a Running state. Please refer to the Troubleshooting section here if you encounter any issues during installation.\nChanges in installation for latest CSI drivers If you are installing the latest versions of the CSI drivers, the driver controller will be installed as a Kubernetes Deployment instead of a Statefulset. These installations can also run multiple replicas for the driver controller pods(not supported for StatefulSets) to support High Availability for the Controller.\nUpdate CSI Drivers The CSI Drivers installed by the Dell CSI Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include –\n Modifying the installation directly via kubectl edit For e.g. - If the name of the installed unity driver is unity, then run # Replace driver-namespace with the namespace where the Unity driver is installed $ kubectl edit csiunity/unity -n \u003cdriver-namespace\u003e and modify the installation\n Modify the API object in-place via kubectl patch  NOTES:\n If you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required. The parameter “dnsPolicy: ClusterFirstWithHostNet” should be added as follows:  driver: configVersion: v5 replicas: 2 dnsPolicy: ClusterFirstWithHostNet common: Configmap needs to be created with command kubectl create -f configmap.yaml using following yaml file.  apiVersion:v1kind:ConfigMapmetadata:name:powerstore-config-paramsnamespace:csi-powerstoredata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"debug\"CSI_LOG_FORMAT:\"JSON\"NOTE: Replicas in the driver CR file should not be greater than or equal to the number of worker nodes when upgrading the driver. If the Replicas count is not less than the worker node count, some of the driver controller pods would land in a pending state, and upgrade will not be successful. Driver controller pods go in a pending state because they have anti-affinity to each other and cannot be scheduled on nodes where there is a driver controller pod already running. Refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity for more details.\nNOTE: Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures.\nNOTE: From v1.4.0 onwards, Dell CSI Operator does not support the creation of StorageClass and VolumeSnapshotClass objects. Although these fields are still present in the various driver CustomResourceDefinitions, they would be ignored by the operator. These fields will be removed from the CustomResourceDefinitions in a future release. If StorageClass and VolumeSnapshotClass need to be retained, you should upgrade the driver as per the recommended way noted above. StorageClass and VolumeSnapshotClass would not be retained on driver uninstallation.\nSupported modifications  Changing environment variable values for driver Adding (supported) environment variables Updating the image of the driver  Limitations  The Dell CSI Operator can’t manage any existing driver installed using Helm charts. If you already have installed one of the DellEMC CSI driver in your cluster and want to use the operator based deployment, uninstall the driver and then redeploy the driver following the installation procedure described above The Dell CSI Operator is not fully compliant with the OperatorHub React UI elements and some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSI Operator  Custom Resource Specification Each CSI Driver installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.\nBelow is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - Refer full list of supported driver for finding out the appropriate config version here replicas - Number of replicas for controller plugin - Must be set to 1 for all drivers\ndnsPolicy - Determines the dnsPolicy for the node daemonset. Accepted values are Default, ClusterFirst, ClusterFirstWithHostNet, None common\nThis field is mandatory and is used to specify common properties for both controller and the node plugin\n image - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values  Optional fields controller - List of environment variables and values which are applicable only for controller\nnode - List of environment variables and values which are applicable only for node\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver\nforceUpdate\nBoolean value which can be set to true in order to force update the status of the CSI Driver\ntolerations List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet\nIt should be set separately in the controller and node sections if you want separate set of tolerations for them\nnodeSelector Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet\nHere is a sample specification annotated with comments to explain each field\napiVersion:storage.dell.com/v1kind:CSIPowerMax# Type of the drivermetadata:name:test-powermax# Name of the drivernamespace:test-powermax# Namespace where driver is installedspec:driver:# Used to specify configuration versionconfigVersion:v3# Refer the table containing the full list of supported drivers to find the appropriate config versionreplicas:1forceUpdate:false# Set to true in case you want to force an update of driver statuscommon:# All common specificationimage:\"dellemc/csi-powermax:v1.4.0.000R\"#driver image for a particular releaseimagePullPolicy:IfNotPresentenvs:- name:X_CSI_POWERMAX_ENDPOINTvalue:\"https://0.0.0.0:8443/\"- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\"XYZ\"You can set the field replicas to a higher number than 1 for the latest driver versions.\nNote - The image field should point to the correct image tag for version of the driver you are installing.\nFor e.g. - If you wish to install v1.4 of the CSI PowerMax driver, use the image tag dellemc/csi-powermax:v1.4.0.000R\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. Any modifications to this should be only done after consulting with Dell EMC support.\nModify the driver specification  Choose the correct configVersion. Refer the table containing the full list of supported drivers and versions. Provide the namespace (in metadata section) where you want to install the driver. Provide a name (in metadata section) for the driver. This will be the name of the Custom Resource. Edit the values for mandatory configuration parameters specific to your installation. Edit/Add any values for optional configuration parameters to customize your installation. If you are installing the latest versions of the CSI drivers, the default number of replicas is set to 2. You can increase/decrease this value.  StorageClass and VolumeSnapshotClass New Installations You should not provide any StorageClass or VolumeSnapshotClass details during driver installation. The sample files for all the drivers have been updated to reflect this change. Even if these details are there in the sample files, StorageClass or VolumeSnapshotClass will not be created.\nWhat happens to my existing StorageClass \u0026 VolumeSnapshotClass objects  In case you are upgrading an existing driver installation by using kubectl edit or by patching the object in place, any existing objects will remain as is. If you added more objects as part of the upgrade, then this request will be ignored by the Operator. If you uninstall the older driver, then any StorageClass or VolumeSnapshotClass objects will be deleted. An uninstall and followed by an install of the driver would also result in StorageClass and VolumeSnapshotClass getting deleted and not getting created again.  NOTE: For more information on pre-requisites and parameters, please refer to the sub-pages below for each driver.\nNOTE: Storage Classes and Volume Snapshot Classes would no longer be created during the installation of the driver via an operator from v1.4.0 and higher.\n","excerpt":"The Dell CSI Operator is a Kubernetes Operator, which can be used to …","ref":"/csm-docs/docs/csidriver/installation/operator/","title":"Dell CSI Operator Installation Process"},{"body":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell EMC for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using the OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\nInstallation Dell CSI Operator has been tested and qualified with\n Upstream Kubernetes or OpenShift (see supported versions)  Before you begin If you have installed an old version of the dell-csi-operator which was available with the name CSI Operator, please refer to this section before continuing.\nFull list of CSI Drivers and versions supported by the Dell CSI Operator    CSI Driver Version ConfigVersion Kubernetes Version OpenShift Version     CSI PowerMax 1.5 v4 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI PowerMax 1.6 v5 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerMax 1.7 v6 1.19, 1.20, 1.21 4.6, 4.7   CSI PowerFlex 1.3 v3 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI PowerFlex 1.4 v4 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerFlex 1.5 v5 1.19, 1.20, 1.21 4.6, 4.7   CSI PowerScale 1.4 v4 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI PowerScale 1.5 v5 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerScale 1.6 v6 1.19, 1.20, 1.21 4.6, 4.7   CSI Unity 1.4 v3 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI Unity 1.5 v4 1.18, 1.19, 1.20 4.6, 4.7   CSI Unity 1.6 v5 1.19, 1.20, 1.21 4.6, 4.7   CSI PowerStore 1.2 v2 1.17, 1.18, 1.19 4.5, 4.6   CSI PowerStore 1.3 v3 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerStore 1.4 v4 1.19, 1.20, 1.21 4.6, 4.7     Dell CSI Operator can be installed via OLM (Operator Lifecycle Manager) and manual installation.\nInstallation Using Operator Lifecycle Manager dell-csi-operator can be installed using Operator Lifecycle Manager (OLM) on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the Operator to -\n Automatic - If you want the Operator to be automatically installed or upgraded (once an upgrade becomes available) Manual - If you want a Cluster Administrator to manually review and approve the InstallPlan for installation/upgrades  NOTE: The recommended version of OLM for upstream Kubernetes is v0.17.0.\nPre-Requisite for installation with OLM Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM.\n$ git clone github.com/dell/dell-csi-operator $ cd dell-csi-operator $ tar -czf config.tar.gz driverconfig/ # Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM $ kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u003coperator-namespace\u003e Upstream Kubernetes  For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page.  Red Hat OpenShift Clusters  For installing via OpenShift with the Operator, go to the OpenShift page.  Manual Installation Steps  Skip step 1 for “offline bundle installation” and continue using the workspace created by untar of dell-csi-operator-bundle.tar.gz.\n  Clone the Dell CSI Operator repository. Run bash scripts/install.sh to install the operator.   NOTE: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default. Any existing installations of Dell CSI Operator (v1.2.0 or later) installed using install.sh to the ‘default’ or ‘dell-csi-operator’ namespace can be upgraded to the new version by running install.sh --upgrade.\n  Run the command oc get pods -n dell-csi-operator to validate the install. If completed successfully, you should be able to see the operator-related pod in the ‘dell-csi-operator’ namespace.   Custom Resource Definitions As part of the Dell CSI Operator installation, a CRD representing each driver installation is also installed.\nList of CRDs which are installed in API Group storage.dell.com\n csipowermax csiunity csivxflexos csiisilon csipowerstore csipowermaxrevproxy  For installation of the supported drivers, a CustomResource has to be created in your cluster.\nPre-Requisites for installation of the CSI Drivers Pre-requisites for upstream Kubernetes Clusters On upstream Kubernetes clusters, make sure to install\n VolumeSnapshot CRDs  On clusters running v1.20 \u0026 v1.21, make sure to install v1 VolumeSnapshot CRDs On clusters running v1.19, make sure to install v1beta1 VolumeSnapshot CRDs   External Volume Snapshot Controller with the correct version  Pre-requisites for Red Hat OpenShift Clusters iSCSI If you are installing a CSI driver which is going to use iSCSI as the transport protocol, please follow the following instructions.\nIn Red Hat OpenShift clusters, you can create a MachineConfig object using the console or oc to ensure that the iSCSI daemon starts on all the Red Hat CoreOS nodes. Here is an example of a MachineConfig object:\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:99-iscsidlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0systemd:units:- name:\"iscsid.service\"enabled:trueOnce the MachineConfig object has been deployed, CoreOS will ensure that iscsid.service starts automatically.\nAlternatively, you can check the status of the iSCSI service by entering the following command on each worker node in the cluster:\nsudo systemctl status iscsid\nThe service should be up and running (i.e. should be active state).\nIf the iscsid.service is not running, then perform the following steps on each worker node in the cluster\n Login to worker nodes and check if the file /etc/iscsi/initiatorname.iscsi has been created properly If the file doesn’t exist or it doesn’t contain a valid ISCSI IQN, then make sure it exists with valid entries Ensure that iscsid service is running - Enable sudo systemctl enable iscsid \u0026 restart sudo systemctl restart iscsid iscsid if necessary. Note: If your worker nodes are running Red Hat CoreOS, make sure that automatic ISCSI login at boot is configured. Please contact RedHat for more details.  MultiPath If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\necho 'defaults { user_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0\nUse the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:workers-multipath-conf-defaultlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0storage:files:- contents:source:data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0Kverification:{}filesystem:rootmode:400path:/etc/multipath.confAfter deploying thisMachineConfig object, CoreOS will start multipath service automatically.\nAlternatively, you can check the status of the multipath service by entering the following command in each worker nodes.\nsudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally, you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer to official documentation of the multipath configuration.\nReplacing CSI Operator with Dell CSI Operator Dell CSI Operator was previously available, with the name CSI Operator, for both manual and OLM installation.\nCSI Operator has been discontinued and has been renamed to Dell CSI Operator. This is just a name change and as a result, the Kubernetes resources created as part of the Operator deployment will use the name dell-csi-operator instead of csi-operator.\nBefore proceeding with the installation of the new Dell CSI Operator, any existing CSI Operator installation has to be completely removed from the cluster.\nNote - This doesn’t impact any of the CSI Drivers which have been installed in the cluster\nIf the old CSI Operator was installed manually, then run the following command from the root of the repository which was used originally for installation\nbash scripts/undeploy.sh  If you don’t have the original repository available, then run the following commands\ngit clone https://github.com/dell/dell-csi-operator.git cd dell-csi-operator git checkout csi-operator-v1.0.0 bash scripts/undeploy.sh  Note - Once you have removed the old CSI Operator, then for installing the new Dell CSI Operator, you will need to pull/checkout the latest code\nIf you had installed the old CSI Operator using OLM, then please follow the uninstallation instructions provided by OperatorHub. This will mostly involve:\n* Deleting the CSI Operator Subscription * Deleting the CSI Operator CSV  Installing CSI Driver via Operator CSI Drivers can be installed by creating a CustomResource object in your cluster.\nSample manifest files for each driver CustomResourceDefintion have been provided in the samples folder to help with the installation of the drivers. These files follow the naming convention\n{driver name}_{driver version}_k8s_{k8 version}.yaml  Or\n{driver name}_{driver version}_ops_{OpenShift version}.yaml  For e.g.\n sample/powermax_v140_k8s_117.yaml* \u003c- To install CSI PowerMax driver v1.4.0 on a Kubernetes 1.17 cluster sample/powermax_v140_ops_46.yaml* \u003c- To install CSI PowerMax driver v1.4.0 on an OpenShift 4.6 cluster  Copy the correct sample file and edit the mandatory \u0026 any optional parameters specific to your driver installation by following the instructions here\n NOTE: A detailed explanation of the various mandatory and optional fields in the CustomResource is available here. Please make sure to read through and understand the various fields.\n Run the following command to install the CSI driver.\nkubectl create -f \u003cdriver-manifest.yaml\u003e Note: If you are using an OLM based installation, the example manifests are available in the OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the installation Once the driver Custom Resource has been created, you can verify the installation\n  Check if Driver CR got created successfully\nFor e.g. – If you installed the PowerMax driver\n$ kubectl get csipowermax -n \u003cdriver-namespace\u003e   Check the status of the Custom Resource to verify if the driver installation was successful\n  If the driver-namespace was set to test-powermax, and the name of the driver is powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the details of the Custom Resource.\nNote: If the state of the CustomResource is Running then all the driver pods have been successfully installed. If the state is SuccessFul, then it means the driver deployment was successful but some driver pods may not be in a Running state. Please refer to the Troubleshooting section here if you encounter any issues during installation.\nChanges in installation for latest CSI drivers If you are installing the latest versions of the CSI drivers, the driver controller will be installed as a Kubernetes Deployment instead of a Statefulset. These installations can also run multiple replicas for the driver controller pods(not supported for StatefulSets) to support High Availability for the Controller.\nUpdate CSI Drivers The CSI Drivers installed by the Dell CSI Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include –\n Modifying the installation directly via kubectl edit For e.g. - If the name of the installed unity driver is unity, then run # Replace driver-namespace with the namespace where the Unity driver is installed $ kubectl edit csiunity/unity -n \u003cdriver-namespace\u003e and modify the installation\n Modify the API object in-place via kubectl patch  NOTE: If you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required.\nNOTE: Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures.\nNOTE: From v1.4.0 onwards, Dell CSI Operator does not support creation of StorageClass and VolumeSnapshotClass objects. Although these fields are still present in the various driver CustomResourceDefinitions, they would be ignored by the operator. These fields will be removed from the CustomResourceDefinitions in a future release. If StorageClass and VolumeSnapshotClass needs to be retained, you should upgrade the driver as per the recommended way noted above. StorageClass and VolumeSnapshotClass would not be retained on driver uninstallation.\nSupported modifications  Changing environment variable values for driver Adding (supported) environment variables Updating the image of the driver  Limitations  The Dell CSI Operator can’t manage any existing driver installed using Helm charts. If you already have installed one of the DellEMC CSI driver in your cluster and want to use the operator based deployment, uninstall the driver and then redeploy the driver following the installation procedure described above The Dell CSI Operator is not fully compliant with the OperatorHub React UI elements and some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSI Operator  Custom Resource Specification Each CSI Driver installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.\nBelow is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - Refer full list of supported driver for finding out the appropriate config version here replicas - Number of replicas for controller plugin - Must be set to 1 for all drivers\ncommon\nThis field is mandatory and is used to specify common properties for both controller and the node plugin\n image - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values  Optional fields controller - List of environment variables and values which are applicable only for controller\nnode - List of environment variables and values which are applicable only for node\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver\nforceUpdate\nBoolean value which can be set to true in order to force update the status of the CSI Driver\ntolerations List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet\nIt should be set separately in the controller and node sections if you want separate set of tolerations for them\nnodeSelector Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet\nHere is a sample specification annotated with comments to explain each field\napiVersion:storage.dell.com/v1kind:CSIPowerMax# Type of the drivermetadata:name:test-powermax# Name of the drivernamespace:test-powermax# Namespace where driver is installedspec:driver:# Used to specify configuration versionconfigVersion:v3# Refer the table containing the full list of supported drivers to find the appropriate config versionreplicas:1forceUpdate:false# Set to true in case you want to force an update of driver statuscommon:# All common specificationimage:\"dellemc/csi-powermax:v1.4.0.000R\"#driver image for a particular releaseimagePullPolicy:IfNotPresentenvs:- name:X_CSI_POWERMAX_ENDPOINTvalue:\"https://0.0.0.0:8443/\"- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\"XYZ\"You can set the field replicas to a higher number than 1 for the latest driver versions.\nNote - The image field should point to the correct image tag for version of the driver you are installing.\nFor e.g. - If you wish to install v1.4 of the CSI PowerMax driver, use the image tag dellemc/csi-powermax:v1.4.0.000R\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. Any modifications to this should be only done after consulting with Dell EMC support.\nModify the driver specification  Choose the correct configVersion. Refer the table containing the full list of supported drivers and versions. Provide the namespace (in metadata section) where you want to install the driver. Provide a name (in metadata section) for the driver. This will be the name of the Custom Resource. Edit the values for mandatory configuration parameters specific to your installation. Edit/Add any values for optional configuration parameters to customize your installation. If you are installing the latest versions of the CSI drivers, the default number of replicas is set to 2. You can increase/decrease this value.  StorageClass and VolumeSnapshotClass New Installations You should not provide any StorageClass or VolumeSnapshotClass details during driver installation. The sample files for all the drivers have been updated to reflect this change. Even if these details are there in the sample files, StorageClass or VolumeSnapshotClass will not be created.\nWhat happens to my existing StorageClass \u0026 VolumeSnapshotClass objects  In case you are upgrading an existing driver installation by using kubectl edit or by patching the object in place, any existing objects will remain as is. If you added more objects as part of the upgrade, then this request will be ignored by the Operator. If you uninstall the older driver, then any StorageClass or VolumeSnapshotClass objects will be deleted. An uninstall and followed by an install of the driver would also result in StorageClass and VolumeSnapshotClass getting deleted and not getting created again.  NOTE: For more information on pre-requisites and parameters, please refer to the sub-pages below for each driver.\nNOTE: Storage Classes and Volume Snapshot Classes would no longer be created during the installation of the driver via an operator from v1.4.0 and higher.\n","excerpt":"The Dell CSI Operator is a Kubernetes Operator, which can be used to …","ref":"/csm-docs/v1/installation/operator/","title":"Dell CSI Operator Installation Process"},{"body":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell EMC for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\nInstallation Dell CSI Operator has been tested and qualified with\n Upstream Kubernetes or OpenShift (see supported versions)  Before you begin If you have installed an old version of the dell-csi-operator which was available with the name CSI Operator, please refer this section before continuing.\nFull list of CSI Drivers and versions supported by the Dell CSI Operator    CSI Driver Version ConfigVersion Kubernetes Version OpenShift Version     CSI PowerMax 1.5 v4 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI PowerMax 1.6 v5 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerFlex 1.3 v3 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI PowerFlex 1.4 v4 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerScale 1.4 v4 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI PowerScale 1.5 v5 1.18, 1.19, 1.20 4.6, 4.7   CSI Unity 1.4 v3 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI Unity 1.5 v4 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerStore 1.2 v2 1.17, 1.18, 1.19 4.5, 4.6   CSI PowerStore 1.3 v3 1.18, 1.19, 1.20 4.6, 4.7     Dell CSI Operator can be installed via OLM (Operator Lifecycle Manager) and manual installation.\nInstallation Using Operator Lifecycle Manager dell-csi-operator can be installed using Operator Lifecycle Manager (OLM) on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the Operator to -\n Automatic - If you want the Operator to be automatically installed or upgraded (once an upgrade becomes available) Manual - If you want a Cluster Administrator to manually review and approve the InstallPlan for installation/upgrades  Pre-Requisite for installation with OLM Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM.\n$ git clone github.com/dell/dell-csi-operator $ cd dell-csi-operator $ tar -czf config.tar.gz driverconfig/ # Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM $ kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u003coperator-namespace\u003e Upstream Kubernetes  For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page.  Red Hat OpenShift Clusters  For installing via OpenShift with the Operator, go to the OpenShift page.  Manual Installation Steps  Clone the Dell CSI Operator repository. Skip this step for Offline Install. And continue using workspace created by untar of dell-csi-operator-bundle.tar.gz. Run bash scripts/install.sh to install the operator  Run the command oc get pods to validate the install completed, should be able to see the operator related pod on default namespace   Custom Resource Definitions As part of the Dell CSI Operator installation, a CRD representing each driver installation is also installed.\nList of CRDs which are installed in API Group storage.dell.com\n csipowermax csiunity csivxflexos csiisilon csipowerstore csipowermaxrevproxy  For installation of the supported drivers, a CustomResource has to be created in your cluster.\nPre-Requisites for installation of the CSI Drivers Pre-requisites for upstream Kubernetes Clusters On upstream Kubernetes clusters, make sure to install\n VolumeSnapshot CRDs  On clusters running v1.20, make sure to install v1 VolumeSnapshot CRDs On clusters running v1.18 \u0026 v1.19, make sure to install v1beta1 VolumeSnapshot CRDs   External Volume Snapshot Controller with correct version  Pre-requisites for Red Hat OpenShift Clusters iSCSI If you are installing a CSI driver which is going to use iSCSI as the transport protocol, please follow the following instructions.\nIn Red Hat OpenShift clusters, you can create a MachineConfig object using the console or oc to ensure that the iSCSI daemon starts on all the Red Hat CoreOS nodes. Here is an example of a MachineConfig object:\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:99-iscsidlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0systemd:units:- name:\"iscsid.service\"enabled:trueOnce the MachineConfig object has been deployed, CoreOS will ensure that iscsid.service starts automatically.\nAlternatively, you can check the status of iSCSI service by entering the following command on each worker node in the cluster:\nsudo systemctl status iscsid\nThe service should be up and running (i.e. should be in active state).\nIf the iscsid.service is not running, then perform the following steps on each worker node in the cluster\n Login to worker nodes and check if the file /etc/iscsi/initiatorname.iscsi has been created properly If the file doesn’t exist or it doesn’t contain a valid ISCSI IQN, then make sure it exists with valid entries Ensure that iscsid service is running - Enable sudo systemctl enable iscsid \u0026 restart sudo systemctl restart iscsid iscsid if necessary. Note: If your worker nodes are running on Red Hat CoreOS , you can refer the URL https://coreos.com/os/docs/latest/iscsi.html#enable-automatic-iscsi-login-at-boot for additional information.  MultiPath If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\necho 'defaults { user_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0\nUse the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:workers-multipath-conf-defaultlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0storage:files:- contents:source:data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0Kverification:{}filesystem:rootmode:400path:/etc/multipath.confAfter deploying thisMachineConfig object, CoreOS will start multipath service automatically.\nAlternatively you can check the status of multipath service by entering the following command in each worker nodes.\nsudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally , you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer official documentation of multipath configuration.\nReplacing CSI Operator with Dell CSI Operator Dell CSI Operator was previously available, with the name CSI Operator, for both manual and OLM installation.\nCSI Operator has been discontinued and has been renamed to Dell CSI Operator. This is just a name change and as a result, the Kubernetes resources created as part of the Operator deployment will use the name dell-csi-operator instead of csi-operator.\nBefore proceeding with the installation of the new Dell CSI Operator, any existing CSI Operator installation has to be completely removed from the cluster.\nNote - This doesn’t impact any of the CSI Drivers which have been installed in the cluster\nIf the old CSI Operator was installed manually, then run the following command from the root of the repository which was used originally for installation\nbash scripts/undeploy.sh  If you don’t have the original repository available, then run the following commands\ngit clone https://github.com/dell/dell-csi-operator.git cd dell-csi-operator git checkout csi-operator-v1.0.0 bash scripts/undeploy.sh  Note - Once you have removed the old CSI Operator, then for installing the new Dell CSI Operator, you will need to pull/checkout the latest code\nIf you had installed old CSI Operator using OLM, then please follow un-installation instructions provided by OperatorHub. This will mostly involve:\n* Deleting the CSI Operator Subscription * Deleting the CSI Operator CSV  Install CSI Driver To install CSI drivers using Dell CSI Operator, please refer here\nNOTE: For more information on pre-requisites and parameters, please refer to the sub-pages below for each driver.\n","excerpt":"The Dell CSI Operator is a Kubernetes Operator, which can be used to …","ref":"/csm-docs/v2/installation/operator/","title":"Dell CSI Operator Installation Process"},{"body":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell EMC for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\n For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page. For installing via OpenShift with the certified Operator, go to the OpenShift page. For installing manually, follow the instructions below.  Manual Installation Pre-requisites Dell CSI Operator has been tested and qualified with\n Upstream Kubernetes cluster v1.17, v1.18, v1.19 OpenShift Clusters 4.5, 4.6 with RHEL 7.x \u0026 RHCOS worker nodes For upstream k8s clusters, make sure to install  Beta VolumeSnapshot CRDs (can be installed using the Operator installation script) External Volume Snapshot Controller     Note- For more insights or detailed pre-requisites refer https://github.com/dell/dell-csi-operator\n Steps  Clone the Dell CSI Operator repository Run ‘bash scripts/install.sh’ to install the operator  Run the command ‘oc get pods’ to validate the install completed  Should be able to see the operator related pod on default namespace     Driver Install via Dell CSI Operator For information on how to install the CSI drivers via the Dell CSI Operator, please refer to the sub-pages below for each driver.\n","excerpt":"The Dell CSI Operator is a Kubernetes Operator, which can be used to …","ref":"/csm-docs/v3/installation/operator/","title":"Dell CSI Operator Installation Process"},{"body":"One of the goals of high availability is to eliminate single points of failure in a storage system. In Kubernetes, this can mean that a single PV represents multiple read/write enabled volumes on different arrays, located at reasonable distances with both the volumes in sync with each other. If one of the volumes goes down, there will still be another volume available for read and write. This kind of high availability can be achieved by using SRDF Metro replication mode supported only by Powermax arrays.\nSRDF Metro Architecture In SRDF metro configurations:\n R2 devices are Read/Write accessible to application hosts. Application host can write to both the R1 and R2 sides of the device pair. R2 devices assume the same external device identity(geometry, device WWN) as the R1 devices. All the above characteristic makes SRDF metro best suited for the scenarios in which high availability of data is desired.  With respect to Kubernetes, the SRDF metro mode works in single cluster scenarios. In the metro, both the arrays—arrays with SRDF metro link setup between them—involved in the replication are managed by the same csi-powermax driver. The replication is triggered by creating a volume using a StorageClass with metro-related parameters. The driver on receiving the metro-related parameters in the CreateVolume call creates a metro replicated volume and the details about both the volumes are returned in the volume context to the Kubernetes cluster. So, the PV created in the process represents a pair of metro replicated volumes. When a PV, representing a pair of metro replicated volumes, is claimed by a pod, the host treats each of the volumes represented by the single PV as a separate data path. The switching between the paths, to read and write the data, is managed by the multipath driver. The switching happens automatically, as configured by the user—in round-robin fashion or otherwise—or it can happen if one of the paths goes down. For details on Linux multipath driver setup, click here.\nThe creation of volumes in SRDF metro mode doesn’t involve the replication sidecar or the common controller, nor does it cause the creation of any replication related custom resources; it just needs a csi-powermax driver that implements the CreateVolume grpc endpoint with SRDF metro capability for it to work.\nUsage The metro replicated volumes are created just like the normal volumes, but the StorageClass contains some extra parameters related to metro replication. A StorageClass to create metro replicated volumes may look as follows:\nkind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:storage-class-metroprovisioner:driver.dellemc.comparameters:SRP:'SRP_1'SYMID:'000000000001'ServiceLevel:'Bronze'replication.storage.dell.com/IsReplicationEnabled:'true'replication.storage.dell.com/RdfGroup:'7'replication.storage.dell.com/RdfMode:'METRO'replication.storage.dell.com/RemoteRDFGroup:'7'replication.storage.dell.com/RemoteSYMID:'000000000002'replication.storage.dell.com/RemoteServiceLevel:'Bronze'reclaimPolicy:DeletevolumeBindingMode:ImmediateSnapshots on SRDF Metro volumes A snapshot can be created on either of the volumes in the metro volume pair depending on the parameters in the VolumeSnapshotClass. The snapshots are by default created on the volumes on the R1 side of the SRDF metro pair, but if a Symmetrix id is specified in the VolumeSnapshotClass parameters, the driver creates the snapshot on the specified array; the specified array can either be the R1 or the R2 array. A VolumeSnapshotClass with symmetrix id specified in parameters may look as follows:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:sample-snapclassdriver:driver.dellemc.comdeletionPolicy:Deleteparameters:SYMID:'000000000001'","excerpt":"One of the goals of high availability is to eliminate single points of …","ref":"/csm-docs/docs/replication/high-availability/","title":"High Availability"},{"body":"Container Storage Modules (CSM) for Observability is part of the open-source suite of Kubernetes storage enablers for Dell EMC products.\nIt is an OpenTelemetry agent that collects array-level metrics for Dell EMC storage so they can be scraped into a Prometheus database. With CSM for Observability, you will gain visibility not only on the capacity of the volumes/file shares you manage with Dell CSM CSI (Container Storage Interface) drivers but also their performance in terms of bandwidth, IOPS, and response time.\nThanks to pre-packaged Grafana dashboards, you will be able to go through these metrics history and see the topology between a Kubernetes PV (Persistent Volume) and its translation as a LUN or file share in the backend array. This module also allows Kubernetes admins to collect array level metrics to check the overall capacity and performance directly from the Prometheus/Grafana tools rather than interfacing directly with the storage system itself.\nMetrics data is collected and pushed to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. SSL certificates for TLS between nodes are handled by cert-manager.\nCSM for Observability is composed of several services, each living in its own GitHub repository. Contributions can be made to this repository or any of the CSM for Observability repositories listed below.\n  Name Repository Description     Performance Metrics for PowerFlex CSM Metrics for PowerFlex Performance Metrics for PowerFlex captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell EMC PowerFlex. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics so they can be visualized in Grafana. Please visit the repository for more information.   Performance Metrics for PowerStore CSM Metrics for PowerStore Performance Metrics for PowerStore captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell EMC PowerStore. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics so they can be visualized in Grafana. Please visit the repository for more information.   Volume Topology CSM Topology Topology provides Kubernetes administrators with the topology data related to containerized storage that is provisioned by a CSI (Container Storage Interface) Driver for Dell EMC storage products. Please visit the repository for more information.    CSM for Observability Capabilities CSM for Observability provides the following capabilities:\n  Capability PowerMax PowerFlex Unity PowerScale PowerStore     Collect and expose Volume Metrics via the OpenTelemetry Collector no yes no no yes   Collect and expose File System Metrics via the OpenTelemetry Collector no no no no yes   Collect and expose export (k8s) node metrics via the OpenTelemetry Collector no yes no no no   Collect and expose filesystem capacity metrics via the OpenTelemetry Collector no no no no yes   Collect and expose block storage capacity metrics via the OpenTelemetry Collector no yes no no yes   Non-disruptive config changes no yes no no yes   Non-disruptive log level changes no yes no no yes   Grafana Dashboards for displaying metrics and topology data no yes no no yes    Supported Operating Systems/Container Orchestrator Platforms   COP/OS Supported Versions     Kubernetes 1.20, 1.21, 1.22   Red Hat OpenShift 4.7, 4.8   Rancher Kubernetes Engine yes   RHEL 7.x, 8.x   CentOS 7.8, 7.9    Supported Storage Platforms    PowerFlex PowerStore     Storage Array 3.5.x, 3.6.x 1.0.x, 2.0.x    Supported CSI Drivers CSM for Observability supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell EMC PowerFlex csi-powerflex v2.0   CSI Driver for Dell EMC PowerStore csi-powerstore v2.0    Topology Data CSM for Observability provides Kubernetes administrators with the topology data related to containerized storage. This topology data is visualized using Grafana:   Field Description     Namespace The namespace associated with the persistent volume claim   Persistent Volume The name of the persistent volume   Status The status of the persistent volume. “Released” indicating the persistent volume has a claim. “Bound” indicating the persistent volume has a claim   Persistent Volume Claim The name of the persistent volume claim associated with the persistent volume   CSI Driver The name of the CSI driver that was responsible for provisioning the volume on the storage system   Created The date the persistent volume was created   Provisioned Size The provisioned size of the persistent volume   Storage Class The storage class associated with the persistent volume   Storage System Volume Name The name of the volume on the storage system that is associated with the persistent volume   Storage Pool The storage pool name the volume/storage class is associated with   Storage System The storage system ID or IP address the volume is associated with   Protocol The storage system protocol type the volume/storage class is associated with    TLS Encryption CSM for Observability deployment relies on cert-manager to manage SSL certificates that are used to encrypt communication between various components. When deploying CSM for Observability, cert-manager is installed and configured automatically. The cert-manager components listed below will be installed alongside CSM for Observability.\n  Component     cert-manager   cert-manager-cainjector   cert-manager-webhook    If desired you may provide your own certificate key pair to be used inside the cluster by providing the path to the certificate and key in the Helm chart config. If you do not provide a certificate, one will be generated for you on installation.\n NOTE: The certificate provided must be a CA certificate. This is to facilitate automated certificate rotation.\n Viewing Logs Logs can be viewed by using the kubectl logs CLI command to output logs for a specific Pod or Deployment.\nFor example, the following script will capture logs of all Pods in the CSM namespace and save the output to one file per Pod.\n#!/bin/bash namespace=[CSM_NAMESPACE] for pod in $(kubectl get pods -n $namespace -o name); do logFileName=$(echo $pod | tr / -).txt kubectl logs -n $namespace $pod --all-containers \u003e $logFileName done ","excerpt":"Container Storage Modules (CSM) for Observability is part of the …","ref":"/csm-docs/docs/observability/","title":"Observability"},{"body":"Replication Enabled Storage Classes In order to create replicated volumes \u0026 volume groups, you need to add some extra parameters to your storage class definition. These extra parameters generally carry the prefix replication.storage.dell.com to differentiate them from other provisioning parameters.\nReplication enabled storage classes are always created in pairs within/across clusters and are generally mirrors of each other. Before provisioning replicated volumes, make sure that these pairs of storage classes are created properly.\nCommon Parameters There are 3 mandatory key/value pairs which should always be present in the storage class parameters -\nreplication.storage.dell.com/isReplicationEnabled:'true'replication.storage.dell.com/remoteClusterID:\u003cRemoteClusterId\u003ereplication.storage.dell.com/remoteStorageClassName:\u003cRemoteScName\u003eremoteClusterID This should contain the Cluster ID of the remote cluster where the replicated volume is going to be created. In case of a single stretched cluster, it should be always set to self\nremoteStorageClassName This should contain the name of the storage class on the remote cluster which is used to create the remote PersistentVolume.\n Note: You still need to create a pair of storage classes even while using a single stretched cluster\n Driver specific parameters Please refer to the driver specific sections for PowerMax \u0026 PowerStore for a detailed list of parameters.\nPV sync Deletion The dell-csm-replicator supports ‘sync deletion’ of replicated PV resources i.e when a replication enabled PV is deleted its corresponding source or target PV can also be deleted.\nThe decision to whether or not sync delete the corresponding PV depends on a Storage Class parameter which can be configured by the user.\nreplication.storage.dell.com/remotePVRetentionPolicy: 'delete' | 'retain' If the remotePVRetentionPolicy is set to ‘delete’, the corresponding PV would be deleted.\nIf the remotePVRetentionPolicy is set to ‘retain’, the corresponding PV would be retained.\nBy default, if the remotePVRetentionPolicy is not specified in the Storage Class, replicated PV resources are retained.\nRG sync Deletion The dell-csm-replicator supports ‘sync deletion’ of RG (DellCSIReplicationGroup) resources i.e when an RG is deleted its corresponding source or target RG can also be deleted.\nThe decision to whether or not sync delete the corresponding RG depends on a Storage Class parameter which can be configured by the user.\nreplication.storage.dell.com/remoteRGRetentionPolicy: 'delete' | 'retain' If the remoteRGRetentionPolicy is set to ‘delete’, the corresponding RG would be deleted.\nIf the remoteRGRetentionPolicy is set to ‘retain’, the corresponding RG would be retained.\nBy default, if the remoteRGRetentionPolicy is not specified in the Storage Class, replicated RG resources are retained.\nExample If you are setting up replication between two clusters with ClusterID set to Cluster A \u0026 Cluster B, then the storage class definitions in both the clusters would look like -\nCluster A apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:rep-srcparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteClusterID:ClusterBreplication.storage.dell.com/remoteStorageClassName:rep-tgt# Some driver specific replication \u0026 non-replication related paramsprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:ImmediateCluster B apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:rep-tgtparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteClusterID:ClusterAreplication.storage.dell.com/remoteStorageClassName:rep-src# Some driver specific replication \u0026 non-replication related paramsprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediate","excerpt":"Replication Enabled Storage Classes In order to create replicated …","ref":"/csm-docs/docs/replication/deployment/storageclasses/","title":"Storage Class"},{"body":"","excerpt":"","ref":"/csm-docs/docs/csidriver/troubleshooting/","title":"Troubleshooting"},{"body":"","excerpt":"","ref":"/csm-docs/v1/troubleshooting/","title":"Troubleshooting"},{"body":"","excerpt":"","ref":"/csm-docs/v2/troubleshooting/","title":"Troubleshooting"},{"body":"","excerpt":"","ref":"/csm-docs/v3/troubleshooting/","title":"Troubleshooting"},{"body":"The dell-csm-replicator supports monitoring of DellCSIReplicationGroup Custom Resources (CRs).\nEach RG is polled at a pre-defined interval and for each RG, a gRPC call is made to the driver which returns the status of the protection group on the array.\nIf an RG doesn’t have any PVs associated with it, the driver will not receive any monitoring request for that RG.\nThis status can be obtained from the RG as under:\nNAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 51d Ready SUSPENDED 2021-09-10T10:48:09Z ","excerpt":"The dell-csm-replicator supports monitoring of DellCSIReplicationGroup …","ref":"/csm-docs/docs/replication/monitoring/","title":"Monitoring"},{"body":"Enabling Replication In CSI PowerMax Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell EMC PowerMax supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Configure SRDF connection between multiple PowerMax instances. Follow instructions by PowerMax storage for creating the SRDF Groups between a set of arrays.\nYou can ensure that you configured remote arrays by navigating to the Data Protection tab and choosing SRDF Groups on the managing Unisphere of your array. You should see a list of remote systems with the SRDF Group number that is configured and the Online field set to a green tick.\nWhile using any SRDF groups, ensure that they are for exclusive use by the CSI PowerMax driver -\n Any SRDF group which will be used by the driver is not in use by any other application If an SRDF group is already in use by a CSI driver, don’t use it for provisioning replicated volumes outside CSI provisioning workflows.  There are some important limitations that apply to how CSI PowerMax driver uses SRDF groups -\n One replicated storage group always contains volumes provisioned from a single namespace While using SRDF mode Async/Metro, a single SRDF group can be used to provision volumes within a single namespace. You can still create multiple storage classes using the same SRDF group for different Service Levels. But all these storage classes will be restricted to provisioning volumes within a single namespace. When using SRDF mode Sync, a single SRDF group can be used to provision volumes from multiple namespaces.  In Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\n Check controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING\n Check that controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target clusters IDs.\n  If you don’t have something installed or something out-of-place please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set helm parameter replication.enabled in your copy of example values.yaml file (usually called my-powermax-settings.yaml, myvalues.yaml etc.).\nHere is an example of how that would look like\n...# Set this to true to enable replicationreplication:enabled:trueimage:dellemc/dell-csi-replicator:v1.0.0replicationContextPrefix:\"powermax\"replicationPrefix:\"replication.storage.dell.com\"...You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerMax following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\n NOTE: you need to install your driver at least on the source cluster, but it is recommended to install drivers on all clusters you will use for replication.\n Creating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nPair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository at ./samples/storageclass/powermax_srdf.yaml.\nIt will look like this:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-srdfprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\u003cSRPName\u003e SYMID: \u003cSYMID\u003eServiceLevel:\u003cServiceLevel\u003e replication.storage.dell.com/RemoteSYMID: \u003cRemoteSYMID\u003ereplication.storage.dell.com/RemoteSRP:\u003cRemoteSRP\u003e replication.storage.dell.com/isReplicationEnabled: \"true\"replication.storage.dell.com/RemoteServiceLevel:\u003cRemoteServiceLevel\u003e replication.storage.dell.com/RdfMode: \u003cRdfMode\u003ereplication.storage.dell.com/Bias:\"false\"replication.storage.dell.com/RdfGroup:\u003cRdfGroup\u003e replication.storage.dell.com/RemoteRDFGroup: \u003cRemoteRDFGroup\u003ereplication.storage.dell.com/remoteStorageClassName:\u003cRemoteStorageClassName\u003e replication.storage.dell.com/remoteClusterID: \u003cRemoteClusterID\u003eLet’s go through each parameter and what it means:\n replication.storage.dell.com/isReplicationEnabled if set to true, will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/RemoteStorageClassName points to the name of the remote storage class, if you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/RemoteClusterID represents the ID of a remote cluster, it is the same id you put in the replication controller config map. replication.storage.dell.com/RemoteSYMID is the Symmetrix id of the remote array. replication.storage.dell.com/RemoteSRP is the storage pool of the remote array. replication.storage.dell.com/RemoteServiceLevel is the service level that will be assigned to remote volumes. replication.storage.dell.com/RdfMode points to the RDF mode you want to use. It should be one out of “ASYNC”, “METRO” and “SYNC”. If mode is set to METRO, driver does not need RemoteStorageClassName and RemoteClusterID as it supports METRO with single cluster configuration. replication.storage.dell.com/Bias when the RdfMode is set to METRO, this parameter is required to indicate driver to use Bias or Witness. If set to true, the driver will configure METRO with Bias, if set to false, the driver will configure METRO with Witness. replication.storage.dell.com/RdfGroup is the local SRDF group number, as configured. replication.storage.dell.com/RemoteRDFGroup is the remote SRDF group number, as configured.  Let’s follow up that with an example, let’s assume we have two Kubernetes clusters and two PowerMax storage arrays:\n Clusters have IDs of cluster-1 and cluster-2 There are two arrays local Symmetrix array: 000000000001 and remote Symmetrix array: 000000000002 Storage arrays are connected to each other via RdfGroup 1 and RemoteRDFGroup 2 Cluster cluster-1 connected to array 000000000001 Cluster cluster-2 connected to array 000000000002 RDF Mode is ASYNC  And this how would our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-srdfprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\"SRP\"SYMID:\"000000000001\"ServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSYMID:\"000000000002\"replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/RemoteSRP:\"SRP\"replication.storage.dell.com/RemoteServiceLevel:\"Optimized\"replication.storage.dell.com/RdfMode:\"ASYNC\"replication.storage.dell.com/Bias:\"false\"replication.storage.dell.com/RdfGroup:\"1\"replication.storage.dell.com/RemoteRDFGroup:\"2\"replication.storage.dell.com/remoteStorageClassName:\"powermax-srdf\"replication.storage.dell.com/remoteClusterID:\"cluster-2\"StorageClass to be created in cluster-2:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-srdfprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\"SRP\"SYMID:\"000000000002\"ServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSYMID:\"000000000001\"replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/RemoteServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSRP:\"SRP\"replication.storage.dell.com/Bias:\"false\"replication.storage.dell.com/RdfMode:\"ASYNC\"replication.storage.dell.com/RdfGroup:\"2\"replication.storage.dell.com/RemoteRDFGroup:\"1\"replication.storage.dell.com/remoteStorageClassName:\"powermax-srdf\"replication.storage.dell.com/remoteClusterID:\"cluster-1\"After figuring out how storage classes would look like you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example in repctl/examples/powermax_example_values.yaml, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from manual installation and see how config would look like\nsourceClusterID:\"cluster-1\"targetClusterID:\"cluster-2\"name:\"powermax-replication\"driver:\"powermax\"reclaimPolicy:\"Retain\"replicationPrefix:\"replication.storage.dell.com\"parameters:rdfMode:\"ASYNC\"srp:source:\"SRP_1\"target:\"SRP_1\"symId:source:\"000000000001\"target:\"000000000002\"serviceLevel:source:\"Optimized\"target:\"Optimized\"rdfGroup:source:\"1\"target:\"2\"After preparing the config you can apply it to both clusters with repctl, just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes would be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl list storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nProvisioning Metro Volumes Here is an example of a storage class configured for Metro mode,\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-metroprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\"SRP\"SYMID:\"000000000001\"ServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSYMID:\"000000000002\"replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/RemoteSRP:\"SRP\"replication.storage.dell.com/RemoteServiceLevel:\"Optimized\"replication.storage.dell.com/RdfMode:\"Metro\"replication.storage.dell.com/Bias:\"true\"replication.storage.dell.com/RdfGroup:\"3\"replication.storage.dell.com/RemoteRDFGroup:\"3\"After installing the driver and creating a storage class with Metro config (as shown above) we can create volumes. On your cluster, create a PersistentVolumeClaim using this storage class. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nSupported Replication Actions The CSI PowerMax driver supports the following list of replication actions:\nBasic Site Specific Actions -  FAILOVER_LOCAL FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL UNPLANNED_FAILOVER_REMOTE REPROTECT_LOCAL REPROTECT_REMOTE  Advanced Site Specific Actions - In this section we are going to refer to “Site A” as the original source site \u0026 “Site B” as the original target site. Any action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\n FAILOVER_WITHOUT_SWAP_LOCAL  You can use this action to do a failover when you are at Site B, and don’t want to swap the replication direction. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_LOCAL. After receiving this request the CSI driver will attempxt to Fail over to Site B which is the local site.   FAILOVER_WITHOUT_SWAP_REMOTE  You can use this action to do a failover when you are at Site A, and don’t want to swap the replication direction. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_REMOTE. After receiving this request the CSI driver will attempt to Fail over to Site B which is the remote site.   FAILBACK_LOCAL  You can use this action to do a failback, and when you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_LOCAL. After receiving this request the CSI driver will attempt to Fail back from Site B to Site A which is the local site.   FAILBACK_REMOTE  You can use this action to do a failback, and when you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_REMOTE. After receiving this request the CSI driver will attempt to Fail back to Site A from Site B which is the local site.   SWAP_LOCAL  You can use this action to swap the replication direction, and you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_LOCAL. After receiving this request the CSI driver will attempt to do SWAP at Site A which is the local site.   SWAP_REMOTE  You can use this action to swap the replication direction, and you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_REMOTE. After receiving this request the CSI driver will attempt to do SWAP at Site B which is the remote site.    Maintenance Actions -  SUSPEND RESUME ESTABLISH SYNC  Deletion of DellCSIReplicationGroup The deletion of DellCSIReplicationGroup custom resource triggers the DeleteStorageProtectionGroup call on the driver. The storage protection group on the array can be deleted only if it has no volumes associated with it. If the deletion is triggered on the storage protection group with volumes, the deletion will fail and the dell-csi-driver will return a final error to the dell-csm-replication sidecar.\n","excerpt":"Enabling Replication In CSI PowerMax Container Storage Modules (CSM) …","ref":"/csm-docs/docs/replication/deployment/powermax/","title":"PowerMax"},{"body":"","excerpt":"","ref":"/csm-docs/docs/csidriver/release/","title":"Release Notes"},{"body":"","excerpt":"","ref":"/csm-docs/v1/release/","title":"Release Notes"},{"body":"","excerpt":"","ref":"/csm-docs/v2/release/","title":"Release Notes"},{"body":"","excerpt":"","ref":"/csm-docs/v3/release/","title":"Release Notes"},{"body":"Container Storage Modules (CSM) for Replication is part of the open-source suite of Kubernetes storage enablers for Dell EMC products.\nCSM for Replication project aims to bring Replication \u0026 Disaster Recovery capabilities of Dell EMC Storage Arrays to Kubernetes clusters. It helps you replicate groups of volumes using the native replication technology available on the storage array and can provide you a way to restart applications in case of both planned and unplanned migration.\nCSM for Replication Capabilities CSM for Replication provides the following capabilities:\n  Capability PowerScale Unity PowerStore PowerFlex PowerMax     Replicate data using native storage array based replication no no yes no yes   Create PersistentVolume objects in the cluster representing the replicated volume no no yes no yes   Create DellCSIReplicationGroup objects in the cluster no no yes no yes   Failover \u0026 Reprotect applications using the replicated volumes no no yes no yes   Provides a command line utility - repctl for configuring \u0026 managing replication related resources across multiple clusters no no yes no yes    Supported Operating Systems/Container Orchestrator Platforms   COP/OS PowerMax PowerStore     Kubernetes 1.20, 1.21, 1.22 1.20, 1.21, 1.22   Red Hat OpenShift 4.7, 4.8 4.7, 4.8   RHEL 7.x, 8.x 7.x, 8.x   CentOS 7.8, 7.9 7.8, 7.9   Ubuntu 20.04 20.04   SLES 15SP2 15SP2    Supported Storage Platforms    PowerMax PowerStore     Storage Array 5978.479.479, 5978.669.669, 5978.711.711, Unisphere 9.2 1.0.x, 2.0.x    Supported CSI Drivers CSM for Replication supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell EMC PowerMax csi-powermax v2.0   CSI Driver for Dell EMC PowerStore csi-powerstore v2.0    Details As on the storage arrays, all replication related Kubernetes entities are required/created in pairs -\n Pair of Kubernetes Clusters Pair of replication enabled Storage classes Pair of PersistentVolumes representing the replicated pair on the storage array Pair of DellCSIReplicationGroup objects representing the replicated protection groups on the storage array  You can also use a single stretched Kubernetes cluster for protecting your applications. Even in this topology, rest of the objects still exist in pairs.\nWhat it does not do  Replicate application manifests within/across clusters Stop applications before the planned/unplanned migration Start applications after the migration Replicate PersistentVolumeClaim objects within/across clusters  CSM for Replication Module Capabilities CSM for Replication provides the following capabilities:\n   Capability PowerMax PowerStore PowerScale PowerFlex Unity     Asynchronous replication of PVs accross K8s clusters yes yes no no no   Synchronous replication of PVs accross K8s clusters yes no no no no   Single cluster (stretched) mode replication yes yes no no no   Replication actions (failover, reprotect) yes yes no no no    Supported Platforms The following matrix provides a list of all supported versions for each Dell EMC Storage product.\n   Platforms PowerMax PowerStore     Kubernetes 1.20, 1.21, 1.22 1.20, 1.21, 1.22   CSI Driver 2.x 2.x    For compatibility with storage arrays please refer to corresponding CSI drivers\nQuickStart  Install all required components:   Enable replication during CSI driver installation Install CSM Replication Controller \u0026 repctl  Create replication enabled storage classes Create PersistentVolumeClaim using the replication enabled storage class  How it works At a high level, the following happens when you create a PersistentVolumeClaim object using a replication enabled storage class -\n CSI driver creates protection group on the storage array (if required) CSI driver creates the volume and adds it to the protection group. There will be a corresponding group and pair on the remote storage array A DellCSIReplicationGroup object is created in the cluster representing the protection group on the storage array A replica of the PersistentVolume \u0026 DellCSIReplicationGroup is created  You can refer this page for more details about the architecture.\nOnce the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), you can exercise the general Disaster Recovery workflows -\n Planned Migration to the target cluster/array Unplanned Migration to the target cluster/array Reprotect volumes at the target cluster/array Maintenance activities like - Suspend, Resume, Establish replication  ","excerpt":"Container Storage Modules (CSM) for Replication is part of the …","ref":"/csm-docs/docs/replication/","title":"Replication"},{"body":"You can exercise native replication control operations from Dell EMC storage arrays by performing “Actions” on the replicated group of volumes using the DellCSIReplicationGroup object.\nYou can patch the DellCSIReplicationGroup Custom Resource and set the action field in the spec to one of the allowed values (refer to tables in this document).\nWhen you set the action field in the Custom Resource object, the following happens:\n State of the RG CR is set to action_in_progress. For e.g. if you set the action field to SYNC, then the state will change to SYNC_IN_PROGRESS, action field will reset to empty dell-csi-replicator sidecar issues the command to the CSI driver to perform the appropriate action Once the CSI driver has completed the operation, State of the RG CR goes back to Ready  While the action is in progress, you shouldn’t update the action field. Any attempt to change the action field will be rejected and it will be reset to empty. There are certain pre-requisites that have to be fulfilled before any action can be done on the RG CR. For e.g. - you can’t perform a Reprotect without doing a Failover first. There are some “Workflows” defined in Section 2 of this document which provide a sequence of operations for some common use-cases. An important exception to these rules is the action UNPLANNED_FAILOVER which can be run at any time.\n Note - Throughout this document, we are going to refer to “Hopkinton” as the original source site \u0026 “Durham” as the original target site.\n Site Specific Actions These actions can be run at any site, but they have some site-specific context included.\nAny action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\nFor e.g. -\n If the CR at Hopkinton is patched with action FAILOVER_REMOTE, it means that the driver will attempt to Fail Over to Durham which is the remote site. If the CR at Durham is patched with action FAILOVER_LOCAL, it means that the driver will attempt to Fail over to Durham which is the local site. If the CR at Durham is patched with REPROTECT_LOCAL, it means that the driver will Re-protect the volumes at Durham which is the local site.  The following table lists details of what actions should be used in different Disaster Recovery workflows \u0026 the equivalent operation done on the storage array:\n   Workflow Actions PowerMax PowerStore     Planned Migration FAILOVER_LOCAL\nFAILOVER_REMOTE symrdf failover -swap FAILOVER (no REPROTECT after FAILOVER)   Reprotect REPROTECT_LOCAL REPROTECT_REMOTE symrdf resume/est REPROTECT   Unplanned Migration UNPLANNED_FAILOVER_LOCAL UNPLANNED_FAILOVER_REMOTE symrdf failover -force FAILOVER (at target site)    Maintenance Actions These actions can be run at any site and are used to change the replication link state for maintenance activities. The following table lists the supported maintenance actions and the equivalent operation done on the storage arrays\n  Action Description PowerMax PowerStore     SUSPEND Temporarily suspend replication symrdf suspend PAUSE   RESUME Resume replication symrdf resume RESUME   SYNC Synchronize all changes from source to target symrdf establish SYNCHRONIZE NOW    How to perform actions We strongly recommend using repctl to perform any actions on DellCSIReplicationGroup objects. You can find detailed steps here\nIf you wish to use kubectl to perform actions, then use kubectl edit/patch operations and set the action field in the Custom Resource. While performing site-specific actions, please consult each driver’s documentation to get an exhaustive list of all the supported actions.\nFor a brief guide on using actions for various DR workflows, please refer to this document\n","excerpt":"You can exercise native replication control operations from Dell EMC …","ref":"/csm-docs/docs/replication/replication-actions/","title":"Replication Actions"},{"body":"Container Storage Modules (CSM) for Resiliency is part of the open-source suite of Kubernetes storage enablers for Dell EMC products.\nUser applications can have problems if you want their Pods to be resilient to node failure. This is especially true of those deployed with StatefulSets that use PersistentVolumeClaims. Kubernetes guarantees that there will never be two copies of the same StatefulSet Pod running at the same time and accessing storage. Therefore, it does not clean up StatefulSet Pods if the node executing them fails.\nFor the complete discussion and rationale, go to https://github.com/kubernetes/community and search for the pod-safety.md file (path: contributors/design-proposals/storage/pod-safety.md). For more background on the forced deletion of Pods in a StatefulSet, please visit Force Delete StatefulSet Pods.\nCSM for Resiliency High-Level Description CSM for Resiliency is designed to make Kubernetes Applications, including those that utilize persistent storage, more resilient to various failures. The first component of the Resiliency module is a pod monitor that is specifically designed to protect stateful applications from various failures. It is not a standalone application, but rather is deployed as a sidecar to CSI (Container Storage Interface) drivers, in both the driver’s controller pods and the driver’s node pods. Deploying CSM for Resiliency as a sidecar allows it to make direct requests to the driver through the Unix domain socket that Kubernetes sidecars use to make CSI requests.\nSome of the methods CSM for Resiliency invokes in the driver are standard CSI methods, such as NodeUnpublishVolume, NodeUnstageVolume, and ControllerUnpublishVolume. CSM for Resiliency also uses proprietary calls that are not part of the standard CSI specification. Currently, there is only one, ValidateVolumeHostConnectivity that returns information on whether a host is connected to the storage system and/or whether any I/O activity has happened in the recent past from a list of specified volumes. This allows CSM for Resiliency to make more accurate determinations about the state of the system and its persistent volumes.\nAccordingly, CSM for Resiliency is adapted to and qualified with each CSI driver it is to be used with. Different storage systems have different nuances and characteristics that CSM for Resiliency must take into account.\nCSM for Resiliency Capabilities CSM for Resiliency provides the following capabilities:\n  Capability PowerScale Unity PowerStore PowerFlex PowerMax     Detect pod failures for the following failure types - Node failure, K8S Control Plane Network failure, Array I/O Network failure no yes no yes no   Cleanup pod artifacts from failed nodes no yes no yes no   Revoke PV access from failed nodes no yes no yes no    Supported Operating Systems/Container Orchestrator Platforms   COP/OS Supported Versions     Kubernetes 1.20, 1.21, 1.22   Red Hat OpenShift 4.7, 4.8   RHEL 7.x, 8.x   CentOS 7.8, 7.9    Supported Storage Platforms    PowerFlex Unity     Storage Array 3.5.x, 3.6.x 5.0.5, 5.0.6, 5.0.7, 5.1.0    Supported CSI Drivers CSM for Authorization supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell EMC PowerFlex csi-powerflex v2.0   CSI Driver for Dell EMC Unity csi-unity v2.0    PowerFlex Support PowerFlex is a highly scalable array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerFlex leverages the following PowerFlex features:\n Very quick detection of Array I/O Network Connectivity status changes (generally takes 1-2 seconds for the array to detect changes) A robust mechanism if Nodes are doing I/O to volumes (sampled over a 5-second period). Low latency REST API supports fast CSI provisioning and de-provisioning operations. A proprietary network protocol provided by the SDC component that can run over the same IP interface as the K8S control plane or over a separate IP interface for Array I/O.  Unity Support Dell EMC Unity is targeted for midsized deployments, remote or branch offices, and cost-sensitive mixed workloads. Unity systems are designed for all-Flash, deliver the best value in the market, and are available in purpose-built (all Flash or hybrid Flash), converged deployment options (through VxBlock), and software-defined virtual edition.\n Unity (purpose-built): A modern midrange storage solution, engineered from the groundup to meet market demands for Flash, affordability and incredible simplicity. The Unity Family is available in 12 All Flash models and 12 Hybrid models. VxBlock (converged): Unity storage options are also available in Dell EMC VxBlock System 1000. UnityVSA (virtual): The Unity Virtual Storage Appliance (VSA) allows the advanced unified storage and data management features of the Unity family to be easily deployed on VMware ESXi servers, for a ‘software defined’ approach. UnityVSA is available in two editions:  Community Edition is a free downloadable 4 TB solution recommended for nonproduction use. Professional Edition is a licensed subscription-based offering available at capacity levels of 10 TB, 25 TB, and 50 TB. The subscription includes access to online support resources, EMC Secure Remote Services (ESRS), and on-call software- and systems-related support.    All three deployment options, i.e. Unity, UnityVSA, and Unity-based VxBlock, enjoy one architecture, one interface with consistent features and rich data services.\nLimitations and Exclusions This file contains information on Limitations and Exclusions that users should be aware of. Additionally, there are driver specific limitations and exclusions that may be called out in the Deploying CSM for Resiliency page.\nSupported and Tested Operating Modes The following provisioning types are supported and have been tested:\n Dynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “FileSystem”. Dynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “Block”. Use of the above volumes with Pods created by StatefulSets. Up to 12 or so protected pods on a given node. Failing up to 3 nodes at a time in 9 worker node clusters, or failing 1 node at a time in smaller clusters. Application recovery times are dependent on the number of pods that need to be moved as a result of the failure. See the section on “Testing and Performance” for some of the details.  Not Tested But Assumed to Work  Deployments with the above volume types, provided two pods from the same deployment do not reside on the same node. At the current time anti-affinity rules should be used to guarantee no two pods accessing the same volumes are scheduled to the same node. Multi-array support  Not Yet Tested or Supported   Pods that use persistent volumes from multiple CSI drivers. This cannot be supported because multiple controller-podmons (one for each driver type) would be trying to manage the failover with conflicting actions.\n  ReadWriteMany volumes. This may have issues if a node has multiple pods accessing the same volumes. In any case once pod cleanup fences the volumes on a node, they will no longer be available to any pods using those volumes on that node. We will endeavor to support this in the future.\n  Multiple instances of the same driver type (for example two CSI driver for Dell EMC PowerFlex deployments.)\n  Deploying and Managing Applications Protected by CSM for Resiliency The first thing to remember about CSM for Resiliency is that it only takes action on pods configured with the designated label. Both the key and the value have to match what is in the podmon helm configuration. CSM for Resiliency emits a log message at startup with the label key and value it is using to monitor pods:\nlabelSelector: {map[podmon.dellemc.com/driver:csi-vxflexos] The above message indicates the key is: podmon.dellemc.com/driver and the label value is csi-vxflexos. To search for the pods that would be monitored, try this:\n[root@lglbx209 podmontest]# kubectl get pods -A -l podmon.dellemc.com/driver=csi-vxflexos NAMESPACE NAME READY STATUS RESTARTS AGE pmtu1 podmontest-0 1/1 Running 0 3m7s pmtu2 podmontest-0 1/1 Running 0 3m8s pmtu3 podmontest-0 1/1 Running 0 3m6s If CSM for Resiliency detects a problem with a pod caused by a node or other failure that it can initiate remediation for, it will add an event to that pod’s events:\nkubectl get events -n pmtu1 ... 61s Warning NodeFailure pod/podmontest-0 podmon cleaning pod [7520ba2a-cec5-4dff-8537-20c9bdafbe26 node.example.com] with force delete ... CSM for Resiliency may also generate events if it is unable to cleanup a pod for some reason. For example, it may not clean up a pod because the pod is still doing I/O to the array.\nImportant Before putting an application into production that relies on CSM for Resiliency monitoring, it is important to do a few test failovers first. To do this take the node that is running the pod offline for at least 2-3 minutes. Verify that there is an event message similar to the one above is logged, and that the pod recovers and restarts normally with no loss of data. (Note that if the node is running many CSM for Resiliency protected pods, the node may need to be down longer for CSM for Resiliency to have time to evacuate all the protected pods.)\nApplication Recommendations   It is recommended that pods that will be monitored by CSM for Resiliency be configured to exit if they receive any I/O errors. That should help achieve the recovery as quickly as possible.\n  CSM for Resiliency does not directly monitor application health. However, if standard Kubernetes health checks are configured, that may help reduce pod recovery time in the event of node failure, as CSM for Resiliency should receive an event that the application is Not Ready. Note that a Not Ready pod is not sufficient to trigger CSM for Resiliency action unless there is also some condition indicating a Node failure or problem, such as the Node is tainted, or the array has lost connectivity to the node.\n  As noted previously in the Limitations and Exclusions section, CSM for Resiliency has not yet been verified to work with ReadWriteMany or ReadOnlyMany volumes. Also, it has not been verified to work with pod controllers other than StatefulSet.\n  Recovering From Failures Normally CSM for Resiliency should be able to move pods that have been impacted by Node Failures to a healthy node. After the failed nodes have come back online, CSM for Resiliency cleans them up (especially any potential zombie pods) and then automatically removes the CSM for Resiliency node taint that prevents pods from being scheduled to the failed node(s). There are a few cases where this cannot be fully automated and operator intervention is required, including:\n  CSM for Resiliency expects that when a node failure occurs, all CSM for Resiliency labeled pods are evacuated and rescheduled on other nodes. This process may not complete however if the node comes back online before CSM for Resiliency has had time to evacuate all the labeled pods. The remaining pods may not restart correctly, going to “Error” or “CrashLoopBackoff”. We are considering some possible remediation for this condition but have not implemented them yet.\nIf this happens, try deleting the pod with “kubectl delete pod …”. In our experience this normally will cause the pod to be restarted and transition to the “Running” state.\n  Podmon-node is responsible for cleaning up failed nodes after the nodes’ communication has been restored. The algorithm checks to see that all the monitored pods have terminated and their volumes and mounts have been cleaned up.\nIf some of the monitored pods are still executing, node-podmon will emit the following log message at the end of a cleanup cycle (and retry the cleanup after a delay):\npods skipped for cleanup because still present: \u003cpod-list\u003e If this happens, DO NOT manually remove the CSM for Resiliency node taint. Doing so could possibly cause data corruption if volumes were not cleaned up, and a pod using those volumes was subsequently scheduled to that node.\nThe correct course of action in this case is to reboot the failed node(s) that have not removed their taints in a reasonable time (5-10 minutes after the node is online again.) The operator can delay executing this reboot until it is convenient, but new pods will not be scheduled to it in the interim. This reboot will cancel any potential zombie pods. After the reboot, node-podmon should automatically remove the node taint after a short time.\n  Testing Methodology and Results A three tier testing methodology is used for CSM for Resiliency:\n Unit testing with high coverage (\u003e90% statement) tests the program logic and is especially used to test the error paths by injecting faults. An integration test describes test scenarios in Gherkin that sets up specific testing scenarios executed against a Kubernetes test cluster. The tests use ranges for many of the parameters to add an element of “chaos testing”. Script based testing supports longevity testing in a Kubernetes cluster. For example, one test repeatedly fails three different lists of nodes in succession and is used to fail 1/3 of the cluster’s worker nodes on a cyclic basis and repeat indefinitely. This test collect statistics on length of time for pod evacuation, pod recovery, and node cleanup.  ","excerpt":"Container Storage Modules (CSM) for Resiliency is part of the …","ref":"/csm-docs/docs/resiliency/","title":"Resiliency"},{"body":"","excerpt":"","ref":"/csm-docs/v1/grasp/","title":"Learn"},{"body":"","excerpt":"","ref":"/csm-docs/v2/grasp/","title":"Learn"},{"body":"","excerpt":"","ref":"/csm-docs/v3/grasp/","title":"Learn"},{"body":"Enabling Replication In CSI PowerStore For the Container Storage Modules (CSM) for Replication sidecar container to work properly it needs to be installed alongside CSI driver that supports replication dell-csi-extensions calls.\nCSI driver for Dell EMC PowerStore supports necessary extension calls from dell-csi-extensions and to be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Be sure to configure replication between multiple PowerStore instances using instructions provided by PowerStore storage.\nYou can ensure that you configured remote systems by navigating to the Protection tab and choosing Remote System in UI of your PowerStore instance.\nYou should see a list of remote systems with both Management State and Data Connection fields set to OK.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\n Check controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING\n Check that controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by list of target clusters IDs.\n  If you don’t have something installed or something is out-of-place, please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set helm parameter controller.replication.enabled in your copy of example values.yaml file (usually called my-powerstore-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like\n...# controller: configure controller specific parameterscontroller:...# replication: allows to configure replicationreplication:enabled:trueimage:dellemc/dell-csi-replicator:v1.0.0replicationContextPrefix:\"powerstore\"replicationPrefix:\"replication.storage.dell.com\"...You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerStore following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\n NOTE: you need to install your driver at least on the source cluster, but it is recommended to install drivers on all clusters you will use for replication.\n Creating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nA pair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository at ./samples/storageclass/powerstore-replication.yaml.\nIt will look like this:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:\"powerstore-replication\"provisioner:\"csi-powerstore.dellemc.com\"reclaimPolicy:RetainvolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"powerstore-replication\"replication.storage.dell.com/remoteClusterID:\"tgt-cluster-id\"replication.storage.dell.com/remoteSystem:\"RT-0000\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"arrayID:\"Unique\"Let’s go through each parameter and what it means:\n replication.storage.dell.com/isReplicationEnabled if set to true will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents ID of a remote cluster. It is the same id you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system as seen from the current PowerStore instance. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. replication.storage.dell.com/ignoreNamespaces, if set to true PowerStore driver, it will ignore in what namespace volumes are created and put every volume created using this storage class into a single volume group. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate them. arrayID is a unique identifier of the storage array you specified in array connection secret.  Let’s follow up that with an example. Let’s assume you have two Kubernetes clusters and two PowerStore storage arrays:\n Clusters have IDs of cluster-1 and cluster-2 Storage arrays connected between each other and show up as remote systems with names RT-0001 and RT-0002 Cluster cluster-1 connected to array RT-0001 Cluster cluster-2 connected to array RT-0002 Storage array RT-0001 has a unique ID of PS000000001 Storage array RT-0002 has a unique ID of PS000000002  And this is what our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:\"powerstore-replication\"provisioner:\"csi-powerstore.dellemc.com\"reclaimPolicy:RetainvolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"powerstore-replication\"replication.storage.dell.com/remoteClusterID:\"cluster-2\"replication.storage.dell.com/remoteSystem:\"RT-0002\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"arrayID:\"PS000000001\"StorageClass to be created in cluster-2:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:\"powerstore-replication\"provisioner:\"csi-powerstore.dellemc.com\"reclaimPolicy:RetainvolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"powerstore-replication\"replication.storage.dell.com/remoteClusterID:\"cluster-1\"replication.storage.dell.com/remoteSystem:\"RT-0001\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"arrayID:\"PS000000002\"After figuring out how storage classes would look, you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example in repctl/examples/powerstore_example_values.yaml, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from manual installation and see how config would look like\nsourceClusterID:\"cluster-1\"targetClusterID:\"cluster-2\"name:\"powerstore-replication\"driver:\"powerstore\"reclaimPolicy:\"Retain\"replicationPrefix:\"replication.storage.dell.com\"parameters:arrayID:source:\"PS000000001\"target:\"PS000000002\"remoteSystem:source:\"RT-0002\"target:\"RT-0001\"rpo:\"Five_Minutes\"ignoreNamespaces:\"false\"volumeGroupPrefix:\"csi\"After preparing the config you can apply it to both clusters with repctl. Just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes would be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl list storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerStore driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerStore driver supports the following list of replication actions:\n FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC  ","excerpt":"Enabling Replication In CSI PowerStore For the Container Storage …","ref":"/csm-docs/docs/replication/deployment/powerstore/","title":"PowerStore"},{"body":"repctl repctl is a command-line client for configuring replication and managing replicated resources between multiple Kubernetes clusters.\nUsage Managing Clusters To begin managing replication with repctl you need to add your Kubernetes clusters, you can do that using cluster add command\n./repctl cluster add -f \u003cconfig-file\u003e -n \u003cname\u003e You can view clusters that are currently being managed by repctl by running cluster list command\n./repctl cluster list Also, you can inject information about all of your current clusters as config maps into the same clusters, so it can be used by dell-csi-replicator\n./repctl cluster inject You can also generate kubeconfigs from existing replication service accounts and inject them in config maps by providing --use-sa flag\n./repctl cluster inject --use-sa Querying Resources After adding clusters you want to manage with repctl you can query resources from multiple clusters at once using list command.\nFor example, this command will list all storage classes in all clusters that currently are being managed by repctl\n./repctl list storageclasses --all If you want to query some particular clusters you can do that by specifying clusters flag\n./repctl list pv --clusters cluster-1,cluster-3 All other different flags for querying resources you can check using included into the tool help flag -h.\nCreating Resources Generic Generic create command allows you to apply provided config file into multiple clusters at once\n/repctl create -f \u003cpath-to-file\u003e PersistentVolumeClaims You can use repctl to create PVCs from Replication Group’s PVs on the target cluster\n./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false  By default, ‘create pvc’ will do a ‘dry-run’ while creating PVCs. If you don’t encounter any issues in the dry-run, then you can re-run the command by turning off the dry-run flag to false.\n Storage Classes repctl can create special replication enabled storage classes from provided config, you can find example configs in examples folder\n./repctl create sc --from-config \u003cconfig-file\u003e` Single Cluster Replication repctl supports working with replication within a single Kubernetes cluster.\nJust add cluster you want to use with cluster add command, and you can list, filter, and create resources.\nVolumes and ReplicationGroups created as “target” resources would be prefixed with replicated- so you can easily differentiate them.\nYou can also differentiate between single cluster replication configured StorageClasses and ReplicationGroups and multi-cluster ones by checking remoteClusterID field, for a single cluster the field would be set to self.\nTo create replication enabled storage classes for single cluster replication using create sc command be sure to set both sourceClusterID and targetClusterID to the same clusterID and continue as usual with executing the command. Name of StorageClass resource that created as “target” will be appended with -tgt.\nExecuting Actions repctl can be used to execute various replication actions on ReplicationGroups.\nFailover This command will perform a planned failover to a cluster or an RG.\nWhen working with multiple clusters, you can perform failover by specifying the target cluster ID. To do that use --to-cluster \u003ctargetClusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e failover --to-cluster cluster2 When working with replication within a single cluster, you can perform failover by specifying the target replication group ID. To do that use --to-rg \u003crg-id\u003e parameter.\n./repctl failover --to-rg \u003crg-id\u003e In both scenarios repctl will patch the CR at the source site with action FAILOVER_REMOTE.\nYou can also provide --unplanned parameter, then repctl will perform an unplanned failover to a given cluster or an RG, instead of FAILOVER_REMOTE repctl will patch CR at target cluster with action UNPLANNED_FAILOVER_LOCAL.\nReprotect This command will perform a reprotect at the specified cluster or the RG.\nWhen working with multiple clusters, you can perform reprotect by specifying the cluster ID. To do that use --to-cluster \u003cclusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e reprotect --to-cluster cluster1 When working with replication within a single cluster, you can perform reprotect by specifying the replication group ID. To do that use --to-rg \u003crg-id\u003e parameter.\n./repctl reprotect --to-rg \u003crg-id\u003e In both scenarios repctl will patch the CR at the source site with action REPROTECT_LOCAL.\nFailback This command will perform a planned failback to a cluster or an RG.\nWhen working with multiple clusters, you can perform failback by specifying the cluster ID, to do that use --to-cluster \u003cclusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e failback --to-cluster cluster1 When working with replication within a single cluster, you can perform failback by specifying the replication group ID. To do that use --to-rg \u003crg-id\u003e parameter.\n./repctl failback --to-rg \u003crg-id\u003e In both scenarios repctl will patch the CR at the source site with action FAILBACK_LOCAL.\nYou can also provide --discard parameter, then repctl will perform a failback but discard any writes at target, instead of FAILBACK_LOCAL repctl will patch CR at target cluster with action ACTION_FAILBACK_DISCARD_CHANGES_LOCAL.\nSwap This command will perform a swap at a specified cluster or an RG.\nWhen working with multiple clusters, you can perform swap by specifying the cluster ID. To do that use --to-cluster \u003cclusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e swap --to-cluster cluster1 When working with replication within a single cluster, you can perform swap by specifying the replication group ID. To do that use --to-rg \u003crg-id\u003e parameter.\n./repctl swap --to-rg \u003crg-id\u003e repctl will patch CR at the source cluster with action SWAP_LOCAL.\nMaintenance Actions You can also use exec command to execute maintenance actions such as suspend, resume, and sync.\nFor single or multi-cluster config:\n./repctl --rg \u003crg-id\u003e exec -a \u003cACTION\u003e Where \u003cACTION\u003e can be one of the following:\n suspend will suspend replication, changes will no longer be synced between replication sites resume will resume replication, canceling the effect of suspend action sync will force synchronization of change between replication sites  ","excerpt":"repctl repctl is a command-line client for configuring replication and …","ref":"/csm-docs/docs/replication/tools/","title":"Tools"},{"body":"Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class   Note: From v1.7, the CSI driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the /samples/volumesnapshotclass folder under respective drivers.\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:csm-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\n","excerpt":"Volume Snapshot Feature In order to use Volume Snapshots, ensure the …","ref":"/csm-docs/docs/snapshots/","title":"Snapshots"},{"body":"   Symptoms Prevention, Resolution or Workaround     Persistent volumes don’t get created on the target cluster. Run kubectl describe on one of the pods of replication controller and see if event says Config update won't be applied because of invalid configmap/secrets. Please fix the invalid configuration. If it does then ensure you correctly populated replication ConfigMap. You can check the current status by running kubectl describe cm -n dell-replication-controller dell-replication-controller-config. If ConfigMap is empty please edit it yourself or use repctl cluster inject command.   Persistent volumes don’t get created on the target cluster. You don’t see any events on the replication-controller pod. Check logs of replication controller by running kubectl logs -n dell-replication-controller dell-replication-controller-manager-\u003cgenerated-symbols\u003e. If you see clusterId - \u003cclusterID\u003e not found errors then be sure to check if you specified the same clusterIDs in both your ConfigMap and replication enabled StorageClass.   You apply replication action by manually editing ReplicationGroup resource field spec.action and don’t see any change of ReplicationGroup state after a while. Check events of the replication-controller pod, if it says Cannot proceed with action \u003cyour-action\u003e. [unsupported action] then check spelling of your action and consult replication-actions page. Alternatively, you can use repctl instead of manually editing ReplicationGroup resources.   You execute failover action using repctl failover command and see failover: error executing failover to source site. This means you tried to failover to a cluster that is already marked source. If you still want to execute failover for RG just choose another cluster.   You’ve created PersistentVolumeClaim using replication enabled StorageClass but don’t see any RGs created in the source cluster. Check annotations of created PersistentVolumeClaim. If it doesn’t have annotations that start with replication.storage.dell.com then please wait for a couple of minutes for them to be added and RG to be created.    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Persistent …","ref":"/csm-docs/docs/replication/troubleshooting/","title":"Troubleshooting"},{"body":"Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes 1.17 and generally available (v1) in Kubernetes version \u003e=1.20.\nIn order to use Volume Snapshots, ensure the following components with appropriate versions have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  ","excerpt":"Volume Snapshot Feature The Volume Snapshot feature was introduced in …","ref":"/csm-docs/v1/concepts/","title":"Concepts"},{"body":"Volume Snapshot Feature The Volume Snapshot feature started in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes version 1.17 and generally available (v1) in Kubernetes version 1.20.\nIn order to use Volume Snapshots, ensure the following components with appropriate versions have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  ","excerpt":"Volume Snapshot Feature The Volume Snapshot feature started in alpha …","ref":"/csm-docs/v2/concepts/","title":"Concepts"},{"body":"","excerpt":"","ref":"/csm-docs/docs/csidriver/partners/","title":"Our Ecosystem Partners"},{"body":"","excerpt":"","ref":"/csm-docs/v1/partners/","title":"Our Ecosystem Partners"},{"body":"","excerpt":"","ref":"/csm-docs/v2/partners/","title":"Our Ecosystem Partners"},{"body":"","excerpt":"","ref":"/csm-docs/v3/partners/","title":"Our Ecosystem Partners"},{"body":"","excerpt":"","ref":"/csm-docs/docs/grasp/","title":"Learn"},{"body":"Troubleshooting links for Container Storage Modules:\nCSI Drivers\nCSM for Authorization\nCSM for Observability\nCSM for Replication\nCSM for Resiliency\n","excerpt":"Troubleshooting links for Container Storage Modules:\nCSI Drivers\nCSM …","ref":"/csm-docs/docs/troubleshooting/","title":"Troubleshooting"},{"body":"For all your support needs or to follow the latest ongoing discussions and updates, join our Slack group. Click Here to request your invite.\nYou can also interact with us on GitHub by creating a GitHub Issue.\n","excerpt":"For all your support needs or to follow the latest ongoing discussions …","ref":"/csm-docs/docs/support/","title":"Support"},{"body":"CSM Docs is an open-source project and we thrive to build a welcoming and open community for anyone who wants to use the project or contribute to it.\nContributing to CSM Docs Become one of the contributors to this project!\nYou can contribute to this project in several ways. Here are some examples:\n Contribute to the CSM documentation. Report an issue. Feature requests.  CSM docs reside in https://github.com/dell/csm-docs.\nCSM project resides in https://github.com/dell/csm.\nDon’t  Break the website view. Commit directly. Compromise backward compatibility. Disrespect your Community Team members. Forget to keep things simple.  Do  Keep it simple. Good work, your best every time. Squash your commits, avoid merges. Keep open communication with other Committers. Ask questions. Test your changes locally and make sure it is not breaking anything.  Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose.\nBranching strategy The CSM documentation portal follows a release branch strategy where a branch is created for each release and all documentation changes made for a release are done on that branch. The release branch is then merged into the main branch at the time of the release. In some situations it may be sufficient to merge a non-release branch to main if it fixes some issue in the documentation for the current released version.\nBranch Naming Convention    Branch Type Example Comment     main main    Release release-1.0 hotfix: release-1.1 patch: release-1.0.1   Feature feature-9-olp-support “9” referring to GitHub issue ID   Bug Fix bugfix-110-remove-docker-compose “110” referring to GitHub issue ID    Steps for working on the main branch  Fork the repository. Create a branch off of the main branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream main branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream main branch. Once your pull request has merged with the required approvals, your branch can be deleted.  Steps for working on a release branch  Fork the repository. Create a branch off of the release branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream release branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream release branch. Once your pull request has merged with the required approvals, your branch can be deleted.  Previewing your changes  Install latest Hugo version extended version.  Note: Please note we have to install an extended version.\n  Create a local copy of the csm-docs repository using git clone. Update docsy submodules inside themes folder using git submodule update --recursive --init Change to the csm-docs folder and run hugo server By default, local changes will be reflected at http://localhost:1313/. Hugo will watch for changes to the content and automatically refreshes the site.\n Note: To bind it to different server address use hugo server --bind 0.0.0.0, default is 127.0.0.1\n  After testing the changes locally, raise a pull request after editing the pages and pushing it to GitHub.  Community guidelines This project follows https://github.com/dell/csm/blob/main/docs/CODE_OF_CONDUCT.md.\nBest Practices Linking the URLs Hardcoded relative links like [troubleshooting observability](../../observability/troubleshooting.md) will behave unexpectedly compared to how they would work on our local file system. To avoid broken links in the portal, use regular relative URLs in links that will be left unchanged by Hugo.\nStyle guide  Use sentence case wherever applicable. Use the numbered lists for items in sequential order and bulletins for the other lists. Check for grammar and spelling. Embed the code within backticks. Use only high-resolution images.  ","excerpt":"CSM Docs is an open-source project and we thrive to build a welcoming …","ref":"/csm-docs/docs/contributionguidelines/","title":"Contribution Guidelines"},{"body":"The Dell Container Storage Modules (CSM) enables simple and consistent integration and automation experiences, extending enterprise storage capabilities to Kubernetes for cloud-native stateful applications. It reduces management complexity so developers can independently consume enterprise storage with ease and automate daily operations such as provisioning, snapshotting, replication, observability, authorization and, resiliency.\nCSM is made up of multiple components including modules (enterprise capabilities), CSI drivers (storage enablement) and, other related applications (deployment, feature controllers, etc).\nCSM Supported Modules and Dell EMC CSI Drivers    Modules/Drivers CSM 1.0 Previous Older Archives     Authorization 1.0 - - -   Observability 1.0 - - -   Replication 1.0 - - -   Resiliency 1.0 - - -   CSI Driver for PowerScale v2.0 v1.6 v1.5 v1.4   CSI Driver for Unity v2.0 v1.6 v1.5 v1.3   CSI Driver for PowerStore v2.0 v1.4 v1.3 v1.2   CSI Driver for PowerFlex v2.0 v1.5 v1.4 v1.3   CSI Driver for PowerMax v2.0 v1.7 v1.6 v1.5    ","excerpt":"The Dell Container Storage Modules (CSM) enables simple and consistent …","ref":"/csm-docs/docs/","title":"Dell EMC Container Storage Modules (CSM)"},{"body":"","excerpt":"","ref":"/csm-docs/blog/news/","title":"News About Docsy"},{"body":"The partnership between Dell Technologies and Google to support Anthos as an on-prem/hybrid Kubernetes platform tightens and expands.\n Anthos 1.5  PowerMax v1.4 PowerStore v1.1   Anthos bare metal  PowerMax PowerStore asdf-bmctl   Go further  Anthos 1.5 First let us talk about Anthos 1.5 that runs on top of VMware hypervisor. Dell is a storage and platform partner since the version 1.1 and it continues !\nBoth drivers (csi-powermax and csi-powerstore are qualified for iSCSI.\nTo ensure the iSCSI daemon is started, you can use the following DaemonSet to take care of it :\nkubectl create -f https://raw.githubusercontent.com/coulof/ds-iscsi/master/ds-iscsi.yaml PowerMax v1.4 As discussed in that post we provide a new installer script for every driver.\nIt never have been easier to install the CSI driver on Anthos. To do so, simply follow the steps of the Product Guide and add --skip-verify for the install command line :\n./csi-install.sh --namespace powermax --values my-powermax-settings.yaml --skip-verify If you come from an existing installation, there is nothing else to do.\nPowerStore v1.1 For the first time, it is my pleasure to announce csi-powerstore qualifies for Anthos v1.5 for iSCSI protocol (NFS will come later).\nPowerStore storage fits particularly well workloads that are on the Edge. Same here the installation on Anthos is the same as what is documented in the product guide with the addition --skip-verify option:\n./csi-install.sh --namespace csi-powerstore --values ./my-powerstoresettings.yaml --skip-verify Anthos bare metal Google recently announced the Anthos for bare metal, which, as its name indicates, brings support for Anthos Kubernetes engine on bare-metal server. This is a great opportunity to leverage specialized hardware or get rid of any kind of constraint on the VM hypervisor.\nPowerMax Thanks to the CSI driver can take full advantage of your Fiber Channel infrastructure and PowerMax end-to-end NVMe capability on Anthos bare metal. That type of architecture fits well with IO intensive workload and business critical application, often tight to transactional data.\nCheckout the installation process in video:\n PowerStore The save level of service is given to PowerStore with a full support for Anthos bare metal.\nCheckout the installation process in video:\n asdf-bmctl During the qualification process I had to juggle with at least 3 different versions of the Anthos bare metal installer.\nBeing sick of doing symlinks anytime I needed to change version, I wrote an asdf plugin to list the available versions, install them, and attach an Anthos bare metal configuration to a specific version.\nYou can :\n install it with asdf plugin-add bmctl git@eos2git.cec.lab.emc.com:coulof/asdf-bmctl.git list the versions with asdf list-all bmctl install them with asdf install bmctl 1.6.0 and then you can set your version locally asdf local bmctl 1.6.0 or globally asdf global bmctl 1.6.0.  Go further If you need a demo or have any question on Dell CSI drivers with Anthos reach out the Dell container community website\n","excerpt":"The partnership between Dell Technologies and Google to support Anthos …","ref":"/csm-docs/blog/2020/10/30/google-anthos-announcements/","title":"Google Anthos announcements"},{"body":"The quaterly update for Dell CSI Driver is there !\n New features  Across portfolio Volume Cloning Volume Expansion online and offline Raw Block Support RedHat CoreOS Docker EE 3.1 Dell CSI Operator CSI Driver for PowerMax CSI Driver for PowerStore CSI Driver for PowerFlex   One more thing ; Ansible for PowerStore v1.1 Useful links  New features Across portfolio This release gives for every driver the :\n Support of OpenShift 4.4 as well as Kubernetes 1.17, 1.18, 1.19 Support for Kubernetes Volume Snapshot Beta API New installer !  With Volume Snapshot’s promotion to beta, one significant change is the CSI external-snapshotter sidecar has been split into two controllers, a common snapshot controller and a CSI external-snapshotter sidecar.\nThe new install script available under dell-csi-helm-installer/csi-install.sh will :\n By default, install of the external-snaphotter for CSI driver. Optionally, install the beta snapshot CRD when the option --snapshot-crd is set during the initial installation.  Most recent Kubernetes distributions like OpenShift or GKE come with the common snapshotter controller installed.\nFor Kubernetes vanilla, you have to deploy the common snapshotter manually. The instructions are available here.\n /!\\ The drivers have validated the external-snapshotter version 1.2 and not the bleeding-edge version\n Volume Cloning Volume cloning is now available for every driver, but PowerFlex (that feature is on the roadmap).\nIt never has been easier [to spin a new environement from the production](({% post_url {% post_url note/dell/2020-05-29-gitlab-powermax %})).\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clone-pvc-0spec:accessModes:- ReadWriteOnceresources:requests:storage:5GistorageClassName:powermaxdataSource:kind:PersistentVolumeClaimname:pvc-0In the PVC definition you must make sure the source of the clone has the same storageClassName, request.storage size, namespace and accessModes.\nVolume Expansion online and offline That feature was already present in csi-powerscale ; it is now available for every Dell CSI driver.\nTo expand a volume, you just have to edit the PV size ; blazing fast example below: {: .size-small}\nRaw Block Support The Raw Block Support was already available with csi-powermax ; it is now available in csi-vxflexos and csi-powerstore.\nThat feature can be used if your application needs a filesystem different from xfs or ext4 or applications that can take advantage of a block device (like HDFS, Oracle ASM, etc.).\nRedHat CoreOS But for PowerFlex, every driver has been qualified with OpenShift 4.3 and 4.4 on CoreOS type of nodes !\nDocker EE 3.1 Docker Enterprise Edition (now part of Mirantis) makes his appearance to the list of officially supported by support.dell.com Kubernetes distributions.\nThe first drivers to qualify Docker EE are : csi-powerscale, csi-unity and csi-vxflexos.\nDell CSI Operator The dell-csi-operator adds support for the installation of the csi-powerstore and the multi-array support for csi-unity.\n At the moment of the publication, the new operator is under the RedHat certification process to get official support. The version 1.1 is not available yet in OperatorHub.io or OpenShift UI. Stay tuned for the update.\n CSI Driver for PowerMax Upon installation, we can enable the CSI PowerMax Reverse Proxy service. The CSI PowerMax Reverse Proxy is a reverse proxy that forwards CSI driver requests to Unisphere servers.\nIt can be used to improve reliability by having redundant Unisphere, or scale-up the number of requests to be sent to Unisphere and the managed PowerMax arrays.\nCSI Driver for PowerStore The csi-powerstore adds NFS to the list of supported protocols. It has all the features that iSCSI and Fiber Channel storage classes have.\nIf you need concurrent filesystem access (i.e. ReadWriteMany access mode) you can use the NFS protocol.\nCSI Driver for PowerFlex The csi-vxflexos is the first driver to bring topology support. It avoids the driver tried to mount a volume when the SDC is not installed (I see you non-CoreOS support ;-))\nOne more thing ; Ansible for PowerStore v1.1 The biggest “hot new feature” is the support for file operation in ansible-powerstore; this means we have access to new modules for:\n File system Snapshot File system NAS server NFS export SMB Share Quota  And of course all the modules conform to Ansible Idempotency requirement.\nUseful links For more details you can check :\n The product guides and release notes in the repositories for csi-powermax, csi-powerscale, csi-powerstore, csi-unity, csi-vxflexos and ansible-powerstore. The FAQs on on Dell container community website:  FAQ for CSI Driver for PowerMax FAQ for CSI Driver for PowerScale FAQ for CSI Driver for PowerStore FAQ for CSI Driver for PowerFlex FAQ for CSI Driver for Unity    ","excerpt":"The quaterly update for Dell CSI Driver is there !\n New features …","ref":"/csm-docs/blog/2020/09/28/csi-drivers-volume-expansion-and-beta-snapshot-support-update/","title":"CSI drivers Volume expansion and beta Snapshot support update !"},{"body":"Every quarter Dell Technologies ships new versions of his CSI Drivers and Ansible modules.\n Dell EMC has anounced new set of CSI Drivers for their storage arrays. Some highliths for these June 2020 releases:\n Qualifications for OpenShift 4.3 and Kubernetes 1.16 for all the drivers Easy upgrade with the CSI Operator for all the drivers Helm 3 support for all the drivers Multi-array support for PowerMax and Unity NFS support for Unity Volume expansion for Isilon Volume cloning for PowerMax CHAP for PowerMax   For the Ansible modules you will have:\n a brand new Ansible module for Unity ! Ansible for Isilon v1.1 brings support for SmartQuotas and is compatible with next OneFS major version.   For more details you can check :\n The product guides and release notes in the repositories for csi-powermax v1.3, csi-isilon v1.2, csi-unity v1.2, csi-vxflexos v1.1.5, ansible-unity v1.0 and ansible-isilon v1.1 The FAQs on on Dell container community website:  FAQ for CSI Driver for PowerMax FAQ for CSI Driver for Unity FAQ for CSI Driver for VxFlexOS FAQ for CSI Driver for Isilon FAQ for Ansible Isilon    ","excerpt":"Every quarter Dell Technologies ships new versions of his CSI Drivers …","ref":"/csm-docs/blog/2020/06/15/june-2020-dell-storage-enablers-big-update/","title":"June 2020 Dell storage enablers big update !"},{"body":"PowerMax is now supported by Google Anthos 1.3\nKeep in mind the snapshot alpha version is not supported in GKE.\nMore details on the support matrix and installation steps, check the official announcement on Dell container community website.\n","excerpt":"PowerMax is now supported by Google Anthos 1.3\nKeep in mind the …","ref":"/csm-docs/blog/2020/05/27/anthos-1.3-qualification-for-powermax/","title":"Anthos 1.3 Qualification for PowerMax"},{"body":"Installing CSI Driver via Operator CSI Drivers can be installed by creating a CustomResource object in your cluster.\nSample manifest files for each driver CustomResourceDefintion have been provided in the samples folder to help with the installation of the drivers. These files follow the naming convention\n{driver name}_{driver version}_k8s_{k8 version}.yaml  Or\n{driver name}_{driver version}_ops_{OpenShift version}.yaml  For e.g.\n sample/powermax_v140_k8s_117.yaml* \u003c- To install CSI PowerMax driver v1.4.0 on a Kubernetes 1.17 cluster sample/powermax_v140_ops_46.yaml* \u003c- To install CSI PowerMax driver v1.4.0 on an OpenShift 4.6 cluster  Copy the correct sample file and edit the mandatory \u0026 any optional parameters specific to your driver installation by following the instructions here\n NOTE: A detailed explanation of the various mandatory and optional fields in the CustomResource is available here. Please make sure to read through and understand the various fields.\n Run the following command to install the CSI driver.\nkubectl create -f \u003cdriver-manifest.yaml\u003e Note: If you are using an OLM based installation, the example manifests are available in the OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the installation Once the driver Custom Resource has been created, you can verify the installation\n  Check if Driver CR got created successfully\nFor e.g. – If you installed the PowerMax driver\n$ kubectl get csipowermax -n \u003cdriver-namespace\u003e   Check the status of the Custom Resource to verify if the driver installation was successful\n  If the driver-namespace was set to test-powermax, and the name of the driver is powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the details of the Custom Resource.\nNote: If the state of the CustomResource is Running then all the driver pods have been successfully installed. If the state is SuccessFul, then it means the driver deployment was successful but some driver pods may not be in a Running state. Please refer to the Troubleshooting section here if you encounter any issues during installation.\nChanges in installation for latest CSI drivers If you are installing the latest versions of the CSI drivers, the driver controller will be installed as a Kubernetes Deployment instead of a Statefulset. These installations can also run multiple replicas for the driver controller pods(not supported for StatefulSets) to support High Availability for the Controller.\nUpdate CSI Drivers The CSI Drivers installed by the Dell CSI Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include –\n Modifying the installation directly via kubectl edit For e.g. - If the name of the installed unity driver is unity, then run # Replace driver-namespace with the namespace where the Unity driver is installed $ kubectl edit csiunity/unity -n \u003cdriver-namespace\u003e and modify the installation\n Modify the API object in-place via kubectl patch  NOTE: If you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required.\nNOTE: Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures.\nSupported modifications  Changing environment variable values for driver Adding (supported) environment variables Updating the image of the driver  Unsupported modifications Kubernetes doesn’t allow to update a storage class once it has been created. Any attempt to update a storage class will result in a failure.\n Note – Any attempt to rename a storage class or snapshot class will result in the deletion of older class and creation of a new class.\n Limitations  The Dell CSI Operator can’t manage any existing driver installed using Helm charts. If you already have installed one of the DellEMC CSI driver in your cluster and want to use the operator based deployment, uninstall the driver and then redeploy the driver following the installation procedure described above The Dell CSI Operator can’t update storage classes as it is prohibited by Kubernetes. Any attempt to do so will cause an error and the driver Custom Resource will be left in a Failed state. Refer the Troubleshooting section to fix the driver CR. The Dell CSI Operator is not fully compliant with the OperatorHub React UI elements and some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSI Operator  Custom Resource Specification Each CSI Driver installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.\nBelow is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - Refer full list of supported driver for finding out the appropriate config version here replicas - Number of replicas for controller plugin - Must be set to 1 for all drivers\ncommon\nThis field is mandatory and is used to specify common properties for both controller and the node plugin\n image - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values  Optional fields controller - List of environment variables and values which are applicable only for controller\nnode - List of environment variables and values which are applicable only for node\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver\nstorageclass\nList of Storage Class fields\n name - name of the Storage Class default - Used to specify if the storage class will be marked as default (only set one storage class as default in a cluster) reclaimPolicy - Sets the PersistentVolumeReclaim Policy for the PVCs. Defaults to Delete if not specified parameters - driver specific parameters. Refer individual driver section for more details allowVolumeExpansion - Set to true for allowing volume expansion for PVC volumeBindingMode - Sets the VolumeBindingMode in the Storage Class. If left blank, it will be set to the default value for the driver version you are installing allowedTopologies - Sets the topology keys and values which allows the pods/and volumes to be scheduled on nodes that have access to the storage.  snapshotclass\nList of Snapshot Class specifications\n name - name of the snapshot class parameters - driver specific parameters. Refer individual driver section for more details  forceUpdate\nBoolean value which can be set to true in order to force update the status of the CSI Driver\ntolerations List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet\nIt should be set separately in the controller and node sections if you want separate set of tolerations for them\nnodeSelector Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet\nHere is a sample specification annotated with comments to explain each field\napiVersion:storage.dell.com/v1kind:CSIPowerMax# Type of the drivermetadata:name:test-powermax# Name of the drivernamespace:test-powermax# Namespace where driver is installedspec:driver:# Used to specify configuration versionconfigVersion:v3# Refer the table containing the full list of supported drivers to find the appropriate config versionreplicas:1forceUpdate:false# Set to true in case you want to force an update of driver statuscommon:# All common specificationimage:\"dellemc/csi-powermax:v1.4.0.000R\"#driver image for a particular releaseimagePullPolicy:IfNotPresentenvs:- name:X_CSI_POWERMAX_ENDPOINTvalue:\"https://0.0.0.0:8443/\"- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\"XYZ\"storageClass:- name:bronzedefault:truereclaimPolicy:Deleteparameters:SYMID:\"000000000001\"SRP:DEFAULT_SRPServiceLevel:BronzeYou can set the field replicas to a higher number than 1 for the latest driver versions.\nNote - The image field should point to the correct image tag for version of the driver you are installing.\nFor e.g. - If you wish to install v1.4 of the CSI PowerMax driver, use the image tag dellemc/csi-powermax:v1.4.0.000R\nNote - The name of the Storage Class or the Volume Snapshot Class (which are created in the Kubernetes/OpenShift cluster) is created using the name of the driver and the name provided for these classes in the manifest. This is done in order to ensure that these names are unique if there are multiple drivers installed in the same cluster.\nFor e.g. - With the above sample manifest, the name of the storage class which is created in the cluster will be test-powermax-bronze.\nYou can get the name of the StorageClass and SnapshotClass created by the operator by running the commands - kubectl get storageclass and kubectl get volumesnapshotclass\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. Any modifications to this should be only done after consulting with Dell EMC support.\nModify the driver specification  Choose the correct configVersion. Refer the table containing the full list of supported drivers and versions. Provide the namespace (in metadata section) where you want to install the driver. Provide a name (in metadata section) for the driver. This will be the name of the Custom Resource. Edit the values for mandatory configuration parameters specific to your installation. Edit/Add any values for optional configuration parameters to customize your installation. If you are installing the latest versions of the CSI drivers, the default number of replicas is set to 2. You can increase/decrease this value.  ","excerpt":"Installing CSI Driver via Operator CSI Drivers can be installed by …","ref":"/csm-docs/v1/installation/operator/installdriver/","title":""},{"body":"Installing CSI Driver via Operator CSI Drivers can be installed by creating a CustomResource object in your cluster.\nSample manifest files for each driver CustomResourceDefintion have been provided in the samples folder to help with the installation of the drivers. These files follow the naming convention\n{driver name}_{driver version}_k8s_{k8 version}.yaml  Or\n{driver name}_{driver version}_ops_{OpenShift version}.yaml  For e.g.\n sample/powermax_v140_k8s_117.yaml* \u003c- To install CSI PowerMax driver v1.4.0 on a Kubernetes 1.17 cluster sample/powermax_v140_ops_46.yaml* \u003c- To install CSI PowerMax driver v1.4.0 on an OpenShift 4.6 cluster  Copy the correct sample file and edit the mandatory \u0026 any optional parameters specific to your driver installation by following the instructions here\n NOTE: A detailed explanation of the various mandatory and optional fields in the CustomResource is available here. Please make sure to read through and understand the various fields.\n Run the following command to install the CSI driver.\nkubectl create -f \u003cdriver-manifest.yaml\u003e Note: If you are using an OLM based installation, the example manifests are available in the OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the installation Once the driver Custom Resource has been created, you can verify the installation\n  Check if Driver CR got created successfully\nFor e.g. – If you installed the PowerMax driver\n$ kubectl get csipowermax -n \u003cdriver-namespace\u003e   Check the status of the Custom Resource to verify if the driver installation was successful\n  If the driver-namespace was set to test-powermax, and the name of the driver is powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the details of the Custom Resource.\nNote: If the state of the CustomResource is Running then all the driver pods have been successfully installed. If the state is SuccessFul, then it means the driver deployment was successful but some driver pods may not be in a Running state. Please refer to the Troubleshooting section here if you encounter any issues during installation.\nChanges in installation for latest CSI drivers If you are installing the latest versions of the CSI drivers, the driver controller will be installed as a Kubernetes Deployment instead of a Statefulset. These installations can also run multiple replicas for the driver controller pods(not supported for StatefulSets) to support High Availability for the Controller.\nUpdate CSI Drivers The CSI Drivers installed by the Dell CSI Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include –\n Modifying the installation directly via kubectl edit For e.g. - If the name of the installed unity driver is unity, then run # Replace driver-namespace with the namespace where the Unity driver is installed $ kubectl edit csiunity/unity -n \u003cdriver-namespace\u003e and modify the installation\n Modify the API object in-place via kubectl patch  NOTE: If you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required.\nNOTE: Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures.\nSupported modifications  Changing environment variable values for driver Adding (supported) environment variables Updating the image of the driver  Unsupported modifications Kubernetes doesn’t allow to update a storage class once it has been created. Any attempt to update a storage class will result in a failure.\n Note – Any attempt to rename a storage class or snapshot class will result in the deletion of older class and creation of a new class.\n Limitations  The Dell CSI Operator can’t manage any existing driver installed using Helm charts. If you already have installed one of the DellEMC CSI driver in your cluster and want to use the operator based deployment, uninstall the driver and then redeploy the driver following the installation procedure described above The Dell CSI Operator can’t update storage classes as it is prohibited by Kubernetes. Any attempt to do so will cause an error and the driver Custom Resource will be left in a Failed state. Refer the Troubleshooting section to fix the driver CR. The Dell CSI Operator is not fully compliant with the OperatorHub React UI elements and some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSI Operator  Custom Resource Specification Each CSI Driver installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.\nBelow is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - Refer full list of supported driver for finding out the appropriate config version here replicas - Number of replicas for controller plugin - Must be set to 1 for all drivers\ncommon\nThis field is mandatory and is used to specify common properties for both controller and the node plugin\n image - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values  Optional fields controller - List of environment variables and values which are applicable only for controller\nnode - List of environment variables and values which are applicable only for node\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver\nstorageclass\nList of Storage Class fields\n name - name of the Storage Class default - Used to specify if the storage class will be marked as default (only set one storage class as default in a cluster) reclaimPolicy - Sets the PersistentVolumeReclaim Policy for the PVCs. Defaults to Delete if not specified parameters - driver specific parameters. Refer individual driver section for more details allowVolumeExpansion - Set to true for allowing volume expansion for PVC volumeBindingMode - Sets the VolumeBindingMode in the Storage Class. If left blank, it will be set to the default value for the driver version you are installing allowedTopologies - Sets the topology keys and values which allows the pods/and volumes to be scheduled on nodes that have access to the storage.  snapshotclass\nList of Snapshot Class specifications\n name - name of the snapshot class parameters - driver specific parameters. Refer individual driver section for more details  forceUpdate\nBoolean value which can be set to true in order to force update the status of the CSI Driver\ntolerations List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet\nIt should be set separately in the controller and node sections if you want separate set of tolerations for them\nnodeSelector Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet\nHere is a sample specification annotated with comments to explain each field\napiVersion:storage.dell.com/v1kind:CSIPowerMax# Type of the drivermetadata:name:test-powermax# Name of the drivernamespace:test-powermax# Namespace where driver is installedspec:driver:# Used to specify configuration versionconfigVersion:v3# Refer the table containing the full list of supported drivers to find the appropriate config versionreplicas:1forceUpdate:false# Set to true in case you want to force an update of driver statuscommon:# All common specificationimage:\"dellemc/csi-powermax:v1.4.0.000R\"#driver image for a particular releaseimagePullPolicy:IfNotPresentenvs:- name:X_CSI_POWERMAX_ENDPOINTvalue:\"https://0.0.0.0:8443/\"- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\"XYZ\"storageClass:- name:bronzedefault:truereclaimPolicy:Deleteparameters:SYMID:\"000000000001\"SRP:DEFAULT_SRPServiceLevel:BronzeYou can set the field replicas to a higher number than 1 for the latest driver versions.\nNote - The image field should point to the correct image tag for version of the driver you are installing.\nFor e.g. - If you wish to install v1.4 of the CSI PowerMax driver, use the image tag dellemc/csi-powermax:v1.4.0.000R\nNote - The name of the Storage Class or the Volume Snapshot Class (which are created in the Kubernetes/OpenShift cluster) is created using the name of the driver and the name provided for these classes in the manifest. This is done in order to ensure that these names are unique if there are multiple drivers installed in the same cluster.\nFor e.g. - With the above sample manifest, the name of the storage class which is created in the cluster will be test-powermax-bronze.\nYou can get the name of the StorageClass and SnapshotClass created by the operator by running the commands - kubectl get storageclass and kubectl get volumesnapshotclass\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. Any modifications to this should be only done after consulting with Dell EMC support.\nModify the driver specification  Choose the correct configVersion. Refer the table containing the full list of supported drivers and versions. Provide the namespace (in metadata section) where you want to install the driver. Provide a name (in metadata section) for the driver. This will be the name of the Custom Resource. Edit the values for mandatory configuration parameters specific to your installation. Edit/Add any values for optional configuration parameters to customize your installation. If you are installing the latest versions of the CSI drivers, the default number of replicas is set to 2. You can increase/decrease this value.  ","excerpt":"Installing CSI Driver via Operator CSI Drivers can be installed by …","ref":"/csm-docs/v2/installation/operator/installdriver/","title":""},{"body":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes\n-Product Guide\nPowerMax v1.4 -Release Notes\n-Product Guide\nPowerFlex v1.2 -Release Notes\n-Product Guide\nPowerStore v1.1 -Release Notes\n-Product Guide\nUnity v1.3 -Release Notes\n-Product Guide\n","excerpt":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes …","ref":"/csm-docs/docs/csidriver/archives/","title":"Archives"},{"body":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes\n-Product Guide\nPowerMax v1.4 -Release Notes\n-Product Guide\nPowerFlex v1.2 -Release Notes\n-Product Guide\nPowerStore v1.1 -Release Notes\n-Product Guide\nUnity v1.3 -Release Notes\n-Product Guide\n","excerpt":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes …","ref":"/csm-docs/v1/archives/","title":"Archives"},{"body":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes\n-Product Guide\nPowerMax v1.4 -Release Notes\n-Product Guide\nPowerFlex v1.2 -Release Notes\n-Product Guide\nPowerStore v1.1 -Release Notes\n-Product Guide\nUnity v1.3 -Release Notes\n-Product Guide\n","excerpt":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes …","ref":"/csm-docs/v2/archives/","title":"Archives"},{"body":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes\n-Product Guide\nPowerMax v1.4 -Release Notes\n-Product Guide\nPowerFlex v1.2 -Release Notes\n-Product Guide\nPowerStore v1.1 -Release Notes\n-Product Guide\nUnity v1.3 -Release Notes\n-Product Guide\n","excerpt":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes …","ref":"/csm-docs/v3/archives/","title":"Archives"},{"body":"This is the blog section. It has two categories: News and Releases.\n","excerpt":"This is the blog section. It has two categories: News and Releases.\n","ref":"/csm-docs/blog/","title":"Blog"},{"body":"   Before installing the drivers, Dell CSI Operator tries to validate the Custom Resource being created. If some mandatory environment variables are missing or there is a type mismatch, then the Operator will report an error during the reconciliation attempts.\nBecause of this, the status of the Custom Resource will change to “Failed” and the error captured in the “ErrorMessage” field in the status.\nFor example - If the PowerMax driver was installed in the namespace test-powermax and has the name powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the Custom Resource details.\nIf there was an error while installing the driver, then you would see a status like this:\nstatus:status:errorMessage:mandatoryEnv- X_CSI_K8S_CLUSTER_PREFIXnotspecifiedinuserspecstate:FailedThe state of the Custom Resource can also change to Failed because of any other prohibited updates or any failure while installing the driver. In order to recover from this failure, fix the error in the manifest and update/patch the Custom Resource\n  After an update to the driver, the controller pod may not have the latest desired specification.\nThis happens when the controller pod was in a failed state before applying the update. Even though the Dell CSI Operator updates the pod template specification for the StatefulSet, the StatefulSet controller does not apply the update to the pod. This happens because of the unique nature of StatefulSets where the controller tries to retain the last known working state.\nTo get around this problem, the Dell CSI Operator forces an update of the pod specification by deleting the older pod. In case the Dell CSI Operator fails to do so, delete the controller pod to force an update of the controller pod specification\n  The Status of the CSI Driver Custom Resource shows the state of the driver pods after installation. This state will not be updated automatically if there are any changes to the driver pods outside any Operator operations.\nAt times because of inconsistencies in fetching data from the Kubernetes cache, the state of some driver pods may not be updated correctly in the status. To force an update of the state, you can update the Custom Resource forcefully by setting forceUpdate to true. If all the driver pods are in the Available State, then the state of the Custom Resource will be updated as Running\n  ","excerpt":"   Before installing the drivers, Dell CSI Operator tries to validate …","ref":"/csm-docs/docs/csidriver/troubleshooting/operator/","title":"Dell CSI Operator"},{"body":"   Before installing the drivers, Dell CSI Operator tries to validate the Custom Resource being created. If some mandatory environment variables are missing or there is a type mismatch, then the Operator will report an error during the reconciliation attempts.\nBecause of this, the status of the Custom Resource will change to “Failed” and the error captured in the “ErrorMessage” field in the status.\nFor example - If the PowerMax driver was installed in the namespace test-powermax and has the name powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the Custom Resource details.\nIf there was an error while installing the driver, then you would see a status like this:\nstatus:status:errorMessage:mandatoryEnv- X_CSI_K8S_CLUSTER_PREFIXnotspecifiedinuserspecstate:FailedThe state of the Custom Resource can also change to Failed because of any other prohibited updates or any failure while installing the driver. In order to recover from this failure, fix the error in the manifest and update/patch the Custom Resource\n  After an update to the driver, the controller pod may not have the latest desired specification.\nThis happens when the controller pod was in a failed state before applying the update. Even though the Dell CSI Operator updates the pod template specification for the StatefulSet, the StatefulSet controller does not apply the update to the pod. This happens because of the unique nature of StatefulSets where the controller tries to retain the last known working state.\nTo get around this problem, the Dell CSI Operator forces an update of the pod specification by deleting the older pod. In case the Dell CSI Operator fails to do so, delete the controller pod to force an update of the controller pod specification\n  The Status of the CSI Driver Custom Resource shows the state of the driver pods after installation. This state will not be updated automatically if there are any changes to the driver pods outside any Operator operations.\nAt times because of inconsistencies in fetching data from the Kubernetes cache, the state of some driver pods may not be updated correctly in the status. To force an update of the state, you can update the Custom Resource forcefully by setting forceUpdate to true. If all the driver pods are in the Available State, then the state of the Custom Resource will be updated as Running\n  ","excerpt":"   Before installing the drivers, Dell CSI Operator tries to validate …","ref":"/csm-docs/v1/troubleshooting/operator/","title":"Dell CSI Operator"},{"body":"   Before installing the drivers, Dell CSI Operator tries to validate the Custom Resource being created. If some mandatory environment variables are missing or there is a type mismatch, then the Operator will report an error during the reconciliation attempts.\nBecause of this, the status of the Custom Resource will change to “Failed” and the error captured in the “ErrorMessage” field in the status.\nFor e.g. - If the PowerMax driver was installed in the namespace test-powermax and has the name powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the Custom Resource details.\nIf there was an error while installing the driver, then you would see a status like this -\nstatus:status:errorMessage:mandatoryEnv- X_CSI_K8S_CLUSTER_PREFIXnotspecifiedinuserspecstate:FailedThe state of the Custom Resource can also change to Failed because of any other prohibited updates or any failure while installing the driver. In order to recover from this failure, fix the error in the manifest and update/patch the Custom Resource\n  After an update to the driver, the controller pod may not have the latest desired specification\nThe above happens when the controller pod was in a failed state before applying the update. Even though the Dell CSI Operator updates the pod template specification for the StatefulSet, the StatefulSet controller does not apply the update to the pod. This happens because of the unique nature of StatefulSets where the controller tries to retain the last known working state.\nTo get around this problem, the Dell CSI Operator forces an update of the pod specification by deleting the older pod. In case the Dell CSI Operator fails to do so, delete the controller pod to force an update of the controller pod specification\n  The Status of the CSI Driver Custom Resource shows the state of the driver pods after installation. This state will not be updated automatically if there are any changes to the driver pods outside any Operator operations At times because of inconsistencies in fetching data from the Kubernetes cache, state of some driver pods may not be updated correctly in the status. To force an update of the state, you can update the Custom Resource forcefully by setting forceUpdate to true. If all the driver pods are in Available State, then the state of the Custom Resource will be updated as Running\n  ","excerpt":"   Before installing the drivers, Dell CSI Operator tries to validate …","ref":"/csm-docs/v2/troubleshooting/operator/","title":"Dell CSI Operator"},{"body":" Welcome to Dell Technologies Container Storage Modules documentation! Learn More          ","excerpt":" Welcome to Dell Technologies Container Storage Modules documentation! …","ref":"/csm-docs/","title":"Dell Technologies"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Docker Universal Control Plane (UCP).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn UCP based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on UCP backed clusters may run any of the OSs which we support with upstream clusters.\nDocker EE UI Examples ","excerpt":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and …","ref":"/csm-docs/v2/partners/docker/","title":"Docker EE"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Docker Universal Control Plane (UCP).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn UCP based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on UCP backed clusters may run any of the OSs which we support with upstream clusters.\nDocker EE UI Examples ","excerpt":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and …","ref":"/csm-docs/v3/partners/docker/","title":"Docker EE"},{"body":" This document version is no longer actively maintained. The site that you are currently viewing is an archived snapshot. For up-to-date documentation, see the latest version\n ","excerpt":" This document version is no longer actively maintained. The site that …","ref":"/csm-docs/v1/","title":"Documentation"},{"body":" This document version is no longer actively maintained. The site that you are currently viewing is an archived snapshot. For up-to-date documentation, see the latest version\n ","excerpt":" This document version is no longer actively maintained. The site that …","ref":"/csm-docs/v2/","title":"Documentation"},{"body":" This document version is no longer actively maintained. The site that you are currently viewing is an archived snapshot. For up-to-date documentation, see the latest version\n ","excerpt":" This document version is no longer actively maintained. The site that …","ref":"/csm-docs/v3/","title":"Documentation"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Mirantis Kubernetes Engine (MKE).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn MKE-based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on MKE-backed clusters may run any of the OS which we support with upstream clusters.\nMKE UI Examples ","excerpt":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and …","ref":"/csm-docs/docs/csidriver/partners/docker/","title":"MKE"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Mirantis Kubernetes Engine (MKE).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn MKE-based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on MKE-backed clusters may run any of the OS which we support with upstream clusters.\nMKE UI Examples ","excerpt":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and …","ref":"/csm-docs/v1/partners/docker/","title":"MKE"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell EMC CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\n PowerFlex PowerMax PowerScale PowerStore Unity  As well as the Dell CSI Operator\n Dell CSI Operator  Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\n One Linux-based system, with internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry  If one Linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry.    One of these will be required on both the system building the offline bundle as well as the system preparing for installation.    Tested version(s) are docker 19.03+ and podman 1.6.4+   git git will be used to manually clone one of the above repositories in order to create an offline bundle.    This is only needed on the system preparing the offline bundle.    Tested version(s) are git 1.8+ but any version should work.    Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\n Build an offline bundle Unpacking the offline bundle created in Step 1 and preparing for installation Perform either a Helm installation or Operator installation using the files obtained after unpacking in Step 2  Building an offline bundle This needs to be performed on a Linux system with access to the internet as a git repo will need to be cloned, and container images pulled from public registries.\nTo build an offline bundle, the following steps are needed:\n Perform a git clone of the desired repository. For a helm-based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle   For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory  The script will perform the following steps:\n Determine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator  The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\n[user@anothersystem /home/user]# git clone https://github.com/dell/dell-csi-operator.git [user@anothersystem /home/user]# cd dell-csi-operator [user@system /home/user/dell-csi-operator]# scripts/csi-offline-bundle.sh -c * * Building image manifest file * * Pulling container images dellemc/csi-isilon:v1.4.0.000R dellemc/csi-isilon:v1.5.0 dellemc/csi-isilon:v1.6.0 dellemc/csipowermax-reverseproxy:v1.3.0 dellemc/csi-powermax:v1.5.0.000R dellemc/csi-powermax:v1.6.0 dellemc/csi-powermax:v1.7.0 dellemc/csi-powerstore:v1.2.0.000R dellemc/csi-powerstore:v1.3.0 dellemc/csi-powerstore:v1.4.0 dellemc/csi-unity:v1.4.0.000R dellemc/csi-unity:v1.5.0 dellemc/csi-unity:v1.6.0 dellemc/csi-vxflexos:v1.3.0.000R dellemc/csi-vxflexos:v1.4.0 dellemc/csi-vxflexos:v1.5.0 dellemc/dell-csi-operator:v1.4.0 dellemc/sdc:3.5.1.1 dellemc/sdc:3.5.1.1-1 docker.io/busybox:1.32.0 k8s.gcr.io/sig-storage/csi-attacher:v3.0.0 k8s.gcr.io/sig-storage/csi-attacher:v3.1.0 k8s.gcr.io/sig-storage/csi-attacher:v3.2.1 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.1.0 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.0.2 k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1 k8s.gcr.io/sig-storage/csi-resizer:v1.2.0 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3 k8s.gcr.io/sig-storage/csi-snapshotter:v4.0.0 k8s.gcr.io/sig-storage/csi-snapshotter:v4.1.0 quay.io/k8scsi/csi-resizer:v1.0.0 quay.io/k8scsi/csi-resizer:v1.1.0 * * Saving images * * Copying necessary files /dell/git/dell-csi-operator/config /dell/git/dell-csi-operator/deploy /dell/git/dell-csi-operator/samples /dell/git/dell-csi-operator/scripts /dell/git/dell-csi-operator/README.md /dell/git/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md * * Complete Offline bundle file is: /dell/git/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking the offline bundle and preparing for installation This needs to be performed on a Linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for the driver or Operator installation, the following steps need to be performed:\n Copy the offline bundle file created from the previous step to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option  The script will then perform the following steps:\n Load the required container images into the local system Tag the images according to the user-supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images  An example of preparing the bundle for installation (192.168.75.40:5000 refers to an image registry accessible to Kubernetes/OpenShift):\n[user@anothersystem /tmp]# tar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md [user@anothersystem /tmp]# cd dell-csi-operator-bundle [user@anothersystem /tmp/dell-csi-operator-bundle]# scripts/csi-offline-bundle.sh -p -r 192.168.75.40:5000/operator Preparing an offline bundle for installation * * Loading docker images * * Tagging and pushing images dellemc/csi-isilon:v1.4.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.4.0.000R dellemc/csi-isilon:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.5.0 dellemc/csi-isilon:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.6.0 dellemc/csipowermax-reverseproxy:v1.3.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csipowermax-reverseproxy:v1.3.0 dellemc/csi-powermax:v1.5.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.5.0.000R dellemc/csi-powermax:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.6.0 dellemc/csi-powermax:v1.7.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.7.0 dellemc/csi-powerstore:v1.2.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.2.0.000R dellemc/csi-powerstore:v1.3.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.3.0 dellemc/csi-powerstore:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.4.0 dellemc/csi-unity:v1.4.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.4.0.000R dellemc/csi-unity:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.5.0 dellemc/csi-unity:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.6.0 dellemc/csi-vxflexos:v1.3.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.3.0.000R dellemc/csi-vxflexos:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.4.0 dellemc/csi-vxflexos:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.5.0 dellemc/dell-csi-operator:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/dell-csi-operator:v1.4.0 dellemc/sdc:3.5.1.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/sdc:3.5.1.1 dellemc/sdc:3.5.1.1-1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/sdc:3.5.1.1-1 docker.io/busybox:1.32.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/busybox:1.32.0 k8s.gcr.io/sig-storage/csi-attacher:v3.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.0.0 k8s.gcr.io/sig-storage/csi-attacher:v3.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.1.0 k8s.gcr.io/sig-storage/csi-attacher:v3.2.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.2.1 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.0.1 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.1.0 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.2.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.0.2 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.0.2 k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.1.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.2.1 k8s.gcr.io/sig-storage/csi-resizer:v1.2.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.2.0 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v3.0.2 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v3.0.3 k8s.gcr.io/sig-storage/csi-snapshotter:v4.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v4.0.0 k8s.gcr.io/sig-storage/csi-snapshotter:v4.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v4.1.0 quay.io/k8scsi/csi-resizer:v1.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.0.0 quay.io/k8scsi/csi-resizer:v1.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.1.0 * * Preparing operator files within /tmp/dell-csi-operator-bundle changing: dellemc/csi-isilon:v1.4.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.4.0.000R changing: dellemc/csi-isilon:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.5.0 changing: dellemc/csi-isilon:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.6.0 changing: dellemc/csipowermax-reverseproxy:v1.3.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csipowermax-reverseproxy:v1.3.0 changing: dellemc/csi-powermax:v1.5.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.5.0.000R changing: dellemc/csi-powermax:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.6.0 changing: dellemc/csi-powermax:v1.7.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.7.0 changing: dellemc/csi-powerstore:v1.2.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.2.0.000R changing: dellemc/csi-powerstore:v1.3.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.3.0 changing: dellemc/csi-powerstore:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.4.0 changing: dellemc/csi-unity:v1.4.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.4.0.000R changing: dellemc/csi-unity:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.5.0 changing: dellemc/csi-unity:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.6.0 changing: dellemc/csi-vxflexos:v1.3.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.3.0.000R changing: dellemc/csi-vxflexos:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.4.0 changing: dellemc/csi-vxflexos:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.5.0 changing: dellemc/dell-csi-operator:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/dell-csi-operator:v1.4.0 changing: dellemc/sdc:3.5.1.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/sdc:3.5.1.1 changing: dellemc/sdc:3.5.1.1-1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/sdc:3.5.1.1-1 changing: docker.io/busybox:1.32.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/busybox:1.32.0 changing: k8s.gcr.io/sig-storage/csi-attacher:v3.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.0.0 changing: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.1.0 changing: k8s.gcr.io/sig-storage/csi-attacher:v3.2.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.2.1 changing: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.0.1 changing: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.1.0 changing: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.2.0 changing: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.2 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.0.2 changing: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.1.0 changing: k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.2.1 changing: k8s.gcr.io/sig-storage/csi-resizer:v1.2.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.2.0 changing: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v3.0.2 changing: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v3.0.3 changing: k8s.gcr.io/sig-storage/csi-snapshotter:v4.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v4.0.0 changing: k8s.gcr.io/sig-storage/csi-snapshotter:v4.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v4.1.0 changing: quay.io/k8scsi/csi-resizer:v1.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.0.0 changing: quay.io/k8scsi/csi-resizer:v1.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.1.0 * * Complete Perform either a Helm installation or Operator installation Now that the required images are available and the Helm Charts/Operator configuration updated, you can proceed by following the usual installation procedure as documented either via Helm or Operator.\nNOTES:\n Offline bundle installation is only supported with manual installs i.e. without using Operator Lifecycle Manager. Installation should be done using the files that are obtained after unpacking the offline bundle (dell-csi-operator-bundle.tar.gz) as the image tags in the manifests are modified to point to the internal registry. Offline bundle installs operator in default namespace via install.sh script. Make sure that the current context in kubeconfig file has the namespace set to default.  ","excerpt":"The csi-offline-bundle.sh script can be used to create a package …","ref":"/csm-docs/docs/csidriver/installation/offline/","title":"Offline Installation of Dell EMC CSI Storage Providers"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell EMC CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\n PowerFlex PowerMax PowerScale PowerStore Unity  As well as the Dell CSI Operator\n Dell CSI Operator  Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\n One Linux-based system, with internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry  If one Linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry.    One of these will be required on both the system building the offline bundle as well as the system preparing for installation.    Tested version(s) are docker 19.03+ and podman 1.6.4+   git git will be used to manually clone one of the above repositories in order to create an offline bundle.    This is only needed on the system preparing the offline bundle.    Tested version(s) are git 1.8+ but any version should work.    Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\n Build an offline bundle Unpacking the offline bundle created in Step 1 and preparing for installation Perform either a Helm installation or Operator installation using the files obtained after unpacking in Step 2  Building an offline bundle This needs to be performed on a Linux system with access to the internet as a git repo will need to be cloned, and container images pulled from public registries.\nTo build an offline bundle, the following steps are needed:\n Perform a git clone of the desired repository. For a helm-based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle   For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory  The script will perform the following steps:\n Determine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator  The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\n[user@anothersystem /home/user]# git clone https://github.com/dell/dell-csi-operator.git [user@anothersystem /home/user]# cd dell-csi-operator [user@system /home/user/dell-csi-operator]# scripts/csi-offline-bundle.sh -c * * Building image manifest file * * Pulling container images dellemc/csi-isilon:v1.4.0.000R dellemc/csi-isilon:v1.5.0 dellemc/csi-isilon:v1.6.0 dellemc/csipowermax-reverseproxy:v1.3.0 dellemc/csi-powermax:v1.5.0.000R dellemc/csi-powermax:v1.6.0 dellemc/csi-powermax:v1.7.0 dellemc/csi-powerstore:v1.2.0.000R dellemc/csi-powerstore:v1.3.0 dellemc/csi-powerstore:v1.4.0 dellemc/csi-unity:v1.4.0.000R dellemc/csi-unity:v1.5.0 dellemc/csi-unity:v1.6.0 dellemc/csi-vxflexos:v1.3.0.000R dellemc/csi-vxflexos:v1.4.0 dellemc/csi-vxflexos:v1.5.0 dellemc/dell-csi-operator:v1.4.0 dellemc/sdc:3.5.1.1 dellemc/sdc:3.5.1.1-1 docker.io/busybox:1.32.0 k8s.gcr.io/sig-storage/csi-attacher:v3.0.0 k8s.gcr.io/sig-storage/csi-attacher:v3.1.0 k8s.gcr.io/sig-storage/csi-attacher:v3.2.1 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.1.0 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.0.2 k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1 k8s.gcr.io/sig-storage/csi-resizer:v1.2.0 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3 k8s.gcr.io/sig-storage/csi-snapshotter:v4.0.0 k8s.gcr.io/sig-storage/csi-snapshotter:v4.1.0 quay.io/k8scsi/csi-resizer:v1.0.0 quay.io/k8scsi/csi-resizer:v1.1.0 * * Saving images * * Copying necessary files /dell/git/dell-csi-operator/config /dell/git/dell-csi-operator/deploy /dell/git/dell-csi-operator/samples /dell/git/dell-csi-operator/scripts /dell/git/dell-csi-operator/README.md /dell/git/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md * * Complete Offline bundle file is: /dell/git/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking the offline bundle and preparing for installation This needs to be performed on a Linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for the driver or Operator installation, the following steps need to be performed:\n Copy the offline bundle file created from the previous step to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option  The script will then perform the following steps:\n Load the required container images into the local system Tag the images according to the user-supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images  An example of preparing the bundle for installation (192.168.75.40:5000 refers to an image registry accessible to Kubernetes/OpenShift):\n[user@anothersystem /tmp]# tar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md [user@anothersystem /tmp]# cd dell-csi-operator-bundle [user@anothersystem /tmp/dell-csi-operator-bundle]# scripts/csi-offline-bundle.sh -p -r 192.168.75.40:5000/operator Preparing an offline bundle for installation * * Loading docker images * * Tagging and pushing images dellemc/csi-isilon:v1.4.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.4.0.000R dellemc/csi-isilon:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.5.0 dellemc/csi-isilon:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.6.0 dellemc/csipowermax-reverseproxy:v1.3.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csipowermax-reverseproxy:v1.3.0 dellemc/csi-powermax:v1.5.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.5.0.000R dellemc/csi-powermax:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.6.0 dellemc/csi-powermax:v1.7.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.7.0 dellemc/csi-powerstore:v1.2.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.2.0.000R dellemc/csi-powerstore:v1.3.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.3.0 dellemc/csi-powerstore:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.4.0 dellemc/csi-unity:v1.4.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.4.0.000R dellemc/csi-unity:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.5.0 dellemc/csi-unity:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.6.0 dellemc/csi-vxflexos:v1.3.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.3.0.000R dellemc/csi-vxflexos:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.4.0 dellemc/csi-vxflexos:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.5.0 dellemc/dell-csi-operator:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/dell-csi-operator:v1.4.0 dellemc/sdc:3.5.1.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/sdc:3.5.1.1 dellemc/sdc:3.5.1.1-1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/sdc:3.5.1.1-1 docker.io/busybox:1.32.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/busybox:1.32.0 k8s.gcr.io/sig-storage/csi-attacher:v3.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.0.0 k8s.gcr.io/sig-storage/csi-attacher:v3.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.1.0 k8s.gcr.io/sig-storage/csi-attacher:v3.2.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.2.1 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.0.1 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.1.0 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.2.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.0.2 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.0.2 k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.1.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.2.1 k8s.gcr.io/sig-storage/csi-resizer:v1.2.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.2.0 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v3.0.2 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v3.0.3 k8s.gcr.io/sig-storage/csi-snapshotter:v4.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v4.0.0 k8s.gcr.io/sig-storage/csi-snapshotter:v4.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v4.1.0 quay.io/k8scsi/csi-resizer:v1.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.0.0 quay.io/k8scsi/csi-resizer:v1.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.1.0 * * Preparing operator files within /tmp/dell-csi-operator-bundle changing: dellemc/csi-isilon:v1.4.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.4.0.000R changing: dellemc/csi-isilon:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.5.0 changing: dellemc/csi-isilon:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.6.0 changing: dellemc/csipowermax-reverseproxy:v1.3.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csipowermax-reverseproxy:v1.3.0 changing: dellemc/csi-powermax:v1.5.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.5.0.000R changing: dellemc/csi-powermax:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.6.0 changing: dellemc/csi-powermax:v1.7.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.7.0 changing: dellemc/csi-powerstore:v1.2.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.2.0.000R changing: dellemc/csi-powerstore:v1.3.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.3.0 changing: dellemc/csi-powerstore:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.4.0 changing: dellemc/csi-unity:v1.4.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.4.0.000R changing: dellemc/csi-unity:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.5.0 changing: dellemc/csi-unity:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.6.0 changing: dellemc/csi-vxflexos:v1.3.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.3.0.000R changing: dellemc/csi-vxflexos:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.4.0 changing: dellemc/csi-vxflexos:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.5.0 changing: dellemc/dell-csi-operator:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/dell-csi-operator:v1.4.0 changing: dellemc/sdc:3.5.1.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/sdc:3.5.1.1 changing: dellemc/sdc:3.5.1.1-1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/sdc:3.5.1.1-1 changing: docker.io/busybox:1.32.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/busybox:1.32.0 changing: k8s.gcr.io/sig-storage/csi-attacher:v3.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.0.0 changing: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.1.0 changing: k8s.gcr.io/sig-storage/csi-attacher:v3.2.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.2.1 changing: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.0.1 changing: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.1.0 changing: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.2.0 changing: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.2 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.0.2 changing: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.1.0 changing: k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.2.1 changing: k8s.gcr.io/sig-storage/csi-resizer:v1.2.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.2.0 changing: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v3.0.2 changing: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v3.0.3 changing: k8s.gcr.io/sig-storage/csi-snapshotter:v4.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v4.0.0 changing: k8s.gcr.io/sig-storage/csi-snapshotter:v4.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v4.1.0 changing: quay.io/k8scsi/csi-resizer:v1.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.0.0 changing: quay.io/k8scsi/csi-resizer:v1.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.1.0 * * Complete Perform either a Helm installation or Operator installation Now that the required images are available and the Helm Charts/Operator configuration updated, you can proceed by following the usual installation procedure as documented either via Helm or Operator.\nNOTES:\n Offline bundle installation is only supported with manual installs i.e. without using Operator Lifecycle Manager. Installation should be done using the files that are obtained after unpacking the offline bundle (dell-csi-operator-bundle.tar.gz) as the image tags in the manifests are modified to point to the internal registry. Offline bundle installs operator in default namespace via install.sh script. Make sure that the current context in kubeconfig file has the namespace set to default.  ","excerpt":"The csi-offline-bundle.sh script can be used to create a package …","ref":"/csm-docs/v1/installation/offline/","title":"Offline Installation of Dell EMC CSI Storage Providers"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell EMC CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\n PowerFlex PowerMax PowerScale PowerStore Unity  As well as the Dell CSI Operator\n Dell CSI Operator  Dependencies Multiple linux based systems may be required to create and process an offline bundle for use.\n One linux based system, with internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One linux based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry  If one linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry.    One of these will be required on both the system building the offline bundle as well as the system preparing for installation.    Tested version(s) are docker 19.03+ and podman 1.6.4+   git git will be used to manually clone one of the above repos in order to create and offline bundle.    This is only needed on the system preparing the offline bundle.    Tested version(s) are git 1.8+ but any version should work.    Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\n Build an offline bundle Unpacking the offline bundle created in Step 1 and preparing for installation Perform either a Helm installation or Operator installation using the files obtained after unpacking in Step 2  Building an offline bundle This needs to be performed on a linux system with access to the internet as a git repo will need to be cloned, and container images pulled from public registries.\nTo build an offline bundle, the following steps are needed:\n Perform a git clone of the desired repository. For a helm based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle   For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory  The script will perform the following steps:\n Determine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator  The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\n[user@anothersystem /home/user]# git clone https://github.com/dell/dell-csi-operator.git [user@anothersystem /home/user]# cd dell-csi-operator [user@system /home/user/dell-csi-operator]# scripts/csi-offline-bundle.sh -c * * Building image manifest file * * Pulling container images dellemc/csi-isilon:v1.2.0 dellemc/csi-isilon:v1.3.0.000R dellemc/csipowermax-reverseproxy:v1.0.0.000R dellemc/csi-powermax:v1.2.0.000R dellemc/csi-powermax:v1.4.0.000R dellemc/csi-powerstore:v1.1.0.000R dellemc/csi-unity:v1.3.0.000R dellemc/csi-vxflexos:v1.1.5.000R dellemc/csi-vxflexos:v1.2.0.000R dellemc/dell-csi-operator:v1.1.0.000R quay.io/k8scsi/csi-attacher:v2.0.0 quay.io/k8scsi/csi-attacher:v2.2.0 quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 quay.io/k8scsi/csi-provisioner:v1.4.0 quay.io/k8scsi/csi-provisioner:v1.6.0 quay.io/k8scsi/csi-resizer:v0.5.0 quay.io/k8scsi/csi-snapshotter:v2.1.1 * * Saving images * * Copying necessary files /dell/git/dell-csi-operator/config /dell/git/dell-csi-operator/deploy /dell/git/dell-csi-operator/samples /dell/git/dell-csi-operator/scripts /dell/git/dell-csi-operator/README.md /dell/git/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md * * Complete Offline bundle file is: /dell/git/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking the offline bundle and preparing for installation This needs to be performed on a linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for driver or Operator installation, the following steps need to be performed:\n Copy the offline bundle file created from the previous step to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option  The script will then perform the following steps:\n Load the required container images into the local system Tag the images according to the user supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images  An example of preparing the bundle for installation (192.168.75.40:5000 refers to a image registry accessible to Kubernetes/OpenShift):\n[user@anothersystem /tmp]# tar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md [user@anothersystem /tmp]# cd dell-csi-operator-bundle [user@anothersystem /tmp/dell-csi-operator-bundle]# scripts/csi-offline-bundle.sh -p -r 192.168.75.40:5000/operator Preparing a offline bundle for installation * * Loading docker images * * Tagging and pushing images dellemc/csi-isilon:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.2.0 dellemc/csi-isilon:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.3.0.000R dellemc/csipowermax-reverseproxy:v1.0.0.000R -\u003e 192.168.75.40:5000/operator/csipowermax-reverseproxy:v1.0.0.000R dellemc/csi-powermax:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.2.0.000R dellemc/csi-powermax:v1.4.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.4.0.000R dellemc/csi-powerstore:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/csi-powerstore:v1.1.0.000R dellemc/csi-unity:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-unity:v1.3.0.000R dellemc/csi-vxflexos:v1.1.5.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.1.5.000R dellemc/csi-vxflexos:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.2.0.000R dellemc/dell-csi-operator:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/dell-csi-operator:v1.1.0.000R quay.io/k8scsi/csi-attacher:v2.0.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.0.0 quay.io/k8scsi/csi-attacher:v2.2.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.2.0 quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-node-driver-registrar:v1.2.0 quay.io/k8scsi/csi-provisioner:v1.4.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.4.0 quay.io/k8scsi/csi-provisioner:v1.6.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.6.0 quay.io/k8scsi/csi-resizer:v0.5.0 -\u003e 192.168.75.40:5000/operator/csi-resizer:v0.5.0 quay.io/k8scsi/csi-snapshotter:v2.1.1 -\u003e 192.168.75.40:5000/operator/csi-snapshotter:v2.1.1 * * Preparing operator files within /tmp/dell-csi-operator-bundle changing: dellemc/csi-isilon:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.2.0 changing: dellemc/csi-isilon:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.3.0.000R changing: dellemc/csipowermax-reverseproxy:v1.0.0.000R -\u003e 192.168.75.40:5000/operator/csipowermax-reverseproxy:v1.0.0.000R changing: dellemc/csi-powermax:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.2.0.000R changing: dellemc/csi-powermax:v1.4.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.4.0.000R changing: dellemc/csi-powerstore:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/csi-powerstore:v1.1.0.000R changing: dellemc/csi-unity:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-unity:v1.3.0.000R changing: dellemc/csi-vxflexos:v1.1.5.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.1.5.000R changing: dellemc/csi-vxflexos:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.2.0.000R changing: dellemc/dell-csi-operator:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/dell-csi-operator:v1.1.0.000R changing: quay.io/k8scsi/csi-attacher:v2.0.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.0.0 changing: quay.io/k8scsi/csi-attacher:v2.2.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.2.0 changing: quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-node-driver-registrar:v1.2.0 changing: quay.io/k8scsi/csi-provisioner:v1.4.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.4.0 changing: quay.io/k8scsi/csi-provisioner:v1.6.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.6.0 changing: quay.io/k8scsi/csi-resizer:v0.5.0 -\u003e 192.168.75.40:5000/operator/csi-resizer:v0.5.0 changing: quay.io/k8scsi/csi-snapshotter:v2.1.1 -\u003e 192.168.75.40:5000/operator/csi-snapshotter:v2.1.1 * * Complete Perform either a Helm installation or Operator installation Now that the required images are available and the Helm Charts/Operator configuration updated, you can proceed by following the usual installation procedure as documented either via Helm or Operator.\nNOTE: Installation should be done using the files that was obtained after unpacking the bundle as the image tags in the manifests are modifed to point to the internal registry.\n","excerpt":"The csi-offline-bundle.sh script can be used to create a package …","ref":"/csm-docs/v2/installation/offline/","title":"Offline Installation of Dell EMC CSI Storage Providers"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell EMC CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\n PowerFlex PowerMax PowerScale PowerStore Unity  As well as the Dell CSI Operator\n Dell CSI Operator  Dependencies Multiple linux based systems may be required to create and process an offline bundle for use.\n One linux based system, with internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One linux based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry  If one linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry.    One of these will be required on both the system building the offline bundle as well as the system preparing for installation.    Tested version(s) are docker 19.03+ and podman 1.6.4+   git git will be used to manually clone one of the above repos in order to create and offline bundle.    This is only needed on the system preparing the offline bundle.    Tested version(s) are git 1.8+ but any version should work.    Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\n Build an offline bundle Unpacking an offline bundle and preparing for installation Perform either a Helm installation or Operator installation  Building an offline bundle This needs to be performed on a linux system with access to the internet as a git repo will need to be cloned, and container images pulled from public registries.\nThe build an offline bundle, the following steps are needed:\n Perform a git clone of the desired repository. For a helm based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle   For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory  The script will perform the following steps:\n Determine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator  The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\n[user@anothersystem /home/user]# git clone https://github.com/dell/dell-csi-operator.git [user@anothersystem /home/user]# cd dell-csi-operator [user@system /home/user/dell-csi-operator]# scripts/csi-offline-bundle.sh -c * * Building image manifest file * * Pulling container images dellemc/csi-isilon:v1.2.0 dellemc/csi-isilon:v1.3.0.000R dellemc/csipowermax-reverseproxy:v1.0.0.000R dellemc/csi-powermax:v1.2.0.000R dellemc/csi-powermax:v1.4.0.000R dellemc/csi-powerstore:v1.1.0.000R dellemc/csi-unity:v1.3.0.000R dellemc/csi-vxflexos:v1.1.5.000R dellemc/csi-vxflexos:v1.2.0.000R dellemc/dell-csi-operator:v1.1.0.000R quay.io/k8scsi/csi-attacher:v2.0.0 quay.io/k8scsi/csi-attacher:v2.2.0 quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 quay.io/k8scsi/csi-provisioner:v1.4.0 quay.io/k8scsi/csi-provisioner:v1.6.0 quay.io/k8scsi/csi-resizer:v0.5.0 quay.io/k8scsi/csi-snapshotter:v2.1.1 * * Saving images * * Copying necessary files /dell/git/dell-csi-operator/config /dell/git/dell-csi-operator/deploy /dell/git/dell-csi-operator/samples /dell/git/dell-csi-operator/scripts /dell/git/dell-csi-operator/README.md /dell/git/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md * * Complete Offline bundle file is: /dell/git/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking an offline bundle and preparing for installation This needs to be performed on a linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for driver or Operator installation, the following steps need to be performed:\n Copy the offline bundle file to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option  The script will then perform the following steps:\n Load the required container images into the local system Tag the images according to the user supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images  An example of preparing the bundle for installation (192.168.75.40:5000 refers to a image registry accessible to Kubernetes/OpenShift):\n[user@anothersystem /tmp]# tar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md [user@anothersystem /tmp]# cd dell-csi-operator-bundle [user@anothersystem /tmp/dell-csi-operator-bundle]# scripts/csi-offline-bundle.sh -p -r 192.168.75.40:5000/operator Preparing a offline bundle for installation * * Loading docker images * * Tagging and pushing images dellemc/csi-isilon:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.2.0 dellemc/csi-isilon:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.3.0.000R dellemc/csipowermax-reverseproxy:v1.0.0.000R -\u003e 192.168.75.40:5000/operator/csipowermax-reverseproxy:v1.0.0.000R dellemc/csi-powermax:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.2.0.000R dellemc/csi-powermax:v1.4.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.4.0.000R dellemc/csi-powerstore:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/csi-powerstore:v1.1.0.000R dellemc/csi-unity:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-unity:v1.3.0.000R dellemc/csi-vxflexos:v1.1.5.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.1.5.000R dellemc/csi-vxflexos:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.2.0.000R dellemc/dell-csi-operator:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/dell-csi-operator:v1.1.0.000R quay.io/k8scsi/csi-attacher:v2.0.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.0.0 quay.io/k8scsi/csi-attacher:v2.2.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.2.0 quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-node-driver-registrar:v1.2.0 quay.io/k8scsi/csi-provisioner:v1.4.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.4.0 quay.io/k8scsi/csi-provisioner:v1.6.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.6.0 quay.io/k8scsi/csi-resizer:v0.5.0 -\u003e 192.168.75.40:5000/operator/csi-resizer:v0.5.0 quay.io/k8scsi/csi-snapshotter:v2.1.1 -\u003e 192.168.75.40:5000/operator/csi-snapshotter:v2.1.1 * * Preparing operator files within /tmp/dell-csi-operator-bundle changing: dellemc/csi-isilon:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.2.0 changing: dellemc/csi-isilon:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-isilon:v1.3.0.000R changing: dellemc/csipowermax-reverseproxy:v1.0.0.000R -\u003e 192.168.75.40:5000/operator/csipowermax-reverseproxy:v1.0.0.000R changing: dellemc/csi-powermax:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.2.0.000R changing: dellemc/csi-powermax:v1.4.0.000R -\u003e 192.168.75.40:5000/operator/csi-powermax:v1.4.0.000R changing: dellemc/csi-powerstore:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/csi-powerstore:v1.1.0.000R changing: dellemc/csi-unity:v1.3.0.000R -\u003e 192.168.75.40:5000/operator/csi-unity:v1.3.0.000R changing: dellemc/csi-vxflexos:v1.1.5.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.1.5.000R changing: dellemc/csi-vxflexos:v1.2.0.000R -\u003e 192.168.75.40:5000/operator/csi-vxflexos:v1.2.0.000R changing: dellemc/dell-csi-operator:v1.1.0.000R -\u003e 192.168.75.40:5000/operator/dell-csi-operator:v1.1.0.000R changing: quay.io/k8scsi/csi-attacher:v2.0.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.0.0 changing: quay.io/k8scsi/csi-attacher:v2.2.0 -\u003e 192.168.75.40:5000/operator/csi-attacher:v2.2.0 changing: quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 -\u003e 192.168.75.40:5000/operator/csi-node-driver-registrar:v1.2.0 changing: quay.io/k8scsi/csi-provisioner:v1.4.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.4.0 changing: quay.io/k8scsi/csi-provisioner:v1.6.0 -\u003e 192.168.75.40:5000/operator/csi-provisioner:v1.6.0 changing: quay.io/k8scsi/csi-resizer:v0.5.0 -\u003e 192.168.75.40:5000/operator/csi-resizer:v0.5.0 changing: quay.io/k8scsi/csi-snapshotter:v2.1.1 -\u003e 192.168.75.40:5000/operator/csi-snapshotter:v2.1.1 * * Complete Perform either a Helm installation or Operator installation Now that the required images have been made available and the Helm Charts/Operator configuration updated, installation can proceed by following the usual installation procedure as documented.\n","excerpt":"The csi-offline-bundle.sh script can be used to create a package …","ref":"/csm-docs/v3/installation/offline/","title":"Offline Installation of Dell EMC CSI Storage Providers"},{"body":"Release Notes - Dell CSI Operator 1.5.0  Note: There will be a delay in certification of Dell CSI Operator 1.5.0 and it will not be available for download from the Red Hat OpenShift certified catalog right away. The operator will still be available for download from the Red Hat OpenShift Community Catalog soon after the 1.5.0 release.\n New Features/Changes  Added support for Kubernetes v1.22 Added support for OpenShift v4.8.  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     A warning message will be listed in the events for cluster scoped objects if the driver is not upgraded after an operator upgrade. This happens because of the fix provided by Kubernetes in 1.20 for one of the known issue. After an operator upgrade, the objects will get updated automatically after 45 mins in case of no driver upgrade.    Support The Dell CSI Operator image is available on Dockerhub and is officially supported by Dell EMC. For any CSI operator and driver issues, questions or feedback, please follow our support process.\n","excerpt":"Release Notes - Dell CSI Operator 1.5.0  Note: There will be a delay …","ref":"/csm-docs/docs/csidriver/release/operator/","title":"Operator"},{"body":"Release Notes - Dell CSI Operator 1.4.0  Note: There will be a delay in certification of Dell CSI Operator 1.4.0 and it will not be available for download from the Red Hat OpenShift certified catalog. The operator will still be available for download from the Red Hat OpenShift Community Catalog soon after the 1.4.0 release.\n New Features/Changes  Added support for Kubernetes v1.21 Deprecated support for Kubernetes v1.18 Migrated to Operator SDK v1.5.0 Deprecated Storage Class Creation and Support Deprecated Volume Snapshot Class Creation and Support  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     A warning message will be listed in the events for cluster scoped objects if the driver is not upgraded after an operator upgrade. This happens because of the fix provided by Kubernetes in 1.20 for one of the known issue. After an operator upgrade, the objects will get updated automatically after 45 mins in case of no driver upgrade.    Support The Dell CSI Operator image is available on Dockerhub and is officially supported by Dell EMC. For any CSI operator and driver issues, questions or feedback, join the Dell EMC Container community.\n","excerpt":"Release Notes - Dell CSI Operator 1.4.0  Note: There will be a delay …","ref":"/csm-docs/v1/release/operator/","title":"Operator"},{"body":"Release Notes - Dell CSI Operator 1.3.0  Note: There will be a delay in certification of Dell CSI Operator 1.3.0 and it will not be available for download from the Red Hat OpenShift certified catalog. The operator will still be available for download from the Red Hat OpenShift Community Catalog soon after the 1.3.0 release.\n New Features/Changes  Added support for OpenShift 4.6, 4.7 with RHEL and CoreOS worker nodes Added support for Upstream Kubernetes cluster v1.18, v1.19, v1.20 Migrated to Operator SDK 1.0 Added support for CSI Ephemeral Inline Volumes Changed driver controller installation from StatefulSet to Deployment Added Support for multiple replicas for driver controller Deployment Added Support for setting volumeBindingMode for Storage Classes Added Support for setting topology keys for Storage Classes  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     A warning message will be listed in the events for StorageClasses if the driver is not upgraded after an operator upgrade. This happens because of the fix provided by Kubernetes in 1.20 for one of the known issue. StorageClasses will get updated automatically after 45 mins if there is no driver upgrade, after an operator upgrade.    Support The Dell CSI Operator image is available on Dockerhub and is officially supported by Dell EMC. For any CSI operator and driver issues, questions or feedback, join the Dell EMC Container community.\n","excerpt":"Release Notes - Dell CSI Operator 1.3.0  Note: There will be a delay …","ref":"/csm-docs/v2/release/operator/","title":"Operator"},{"body":"Release Notes - Dell CSI Operator 1.2.0  Note: There is a delay in Operator 1.2.0 certification hence it will not be visible in Red Hat OpenShift certified catalogue immediately after release on GitHub.\n New Features/Changes  Added support for OpenShift 4.5, 4.6 with RHEL and CoreOS worker nodes Migrated to Operator SDK 1.0 Added support for CSI Ephemeral Inline Volumes Changed driver controller installation from StatefulSet to Deployment Added Support for multiple replicas for driver controller Deployment Added Support for setting volumeBindingMode for Storage Classes Added Support for setting topology keys for Storage Classes  Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no Known issues in this release.\nSupport The Dell CSI Operator image is available on Dockerhub and is officially supported by Dell EMC. For any CSI operator and driver issues, questions or feedback, join the Dell EMC Container community.\n","excerpt":"Release Notes - Dell CSI Operator 1.2.0  Note: There is a delay in …","ref":"/csm-docs/v3/release/operator/","title":"Operator"},{"body":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell EMC PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell EMC PowerFlex Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements that must be met before installing the CSI Driver for Dell EMC PowerFlex:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Enable Zero Padding on PowerFlex Mount propagation is enabled on container runtime that is being used Install PowerFlex Storage Data Client If using Snapshot feature, satisfy all Volume Snapshot requirements A user must exist on the array with a role \u003e= FrontEndConfigure  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nEnable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. For more information to configure this setting, see Dell EMC PowerFlex documentation.\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run the node portion of the CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions.\nOptional: For a typical install, you will pull SDC kernel modules from the Dell EMC FTP site, which is set up by default. Some users might want to mirror this repository to a local location. The PowerFlex KB article has instructions on how to do this.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  (Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\ncontroller:snapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here: v4.2.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v4.2.x\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n When using Kubernetes 1.20/1.21/1.22 it is recommended to use 4.2.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone -b v2.0.0 https://github.com/dell/csi-powerflex.git to clone the git repository.\n  Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace vxflexos to create a new one.\n  Check helm/csi-vxflexos/driver-image.yaml and confirm the driver image points to a new image.\n  Collect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the top-level helm directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the value for these parameters as they must be entered in the config.yaml file in the samples directory.\n  Prepare the samples/config.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing PowerFlex system. true -   password Password for accessing PowerFlex system. true -   systemID System name/ID of PowerFlex system. true -   allSystemNames List of previous names of powerflex array if used for PV create false -   endpoint REST API gateway HTTPS endpoint for PowerFlex system. true -   skipCertificateValidation Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. true true   isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list. false false   mdm mdm defines the MDM(s) that SDC should register with on start. This should be a list of MDM IP addresses or hostnames separated by comma. true -    Example: samples/config.yaml\n# Username for accessing PowerFlex system.\t- username:\"admin\"# Password for accessing PowerFlex system.\tpassword:\"password\"# System name/ID of PowerFlex system.\tsystemID:\"ID1\"# Previous names of PowerFlex system if used for PV.allSystemNames:\"pflex-1,pflex-2\"# REST API gateway HTTPS endpoint for PowerFlex system.endpoint:\"https://127.0.0.1\"# Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface.# Allowed values: true or false# Default value: trueskipCertificateValidation:true# indicates if this array is the default array# needed for backwards compatibility# only one array is allowed to have this set to true # Default value: falseisDefault:true# defines the MDM(s) that SDC should register with on start.# Allowed values: a list of IP addresses or hostnames separated by comma.# Default value: none mdm:\"10.0.0.1,10.0.0.2\"- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"After editing the file, run the following command to create a secret called vxflexos-config:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/config.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/config.yaml -o yaml --dry-run=client | kubectl replace -f -\nNOTE:\n The user needs to validate the YAML syntax and array-related key/values while replacing the vxflexos-creds secret. If you want to create a new array or update the MDM values in the secret, you will need to reinstall the driver. If you change other details, such as login information, the secret will dynamically update – see dynamic-array-configuration for more details. Old json format of the array configuration file is still supported in this release. If you already have your configuration in json format, you may continue to maintain it or you may transfer this configuration to yaml format and replace/update the secret. “insecure” parameter has been changed to “skipCertificateValidation” as insecure is deprecated and will be removed from use in config.yaml or secret.yaml in a future release. Users can continue to use any one of “insecure” or “skipCertificateValidation” for now. The driver would return an error if both parameters are used. Please note that log configuration parameters from v1.5 will no longer work in v2.0. Please refer to the Dynamic Logging Configuration section in Features for more information.    Default logging options are set during helm install. To see possible configuration options, see the Dynamic Logging Configuration section in Features.\n  If using automated SDC deployment:\n Check the SDC container image is the correct version for your version of PowerFlex.    Copy the default values.yaml file cd helm \u0026\u0026 cp csi-vxflexos/values.yaml myvalues.yaml\n  Edit the newly created values file and provide values for the following parameters vi myvalues.yaml:\n     Parameter Description Required Default     version Set to verify the values file version matches driver version. Yes 2.0.0   certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. No 0   logLevel CSI driver log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes “debug”   logFormat CSI driver log format. Allowed values: “TEXT” or “JSON”. Yes “TEXT”   kubeletConfigDir kubelet config directory path. Ensure that the config.yaml file is present at this path. Yes /var/lib/kubelet   defaultFsType Used to set the default FS type which will be used for mount volumes if FsType is not specified in the storage class. Allowed values: ext4, xfs. Yes ext4   imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Allowed values: Always, IfNotPresent, Never. Yes IfNotPresent   enablesnapshotcgdelete A boolean that, when enabled, will delete all snapshots in a consistency group everytime a snap in the group is deleted. Yes false   enablelistvolumesnapshot A boolean that, when enabled, will allow list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap). It is recommend this be false unless instructed otherwise. Yes false   allowRWOMultiPodAccess Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. Yes false   controller This section allows the configuration of controller-specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - -   volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. Yes “k8s”   controllerCount Set to deploy multiple controller instances. If the controller count is greater than the number of available nodes, excess pods remain in a pending state. It should be greater than 0. You can increase the number of available nodes by configuring the “controller” section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2   snapshot.enabled A boolean that enable/disable volume snapshot feature. No true   resizer.enabled A boolean that enable/disable volume expansion feature. No true   nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. Yes \" \"   tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install the controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. Yes \" \"   node This section allows the configuration of node-specific parameters. - -   nodeSelector Defines what nodes would be selected for pods of node daemonset. Leave as blank to use all nodes. Yes \" \"   tolerations Defines tolerations that would be applied to node daemonset. Leave as blank to install node driver only on worker nodes. Yes \" \"   monitor This section allows the configuration of the SDC monitoring pod. - -   enabled Set to enable the usage of the monitoring pod. Yes false   hostNetwork Set whether the monitor pod should run on the host network or not. Yes true   hostPID Set whether the monitor pod should run in the host namespace or not. Yes true   vgsnapshotter This section allows the configuration of the volume group snapshotter(vgsnapshotter) pod. - -   enabled A boolean that enable/disable vg snapshotter feature. No false   image Image for vg snapshotter. No \" \"   podmon Podmon is an optional feature under development and tech preview. Enable this feature only after contact support for additional information. - -   enabled A boolean that enable/disable podmon feature. No false   image image for podmon. No \" \"    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml  NOTE:\n  For detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder.\n  Install script will validate MDM IP(s) in vxflexos-config secret and creates a new field consumed by the init container and sdc-monitor container\n  This install script also runs the verify.sh script. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes.\n  It is mandatory to run install script after changes to MDM configuration in vxflexos-config secret. Refer dynamic-array-configuration\n  (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.\n Mount options are specified in storageclass yaml under mkfsFormatOption. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Certificate validation for PowerFlex Gateway REST API calls This topic provides details about setting up the certificate for the CSI Driver for Dell EMC PowerFlex.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name vxflexos-certs-0 to vxflexos-certs-n based on the “.Values.certSecretCount” parameter present in the namespace vxflexos.\nThis secret contains the X509 certificates of the CA which signed PowerFlex gateway SSL certificate in PEM format.\nThe CSI driver exposes an install parameter in config.yaml, skipCertificateValidation, which determines if the driver performs client-side verification of the gateway certificates.\nskipCertificateValidation parameter is set to true by default, and the driver does not verify the gateway certificates.\nIf skipCertificateValidation is set to false, then the secret vxflexos-certs-n must contain the CA certificate for the array gateway.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the gateway certificate is self-signed or if you are using an embedded gateway, then perform the following steps.\n  To fetch the certificate, run the following command.\n `openssl s_client -showcerts -connect \u003cGateway IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem`  Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem\n  Run the following command to create the cert secret with index ‘0’:\n `kubectl create secret generic vxflexos-certs-0 --from-file=cert-0=ca_cert_0.pem -n vxflexos`  Use the following command to replace the secret:\n `kubectl create secret generic vxflexos-certs-0 -n vxflexos --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -`    Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: vxflexos-certs-1, vxflexos-certs-2, etc)\n  Note:\n “vxflexos” is the namespace for helm-based installation but namespace can be user-defined in operator-based installation. User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation. Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver. Updating vxflexos-certs-n secrets is a manual process, unlike vxflexos-config. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.  Storage Classes For CSI driver for PowerFlex version 1.4 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class: There are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\n Edit storageclass.yaml if you need ext4 filesystem and storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have. Replace \u003cSYSTEM_ID\u003e with the system ID you have. Note there are two appearances in the file. Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f storageclass.yaml or kubectl create -f storageclass-xfs.yaml  NOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerFlex v1.5, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nNOTE Support for v1beta1 snapshots is being discontinued in this release.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerFlex v1.5 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerFlex to 1.5 before upgrading to 2.0.\n","excerpt":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the …","ref":"/csm-docs/docs/csidriver/installation/helm/powerflex/","title":"PowerFlex"},{"body":"Installing CSI Driver for PowerFlex via Operator The CSI Driver for Dell EMC PowerFlex can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: SDC Deployment for Operator  This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non-supported versions of the OS also do the manual SDC deployment steps given below. Refer to https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When the driver is created, MDM value for initContainers in driver CR is set by the operator from mdm attributes in the driver configuration file, config.yaml. An example of config.yaml is below in this document. Do not set MDM value for initContainers in the driver CR file manually. Note: To use an sdc-binary module from customer ftp site:  Create a secret, sdc-repo-secret.yaml to contain the credentials for the private repo. To generate the base64 encoding of a credential:    echo-n\u003ccredential\u003e|base64-isecret sample to use:\napiVersion:v1kind:Secretmetadata:name:sdc-repo-credsnamespace:vxflexostype:Opaquedata:# set username to the base64 encoded username, sdc default isusername:\u003cusernameinbase64\u003e # set password to the base64 encoded password, sdc default ispassword:\u003cpasswordinbase64\u003e Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml. Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml.  Example CR: config/samples/vxflex_v210_ops_48.yaml sideCars:# Comment the following section if you don't want to run the monitoring sidecar- name:sdc-monitorenvs:- name:HOST_PIDvalue:\"1\"- name:MDMvalue:\"\"- name:external-health-monitorargs:[\"--monitor-interval=60s\"]initContainers:- image:dellemc/sdc:3.6imagePullPolicy:IfNotPresentname:sdcenvs:- name:MDMvalue:\"10.x.x.x,10.x.x.x\"Note: Please comment the sdc-monitor sidecar section if you are not using it. Blank values for MDM will result in error. Do not comment the external-health-monitor argument.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  Install Driver   Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e command using the desired name to create the namespace.\n  Prepare the config.yaml for driver configuration.\nExample: config.yaml\n# Username for accessing PowerFlex system.\t# Required: true- username:\"admin\"# Password for accessing PowerFlex system.\t# Required: truepassword:\"password\"# System name/ID of PowerFlex system.\t# Required: truesystemID:\"ID1\"# REST API gateway HTTPS endpoint for PowerFlex system.# Required: trueendpoint:\"https://127.0.0.1\"# Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface.# Allowed values: true or false# Required: true# Default value: trueskipCertificateValidation:true# indicates if this array is the default array# needed for backwards compatibility# only one array is allowed to have this set to true # Required: false# Default value: falseisDefault:true# defines the MDM(s) that SDC should register with on start.# Allowed values: a list of IP addresses or hostnames separated by comma.# Required: true# Default value: none mdm:\"10.0.0.1,10.0.0.2\"# Defines all system names used to create powerflex volumes# Required: false# Default value: noneAllSystemNames:\"name1,name2\"- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"AllSystemNames:\"name1,name2\"After editing the file, run the following command to create a secret called vxflexos-config kubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.yaml -o yaml --dry-run=client | kubectl replace -f -\nNote:\n System ID, MDM configuration, etc. now are taken directly from config.yaml. MDM provided in the input_sample_file.yaml will be overidden with MDM values in config.yaml.    Create a Custom Resource (CR) for PowerFlex using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\n   Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, excess pods will become stay in a pending state. Defaults are 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false   X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false   X_CSI_DEBUG To enable debug mode No true   X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false      Execute the kubectl create -f \u003cinput_sample_file.yaml\u003e command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.\n Example CR for PowerFlex Driver apiVersion:storage.dell.com/v1kind:CSIVXFlexOSmetadata:name:test-vxflexosnamespace:test-vxflexosspec:driver:configVersion:v6replicas:1dnsPolicy:ClusterFirstWithHostNetforceUpdate:falsecommon:image:\"dellemc/csi-vxflexos:v2.0.0\"imagePullPolicy:IfNotPresentenvs:- name:X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOTvalue:\"false\"- name:X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETEvalue:\"false\"- name:X_CSI_DEBUGvalue:\"true\"- name:X_CSI_ALLOW_RWO_MULTI_POD_ACCESSvalue:\"false\"sideCars:# Uncomment the following section if you want to run the monitoring sidecar# - name: sdc-monitor# envs:# - name: HOST_PID# value: \"1\"# - name: MDM# value: \"\"- name:external-health-monitorargs:[\"--monitor-interval=60s\"]initContainers:- image:dellemc/sdc:3.6imagePullPolicy:IfNotPresentname:sdcenvs:- name:MDMvalue:\"10.x.x.x,10.x.x.x\"#provide MDM value---apiVersion:v1kind:ConfigMapmetadata:name:vxflexos-config-paramsnamespace:test-vxflexosdata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"debug\"CSI_LOG_FORMAT:\"TEXT\"    Pre-Requisite for installation with OLM  Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM. 1. git clone https://github.com/dell/dell-csi-operator.git 2. cd dell-csi-operator 3. tar -czf config.tar.gz driverconfig/ # Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM 4. kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u003coperator-namespace\u003e  ","excerpt":"Installing CSI Driver for PowerFlex via Operator The CSI Driver for …","ref":"/csm-docs/docs/csidriver/installation/operator/powerflex/","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\n Navigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following:  Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \"docker.io/centos:latest\" Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured.  Results\nAn outline of this workflow is described below:\n The 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below:  kind:PersistentVolumeClaimapiVersion:v1metadata:name:pvolnamespace:helmtest-vxflexosspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GistorageClassName:vxflexosThe volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use a storage class named vxflexos. This step yields a mounted ext4 file system. You can create the vxflexos and vxflexos-xfs storage classes by using the yamls located in samples/storageclass. If you compare pvol0.yaml and pvol1.yaml, you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos.   NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.\n Test creating snapshots Test the workflow for snapshot creation.\n NOTE: Starting with version 2.0, CSI Driver for PowerFlex helm tests are designed to work exclusively with v1 snapshots.\n Steps\n Start the 2vols container and leave it running.  Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly.   Run sh snaptest.sh to start the test.  This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in snap1.yaml and snap2.yaml. The following are the contents of snap1.yaml:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1namespace:helmtest-vxflexosspec:volumeSnapshotClassName:vxflexos-snapclasssource:persistentVolumeClaimName:pvol0Results\nThe snaptest.sh script will create a snapshot using the definitions in the snap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0, then the created snapshot is named pvol0-snap1.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n helmtest-vxflexos.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell EMC PowerFlex installation does not create this class. You will need to create instance of VolumeSnapshotClass from one of default samples in `samples/volumesnapshotclass’ directory.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\n Run sh snaprestoretest.sh to start the test.  This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updated directory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\n Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly.  Results\nAn outline of this workflow is described below:\n The snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim, which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below:  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:helmtest-vxflexosspec:storageClassName:vxflexosdataSource:name:pvol0-snap1kind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiNOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in snap1.yaml.\n","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/docs/csidriver/installation/test/powerflex/","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v2.0.0 New Features/Changes  Added support for Kubernetes v1.22. Added support for OpenShift v4.8. Added support for dynamic multi-array configuration. Added certificate validation for PowerFlex Gateway REST API calls. Aligned configuration parameters with those in other CSI drivers in effort to have consistent config experience across drivers. Added the ability to configure kubelet directory path. Added the ability to enable/disable installation of resizer sidecar with driver installation. Added the ability to enable/disable installation of snapshotter sidecar with driver installation. Added the ability to use system names instead of system IDs in config files.  Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\n","excerpt":"Release Notes - CSI PowerFlex v2.0.0 New Features/Changes  Added …","ref":"/csm-docs/docs/csidriver/release/powerflex/","title":"PowerFlex"},{"body":"   Symptoms Prevention, Resolution or Workaround     The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that need to pull an image of the driver.   The standalone Helm chart installation fails with Error: couldn't find key MDM in Secret vxflexos/vxflexos-config Make sure that you have ssh keys set up between the master and worker nodes.   When you run the command kubectl describe pods vxflexos-controller-* –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n vxflexos vxflexos-controller-* driver logs show that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system.   The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script.   CreateVolume error System  is not configured in the driver Powerflex name if used for systemID in StorageClass ensure same name is also used in array config systemID   Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on a worker node, and ensure your container run time manager is properly configured to be utilized with SElinux.   Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux.   Installation of the driver on Kubernetes v1.20/v1.21/v1.22 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.20/1.21/v1.22 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements   The kubectl logs -n vxflexos vxflexos-controller-* driver logs show x509: certificate signed by unknown authority A self assigned certificate is used for PowerFlex array. See certificate validation for PowerFlex Gateway   When you run the command kubectl apply -f snapclass-v1.yaml, you get the error error: unable to recognize \"snapclass-v1.yaml\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Check to make sure that the v1 snapshotter CRDs are installed, and not the v1beta1 CRDs, which are no longer supported.   The controller pod is stuck and producing errors such as” Failed to watch *v1.VolumeSnapshotContent: failed to list *v1.VolumeSnapshotContent: the server could not find the requested resource (get volumesnapshotcontents.snapshot.storage.k8s.io) Make sure that v1 snapshotter CRDs and v1 snapclass are installed, and not v1beta1, which is no longer supported.     Note: vxflexos-controller-* is the controller pod that acquires leader lease\n ","excerpt":"   Symptoms Prevention, Resolution or Workaround     The installation …","ref":"/csm-docs/docs/csidriver/troubleshooting/powerflex/","title":"PowerFlex"},{"body":"Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes 1.17 and is generally available (v1) in Kubernetes version \u003e=1.20.\nThe CSI PowerFlex driver version 1.5 supports v1beta1 snapshots on Kubernetes 1.19 and v1 snapshots on Kubernetes 1.20 and 1.21.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  Volume Snapshot Class Before PowerFlex driver v1.5, the installation of the driver created default instance of VolumeSnapshotClass. API version of this VolumeSnapshotClass instance was defined based on Kubernetes version, as below:\nFollowing is the manifest for the Volume Snapshot Class created during installation, prior to PowerFlex driver v1.5:\n{{- if eq .Values.kubeversion \"v1.20\" }} apiVersion: snapshot.storage.k8s.io/v1 {{- else }} apiVersion: snapshot.storage.k8s.io/v1beta1 {{- end}} kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com deletionPolicy: Delete Installation of PowerFlex driver v1.5 does not create VolumeSnapshotClass. You can find samples of default v1beta1 and v1 VolumeSnapshotClass instances in helm/samples/volumesnapshotclass directory. There are two samples, one for v1beta1 version and the other for v1 snapshot version. If needed, install appropriate default sample, based on the version of snapshot CRDs in your cluster.\nCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Create Consistent Snapshot of Group of Volumes This feature extends CSI specification to add the capability to create crash-consistent snapshots of a group of volumes. PowerFlex driver implements this extension in v1.5. This feature is currently in Technical Preview. To use this feature users have to deploy csi-volumegroupsnapshotter side-car as part of the PowerFlex driver.\nMore details can be found here: dell-csi-volumegroup-snapshotter.\nVolume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, which is when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true.\nFollowing is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound  NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the next highest multiple of 8.\n Volume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nCustom File System Format Options The CSI PowerFlex driver version 1.5 supports additional mkfs format options. A user is able to specify additional format options as needed for the driver. Format options are specified in storageclass yaml under mkfsFormatOption as in the following example:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \u003cSTORAGE_POOL\u003e # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID mkfsFormatOption: \"\u003cmkfs_format_option\u003e\" # Insert file system format option volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e # Insert System ID values: - csi-vxflexos.dellemc.com  WARNING: Before utilizing format options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.  Topology Support The CSI PowerFlex driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\n The PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed.  This Topology support does not include customer-defined topology, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that the pod schedule takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.beta.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2020-05-27T13:24:55Z\" labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \"170198\" selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\n NOTE: In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\n Controller HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\n NOTE: If the controller count is greater than the number of available nodes, excess controller pods will be stuck in a pending state.\n If you are using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file (helm install):\nTo assign controller pods to worker nodes only (Default):\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install the controller on worker nodes tolerations: # - key: \"node-role.kubernetes.io/master\" # operator: \"Exists\" # effect: \"NoSchedule\" To assign controller pods to master and worker nodes:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install the controller on worker nodes tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" To assign controller pods to master nodes only:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" For configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nSDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Kubernetes nodes which run the node portion of the CSI driver. The deployment of the SDC kernel module occurs on these nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\n On Kubernetes nodes which run the node portion of the CSI driver, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the nodes with OS version which supports automatic SDC deployment. If there is an SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On nodes that do not support automatic SDC deployment by SDC init container, manual installation steps must be followed. The SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions. There is no automated uninstallation of the SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from the node.  Multiarray Support The CSI PowerFlex driver version 1.5 adds support for managing multiple PowerFlex arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration There is a sample yaml file under the top directory named config.yaml with the following content:\n- username:\"admin\"# username for connecting to APIpassword:\"password\"# password for connecting to APIsystemID:\"ID1\"# system ID for systemendpoint:\"https://127.0.0.1\"# full URL path to the PowerFlex APIskipCertificateValidation:true# skip array certificate validation or notisDefault:true# treat current array as default (would be used by storage class without arrayIP parameter)mdm:\"10.0.0.1,10.0.0.2\"# MDM IPs for the system- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"Here we specify that we want the CSI driver to manage two arrays: one with an IP 127.0.0.1 and the other with an IP 127.0.0.2.\nTo use this config we need to create a Kubernetes secret from it. To do so, run the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml\nCreating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nFind the sample yaml files under helm/samples/storageclass. Edit storageclass.yaml if you want ext4 filesystem, and use storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have, and replace \u003cSYSTEM_ID\u003e with the system ID you have.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use the storage class for the corresponding array.\nEphemeral Inline Volume Starting from version 1.4, CSI PowerFlex driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest (found in csi-vxflexos/test/helm/ephemeral) for creating ephemeral volume in pod manifest with CSI PowerFlex driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumesspec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data0\"name:my-csi-volume- mountPath:\"/data1\"name:my-csi-volume-xfsvolumes:- name:my-csi-volumecsi:driver:csi-vxflexos.dellemc.comfsType:\"ext4\"volumeAttributes:volumeName:\"my-csi-volume\"size:\"8Gi\"storagepool:samplesystemID:sample- name:my-csi-volume-xfscsi:driver:csi-vxflexos.dellemc.comfsType:\"xfs\"volumeAttributes:volumeName:\"my-csi-volume-xfs\"size:\"10Gi\"storagepool:samplesystemID:sampleThis manifest creates a pod and attach two newly created ephemeral inline csi volumes to it, one ext4 and the other xfs.\nTo run the corresponding helm test, go to csi-vxflexos/test/helm/ephemeral and fill in the values for storagepool and systemID in sample.yaml.\nThen run:\n./testEphemeral.sh this test deploys the pod with two ephemeral volumes, and write some data to them before deleting the pod.\nWhen creating ephemeral volumes, it is important to specify the following within the volumeAttributes section: volumeName, size, storagepool, and if you want to use a non-default array, systemID.\nDynamic Logging Configuration This feature is introduced in version 1.5, CSI Driver for PowerFlex now supports dynamic logging configuration.\nTo accomplish this, we utilize two fields in logConfig.yaml: LOG_LEVEL and LOG_FORMAT.\nLOG_LEVEL: minimum level that the driver will log LOG_FORMAT: format the driver should log in, either text or JSON  NOTE: To see the available options for LOG_LEVEL, consult: https://github.com/sirupsen/logrus#level-logging\n If the configmap does not exist yet, simply edit logConfig.yaml, changing the values for LOG_LEVEL and LOG_FORMAT as you see fit.\nIf the configmap already exists, you can use this command to edit the configmap:\nkubectl edit configmap -n vxflexos driver-config\nor you could edit logConfig.yaml, and use this command:\nkubectl apply -f logConfig.yaml\nand then make the necessary adjustments for LOG_LEVEL and LOG_FORMAT.\nIf LOG_LEVEL or LOG_FORMAT are set to options outside of what is supported, the driver will use the default values of “info” and “text” .\n","excerpt":"Volume Snapshot Feature The Volume Snapshot feature was introduced in …","ref":"/csm-docs/v1/features/powerflex/","title":"PowerFlex"},{"body":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell EMC PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell EMC PowerFlex Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements that must be met before installing the CSI Driver for Dell EMC PowerFlex:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Enable Zero Padding on PowerFlex Mount propagation is enabled on container runtime that is being used Install PowerFlex Storage Data Client If using Snapshot feature, satisfy all Volume Snapshot requirements A user must exist on the array with a role \u003e= FrontEndConfigure  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nEnable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. For more information to configure this setting, see Dell EMC PowerFlex documentation.\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run the node portion of the CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for supported OS versions.\nSDC Deployment The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run the node portion of the CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS).\nOn Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\nOptional: For a typical install, you will pull SDC kernel modules from the Dell EMC FTP site, which is set up by default. Some users might want to mirror this repository to a local location. The PowerFlex KB article (https://www.dell.com/support/kbdoc/en-us/000184206/how-to-use-a-private-repository-for) has instructions on how to do this.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  (Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20/1.21 (v1 snapshots) use v4.0.x  Volume Snapshot Controller The beta Volume Snapshots in Kubernetes version 1.17 and later, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20 and 1.21 (v1 snapshots) use v4.0.x  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.x quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 3.0.x version of snapshotter/snapshot-controller when using Kubernetes 1.19 When using Kubernetes 1.20/1.21 it is recommended to use 4.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository.\n  Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace vxflexos to create a new one.\n  Check helm/csi-vxflexos/driver-image.yaml and confirm the driver image points to a new image.\n  Collect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the top-level helm directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the value for these parameters as they must be entered in the config.yaml file in the top-level directory.\n  Prepare the config.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing PowerFlex system. true -   password Password for accessing PowerFlex system. true -   systemID System name/ID of PowerFlex system. true -   endpoint REST API gateway HTTPS endpoint for PowerFlex system. true -   skipCertificateValidation Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. true true   isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list. false false   mdm mdm defines the MDM(s) that SDC should register with on start. This should be a list of MDM IP addresses or hostnames separated by comma. true -    Example: config.yaml\n- username:\"admin\"password:\"password\"systemID:\"ID1\"endpoint:\"https://127.0.0.1\"skipCertificateValidation:trueisDefault:truemdm:\"10.0.0.1,10.0.0.2\"- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"After editing the file, run the following command to create a secret called vxflexos-config kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml -o yaml --dry-run=client | kubectl replace -f -\nNOTE:\n The user needs to validate the YAML syntax and array-related key/values while replacing the vxflexos-creds secret. If you update the secret, you will have to reinstall the driver. Old json format of the array configuration file is still supported in this release. If you already have your configuration in json format, you may continue to maintain it or you may transfer this configuration to yaml format and replace/update the secret.    Create the config map for use by the driver. The config map controls the level and format of the driver’s logging. To create it:\nkubectl create -f logConfig.yaml\nTo see possible configuration options, see the ‘Dynamic Logging Configuration” section in Features.\n  If using automated SDC deployment:\n Check the SDC container image is the correct version for your version of PowerFlex.    Copy the default values.yaml file cd helm \u0026\u0026 cp csi-vxflexos/values.yaml myvalues.yaml\n  Edit the newly created values file and provide values for the following parameters vi myvalues.yaml:\n     Parameter Description Required Default     volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. No “k8s”   controllerCount Set to deploy multiple controller instances. If the controller count is greater than the number of available nodes, excess pods remain in a pending state. You can increase the number of available nodes by configuring the “controller” section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2   enablelistvolumesnapshot Set to have snapshots included in the CSI operation ListVolumes. Disabled by default. No FALSE   allowRWOMultiPodAccess Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No FALSE   controller This section allows the configuration of controller-specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - -   nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. No \" \"   tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install the controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. No \" \"   monitor This section allows the configuration of the SDC monitoring pod. - -   enabled Set to enable the usage of the monitoring pod. No FALSE   hostNetwork Set whether the monitor pod should run on the host network or not. No TRUE   hostPID Set whether the monitor pod should run in the host namespace or not. No TRUE   podmon Podmon is an optional feature under development and tech preview. Enable this feature only after contact support for additional information. - -   enabled  No FALSE   vgsnapshotter Volume Group Snapshotter(vgsnapshotter) is an optional feature under development and tech preview. Enable this feature only after contact support for additional information. - -   enabled  No FALSE    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml  NOTE:\n For detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder. This script runs the verify-csi-vxflexos.sh script that is present in the same directory. It will validate MDM IP(s) in vxflexos-config secret and creates a new field consumed by the init container and sdc-monitor container This script also runs the verify.sh script. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes. It is mandatory to run the first installation and installation after changes to MDM configuration in vxflexos-config secret without skipping the verification. After that, you can use --skip-verify-node or --skip-verify . (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mkfsFormatOption. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes For CSI driver for PowerFlex version 1.4 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the helm/samples folder. Use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerFlex v1.4 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n NOTE: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class: There are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\n Edit storageclass.yaml if you need ext4 filesystem and storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have. Replace \u003cSYSTEM_ID\u003e with the system ID you have. Note there are two appearances in the file. Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f storageclass.yaml or kubectl create -f storageclass-xfs.yaml  NOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerFlex v1.5, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the helm/samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\n","excerpt":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the …","ref":"/csm-docs/v1/installation/helm/powerflex/","title":"PowerFlex"},{"body":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell EMC PowerFlex can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: SDC Deployment for Operator  This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non-supported versions of the OS also do the manual SDC deployment steps given below. Refer to https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When the driver is created, MDM value for initContainers in driver CR is set by the operator from mdm attributes in the driver configuration file, config.json. An example of config.json is below in this document. Do not set MDM value for initContainers in the driver CR file manually. Note: To use an sdc-binary module from customer ftp site:  Create a secret, sdc-repo-secret.yaml to contain the credentials for the private repo. To generate the base64 encoding of a credential:    echo-n\u003ccredential\u003e|base64-isecret sample to use:\napiVersion:v1kind:Secretmetadata:name:sdc-repo-credsnamespace:vxflexostype:Opaquedata:# set username to the base64 encoded username, sdc default isusername:\u003cusernameinbase64\u003e # set password to the base64 encoded password, sdc default ispassword:\u003cpasswordinbase64\u003e Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml. Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml.  Example CR: config/samples/vxflex_v140_ops_46.yaml sideCars:# Uncomment the following section if you want to run the monitoring sidecar- name:sdc-monitorenvs:- name:HOST_PIDvalue:\"1\"- name:MDMvalue:\"\"initContainers:- image:dellemc/sdc:3.5.1.1imagePullPolicy:IfNotPresentname:sdcenvs:- name:MDMvalue:\"\"Manual SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  Install Driver   Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e command using the desired name to create the namespace.\n  Prepare the config.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing PowerFlex system true -   password Password for accessing PowerFlex system true -   systemID System name/ID of PowerFlex system true -   endpoint REST API gateway HTTPS endpoint for PowerFlex system true -   insecure Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface true true   isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list false false   mdm mdm defines the MDM(s) that SDC should register with on start. This should be a list of MDM IP addresses or hostnames separated by comma true -    Example: config.json\n[ { \"username\": \"admin\", \"password\": \"password\", \"systemID\": \"ID1\", \"endpoint\": \"https://127.0.0.1\", \"insecure\": true, \"isDefault\": true, \"mdm\": \"10.0.0.1,10.0.0.2\" }, { \"username\": \"admin\", \"password\": \"password\", \"systemID\": \"ID2\", \"endpoint\": \"https://127.0.0.2\", \"insecure\": true, \"mdm\": \"10.0.0.3,10.0.0.4\" } ] After editing the file, run the following command to create a secret called vxflexos-config kubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.json\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.json -o yaml --dry-run=client | kubectl replace -f -\nNote:\n The user needs to validate the JSON syntax and array-related key/values while replacing the vxflexos-creds secret. If you update the secret, you must reinstall the driver. System ID, MDM configuration, etc. now are taken directly from config.json. MDM provided in the input_sample_file.yaml will be overidden with MDM values in config.json.    Create a Custom Resource (CR) for PowerFlex using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\n   Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, excess pods will become stay in a pending state. Defaults are 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false   X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false   X_CSI_DEBUG To enable debug mode No false   X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false      Execute the kubectl create -f \u003cinput_sample_file.yaml\u003e command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.\n  ","excerpt":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell …","ref":"/csm-docs/v1/installation/operator/powerflex/","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\n Navigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following:  Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \"docker.io/centos:latest\" Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured.  Results\nAn outline of this workflow is described below:\n The 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below:  kind:PersistentVolumeClaimapiVersion:v1metadata:name:pvolnamespace:helmtest-vxflexosspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GistorageClassName:vxflexosThe volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use a storage class named vxflexos. This step yields a mounted ext4 file system. You can create the vxflexos and vxflexos-xfs storage classes by using the yamls located in helm/samples/storageclass. If you compare pvol0.yaml and pvol1.yaml, you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos. NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.  Test creating snapshots Test the workflow for snapshot creation.\nSteps\n Start the 2vols container and leave it running.  Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update betasnap1.yaml and betasnap2.yaml accordingly.   Run sh snaptest.sh to start the test.  This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in betasnap1.yaml and betasnap2.yaml. The following are the contents of betasnap1.yaml:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvol0-snap1namespace:helmtest-vxflexosspec:volumeSnapshotClassName:vxflexos-snapclasssource:persistentVolumeClaimName:pvol0 NOTE: apiVersion in both betasnap yamls must match apiVersion in your VolumeSnapshotClass, if version in VolumeSnapshotClass is v1, adjust apiVersion like so: apiVersion: snapshot.storage.k8s.io/v1 in betasnap1.yaml and betasnap2.yaml\n Results\nThe snaptest.sh script will create a snapshot using the definitions in the betasnap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0, then the created snapshot is named pvol0-snap1.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n helmtest-vxflexos.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell EMC PowerFlex installation does not create this class. You will need to create instance of VolumeSnapshotClass from one of default samples in `helm/samples/volumesnapshotclass’ directory.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\n NOTE: apiVersion in both betasnap yamls must match apiVersion in your VolumeSnapshotClass, if version in vxflexos-snapclass is v1, adjust apiVersion like so: apiVersion: snapshot.storage.k8s.io/v1\n  Run sh snaprestoretest.sh to start the test.  This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updated directory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\n Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update betasnap1.yaml and betasnap2.yaml accordingly.  Results\nAn outline of this workflow is described below:\n The snapshot is taken using betasnap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim, which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below:  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:helmtest-vxflexosspec:storageClassName:vxflexosdataSource:name:pvol0-snap1kind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiNOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in betasnap1.yaml.\n","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v1/installation/test/powerflex/","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v1.5.0 New Features/Changes  Added support for Kubernetes v1.21 Added support for OpenShift 4.6 EUS with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 8.4 Added support for PowerFlex 3.6 Added support for custom file system format options Added support for dynamic log configuration Removed volume snapshot classes from helm template  Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\n","excerpt":"Release Notes - CSI PowerFlex v1.5.0 New Features/Changes  Added …","ref":"/csm-docs/v1/release/powerflex/","title":"PowerFlex"},{"body":"   Symptoms Prevention, Resolution or Workaround     The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that need to pull an image of the driver.   When you run the command kubectl describe pods vxflexos-controller-0 –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n vxflexos vxflexos-controller-0 driver logs show that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system.   The kubectl logs vxflexos-controller-0 -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script.   Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on a worker node, and ensure your container run time manager is properly configured to be utilized with SElinux.   Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux.   Installation of the driver on Kubernetes v1.20/v1.21 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.20/v1.21 requires v1 version of snapshot CRDs. If on Kubernetes 1.20/1.21 (v1 snapshots) install CRDs from v4.0.0, see the Volume Snapshot Requirements   Driver pods are not ready, with the error message: MountVolume.SetUp failed for volume \"log-config\" : configmap \"driver-config\" not found Create the configmap:  kubectl create -f logConfig.yaml    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     The installation …","ref":"/csm-docs/v1/troubleshooting/powerflex/","title":"PowerFlex"},{"body":"Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes 1.17 and is generally available (v1) in Kubernetes version 1.20.\nThe CSI PowerFlex driver version 1.4 supports v1beta1 snapshots on Kubernetes 1.18/1.19 and v1 snapshots on Kubernetes 1.20.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of CSI PowerFlex 1.4 driver, a Volume Snapshot Class is created. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\n{{- if eq .Values.kubeversion \"v1.20\" }} apiVersion: snapshot.storage.k8s.io/v1 {{- else }} apiVersion: snapshot.storage.k8s.io/v1beta1 {{- end}} kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Volume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, that is, when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIn case you are creating more storage classes, ensure that this attribute is set to true if you wish to expand any Persistent Volumes created using these new storage classes.\nFollowing is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the next highest multiple of 8.\nVolume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce , ReadWriteMany , and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nTopology Support The CSI PowerFlex driver version 1.2 and later support Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\n The PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed.  This Topology support does not include customer defined topology, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that pod scheduling takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.beta.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2020-05-27T13:24:55Z\" labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \"170198\" selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\nNOTE: In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\nController HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\n NOTE: If controller count is greater than the number of available nodes, excess controller pods will be stuck in pending state.\n If you are using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file (helm install):\nTo assign controller pods to worker nodes only (Default):\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: # - key: \"node-role.kubernetes.io/master\" # operator: \"Exists\" # effect: \"NoSchedule\" To assign controller pods to master and worker nodes:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" To assign controller pods to master nodes only:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" For configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nSDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Kubernetes nodes which run node portion of CSI driver. The deployment of the SDC kernel module occurs on these nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\n On Kubernetes nodes which run node portion of CSI driver, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the nodes with OS version which supports automatic SDC deployment . If there is a SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On nodes which do not support automatic SDC deployment by SDC init container, manuall installation steps must be followed. The SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC. Refer https://hub.docker.com/r/dellemc/sdc for supported OS versions. There is no automated uninstall of SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from node.  Multiarray Support The CSI PowerFlex driver version 1.4 adds support for managing multiple PowerFlex arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration There is a sample json file under the top directory named config.json with the following content:\n[ { \"username\": \"admin\", # username for connecting to API \"password\": \"password\", # password for connecting to API \"systemID\": \"ID1\",\t# system ID for system \"endpoint\": \"http://127.0.0.1\", # full URL path to the PowerFlex API \"insecure\": true, # use insecure connection or not \"isDefault\": true, # treat current array as default (would be used by storage class without arrayIP parameter) \"mdm\": \"10.0.0.1,10.0.0.2\" # MDM IP for the system }, { \"username\": \"admin\", \"password\": \"password\", \"systemID\": \"ID2\", \"endpoint\": \"https://127.0.0.2\", \"insecure\": true, \"mdm\": \"10.0.0.3,10.0.0.4\" } ] Here we specify that we want CSI driver to manage two arrays: one with an IP 127.0.0.1 and the other with an IP 127.0.0.2.\nTo use this config we need to create a Kubernetes secret from it. To do so run the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.json\nCreating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nFind the sample yaml files under helm/samples/storageclass. Edit storageclass.yaml if you want ext4 filesystem, and use storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have, and replace \u003cSYSTEM_ID\u003e with the system ID you have.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use the storage class for the corresponding array.\nEphemeral Inline Volume The CSI PowerFlex driver version 1.4 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest (found in csi-vxflexos/test/helm/ephemeral) for creating ephemeral volume in pod manifest with CSI PowerFlex driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumesspec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data0\"name:my-csi-volume- mountPath:\"/data1\"name:my-csi-volume-xfsvolumes:- name:my-csi-volumecsi:driver:csi-vxflexos.dellemc.comfsType:\"ext4\"volumeAttributes:volumeName:\"my-csi-volume\"size:\"8Gi\"storagepool:samplesystemID:sample- name:my-csi-volume-xfscsi:driver:csi-vxflexos.dellemc.comfsType:\"xfs\"volumeAttributes:volumeName:\"my-csi-volume-xfs\"size:\"10Gi\"storagepool:samplesystemID:sampleThis manifest will create a pod and attach two newly created ephemeral inline csi volumes to it, one ext4 and the other xfs.\nTo run the corresponding helm test, go to csi-vxflexos/test/helm/ephemeral and fill in the values for storagepool and systemID in sample.yaml.\nThen run:\n./testEphemeral.sh this test will deploy the pod with two ephemeral volumes, and write some data to them before deleting the pod.\nWhen creating ephemeral volumes, it is important to specify the following within the volumeAttributes section: volumeName, size, storagepool, and if you want to use a non-default array, systemID.\n","excerpt":"Volume Snapshot Feature The Volume Snapshot feature was introduced in …","ref":"/csm-docs/v2/features/powerflex/","title":"PowerFlex"},{"body":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, please review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the namespace vxflexos:\n CSI Driver for Dell EMC PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace vxflexos:\n CSI Driver for Dell EMC PowerFlex Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements that must be met before installing the CSI Driver for Dell EMC PowerFlex:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Enable Zero Padding on PowerFlex Configure Mount propagation on container runtime (example: Docker) Install PowerFlex Storage Data Client Volume Snapshot requirements A user must exist on the array with a role \u003e= FrontEndConfigure  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nEnable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. See Dell EMC PowerFlex documentation for more information to configure this setting.\nConfigure Mount Propagation on Container Runtime It is required to configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerFlex. The following is instruction on how to do this with Docker. If you use another container runtime, follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n The service section of /etc/systemd/system/multi-user.target.wants/docker.service needs to be edited in a few places. First, the Requires entry under the [Unit] header needs have docker.service added to it, as shown. Second, MountFlags=shared needs to be added under the [Service] header. [Unit] ... Requires=docker.socket containerd.service docker.service [Service] ... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  NOTE: Some distribution, like Ubuntu, already has MountFlags set by default.\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run node portion of CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for supported OS versions.\nSDC Deployment The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run node portion of CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS).\nOn Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\nOptional: For a typical install, you will pull SDC kernel modules from the Dell EMC ftp site, which is setup by default. Some users might want to mirror this repository to a local location. The PowerFlex KB article (https://www.dell.com/support/kbdoc/en-us/000184206/how-to-use-a-private-repository-for) has instructions on how to do this.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and Cent OS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  Volume Snapshot Requirements Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  Volume Snapshot Controller Starting with beta Volume Snapshots in Kubernetes 1.17, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.3 quay.io/k8scsi/csi-snapshotter:v4.0.0   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository.\n  Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace vxflexos to create a new one.\n  Check helm/csi-vxflexos/driver-image.yaml and confirm the driver image points to new image.\n  Collect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the top-level helm directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the value for these parameters as they must be entered in the config.json file in the top-level directory.\n  Prepare the config.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing PowerFlex system true -   password Password for accessing PowerFlex system true -   systemID System name/ID of PowerFlex system true -   endpoint REST API gateway HTTPS endpoint for PowerFlex system true -   insecure Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface true true   isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list false false   mdm mdm defines the MDM(s) that SDC should register with on start. This should be an list of MDM IP addresses or hostnames separated by comma true -    Example: config.json\n[ { \"username\": \"admin\", \"password\": \"password\", \"systemID\": \"ID1\", \"endpoint\": \"http://127.0.0.1\", \"insecure\": true, \"isDefault\": true, \"mdm\": \"10.0.0.1,10.0.0.2\" }, { \"username\": \"admin\", \"password\": \"password\", \"systemID\": \"ID2\", \"endpoint\": \"https://127.0.0.2\", \"insecure\": true, \"mdm\": \"10.0.0.3,10.0.0.4\" } ] After editing the file, run the following command to create a secret called vxflexos-config kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.json\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.json -o yaml --dry-run=client | kubectl replace -f -\nNOTE:\n The user needs to validate the JSON syntax and array related key/values while replacing the vxflexos-creds secret. If you update the secret, you will have to reinstall the driver. System ID, MDM configuration etc. now are taken directly from config.json, and no longer the values file.    If using automated SDC deployment:\n Check the SDC container image is the correct version for your version of PowerFlex.    Copy the default values.yaml file cd helm \u0026\u0026 cp csi-vxflexos/values.yaml myvalues.yaml\n  Edit the newly created values file and provide values for the following parameters vi myvalues.yaml:\n     Parameter Description Required Default     volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. No “k8s”   controllerCount Set to deploy multiple controller instances. If controller count is greater than the number of available nodes, excess pods will be left in pending state. You can increase number of available nodes by configuring the “controller” section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2   enablelistvolumesnapshot Set to have snapshots included in the CSI operation ListVolumes. Disabled by default. No FALSE   allowRWOMultiPodAccess Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No FALSE   controller This section allows configuration of controller specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - -   nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. No \" \"   tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. No \" \"   monitor This section allows configuration of the SDC monitoring pod. - -   enabled Set to enable the usage of the monitoring pod. No FALSE   hostNetwork Set whether the monitor pod should run on the host network or not. No TRUE   hostPID Set whether the monitor pod should run in the host namespace or not. No TRUE   podmon Podmon is an optional feature under development and tech preview. Enable this feature only after contact support for additional information - -   enabled  No FALSE    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml  NOTE:\n For detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder. This script runs verify-csi-vxflexos.sh script that is present in the same directory. It will validate MDM IP(s) in vxflexos-config secret and creates a new field consumed by init container and sdc-monitor container This script also runs the verify.sh script. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes. It is mandatory to run the first installation and installation after changes to MDM configuration in vxflexos-config secret without skipping the verification. After that you can use --skip-verify-node or --skip-verify . (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes Starting in CSI PowerFlex v1.4, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the helm/samples folder. Please use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerFlex v1.4 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNOTE: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class: There are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\n Edit storageclass.yaml if you need ext4 filesystem and storageclass-xfs.yaml if you want xfs filesystem Replace \u003cSTORAGE_POOL\u003e with the storage pool you have Replace \u003cSYSTEM_ID\u003e with the system ID you have. Note there are two appearances in the file Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f storageclass.yaml or kubectl create -f storageclass-xfs.yaml  NOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You will not be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the …","ref":"/csm-docs/v2/installation/helm/powerflex/","title":"PowerFlex"},{"body":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell EMC PowerFlex v1.4.0 can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: SDC Deployment for Operator  This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non supported verisons of the OS also do the manual SDC deployment steps given below. Refer https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When driver is created, MDM value for initContainers in driver CR is set by operator from mdm attributes in driver configuration file, config.json. Example of config.json is below in this document. Do not set MDM value for initContainers in driver CR file manually. Note: To use a sdc-binary module from customer ftp site:  Create a secret, sdc-repo-secret.yaml to contain the credentials for the private repo. To generate the base64 encoding of a credential:    echo-n\u003ccredential\u003e|base64-isecret sample to use:\napiVersion:v1kind:Secretmetadata:name:sdc-repo-credsnamespace:vxflexostype:Opaquedata:# set username to the base64 encoded username, sdc default isusername:\u003cusernameinbase64\u003e # set password to the base64 encoded password, sdc default ispassword:\u003cpasswordinbase64\u003e Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml. Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml.  Example CR: config/samples/vxflex_v140_ops_46.yaml sideCars:# Uncomment the following section if you want to run the monitoring sidecar- name:sdc-monitorenvs:- name:HOST_PIDvalue:\"1\"- name:MDMvalue:\"\"initContainers:- image:dellemc/sdc:3.5.1.1imagePullPolicy:IfNotPresentname:sdcenvs:- name:MDMvalue:\"\"Manual SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and Cent OS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  Install Driver   Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e command using the desired name to create the namespace.\n  Prepare the config.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing PowerFlex system true -   password Password for accessing PowerFlex system true -   systemID System name/ID of PowerFlex system true -   endpoint REST API gateway HTTPS endpoint for PowerFlex system true -   insecure Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface true true   isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list false false   mdm mdm defines the MDM(s) that SDC should register with on start. This should be an list of MDM IP addresses or hostnames separated by comma true -    Example: config.json\n[ { \"username\": \"admin\", \"password\": \"password\", \"systemID\": \"ID1\", \"endpoint\": \"http://127.0.0.1\", \"insecure\": true, \"isDefault\": true, \"mdm\": \"10.0.0.1,10.0.0.2\" }, { \"username\": \"admin\", \"password\": \"password\", \"systemID\": \"ID2\", \"endpoint\": \"https://127.0.0.2\", \"insecure\": true, \"mdm\": \"10.0.0.3,10.0.0.4\" } ] After editing the file, run the following command to create a secret called vxflexos-config kubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.json\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.json -o yaml --dry-run=client | kubectl replace -f -\nNote:\n The user needs to validate the JSON syntax and array related key/values while replacing the vxflexos-creds secret. If you update the secret, you must reinstall the driver. System ID, MDM configuration etc. now are taken directly from config.json. MDM provided in the input_sample_file.yaml will be overided with MDM values in config.json.    Create a Custom Resource (CR) for PowerFlex using the sample files provided here .\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\n   Parameter Description Required Default     replicas Controls the amount of controller pods you deploy. If the number of controller pods are greater than number of available nodes, excess pods will become stay in a pending state. Defaults is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false   X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false   X_CSI_DEBUG To enable debug mode No false   X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false   StorageClass parameters      storagePool Defines the PowerFlex storage pool from which this driver will provision volumes. You must set this for the primary storage pool to be used Yes pool1   allowVolumeExpansion Once the allowed topology is modified in storage class, pods/and volumes will always be scheduled on nodes that have access to the storage No true   allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to replace the X_CSI_VXFLEXOS_SYSTEMNAME in the key with the actual systemname value No X_CSI_VXFLEXOS_SYSTEMNAME   initContainers:value Set the MDM IP’s here if installing on CoreOS to enable automatic SDC installation Yes (OpenShift) “10.xx.xx.xx,10.xx.xx.xx”      Execute the kubectl create -f \u003cinput_sample_file.yaml\u003e command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.\n  ","excerpt":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell …","ref":"/csm-docs/v2/installation/operator/powerflex/","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\n Navigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Dell CSI Operator, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following:  Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \"docker.io/centos:latest\" Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured.  Results\nAn outline of this workflow is described below:\n The 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below:  kind:PersistentVolumeClaimapiVersion:v1metadata:name:pvolnamespace:helmtest-vxflexosspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GistorageClassName:vxflexosThe volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use one of the pre-defined storage classes created by the CSI Driver for Dell EMC PowerFlex installation process. This step yields a mounted ext4 file system. You can see the storage class definitions in the PowerFlex installation helm chart files storageclass.yaml and storageclass-xfs.yaml. If you compare pvol0.yaml and pvol1.yaml , you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos. NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.  Test creating snapshots Test the workflow for snapshot creation.\nSteps\n Start the 2vols container and leave it running.  Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Operator, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the default snapshotclass name. If your snapshotclass names differ from the default values, update snap1.yaml and snap2.yaml accordingly.   Run sh snaptest.sh to start the test.  This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in snap1.yaml and snap2.yaml. The following are the contents of snap1.yaml:\napiVersion:snapshot.storage.k8s.io/v1alpha1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:helmtest-vxflexosspec:snapshotClassName:vxflexos-snapclasssource:name:pvolkind:PersistentVolumeClaimResults\nThe snaptest.sh script will create a snapshot using the definitions in the snap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0 , then the created snapshot is named pvol0-snap.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n test.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell EMC PowerFlex installation creates this class as its default snapshot class. You can see its definition in the installation directory file volumesnapshotclass.yaml.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\n Run sh snaprestoretest.sh to start the test.  This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updateddirectory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\n Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Dell CSI Operator, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the default snapshotclass name. If your snapshotclass names differ from the default values, update snap1.yaml and snap2.yaml accordingly.  Results\nAn outline of this workflow is described below:\n The snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim , which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below:  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:helmtest-vxflexosspec:storageClassName:vxflexosdataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiNOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in snap1.yaml.\n","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v2/installation/test/powerflex/","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v1.4.0 New Features/Changes  Added support for Kubernetes v1.20 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 8.3 Added support for Fedora CoreOS Added SDC deployment on Fedora CoreOS nodes Added support for Ephemeral Inline Volume Added support multi-mount volumes Added support for managing multiple PowerFlex arrays from one driver Removed storage classes from helm template  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.18.5+   Installation warning: “OpenShift version 4.7, is newer than the version that has been tested. Latest tested version is: 4.6” Ignore this warning and continue with the installation. v1.4.0 release of the driver supports OpenShift 4.6/4.7 .    ","excerpt":"Release Notes - CSI PowerFlex v1.4.0 New Features/Changes  Added …","ref":"/csm-docs/v2/release/powerflex/","title":"PowerFlex"},{"body":"   Symptoms Prevention, Resolution or Workaround     The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that needs to pull an image of the driver.   When you run the command kubectl describe pods vxflexos-controller-0 –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n vxflexos vxflexos-controller-0 driver logs shows that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system.   The kubectl logs vxflexos-controller-0 -n vxflexos driver logs shows that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script.   Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on worker node, and ensure your container run time manager is properly configured to be utilized with SElinux.   Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux.   Installation of the driver on Kubernetes v1.20 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.20 requires v1 version of snapshot CRDs. If on Kubernetes 1.20 (v1 snapshots) install CRDs from v4.0.0, see the Volume Snapshot Requirements    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     The installation …","ref":"/csm-docs/v2/troubleshooting/powerflex/","title":"PowerFlex"},{"body":"Volume Snapshot Feature The CSI PowerFlex driver version 1.2 and later support beta snapshots. Earlier versions of the driver supported alpha snapshots.\nVolume Snapshots feature in Kubernetes has moved to beta in Kubernetes version 1.17. It was an alpha feature in earlier releases (1.13 onwards). The snapshot API version has changed from v1alpha1 to v1beta1 with this migration.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of CSI PowerFlex 1.3 driver, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: pvol0-snap namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Volume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, that is, when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIn case you are creating more storage classes, make sure that this attribute is set to true if you wish to expand any Persistent Volumes created using these new storage classes.\nFollowing is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the closest multiple of 8.\nVolume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce , ReadWriteMany , and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nTopology Support The CSI PowerFlex driver version 1.2 and later support Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\n The PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed.  This Topology support does not include customer defined topology, users cannot create their own labels for nodes and storage classed and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that pod scheduling takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.beta.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2020-05-27T13:24:55Z\" labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \"170198\" selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\nNOTE In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\nController HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\n NOTE: If controller count is greater than the number of available nodes, excess controller pods will be stuck in pending state.\n If you’re using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file (helm install):\nTo assign controller pods to worker nodes only (Default):\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: # - key: \"node-role.kubernetes.io/master\" # operator: \"Exists\" # effect: \"NoSchedule\" To assign controller pods to master and worker nodes:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" To assign controller pods to master nodes only:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" For configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nAutomated SDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Red Hat CoreOS (RHCOS) nodes in an OpenShift cluster. Only RHCOS is supported at this time. The deployment of the SDC kernel module on RHCOS nodes is done via an init container. Automated installation is supported in both via Helm and Dell CSI Operator based installs. The following describes further details of this feature:\n On RHCOS nodes, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the node. If there is a SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On non-RHCOS nodes, the SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC There is no automated uninstall of SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from node.  ","excerpt":"Volume Snapshot Feature The CSI PowerFlex driver version 1.2 and later …","ref":"/csm-docs/v3/features/powerflex/","title":"PowerFlex"},{"body":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, please review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the namespace vxflexos:\n CSI Driver for Dell EMC PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace vxflexos:\n CSI Driver for Dell EMC PowerFlex Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements must be met before installing the CSI Driver for Dell EMC PowerFlex:\n Install Kubernetes (1.17, 1.18, 1.19) or OpenShift (4.5 or 4.6) Install Helm 3 Enable Zero Padding on PowerFlex Configure Mount propagation on container runtime (i.e. Docker) Install PowerFlex Storage Data Client Volume Snapshot requirements A user must exist on the array with a role \u003e= FrontEndConfigure  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nEnable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. See Dell EMC PowerFlex documentation for more information to configure this setting.\nConfigure Mount Propagation on Container Runtime It is required to configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerFlex. The following is instruction on how to do this with Docker. If you use another container runtime, follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  Note: Some distribution, like Ubuntu, already has MountFlags set by default\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all worker nodes. If installing on Red Hat CoreOS (RHCOS) nodes on OpenShift you can install using the automated SDC deployment feature. If installing on non-RHCOS nodes, you must install SDC manually.\nAutomatic SDC Deployment The automated deployment of the SDC runs by default when installing the driver. It installs an SDC container to faciliate the installation. While the install is automated there are a few configuration options for this feature. Those are referenced in the Install the Driver section. More details on how the automatic SDC deployment works can be found in the Feature section of this site on the PowerFlex page.\nOptional: For a typical install, you will pull SDC kernel modules from the Dell EMC ftp site, which is setup by default. Some users might want to mirror this repository to a local location. The PowerFlex documentation has instructions on how to do this. If a mirror is used, you need to create an SDC repo secret for managing the credentials to the mirror. Details on how to create the secret are in the Install the Driver section.\nManually SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and Cent OS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.    Volume Snapshot requirements Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\nYou can also install the CRDs by supplying the option –snapshot-crd while installing the driver using the csi-install.sh script. If you are installing the driver using the Dell CSI Operator, there is a helper script provided to install the snapshot CRDs - scripts/install_snap_crds.sh.\nVolume Snapshot Controller Starting with beta Volume Snapshots in Kubernetes 1.17, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests available on GitHub.\nNOTE:\n The manifests available on GitHub install v3.0.2 of the snapshotter image - quay.io/k8scsi/csi-snapshotter:v3.0.2 Dell recommends using v3.0.2 image of the snapshot-controller - quay.io/k8scsi/snapshot-controller:v3.0.2 The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository. Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace vxflexos to create a new one. Check helm/csi-vxflexos/driver-image.yaml and confirm the driver image points to new image. Edit the helm/secret.yaml, point to correct namespace and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername \u0026 mypassword are credentials for a user with PowerFlex priviledges.\n Create the secret by running kubectl create -f secret.yaml If not using automated SDC deployment, create a dummy SDC repo secret file: kubectl create -f sdc-repo-secret.yaml If using automated SDC deployment:  Check the SDC container image is the correct version for your version of PowerFlex. Create a secret for the SDC repo credentials and provide the URL for the repo.  To create the secret, you must update the details in helm/sdc-repo-secret.yaml file and running kubectl create -f sdc-repo-secret.yaml. To set the repo URL, you must set the repoUrl parameter in the myvalues.yaml file.     Collect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the top-level helm directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the value for these parameters as they must be entered in the myvalues.yaml file.  NOTE: Your SDC might have multiple VxFlex OS systems registered. Ensure that you choose the correct values.   Copy the default values.yaml file cd helm \u0026\u0026 cp csi-vxflexos/values.yaml myvalues.yaml Edit the newly created values file and provide values for the following parameters vi myvalues.yaml:     Parameter Description Required Default     systemName Set to the PowerFlex/VxFlex OS system name or system ID to be used with the driver. Yes “systemname”   restGateway Set to the URL of your system’s REST API Gateway. You can obtain this value from the PowerFlex administrator. Yes “https://123.0.0.1”   storagePool Set to a default (existing) storage pool name in your PowerFlex/VxFlex OS system. Yes “sp”   volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. No “k8s”   controllerCount Set to deploy multiple controller instances. If controller count is greater than the number of available nodes, excess pods will be left in pending state. You can increase number of available nodes by configuring the “controller” section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2   enablelistvolumesnapshot Set to have snapshots included in the CSI operation ListVolumes. Disabled by default. No FALSE   StorageClass Helm charts create a Kubernetes StorageClass while deploying CSI Driver for Dell EMC PowerFlex. This section includes relevant variables. - -   name Defines the name of the Kubernetes storage class that the Helm charts will create. For example, the vxflexos base name will be used to generate names such as vxflexos and vxflexos-xfs. No “vxflexos”   isDefault Sets the newly created storage class as default for Kubernetes. Set this value to true only if you expect PowerFlex to be your principle storage provider, as it will be used in PersitentVolumeClaims where no storageclass is provided. After installation, you can add custom storage classes, if desired. No TRUE   reclaimPolicy Defines whether the volumes will be retained or deleted when the assigned pod is destroyed. The valid values for this variable are Retain or Delete. No “Delete”   controller This section allows configuration of controller specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - -   nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. No \" \"   tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. No \" \"   monitor This section allows configuration of the SDC monitoring pod. - -   enabled Set to enable the usage of the monitoring pod. No FALSE   hostNetwork Set whether the monitor pod should run on the host network or not. No TRUE   hostPID Set whether the monitor pod should run in the host namespace or not. No TRUE   sdcKernelMirror [RHCOS only] The PowerFlex SDC may need to pull a new module that is known to work with newer Linux kernels. The default location of this mirror os at ftp.emc.com. The PowerFlex documentation has instructions for methods to mirror this repository to a local location if necessary. - -   repoUrl Set the URL of the ftp mirror containing SDC kernel modules. Only ftp locations are allowed. A blank string signifies the default mirror, which is “ftp://ftp.emc.com”. No \" \"   11. Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml       NOTE:\n For detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder. This script also runs the verify.sh script that is present in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes. You can also skip the verification step by specifiying the --skip-verify-node option. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and can’t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting in Q4 of 2020, an annotation \"helm.sh/resource-policy\": keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNOTE: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\n Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, make sure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You won’t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the …","ref":"/csm-docs/v3/installation/helm/powerflex/","title":"PowerFlex"},{"body":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell EMC PowerFlex can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: Automated SDC Deployment for Operator  This applies to OpenShift with RHCOS Nodes Only. This feature deploys the sdc kernel modules on CoreOS nodes with the help of an init container. Required: MDM value need to be provided in CR file for the sdc init container to work. Expect error if not in proper format. To use a specific image from ftp site, pass in repo url, repo password and repo username.  Repo username and repo password are to be encrypted by a secret and passed in. Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml.   Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml.  Example CR: config/samples/vxflexos_v130_ops_46.yaml #sideCars:# Uncomment the following section if you want to run the monitoring sidecar# - name: sdc-monitor# envs:# - name: HOST_PID# value: \"1\"initContainers:- image:dellemc/sdc:3.6.0.176-3.5.1000.176imagePullPolicy:IfNotPresentname:sdcenvs:- name:MDMvalue:\"10.xx.xx.xx,10.xx.xx.xx\"Install Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e command using the desired name to create the namespace. Create PowerFlex credentials: Create a file called vxflexos-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:vxflexos-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003eReplace the values for the username and password parameters. These values can be optioned using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 Run kubectl create -f vxflexos-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerFlex using the sample files provided here . Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:    Parameter Description Required Default     replicas Controls the amount of controller pods you deploy. If controller pods is greater than number of available nodes, excess pods will become stuck in pending. Defaults is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_VXFLEXOS_SYSTEMNAME Defines the name of the PowerFlex system from which volumes will be provisioned. This must either be set to the PowerFlex system name or system ID Yes systemname   X_CSI_VXFLEXOS_ENDPOINT Defines the PowerFlex REST API endpoint, with full URL, typically leveraging HTTPS. You must set this for your PowerFlex installations REST gateway Yes https://127.0.0.1   CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false   X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false   X_CSI_DEBUG To enable debug mode No false   StorageClass parameters      storagePool Defines the PowerFlex storage pool from which this driver will provision volumes. You must set this for the primary storage pool to be used Yes pool1   allowVolumeExpansion Once the allowed topology is modified in storage class, pods/and volumes will always be scheduled on nodes that have access to the storage No true   allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to replace the X_CSI_VXFLEXOS_SYSTEMNAME in the key with the actual systemname value No X_CSI_VXFLEXOS_SYSTEMNAME   initContainers:value Set the MDM IP’s here if installing on CoreOS to enable automatic SDC installation Yes (OpenShift) “10.xx.xx.xx,10.xx.xx.xx”     Execute the kubectl create -f \u003cinput_sample_file.yaml\u003e command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.  ","excerpt":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell …","ref":"/csm-docs/v3/installation/operator/powerflex/","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\n Navigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Dell CSI Operator, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following:  Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \"docker.io/centos:latest\" Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured.  Results\nAn outline of this workflow is described below:\n The 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below:  kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol namespace: helmtest-vxflexos spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: vxflexos The volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use one of the pre-defined storage classes created by the CSI Driver for Dell EMC PowerFlex installation process. This step yields a mounted ext4 file system. You can see the storage class definitions in the PowerFlex installation helm chart files storageclass.yaml and storageclass-xfs.yaml. If you compare pvol0.yaml and pvol1.yaml , you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos. NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.  Test creating snapshots Test the workflow for snapshot creation.\nSteps\n Start the 2vols container and leave it running.  Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Operator, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the default snapshotclass name. If your snapshotclass names differ from the default values, update snap1.yaml and snap2.yaml accordingly.   Run sh snaptest.sh to start the test.  This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in snap1.yaml and snap2.yaml. The following are the contents of snap1.yaml:\napiVersion: snapshot.storage.k8s.io/v1alpha1 kind: VolumeSnapshot metadata: name: pvol0-snap namespace: helmtest-vxflexos spec: snapshotClassName: vxflexos-snapclass source: name: pvol kind: PersistentVolumeClaim Results\nThe snaptest.sh script will create a snapshot using the definitions in the snap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0 , then the created snapshot is named pvol0-snap.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n test.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell EMC PowerFlex installation creates this class as its default snapshot class. You can see its definition in the installation directory file volumesnapshotclass.yaml.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\n Run sh snaprestoretest.sh to start the test.  This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updateddirectory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\n Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Dell CSI Operator, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the default snapshotclass name. If your snapshotclass names differ from the default values, update snap1.yaml and snap2.yaml accordingly.  Results\nAn outline of this workflow is described below:\n The snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim , which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below:  apiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi NOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in snap1.yaml.\n","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v3/installation/test/powerflex/","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v1.3.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL and CoreOS worker nodes Added automatic SDC deployment on OpenShift CoreOS nodes Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Ubuntu 20.04 Added support for volume cloning Added support for Controller high availability (multiple-controllers)  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.17 or 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.17.8+, 1.18.5+    ","excerpt":"Release Notes - CSI PowerFlex v1.3.0 New Features/Changes  Added …","ref":"/csm-docs/v3/release/powerflex/","title":"PowerFlex"},{"body":"   Symptoms Prevention, Resolution or Workaround     The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that needs to pull an image of the driver.   When you run the command kubectl describe pods vxflexos-controller-0 –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n vxflexos vxflexos-controller-0 driver logs shows that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system.   The kubectl logs vxflexos-controller-0 -n vxflexos driver logs shows that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script.   Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on worker node, and ensure your container run time manager is properly configured to be   utilized with SElinux.    Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux.    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     The installation …","ref":"/csm-docs/v3/troubleshooting/powerflex/","title":"PowerFlex"},{"body":"CSI Driver for Dell EMC PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, see the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell EMC PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume CSI PowerMax ReverseProxy (optional)  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell EMC PowerMax Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following requirements must be met before installing CSI Driver for Dell EMC PowerMax:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Fibre Channel requirements iSCSI requirements Certificate validation for Unisphere REST API calls Mount propagation is enabled on container runtime that is being used Linux multipathing requirements If using Snapshot feature, satisfy all Volume Snapshot requirements  Install Helm 3 Install Helm 3 on the master node before you install CSI Driver for Dell EMC PowerMax.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.\nFibre Channel Requirements CSI Driver for Dell EMC PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install CSI Driver:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs.  iSCSI Requirements The CSI Driver for Dell EMC PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\n All Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell EMC PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell EMC PowerMax array. All the port group names supplied to the driver must exist on each Dell EMC PowerMax with the same name.  For more information about configuring iSCSI, see Dell EMC Host Connectivity guide.\nCertificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\n  To fetch the certificate, run openssl s_client -showcerts -connect [Unisphere IP]:8443 \u003c/dev/null 2\u003e /dev/null | openssl x509 -outform PEM \u003e ca_cert.pem\nNOTE: The IP address varies for each user.\n  To create the secret, run kubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax\n  Ports in the port group There are no restrictions to how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell EMC PowerMax to ensure that you have multiple paths to your data volumes.\nLinux multipathing requirements CSI Driver for Dell EMC PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\n All the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file.  As a best practice, use the following options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 (Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. For installation, use v4.2.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers to support Volume snapshots.\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster, irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v4.2.x\nNOTE:\n The manifests available on GitHub install the snapshotter image: quay.io/k8scsi/csi-snapshotter:v4.0.x The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication:enabled:trueReplication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in the csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstallation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 4.2.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone -b v2.0.0 https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the `samples/secret/secret.yaml file, point to the correct namespace, and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with PowerMax privileges.\n Create the secret by running kubectl create -f samples/secret/secret.yaml. If you are going to install the new CSI PowerMax ReverseProxy service, create a TLS secret with the name - csireverseproxy-tls-secret which holds an SSL certificate and the corresponding private key in the namespace where you are installing the driver. Copy the default values.yaml file cd helm \u0026\u0026 cp csi-powermax/values.yaml my-powermax-settings.yaml Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml     Parameter Description Required Default     kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent   clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes “ABC”   defaultFsType Used to set the default FS type for external provisioner Yes ext4   portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only “PortGroup1, PortGroup2, PortGroup3”   storageResourcePool This parameter must mention one of the SRPs on the PowerMax array that the symmetrixID specifies. This value is used to create the default storage class. Yes “SRP_1”   serviceLevel This parameter must mention one of the Service Levels on the PowerMax array. This value is used to create the default storage class. Yes “Bronze”   skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No “True”   transportProtocol Set the preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty   nodeNameTemplate Used to specify a template that will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, leave this value empty. No Empty   controller Allows configuration of the controller-specific parameters. - -   controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2   volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s”   snapshot.enabled Enable/Disable volume snapshot feature Yes true   snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot”   resizer.enabled Enable/Disable volume expansion feature Yes true   nodeSelector Define node selection constraints for pods of controller deployment No    tolerations Define tolerations for the controller deployment, if required No    node Allows configuration of the node-specific parameters. - -   tolerations Add tolerations as per requirement No -   nodeSelector Add node selectors as per requirement No -   global This section refers to configuration options for both CSI PowerMax Driver and Reverse Proxy - -   defaultCredentialsSecret This secret name refers to:\n1. The Unisphere credentials if the driver is installed without proxy or with proxy in Linked mode.\n2. The proxy credentials if the driver is installed with proxy in StandAlone mode.\n3. The default Unisphere credentials if credentialsSecret is not specified for a management server. Yes powermax-creds   storageArrays This section refers to the list of arrays managed by the driver and Reverse Proxy in StandAlone mode. - -   storageArrayId This refers to PowerMax Symmetrix ID. Yes 000000000001   endpoint This refers to the URL of the Unisphere server managing storageArrayId Yes if Reverse Proxy mode is StandAlone https://primary-1.unisphe.re:8443   backupEndpoint This refers to the URL of the backup Unisphere server managing storageArrayId, if Reverse Proxy is installed in StandAlone mode. No https://backup-1.unisphe.re:8443   managementServers This section refers to the list of configurations for Unisphere servers managing powermax arrays. - -   endpoint This refers to the URL of the Unisphere server Yes https://primary-1.unisphe.re:8443   credentialsSecret This refers to the user credentials for endpoint No primary-1-secret   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty   limits This refers to various limits for Reverse Proxy No -   maxActiveRead This refers to the maximum concurrent READ request handled by the reverse proxy. No 5   maxActiveWrite This refers to the maximum concurrent WRITE request handled by the reverse proxy. No 4   maxOutStandingRead This refers to maximum queued READ request when reverse proxy receives more than maxActiveRead requests. No 50   maxOutStandingWrite This refers to maximum queued WRITE request when reverse proxy receives more than maxActiveWrite requests. No 50   csireverseproxy This section refers to the configuration options for CSI PowerMax Reverse Proxy - -   enabled Boolean parameter which indicates if CSI PowerMax Reverse Proxy is going to be configured and installed.\nNOTE: If not enabled, then there is no requirement to configure any of the following values. No “False”   image This refers to the image of the CSI Powermax Reverse Proxy container. Yes dellemc/csipowermax-reverseproxy:v1.4.0   tlsSecret This refers to the TLS secret of the Reverse Proxy Server. Yes csirevproxy-tls-secret   deployAsSidecar If set to true, the Reverse Proxy is installed as a sidecar to the driver’s controller pod otherwise it is installed as a separate deployment. Yes “True”   port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation Yes 2222   mode This refers to the installation mode of Reverse Proxy. It can be set to:\n1. Linked: In this mode, the Reverse Proxy communicates with a primary or a backup Unisphere managing the same set of arrays.\n2. StandAlone: In this mode, the Reverse Proxy communicates with multiple arrays managed by different Unispheres. Yes “StandAlone”    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml  Note:\n For detailed instructions on how to run the install scripts, see the readme document in the dell-csi-helm-installer folder. There are a set of samples provided here to help you configure the driver with reverse proxy This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option  Storage Classes Starting CSI PowerMax v1.6, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests has been provided in the samples/storageclass folder. Please use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. To continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Volume Snapshot Class Starting with CSI PowerMax v1.7, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerMax v1.7 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerMax to 1.7 before upgrading to 2.0.\nSample values file The following sections have useful snippets from values.yaml file which provides more information on how to configure the CSI PowerMax driver along with CSI PowerMax ReverseProxy in various modes\nCSI PowerMax driver without Proxy In this mode, the CSI PowerMax driver can only connect to a single Unisphere server. So, you just specify a list of storage arrays and the address of the Unisphere server\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"- storageArrayId:\"000000000002\"managementServers:- endpoint:https://unisphere-address:8443 Note: If you provide multiple endpoints in the list of management servers, the installer will only use the first server in the list\n CSI PowerMax driver with Proxy in Linked mode In this mode, the CSI PowerMax ReverseProxy acts as a passthrough for the RESTAPI calls and only provides limited functionality such as rate limiting, backup Unisphere server. The CSI PowerMax driver is still responsible for the authentication with the Unisphere server.\nThe first endpoint in the list of management servers is the primary Unisphere server and if you provide a second endpoint, then it will be considered as the backup Unisphere’s endpoint.\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"- storageArrayId:\"000000000002\"managementServers:- endpoint:https://primary-unisphere:8443skipCertificateValidation:falsecertSecret:primary-certlimits:maxActiveRead:5maxActiveWrite:4maxOutStandingRead:50maxOutStandingWrite:50- endpoint:https://backup-unisphere:8443#Optional# \"csireverseproxy\" refers to the subchart csireverseproxycsireverseproxy:# Set enabled to true if you want to use proxyenabled:trueimage:dellemc/csipowermax-reverseproxy:v1.4.0tlsSecret:csirevproxy-tls-secretdeployAsSidecar:trueport:2222mode:Linked Note: Since the driver is still responsible for authentication when used with Proxy in Linked mode, the credentials for both primary and backup Unisphere need to be the same.\n CSI PowerMax driver with Proxy in StandAlone mode This is the most advanced configuration which provides you with the capability to connect to Multiple Unisphere servers. You can specify primary and backup Unisphere servers for each storage array. If you have different credentials for your Unisphere servers, you can also specify different credential secrets.\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"endpoint:https://primary-1.unisphe.re:8443backupEndpoint:https://backup-1.unisphe.re:8443- storageArrayId:\"000000000002\"endpoint:https://primary-2.unisphe.re:8443backupEndpoint:https://backup-2.unisphe.re:8443managementServers:- endpoint:https://primary-1.unisphe.re:8443credentialsSecret:primary-1-secretskipCertificateValidation:falsecertSecret:primary-certlimits:maxActiveRead:5maxActiveWrite:4maxOutStandingRead:50maxOutStandingWrite:50- endpoint:https://backup-1.unisphe.re:8443credentialsSecret:backup-1-secretskipCertificateValidation:true- endpoint:https://primary-2.unisphe.re:8443credentialsSecret:primary-2-secretskipCertificateValidation:true- endpoint:https://backup-2.unisphe.re:8443credentialsSecret:backup-2-secretskipCertificateValidation:true# \"csireverseproxy\" refers to the subchart csireverseproxycsireverseproxy:# Set enabled to true if you want to use proxyenabled:trueimage:dellemc/csipowermax-reverseproxy:v1.4.0tlsSecret:csirevproxy-tls-secretdeployAsSidecar:trueport:2222mode:StandAlone Note: If the credential secret is missing from any management server details, the installer will try to use the defaultCredentialsSecret\n ","excerpt":"CSI Driver for Dell EMC PowerMax can be deployed by using the provided …","ref":"/csm-docs/docs/csidriver/installation/helm/powermax/","title":"PowerMax"},{"body":"Installing CSI Driver for PowerMax via Operator CSI Driver for Dell EMC PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Create secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. See the detailed documentation on how to create this secret here.\nInstall Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace. Create PowerMax credentials: Create a file called powermax-creds.yaml with the following content: apiVersion:v1kind:Secretmetadata:name:powermax-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards)# chapsecret: \u003cbase64 CHAP secret\u003eReplace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run the kubectl create -f powermax-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerMax using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:    Parameter Description Required Default     replicas Controls the number of controller Pods you deploy. If controller Pods are greater than the number of available nodes, excess Pods will become stuck in pending. The default is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended to all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ   X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443   X_CSI_TRANSPORT_PROTOCOL Choose which transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No -   X_CSI_MANAGED_ARRAYS List of comma-separated array ID(s) which will be managed by the driver Yes -   X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Leave blank if not using reverse proxy No -   X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4   X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature see the related documentation No -   Node parameters      X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature see the related documentation No false     Execute the following command to create the PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.  CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is an optional component that can be installed with the CSI PowerMax driver. For more details on this feature see the related documentation.\nWhen you install CSI PowerMax ReverseProxy, dell-csi-operator will create a Deployment and ClusterIP service as part of the installation\nNote - To use the ReverseProxy with the CSI PowerMax driver, the ReverseProxy service should be created before you install the CSIPowerMax driver.\nPre-requisites Create a TLS secret that holds an SSL certificate and a private key which is required by the reverse proxy server. Use a tool such as openssl to generate this secret using the example below:\n openssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n powermax tls revproxy-certs --cert=tls.crt --key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec  tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs config : This section contains the details of the Reverse Proxy configuration mode : This value is set to Linked by default. Do not change this value linkConfig : This section contains the configuration of the Linked mode primary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if the primary Unisphere is unreachable url : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false standAloneConfig : This section contains the configuration of the StandAlone mode. Refer to the sample below for the detailed config   Note: Only one of the Linked or StandAlone configurations needs to be supplied. The appropriate mode needs to be set in the spec as well.\n Here is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion:storage.dell.com/v1kind:CSIPowerMaxRevProxymetadata:name:powermax-reverseproxy# \u003c- Name of the CSIPowerMaxRevProxy objectnamespace:test-powermax# \u003c- Set the namespace to where you will install the CSI PowerMax driverspec:# Image for CSI PowerMax ReverseProxyimage:dellemc/csipowermax-reverseproxy:v1.4.0# \u003c- CSI PowerMax Reverse Proxy imageimagePullPolicy:Always# TLS secret which contains SSL certificate and private key for the Reverse Proxy servertlsSecret:csirevproxy-tls-secretconfig:mode:LinkedlinkConfig:primary:url:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:true# This setting determines if client side Unisphere certificate validation is to be skippedcertSecret:\"\"# Provide this value if skipCertificateValidation is set to falsebackup:# This is an optional field and lets you configure a backup unisphere which can be used by proxy serverurl:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:truestandAloneConfig:# Set mode to \"StandAlone\" in order to use this configstorageArrays:- storageArrayId:\"000000000001\"# Unisphere server managing the PowerMax arrayprimaryURL:https://unisphere-1-addr:8443# proxyCredentialSecrets are used by the clients of the proxy to connect to it# If using proxy in the stand alone mode, then the driver must be provided the# same secret.# The format of the proxy credential secret are exactly the same as the unisphere credential secret# For using the proxy with the driver, use the same proxy credential secrets for# all the managed storage arraysproxyCredentialSecrets:- proxy-creds- storageArrayId:\"000000000002\"primaryURL:https://unisphere-2-addr:8443# An optional backup Unisphere server managing the same array# This can be used by the proxy to fall back to in case the primary# Unisphere is inaccessible temporarilybackupURL:unisphere-3-addr:8443proxyCredentialSecrets:- proxy-credsmanagementServers:- url:https://unisphere-1-addr:8443# Secret containing the credentials of the Unisphere serverarrayCredentialSecret:unsiphere-1-credsskipCertificateValidation:true- url:https://unisphere-2-addr:8443arrayCredentialSecret:unsiphere-2-credsskipCertificateValidation:true- url:https://unisphere-3-addr:8443arrayCredentialSecret:unsiphere-3-credsskipCertificateValidation:trueInstallation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service:\nkubectl create -f powermax_reverseproxy.yaml  You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u003cnamespace\u003e kubectl get svc -n \u003cnamespace\u003e  There is a new sample file - powermax_revproxy_standalone_with_driver.yaml in the samples folder which enables installation of CSI PowerMax ReverseProxy in StandAlone mode along with the CSI PowerMax driver. This mode enables the CSI PowerMax driver to connect to multiple Unisphere servers for managing multiple PowerMax arrays. Please follow the same steps described above to install ReverseProxy with this new sample file.\nDynamic Logging Configuration This feature is introduced in CSI Driver for powermax version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Sample CRD file for powermax apiVersion:storage.dell.com/v1kind:CSIPowerMaxmetadata:name:test-powermaxnamespace:test-powermaxspec:driver:# Config version for CSI PowerMax v2.0.0 driverconfigVersion:v2.0.0# replica: Define the number of PowerMax controller nodes# to deploy to the Kubernetes release# Allowed values: n, where n \u003e 0# Default value: Nonereplicas:2dnsPolicy:ClusterFirstWithHostNetforceUpdate:falsecommon:# Image for CSI PowerMax driver v2.0.0image:dellemc/csi-powermax:v2.0.0# imagePullPolicy: Policy to determine if the image should be pulled prior to starting the container.# Allowed values:# Always: Always pull the image.# IfNotPresent: Only pull the image if it does not already exist on the node.# Never: Never pull the image.# Default value: NoneimagePullPolicy:IfNotPresentenvs:# X_CSI_MANAGED_ARRAYS: Serial ID of the arrays that will be used for provisioning# Default value: None# Examples: \"000000000001\", \"000000000002\"- name:X_CSI_MANAGED_ARRAYSvalue:\"000000000000,000000000001\"# X_CSI_POWERMAX_ENDPOINT: Address of the Unisphere server that is managing the PowerMax arrays# Default value: None# Example: https://0.0.0.1:8443- name:X_CSI_POWERMAX_ENDPOINTvalue:\"https://0.0.0.0:8443/\"# X_CSI_K8S_CLUSTER_PREFIX: Define a prefix that is appended onto# all resources created in the Array# This should be unique per K8s/CSI deployment# maximum length of this value is 3 characters# Default value: None# Examples: \"XYZ\", \"EMC\"# Examples: \"XYZ\", \"EMC\"- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\"XYZ\"# X_CSI_POWERMAX_PORTGROUPS: Define the set of existing port groups that the driver will use.# It is a comma separated list of portgroup names.# Required only in case of iSCSI port groups# Allowed values: iSCSI Port Group names# Default value: None# Examples: \"pg1\", \"pg1, pg2\"- name:\"X_CSI_POWERMAX_PORTGROUPS\"value:\"\"# \"X_CSI_TRANSPORT_PROTOCOL\" can be \"FC\" or \"FIBRE\" for fibrechannel,# \"ISCSI\" for iSCSI, or \"\" for autoselection.# Allowed values:# \"FC\" - Fiber Channel protocol# \"FIBER\" - Fiber Channel protocol# \"ISCSI\" - iSCSI protocol# \"\" - Automatic selection of transport protocol# Default value: \"\" \u003cempty\u003e- name:\"X_CSI_TRANSPORT_PROTOCOL\"value:\"\"# X_CSI_POWERMAX_PROXY_SERVICE_NAME: Refers to the name of the proxy service in kubernetes# Set this to \"powermax-reverseproxy\" if you are installing the proxy# Allowed values: \"powermax-reverseproxy\"# default values: \"\" \u003cempty\u003e- name:\"X_CSI_POWERMAX_PROXY_SERVICE_NAME\"value:\"\"# X_CSI_GRPC_MAX_THREADS: Defines the maximum number of concurrent grpc requests.# Set this value to a higher number (max 50) if you are using the proxy# Allowed values: n, where n \u003e 4# default values: None- name:\"X_CSI_GRPC_MAX_THREADS\"value:\"4\"node:envs:# X_CSI_POWERMAX_ISCSI_ENABLE_CHAP: Determine if the driver is going to configure# ISCSI node databases on the nodes with the CHAP credentials# If enabled, the CHAP secret must be provided in the credentials secret# and set to the key \"chapsecret\"# Allowed values:# \"true\" - CHAP is enabled# \"false\" - CHAP is disabled# Default value: \"false\"- name:\"X_CSI_POWERMAX_ISCSI_ENABLE_CHAP\"value:\"false\"---apiVersion:v1kind:ConfigMapmetadata:name:powermax-config-paramsnamespace:test-powermaxdata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"debug\"CSI_LOG_FORMAT:\"JSON\"Note:\n dell-csi-operator does not support the installation of CSI PowerMax ReverseProxy as a sidecar to the controller Pod. This facility is only present with dell-csi-helm-installer. Kubelet config dir path is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  ","excerpt":"Installing CSI Driver for PowerMax via Operator CSI Driver for Dell …","ref":"/csm-docs/docs/csidriver/installation/operator/powermax/","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell EMC PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use CSI Driver for Dell EMC PowerMax. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with a different number of volumes in a given namespace using the storageclass provided. To test the installation of the CSI driver, perform these tests:\n Volume clone test Volume test Snapshot test  Volume test Use this procedure to perform a volume test.\n  Create a namespace with the name test.\n  Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\n  Run the starttest.sh script and provide it with a test name. The following sample command can be used to run the 2vols test: ./starttest.sh -t 2vols -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e\nThis script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\n  Run the /stoptest.sh -t 2vols -n \u003ctest_namespace\u003e script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\n   NOTE: Helm tests have been designed assuming that users have created storageclass names like storageclass-name and storageclass-name-xfs. You can use kubectl get sc to check for the storageclass names.\n Volume clone test Use this procedure to perform a volume clone test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: bash volumeclonetest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum, and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test.  Snapshot test Use this procedure to perform a snapshot test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot of that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test.   NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass. You must update the snapshot class name in the file snap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\n Volume Expansion test Use this procedure to perform a volume expansion test.\n Create a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates one PVC, and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC, and then recalculates the checksum Cleans up all the resources that were created as part of the test  ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/docs/csidriver/installation/test/powermax/","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v2.0.0 New Features/Changes  Added support for Kubernetes v1.22. Added support for OpenShift v4.8. Added support for RKE v1.2.8. Added support for consistent config parameters across CSI drivers. Added the ability to change log level and log format of CSI driver and change them dynamically. Added the ability to configure kubelet directory path. Added the ability to enable/disable installation of resizer sidecar with driver installation. Added the ability to enable/disable installation of snapshotter sidecar with driver installation.  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Delete Volume fails with the error message: volume is part of masking view This issue is due to limitations in Unisphere and occurs when Unisphere is overloaded. Currently, there is no workaround for this but it can be avoided by ensuring that Unisphere is not overloaded during such operations. The Unisphere team is assessing a fix for this in a future Unisphere release   Getting initiators list fails with context deadline error The following error can occur during the driver installation if a large number of initiators are present on the array. There is no workaround for this but it can be avoided by deleting stale initiators on the array   Unable to update Host: A problem occurred modifying the host resource This issue occurs when the nodes do not have unique hostnames or when an IP address/FQDN with same sub-domains are used as hostnames. The workaround is to use unique hostnames or FQDN with unique sub-domains    ","excerpt":"Release Notes - CSI PowerMax v2.0.0 New Features/Changes  Added …","ref":"/csm-docs/docs/csidriver/release/powermax/","title":"PowerMax"},{"body":"   Symptoms Prevention, Resolution or Workaround     Warning about feature gates Double check that you have applied all the features to the indicated processes. Restart kubelet when remediated.   kubectl describe pod powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e indicates that the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or log in to the docker registry   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver cannot authenticate Check your secret’s username and password   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver failed to connect to the U4P because it could not verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Warning about …","ref":"/csm-docs/docs/csidriver/troubleshooting/powermax/","title":"PowerMax"},{"body":"Multi Unisphere Support Starting v1.7, the CSI PowerMax driver can communicate with multiple Unisphere for PowerMax servers to manage multiple PowerMax arrays. In order to use this feature, you must install CSI PowerMax ReverseProxy in the StandAlone mode along with the driver. For more details on how to configure the driver along with the ReverseProxy, please refer the section here\nVolume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes version 1.17 and was generally available (v1) in Kubernetes version \u003e= 1.20.\nThe CSI PowerMax driver version 1.7 supports v1beta1 snapshots on Kubernetes 1.19 and v1 snapshots on Kubernetes 1.20/1.21.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class   Note: From v1.7, the CSI PowerMax driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the helm/samples folder\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powermax-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-restore-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-snapshot-demokind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiCreating PVCs with PVCs as source This is a sample manifest for creating a PVC with another PVC as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-clone-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-pvc-demokind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP With version 1.3.0, support has been added for the unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI. To enable CHAP authentication:\n Create secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u003csymid\u003e -iscsi \u003chost iqn\u003e set chap -cred \u003chost IQN\u003e -secret \u003cCHAP secret\u003e\nWhere is the name of the iSCSI initiator of a host IQN, and is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which stored in the host initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name (experimental feature) With version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell EMC PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this experimental feature, set the following values under customDriverName in my-powermax-settings.yaml.\n Value: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template.  If enabled, the driver name is in the following format: \u003cnamespace\u003e.\u003cdriver name\u003e.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax, then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers NOTE: This is an experimental feature and should be used with extreme caution after consulting with Dell EMC Support.\nTo install multiple CSI Drivers for Dell EMC PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions that should be strictly adhered to:\n Only one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations  To install multiple CSI drivers, follow these steps:\n Create (or use) a new namespace. Ensure that all the pre-requisites are met:  powermax-creds secret is created in this namespace Optional) powermax-certs secret is created in this namespace   Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver.  Volume expansion Starting in v1.4, the CSI PowerMax driver supports the expansion of Persistent Volumes (PVs). This expansion is done online, which is when the PVC is attached to any node.\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThis is a sample manifest for a storage class that allows for Volume Expansion.\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true#Set this attribute to true if you plan to expand any PVCscreatedusingthisstorageclassparameters:SYMID:\"000000000001\"SRP:\"DEFAULT_SRP\"ServiceLevel:\"Bronze\"To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC.\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pmax-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:10Gi#Updated size from 5Gi to 10GistorageClassName:powermax-expand-scNOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of the volume, it cannot be used to shrink a volume.\nRaw block support Starting in v1.4, the CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind:StatefulSetapiVersion:apps/v1metadata:name:powermaxtestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powermaxresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy To get the maximum performance out of the CSI driver for PowerMax and Unisphere for PowerMax REST APIs, starting with v1.4 of the driver, you can deploy the optional CSI PowerMax Reverse Proxy application.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server that acts as a reverse proxy for the Unisphere for PowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and the performance of the CSI PowerMax driver.\nOptionally, you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation The CSI PowerMax Reverse Proxy can be installed in two ways:\n It can be installed as a Kubernetes deployment in the same namespace as the driver. It can be installed as a sidecar to the driver’s controller Pod.  It is also configured as a Kubernetes “NodePort” service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is an HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be an X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nHere is an example to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Using Helm installer A new section, csireverseproxy, in the my-powermax-settings.yaml file can be used to deploy and configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. If it is enabled (using the enabled parameter in the csireverseproxy section of the my-powermax-settings.yaml file), the install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name “powermax-reverseproxy”. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation for PowerMax.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two ‘%’ characters. String prefixing first ‘%’ and string suffixing second ‘%’ is used as is before and after every node identifier.\nAlso, there is a new setting, modifyHostName, which could be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u003cClusterPrefix\u003e-\u003cHostName\u003e*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1, then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], ‘-’ and ‘_’, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of the controller Pod. At any time, only one controller Pod is active(leader), and the rest are on standby. In case of a failure, one of the standby Pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two-controller Pods are ever scheduled on the same node.\nTo increase or decrease the number of controller Pods, edit the following value in values.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. We recommend not changing this unless it is really necessary. Also, if the controller count is greater than the number of available nodes (where the Pods can be scheduled), some controller Pods will remain in the Pending state\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, see the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment and driver node Daemonset. There are two new sections in the values file - controller and node - where you can specify these values separately for the controller and node Pods.\ncontroller If you want to apply nodeSelectors and tolerations for the controller Pods, edit the controller section in the values file.\nHere are some examples:\n To schedule controller Pods to worker nodes only (Default):  controller:nodeSelector:tolerations: Set the following values for controller Pods to tolerate the taint NoSchedule on master nodes:  controller:nodeSelector:tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\" Set the following values for controller Pods to be only scheduled on nodes labelled as master (node-role.kubernetes.io/master):  controller:nodeSelector:node-role.kubernetes.io/master:\"\"tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"node If you want to apply nodeSelectors and tolerations for the node Pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add and remove tolerations to this list\n# \"node\" allows to configure node specific parametersnode:# \"node.nodeSelector\" defines what nodes would be selected for Pods of node daemonset# Leave as blank to use all nodesnodeSelector:# node-role.kubernetes.io/master: \"\"# \"node.tolerations\" defines tolerations that would be applied to node daemonset# Add/Remove tolerations as per requirement# Leave as blank if you wish to not apply any tolerationstolerations:- key:\"node.kubernetes.io/memory-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/disk-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/network-unavailable\"operator:\"Exists\"effect:\"NoExecute\"Topology Support Starting from version 1.5, the CSI PowerMax driver supports topology-aware volume provisioning which helps the Kubernetes scheduler place PVCs on worker nodes that have access to the backend storage. When used with nodeSelectors which can be specified for the driver node Pods, it provides an effective way to provision applications on nodes that have access to the PowerMax array.\nAfter a successful installation of the driver, if a node Pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\n csi-powermax.dellemc.com/\\\u003carray-id\\\u003e If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.iscsi  The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\n NOTE: The Topology support does not include any customer-defined topology, that is, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\n Topology Usage To use the Topology feature, the storage classes must be modified as follows:\n volumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section  For example, a PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-fcparameters:SRP:\"SRP_1\"SYMID:\"000000000001\"ServiceLevel:\u003cServiceLevel\u003e#Insert Service Level Nameprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:WaitForFirstConsumerallowVolumeExpansion:trueallowedTopologies:- matchLabelExpressions:- key:csi-powermax.dellemc.com/000000000001values:- csi-powermax.dellemc.com- key:csi-powermax.dellemc.com/000000000001.fcvalues:- csi-powermax.dellemc.comIn the above example, if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\n A set of sample storage class definitions to enable topology-aware volume provisioning has been provided in the csi-powermax/helm/samples/storageclass folder\n For additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\n","excerpt":"Multi Unisphere Support Starting v1.7, the CSI PowerMax driver can …","ref":"/csm-docs/v1/features/powermax/","title":"PowerMax"},{"body":"The CSI Driver for Dell EMC PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, see the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell EMC PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume CSI PowerMax ReverseProxy (optional)  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell EMC PowerMax Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following requirements must be met before installing the CSI Driver for Dell EMC PowerMax:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Fibre Channel requirements iSCSI requirements Certificate validation for Unisphere REST API calls Mount propagation is enabled on container runtime that is being used Linux multipathing requirements If using Snapshot feature, satisfy all Volume Snapshot requirements  Install Helm 3 Install Helm 3 on the master node before you install the CSI Driver for Dell EMC PowerMax.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.\nFibre Channel Requirements CSI Driver for Dell EMC PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install the CSI Driver:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs.  iSCSI Requirements The CSI Driver for Dell EMC PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\n All Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell EMC PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell EMC PowerMax array. All the port group names supplied to the driver must exist on each Dell EMC PowerMax with the same name.  For more information about configuring iSCSI, you can refer Dell EMC Host Connectivity guide.\nCertificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\n  To fetch the certificate, run openssl s_client -showcerts -connect [Unisphere IP]:8443 \u003c/dev/null 2\u003e /dev/null | openssl x509 -outform PEM \u003e ca_cert.pem\nNOTE: The IP address varies for each user.\n  To create the secret, run kubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax\n  Ports in the port group There are no restrictions around how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell EMC PowerMax to ensure that you have multiple paths to your data volumes.\nLinux multipathing requirements CSI Driver for Dell EMC PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\n All the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file.  (Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20/1.21 (v1 snapshots) use v4.0.x  Volume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers to support beta Volume snapshots in Kubernetes 1.17 or later:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster, irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20 and 1.21 (v1 snapshots) use v4.0.x  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.x quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 3.0.x version of snapshotter/snapshot-controller when using Kubernetes v1.19 When using Kubernetes 1.20/1.21 it is recommended to use 4.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the `helm/secret.yaml file, point to the correct namespace, and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with PowerMax privileges.\n Create the secret by running kubectl create -f helm/secret.yaml. If you are going to install the new CSI PowerMax ReverseProxy service, create a TLS secret with the name - csireverseproxy-tls-secret which holds an SSL certificate and the corresponding private key in the namespace where you are installing the driver. Copy the default values.yaml file cd helm \u0026\u0026 cp csi-powermax/values.yaml my-powermax-settings.yaml Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml     Parameter Description Required Default     clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes “ABC”   controller Allows configuration of the controller-specific parameters. - -   node Allows configuration of the node-specific parameters. - -   tolerations Add tolerations as per requirement No -   nodeSelector Add node selectors as per requirement No -   defaultFsType Used to set the default FS type for external provisioner Yes ext4   portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only “PortGroup1, PortGroup2, PortGroup3”   storageResourcePool This parameter must mention one of the SRPs on the PowerMax array that the symmetrixID specifies. This value is used to create the default storage class. Yes “SRP_1”   serviceLevel This parameter must mention one of the Service Levels on the PowerMax array. This value is used to create the default storage class. Yes “Bronze”   skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No “True”   transportProtocol Set preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty   nodeNameTemplate Used to specify a template that will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, leave this value empty. No Empty   global This section refers to configuration options for both CSI PowerMax Driver and Reverse Proxy - -   defaultCredentialsSecret This secret name refers to:\n1. The Unisphere credentials if the driver is installed without proxy or with proxy in Linked mode.\n2. The proxy credentials if the driver is installed with proxy in StandAlone mode.\n3. The default Unisphere credentials if credentialsSecret is not specified for a management server. Yes powermax-creds   storageArrays This section refers to the list of arrays managed by the driver and Reverse Proxy in StandAlone mode. - -   storageArrayId This refers to PowerMax Symmetrix ID. Yes 000000000001   endpoint This refers to the URL of the Unisphere server managing storageArrayId Yes if Reverse Proxy mode is StandAlone https://primary-1.unisphe.re:8443   backupEndpoint This refers to the URL of the backup Unisphere server managing storageArrayId, if Reverse Proxy is installed in StandAlone mode. No https://backup-1.unisphe.re:8443   managementServers This section refers to the list of configurations for Unisphere servers managing powermax arrays. - -   endpoint This refers to the URL of the Unisphere server Yes https://primary-1.unisphe.re:8443   credentialsSecret This refers to the user credentials for endpoint No primary-1-secret   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty   limits This refers to various limits for Reverse Proxy No -   maxActiveRead This refers to the maximum concurrent READ request handled by the reverse proxy. No 5   maxActiveWrite This refers to the maximum concurrent WRITE request handled by the reverse proxy. No 4   maxOutStandingRead This refers to maximum queued READ request when reverse proxy receives more than maxActiveRead requests. No 50   maxOutStandingWrite This refers to maximum queued WRITE request when reverse proxy receives more than maxActiveWrite requests. No 50   csireverseproxy This section refers to the configuration options for CSI PowerMax Reverse Proxy - -   enabled Boolean parameter which indicates if CSI PowerMax Reverse Proxy is going to be configured and installed.\nNOTE: If not enabled, then there is no requirement to configure any of the following values. No “False”   image This refers to the image of the CSI Powermax Reverse Proxy container. Yes dellemc/csipowermax-reverseproxy:v1.3.0   tlsSecret This refers to the TLS secret of the Reverse Proxy Server. Yes csirevproxy-tls-secret   deployAsSidecar If set to true, the Reverse Proxy is installed as a sidecar to the driver’s controller pod otherwise it is installed as a separate deployment. Yes “True”   port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation Yes 2222   mode This refers to the installation mode of Reverse Proxy. It can be set to:\n1. Linked: In this mode, the Reverse Proxy communicates with a primary or a backup Unisphere managing the same set of arrays.\n2. StandAlone: In this mode, the Reverse Proxy communicates with multiple arrays managed by different Unispheres. Yes “StandAlone”    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml  Note:\n For detailed instructions on how to run the install scripts, see the readme document in the dell-csi-helm-installer folder. There are a set of samples provided here to help you configure the driver with reverse proxy This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option  Storage Classes Starting CSI PowerMax v1.6, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests has been provided in the helm/samples/storageclass folder. Please use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerMax v1.5 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nVolume Snapshot Class Starting CSI PowerMax v1.7, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the helm/samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nSample values file The following sections have useful snippets from values.yaml file which provides more information on how to configure the CSI PowerMax driver along with CSI PowerMax ReverseProxy in various modes\nCSI PowerMax driver without Proxy In this mode, the CSI PowerMax driver can only connect to a single Unisphere server. So, you just specify a list of storage arrays and the address of the Unisphere server\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"- storageArrayId:\"000000000002\"managementServers:- endpoint:https://unisphere-address:8443 Note: If you provide multiple endpoints in the list of management servers, the installer will only use the first server in the list\n CSI PowerMax driver with Proxy in Linked mode In this mode, the CSI PowerMax ReverseProxy just acts as a passthrough for the RESTAPI calls and only provides limited functionality like rate limiting, backup Unisphere server. The CSI PowerMax driver is still responsible for the authentication with the Unisphere server.\nThe first endpoint in the list of management servers is the primary Unisphere server and if you provide a second endpoint, then it will be considered as the backup Unisphere’s endpoint.\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"- storageArrayId:\"000000000002\"managementServers:- endpoint:https://primary-unisphere:8443skipCertificateValidation:falsecertSecret:primary-certlimits:maxActiveRead:5maxActiveWrite:4maxOutStandingRead:50maxOutStandingWrite:50- endpoint:https://backup-unisphere:8443#Optional# \"csireverseproxy\" refers to the subchart csireverseproxycsireverseproxy:# Set enabled to true if you want to use proxyenabled:trueimage:dellemc/csipowermax-reverseproxy:v1.3.0tlsSecret:csirevproxy-tls-secretdeployAsSidecar:trueport:2222mode:Linked Note: Since the driver is still responsible for authentication when used with Proxy in Linked mode, the credentials for both primary and backup Unisphere need to be the same.\n CSI PowerMax driver with Proxy in StandAlone mode This is the most advanced configuration which provides you the capability to connect to Multiple Unisphere servers. You can specify primary \u0026 backup Unisphere servers for each storage array. In case you have different credentials for your Unisphere servers, you can also specify different credential secrets.\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"endpoint:https://primary-1.unisphe.re:8443backupEndpoint:https://backup-1.unisphe.re:8443- storageArrayId:\"000000000002\"endpoint:https://primary-2.unisphe.re:8443backupEndpoint:https://backup-2.unisphe.re:8443managementServers:- endpoint:https://primary-1.unisphe.re:8443credentialsSecret:primary-1-secretskipCertificateValidation:falsecertSecret:primary-certlimits:maxActiveRead:5maxActiveWrite:4maxOutStandingRead:50maxOutStandingWrite:50- endpoint:https://backup-1.unisphe.re:8443credentialsSecret:backup-1-secretskipCertificateValidation:true- endpoint:https://primary-2.unisphe.re:8443credentialsSecret:primary-2-secretskipCertificateValidation:true- endpoint:https://backup-2.unisphe.re:8443credentialsSecret:backup-2-secretskipCertificateValidation:true# \"csireverseproxy\" refers to the subchart csireverseproxycsireverseproxy:# Set enabled to true if you want to use proxyenabled:trueimage:dellemc/csipowermax-reverseproxy:v1.3.0tlsSecret:csirevproxy-tls-secretdeployAsSidecar:trueport:2222mode:StandAlone Note: If the credential secret is missing from any management server details, the installer will try to use the defaultCredentialsSecret\n ","excerpt":"The CSI Driver for Dell EMC PowerMax can be deployed by using the …","ref":"/csm-docs/v1/installation/helm/powermax/","title":"PowerMax"},{"body":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell EMC PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Create secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. See the detailed documentation on how to create this secret here.\nInstall Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace. Create PowerMax credentials: Create a file called powermax-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:powermax-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards)# chapsecret: \u003cbase64 CHAP secret\u003eReplace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run the kubectl create -f powermax-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerMax using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:    Parameter Description Required Default     replicas Controls the number of controller Pods you deploy. If controller Pods are greater than the number of available nodes, excess Pods will become stuck in pending. The default is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended to all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ   X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443   X_CSI_TRANSPORT_PROTOCOL Choose what transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No -   X_CSI_MANAGED_ARRAYS List of comma-separated array id(s) which will be managed by the driver Yes -   X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Leave blank if not using reverse proxy No -   X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4   X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature see the related documentation No -   X_CSI_IG_NODENAME_TEMPLATE Template used for creating hosts on PowerMax. Example: “a-b-c-%foo%-xyz” where the text between the % symbols(foo) is replaced by the actual host name No -   X_CSI_IG_MODIFY_HOSTNAME Determines if the node plugin can rename any existing host on the PowerMax array. Use it with the node name template to rename the existing hosts No false   X_CSI_POWERMAX_DEBUG Determines if HTTP Request/Response is logged No false   Node parameters      X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature see the related documentation No false     Execute the following command to create PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.   Note: There is a new mandatory env - X_CSI_MANAGED_ARRAYS which has to be set while installing or upgrading the driver.\n CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is an optional component that can be installed along with the CSI PowerMax driver. For more details on this feature see the related documentation.\nWhen you install CSI PowerMax ReverseProxy, dell-csi-operator will create a Deployment and ClusterIP service as part of the installation\nNote - To use the ReverseProxy with the CSI PowerMax driver, the ReverseProxy service should be created before you install the CSIPowerMax driver.\nPre-requisites Create a TLS secret that holds an SSL certificate and a private key which is required by the reverse proxy server. Use a tool such as openssl to generate this secret using the example below:\n openssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n powermax tls revproxy-certs --cert=tls.crt --key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec  tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs config : This section contains the details of the Reverse Proxy configuration mode : This value is set to Linked by default. Do not change this value linkConfig : This section contains the configuration of the Linked mode primary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if the primary Unisphere is unreachable url : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false standAloneConfig : This section contains the configuration of the StandAlone mode. Refer to the sample below for the detailed config   Note: Only one of the Linked or StandAlone config needs to be supplied. The appropriate mode needs to set in the spec as well.\n Here is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion:storage.dell.com/v1kind:CSIPowerMaxRevProxymetadata:name:powermax-reverseproxy# \u003c- Name of the CSIPowerMaxRevProxy objectnamespace:test-powermax# \u003c- Set the namespace to where you will install the CSI PowerMax driverspec:# Image for CSI PowerMax ReverseProxyimage:dellemc/csipowermax-reverseproxy:v1.3.0# \u003c- CSI PowerMax Reverse Proxy image imagePullPolicy:Always# TLS secret which contains SSL certificate and private key for the Reverse Proxy servertlsSecret:csirevproxy-tls-secretconfig:mode:LinkedlinkConfig:primary:url:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:true# This setting determines if client side Unisphere certificate validation is to be skippedcertSecret:\"\"# Provide this value if skipCertificateValidation is set to falsebackup:# This is an optional field and lets you configure a backup unisphere which can be used by proxy serverurl:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:truestandAloneConfig:# Set mode to \"StandAlone\" in order to use this configstorageArrays:- storageArrayId:\"000000000001\"# Unisphere server managing the PowerMax arrayprimaryURL:https://unisphere-1-addr:8443# proxyCredentialSecrets are used by the clients of the proxy to connect to it# If using proxy in the stand alone mode, then the driver must be provided the# same secret.# The format of the proxy credential secret are exactly the same as the unisphere credential secret# For using the proxy with the driver, use the same proxy credential secrets for# all the managed storage arraysproxyCredentialSecrets:- proxy-creds- storageArrayId:\"000000000002\"primaryURL:https://unisphere-2-addr:8443# An optional backup Unisphere server managing the same array# This can be used by the proxy to fall back to in case the primary# Unisphere is inaccessible temporarilybackupURL:unisphere-3-addr:8443proxyCredentialSecrets:- proxy-credsmanagementServers:- url:https://unisphere-1-addr:8443# Secret containing the credentials of the Unisphere serverarrayCredentialSecret:unsiphere-1-credsskipCertificateValidation:true- url:https://unisphere-2-addr:8443arrayCredentialSecret:unsiphere-2-credsskipCertificateValidation:true- url:https://unisphere-3-addr:8443arrayCredentialSecret:unsiphere-3-credsskipCertificateValidation:trueInstallation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service:\nkubectl create -f powermax_reverseproxy.yaml  You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u003cnamespace\u003e kubectl get svc -n \u003cnamespace\u003e  There is a new sample file - powermax_revproxy_standalone_with_driver.yaml present in the samples folder which enables installation of CSI PowerMax ReverseProxy in StandAlone mode along with the CSI PowerMax driver. This mode enables the CSI PowerMax driver to connect to multiple Unisphere servers for managing multiple PowerMax arrays. Please follow the same steps described above to install ReverseProxy with this new sample file.\n Note: dell-csi-operator doesn’t support the installation of CSI PowerMax ReverseProxy as a sidecar to the controller pod. This facility is only present with the dell-csi-helm-installer\n ","excerpt":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell …","ref":"/csm-docs/v1/installation/operator/powermax/","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell EMC PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use the CSI Driver for Dell EMC PowerMax. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with a different number of volumes in a given namespace using the storageclass provided. To test the installation of the CSI driver, perform these tests:\n Volume clone test Volume test Snapshot test  Volume test Use this procedure to perform a volume test.\n  Create a namespace with the name test.\n  Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\n  Run the starttest.sh script and provide it with a test name. The following is a sample command that can be used to run the 2vols test: ./starttest.sh -t 2vols -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e\nThis script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\n  Run the /stoptest.sh -t 2vols -n \u003ctest_namespace\u003e script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\n   NOTE: Helm tests have been designed assuming that users are created storageclass names like  and . You can use kubectl get sc to check for the storageclass names.\n Volume clone test Use this procedure to perform a volume clone test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: bash volumeclonetest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum, and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test.  Snapshot test Use this procedure to perform a snapshot test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot of that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test.   NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass. You must update the snapshot class name in the file betaSnap1.yaml/snap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\n Volume Expansion test Use this procedure to perform a volume expansion test.\n Create a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates one PVC, and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC, and then recalculates the checksum Cleans up all the resources that were created as part of the test  ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v1/installation/test/powermax/","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v1.7.0 New Features/Changes  Removed Volume Snapshotclass from helm template Added support for Multi Unisphere Added support for Kubernetes v1.21 Added support for Docker MKE 3.4.0 Added support for RHEL 8.4  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Delete Volume fails with the error message: volume is part of masking view This issue is due to limitations in Unisphere and occurs when Unisphere is overloaded. Currently, there is no workaround for this but can be avoided by making sure Unisphere is not overloaded during such operations. The Unisphere team is assessing a fix for this in a future Unisphere release   Getting initiators list fails with context deadline error The following error can occur during the driver installation if a large number of initiators are present on the array. There is no workaround for this but can be avoided by deleting stale initiators on the array    ","excerpt":"Release Notes - CSI PowerMax v1.7.0 New Features/Changes  Removed …","ref":"/csm-docs/v1/release/powermax/","title":"PowerMax"},{"body":"   Symptoms Prevention, Resolution or Workaround     Warning about feature gates Double check that you have applied all the features to the indicated processes. Restart kubelet when remediated.   kubectl describe pod powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e indicates the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show the driver cannot authenticate Check your secret’s username and password   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show the driver failed to connect to the U4P because it couldn’t verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Warning about …","ref":"/csm-docs/v1/troubleshooting/powermax/","title":"PowerMax"},{"body":"Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes version 1.17 and was generally available (v1) in Kubernetes version 1.20.\nThe CSI PowerMax driver version 1.6 supports v1beta1 snapshots on Kubernetes 1.18/1.19 and v1 snapshots on Kubernetes 1.20.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of the CSI PowerMax 1.6 driver, a Volume Snapshot Class is created. This is the only Volume Snapshot Class you will need and there is no need to create any other Volume Snapshot Class.\nThe following is the manifest for the Volume Snapshot Class created during installation (using the default driver name):\napiVersion: snapshot.storage.k8s.io/v1 deletionPolicy: Delete kind: VolumeSnapshotClass metadata: name: powermax-snapclass driver: csi-powermax.dellemc.com Note: The apiVersion for VolumeSnapshotClass object created on clusters running Kubernetes versions \u003c 1.20 will be v1beta1\nCreating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powermax-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-restore-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-snapshot-demokind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiCreating PVCs with PVCs as source This is a sample manifest for creating a PVC with another PVC as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-clone-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-pvc-demokind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP With version 1.3.0, support has been added for unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI. To enable CHAP authentication:\n Create secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u003csymid\u003e -iscsi \u003chost iqn\u003e set chap -cred \u003chost IQN\u003e -secret \u003cCHAP secret\u003e\nWhere is the name of the iSCSI initiator of a host IQN, and is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which stored in the host initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name (experimental feature) With version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell EMC PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this experimental feature, set the following values under customDriverName in my-powermax-settings.yaml.\n Value: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template.  If enabled, the driver name is in the following format: \u003cnamespace\u003e.\u003cdriver name\u003e.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax , then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers NOTE: This is an experimental feature and should be used with extreme caution after consulting with Dell EMC Support.\nTo install multiple CSI Drivers for Dell EMC PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions which should be strictly adhered to:\n Only one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations  To install multiple CSI drivers, follow these steps:\n Create (or use) a new namespace. Ensure that all the pre-requisites are met:  powermax-creds secret is created in this namespace Optional) powermax-certs secret is created in this namespace   Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver.  Volume expansion Starting in v1.4, the CSI PowerMax driver supports expansion of Persistent Volumes (PVs). This expansion is done online, that is, when the PVC is attached to any node.\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThis is a sample manifest for a storage class which allows for Volume Expansion.\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true#Set this attribute to true if you plan to expand any PVCscreatedusingthisstorageclassparameters:SYMID:\"000000000001\"SRP:\"DEFAULT_SRP\"ServiceLevel:\"Bronze\"To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC.\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pmax-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:10Gi#Updated size from 5Gi to 10GistorageClassName:powermax-expand-scNOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of volume, it cannot be used to shrink a volume.\nRaw block support Starting in v1.4, CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind:StatefulSetapiVersion:apps/v1metadata:name:powermaxtestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powermaxresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy To get the maximum performance out of the CSI driver for PowerMax and Unisphere forPowerMax REST APIs, starting with v1.4 of the driver, you can deploy the optional CSI PowerMax Reverse Proxy application.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server which acts as a reverse proxy for the Unisphere forPowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and performance of the CSI PowerMax driver.\nOptionally you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation CSI PowerMax Reverse Proxy is installed as a Kubernetes deployment in the same namespace as the driver.\nIt is also configured as a Kubernetes “NodePort” service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is a HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be a X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nHere is an example to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Using Helm installer A new section, csireverseproxy, in the my-powermax-settings.yaml file can be used to deploy and configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. If it is enabled (using the enabled parameter in the csireverseproxy section of the my-powermax-settings.yaml file), the install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name “powermax-reverseproxy”. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two ‘%’ characters. String prefixing first ‘%’ and string suffixing second ‘%’ is used as is before and after every node identifier.\nAlso, there is a new setting, modifyHostName, which could be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u003cClusterPrefix\u003e-\u003cHostName\u003e*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1 , then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], ‘-’ and ‘_’, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of controller Pod. At any time, only one controller Pod is active(leader), and the rest are on standby. In case of a failure, one of the standby Pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two controller Pods are ever scheduled on the same node.\nTo increase or decrease the number of controller Pods, edit the following value in values.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. We recommend not changing this unless it is really necessary. Also, if the controller count is greater than the number of available nodes (where the Pods can be scheduled), some controller Pods will remain in the Pending state\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, see the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment and driver node Daemonset. There are two new sections in the values file - controller and node - where you can specify these values separately for the controller and node Pods.\ncontroller If you want to apply nodeSelectors and tolerations for the controller Pods, edit the controller section in the values file.\nHere are some examples:\n To schedule controller Pods to worker nodes only (Default):  controller:nodeSelector:tolerations: Set the following values for controller Pods to tolerate the taint NoSchedule on master nodes:  controller:nodeSelector:tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\" Set the following values for controller pods to be only scheduled on nodes labelled as master (node-role.kubernetes.io/master):  controller:nodeSelector:node-role.kubernetes.io/master:\"\"tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"node If you want to apply nodeSelectors and tolerations for the node Pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add and remove tolerations to this list\n# \"node\" allows to configure node specific parametersnode:# \"node.nodeSelector\" defines what nodes would be selected for Pods of node daemonset# Leave as blank to use all nodesnodeSelector:# node-role.kubernetes.io/master: \"\"# \"node.tolerations\" defines tolerations that would be applied to node daemonset# Add/Remove tolerations as per requirement# Leave as blank if you wish to not apply any tolerationstolerations:- key:\"node.kubernetes.io/memory-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/disk-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/network-unavailable\"operator:\"Exists\"effect:\"NoExecute\"Topology Support Starting from version 1.5, the CSI PowerMax driver supports topology-aware volume provisioning which helps Kubernetes scheduler place PVCs on worker nodes which have access to backend storage. When used with nodeSelectors which can be specified for the driver node Pods, it provides an effective way to provision applications on nodes which have access to the PowerMax array.\nAfter a successful installation of the driver, if a node Pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\n csi-powermax.dellemc.com/\\\u003carray-id\\\u003e If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.iscsi  The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\n NOTE: The Topology support does not include any customer-defined topology, that is, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\n Topology Usage To use the Topology feature, the storage classes must be modified as follows:\n volumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section  For example, a PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-fcparameters:SRP:\"SRP_1\"SYMID:\"000000000001\"ServiceLevel:\u003cServiceLevel\u003e#Insert Service Level Nameprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:WaitForFirstConsumerallowVolumeExpansion:trueallowedTopologies:- matchLabelExpressions:- key:csi-powermax.dellemc.com/000000000001values:- csi-powermax.dellemc.com- key:csi-powermax.dellemc.com/000000000001.fcvalues:- csi-powermax.dellemc.comIn the above example if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\n NOTE: The storage classes created during the driver installation (via Helm) do not contain any topology keys and have the volumeBindingMode set to Immediate. A set of sample storage class definitions to enable topology -aware volume provisioning has been provided in the csi-powermax/helm/samples/storageclass folder\n For additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\nIf you are using dell-csi-operator to create storage classes while installing the CSI PowerMax 1.5 driver, you can set the allowedTopologies value appropriately. volumeBindingMode is set to WaitForFirstConsumer if not specified explicitly.\n","excerpt":"Volume Snapshot Feature The Volume Snapshot feature was introduced in …","ref":"/csm-docs/v2/features/powermax/","title":"PowerMax"},{"body":"The CSI Driver for Dell EMC PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, see the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the powermax namespace:\n CSI Driver for Dell EMC PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace powermax:\n CSI Driver for Dell EMC PowerMax Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following requirements must be met before installing the CSI Driver for Dell EMC PowerMax:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Fibre Channel requirements iSCSI requirements Certificate validation for Unisphere REST API calls Configure Mount propagation on container runtime (that is, Docker) Linux multipathing requirements Volume Snapshot requirements  Install Helm 3 Install Helm 3 on the master node before you install the CSI Driver for Dell EMC PowerMax.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.\nFibre Channel Requirements CSI Driver for Dell EMC PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install the CSI Driver:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs.  iSCSI Requirements The CSI Driver for Dell EMC PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\n All Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell EMC PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell EMC PowerMax array. All the port groups names supplied to the driver must exist on each Dell EMC PowerMax with the same name.  For information about configuring iSCSI, see Dell EMC PowerMax documentation on Dell EMC Support.\nCertificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\n  To fetch the certificate, run openssl s_client -showcerts -connect [Unisphere IP]:8443 \u003c/dev/null\u003e /dev/null | openssl x509 -outform PEM \u003e ca_cert.pem\nNOTE: The IP address varies for each user.\n  To create the secret, run kubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax\n  Ports in port group There are no restrictions around how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell EMC PowerMax to ensure that you have multiple paths to your data volumes.\nConfigure Mount Propagation on Container Runtime You must configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerMax. The following steps explain how to do this with Docker. If you use another container runtime please follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes. Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  NOTE: Some distribution, like Ubuntu, already has MountFlags set by default.\nLinux multipathing requirements CSI Driver for Dell EMC PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\n All the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file.  Volume Snapshot Requirements Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  Volume Snapshot Controller Starting with beta Volume Snapshots in Kubernetes 1.17, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.3 quay.io/k8scsi/csi-snapshotter:v4.0.0   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the `helm/secret.yaml file, point to the correct namespace and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with PowerMax priviledges.\n Create the secret by running kubectl create -f helm/secret.yaml If you are going to install the new CSI PowerMax ReverseProxy service, create a TLS secret with the name - csireverseproxy-tls-secret which holds an SSL certificate and the corresponding private key in the namespace where you are installing the driver. Copy the default values.yaml file `cd helm \u0026\u0026 cp csi-powermax/values.yaml my-powermax-settings.yaml Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml     Parameter Description Required Default     unisphere Specifies the URL of the Unisphere for PowerMax server. If using the CSI PowerMax Reverse Proxy, leave this value unchanged at https://127.0.0.1:8443. Yes “https://127.0.0.1:8443”   clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes “ABC”   controller Allows configuration of the controller-specific parameters. - -   node Allows configuration of the node-specific parameters. - -   tolerations Add tolerations as per requirement No -   nodeSelector Add node selectors as per requirement No -   defaultFsType Used to set the default FS type for external provisioner Yes ext4   portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only “PortGroup1, PortGroup2, PortGroup3”   arrayWhitelist List of comma-separated array IDs. If this parameter remains empty, the driver manages all the arrays that are managed by the Unisphere instance that is configured for the driver. Specify the IDs of the arrays that you want to manage, using the driver. No Empty   symmetrixID Specify a Dell EMC PowerMax array that the driver manages. This value is used to create a default storage class. Yes “000000000000”   storageResourcePool This parameter must mention one of the SRPs on the PowerMax array that the symmetrixID specifies. This value is used to create the default storage class. Yes “SRP_1”   serviceLevel This parameter must mention one of the Service Levels on the PowerMax array. This value is used to create the default storage class. Yes “Bronze”   skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No “True”   transportProtocol Set preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty   nodeNameTemplate Used to specify a template which will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, then leave this value empty. No Empty   csireverseproxy This section refers to configuration options for CSI PowerMax Reverse Proxy - -   enabled Boolean parameter which indicates if CSI PowerMax Reverse Proxy is going to be configured and installed.\nNOTE: If not enabled, then there is no requirement to configure any of the following values. No “False”   port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation No 2222   primary Mandatory section for Reverse Proxy - -   unisphere This must specify the URL of the Unisphere for PowerMax server Yes, if using Reverse Proxy “https://0.0.0.0:8443”   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. It is set to true by default. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty   backup Optional section for Reverse Proxy. Specify Unisphere server address which the Reverse Proxy can fall back to if the primary Unisphere is unreachable or unresponsive.\nNOTE: If you do not want to specify a backup Unisphere server, then remove the backup section from the file - -   unisphere Specify the IP address of the Unisphere for PowerMax server which manages the arrays being used by the CSI driver No “https://0.0.0.0:8443”   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. It is set to true by default. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server No Empty    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml  Note:\n For detailed instructions on how to run the install scripts, see the readme document in the dell-csi-helm-installer folder. This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option  Storage Classes Starting in CSI PowerMax v1.6, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests has been provided in the helm/samples folder. Please use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerMax v1.5 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n","excerpt":"The CSI Driver for Dell EMC PowerMax can be deployed by using the …","ref":"/csm-docs/v2/installation/helm/powermax/","title":"PowerMax"},{"body":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell EMC PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire the lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Create secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. See the detailed documentation on how to create this secret here.\nInstall Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace. Create PowerMax credentials: Create a file called powermax-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:powermax-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards)# chapsecret: \u003cbase64 CHAP secret\u003eReplace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run the kubectl create -f powermax-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerMax using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:    Parameter Description Required Default     replicas Controls the number of controller Pods you deploy. If controller Pods are greater than the number of available nodes, excess Pods will become stuck in pending. The default is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended to all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ   X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443   X_CSI_TRANSPORT_PROTOCOL Choose what transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No -   X_CSI_POWERMAX_ARRAYS List of comma-separated array id(s) which will be managed by the driver No -   X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Leave blank if not using reverse proxy No -   X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4   X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature see the related documentation No -   X_CSI_IG_NODENAME_TEMPLATE Template used for creating hosts on PowerMax. Example: “a-b-c-%foo%-xyz” where the text between the % symbols(foo) is replaced by the actual host name No -   X_CSI_IG_MODIFY_HOSTNAME Determines if the node plugin can rename any existing host on the PowerMax array. Use it with the node name template to rename the existing hosts No false   X_CSI_POWERMAX_DEBUG Determines if HTTP Request/Response is logged No false   Node parameters      X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature see the related documentation No false   StorageClass parameters      SYMID Symmetrix ID Yes 000000000001   SRP Storage Resource Pool Name Yes DEFAULT_SRP   ServiceLevel Service Level No Bronze   FsType File System type (xfs/ext4) xfs    allowVolumeExpansion After the allowed topology is modified in storage class, Pods/and volumes will always be scheduled on nodes that have access to the storage No false   allowedTopologies:key This is to enable topology to allow Pods/and volumes to always be scheduled on nodes that have access to the storage. You need to specify the PowerMax array ID and append .fc or .iscsi at the end of it to specify a protocol. For more details on this feature see the related documentation No “000000000001”     Execute the following command to create PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.  CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is an optional component which can be installed along with the CSI PowerMax driver. For more details on this feature see the related documentation.\nWhen you install CSI PowerMax ReverseProxy, dell-csi-operator will create a Deployment and ClusterIP service as part of the installation\nNote - To use the ReverseProxy with CSI PowerMax driver, the ReverseProxy service should be created before you install the CSIPowerMax driver.\nPre-requisites Create a TLS secret which holds a SSL certificate and a private key which is required by the reverse proxy server. Use a tool such as openssl to generate this secret using the example below:\n openssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n powermax tls revproxy-certs --cert=tls.crt --key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs\nconfig : This section contains the details of the Reverse Proxy configuration\nmode : This value is set to Linked by default. Do not change this value\nlinkConfig : This section contains the configuration of the Linked mode\nprimary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if the primary Unisphere is unreachable\nurl : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false\nHere is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion:storage.dell.com/v1kind:CSIPowerMaxRevProxymetadata:name:powermax-reverseproxy# \u003c- Name of the CSIPowerMaxRevProxy objectnamespace:test-powermax# \u003c- Set the namespace to where you will install the CSI PowerMax driverspec:# Image for CSI PowerMax ReverseProxyimage:dellemc/csipowermax-reverseproxy:v1.0.0.000R# \u003c- CSI PowerMax Reverse Proxy image imagePullPolicy:Always# TLS secret which contains SSL certificate and private key for the Reverse Proxy servertlsSecret:csirevproxy-tls-secretconfig:# Mode for the proxy - only supported mode for now is \"Linked\"mode:LinkedlinkConfig:primary:url:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:true# This setting determines if client side Unisphere certificate validation is to be skippedcertSecret:\"\"# Provide this value if skipCertificateValidation is set to falsebackup:# This is an optional field and lets you configure a backup unisphere which can be used by proxy serverurl:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:trueInstallation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service:\nkubectl create -f powermax_reverseproxy.yaml  You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u003cnamespace\u003e kubectl get svc -n \u003cnamespace\u003e  ","excerpt":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell …","ref":"/csm-docs/v2/installation/operator/powermax/","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell EMC PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use the CSI Driver for Dell EMC PowerMax. These examples automate the creation of Pods using the default storage classes that were created during installation. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with different number of volumes. To test the installation of the CSI driver, perform these tests:\n Volume clone test Volume test Snapshot test  Volume test Use this procedure to perform a volume test.\n  Create a namespace with the name test.\n  Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\n  Run the starttest.sh script and provide it with a test name. The following is a sample command that can be used to run the 2vols test: ./starttest.sh -t 2vols -n test\nThis script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\n  Run the ./stoptest.sh -t 2vols -n test script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\n   NOTE: Helm tests have been designed assuming that users are using the default storageclass names (powermax and powermax-xfs). If your storageclass names differ from the default values, such as when deploying with the Operator, update the templates in 2vols accordingly (located in test/helm/2vols/templates/ directory). You can use kubectl get sc to check for the storageclass names.\n Volume clone test Use this procedure to perform a volume clone test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: bash volumeclonetest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test.  Snapshot test Use this procedure to perform a snapshot test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot on that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test.   NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass which is created by the Helm-based installer. If you have an operator-based deployment, the name of the snapshot class will differ. You must update the snapshot class name in the file betaSnap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\n Volume Expansion test Use this procedure to perform a volume expansion test.\n Create a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates one PVC and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC and then recalculates the checksum Cleans up all the resources that were created as part of the test  ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v2/installation/test/powermax/","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v1.6.0 New Features/Changes  Added support for Kubernetes v1.20 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 8.3 Removed storage classes from helm template  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.18.5+   Delete Volume fails with error message: volume is part of masking view This issue is due to limitations in Unisphere and occurs when Unisphere is overloaded. Currently, there is no workaround for this but can be avoided by making sure Unisphere is not overloaded during such operations. The Unisphere team is assessing a fix for this in a future Unisphere release   Driver installation warning: “OpenShift version 4.7, is newer than the version that has been tested. Latest tested version is: 4.6” Ignore this warning and continue with the installation. v1.6.0 release of the driver supports OpenShift 4.6/4.7 .    ","excerpt":"Release Notes - CSI PowerMax v1.6.0 New Features/Changes  Added …","ref":"/csm-docs/v2/release/powermax/","title":"PowerMax"},{"body":"   Symptoms Prevention, Resolution or Workaround     Warning about feature gates Double check that you have applied all the features to the indicated processes. Restart kubelet when remediated.   kubectl describe pod powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e indicates the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs shows the driver cannot authenticate Check your secret’s username and password   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs shows the driver failed to connect to the U4P because it couldn’t verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Warning about …","ref":"/csm-docs/v2/troubleshooting/powermax/","title":"PowerMax"},{"body":"Volume Snapshot Feature The CSI PowerMax driver supports beta snapshots. Driver versions prior to version 1.4 supported alpha snapshots.\nThe Volume Snapshots feature in Kubernetes has moved to beta in Kubernetes version 1.17. It was an alpha feature in earlier releases (1.13 onwards). The snapshot API version has changed from v1alpha1 to v1beta1 with this migration.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  Volume Snapshot Class Starting CSI PowerMax 1.4 driver, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class you will need and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation (using the default driver name):\napiVersion: snapshot.storage.k8s.io/v1beta1 deletionPolicy: Delete kind: VolumeSnapshotClass metadata: name: powermax-snapclass driver: csi-powermax.dellemc.com Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: pmax-snapshot-demo namespace: test spec: volumeSnapshotClassName: powermax-snapclass source: persistentVolumeClaimName: pmax-pvc-demo Once the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmax-restore-pvc-demo namespace: test spec: storageClassName: powermax dataSource: name: pmax-snapshot-demo kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Creating PVCs with PVCs as source This is a sample manifest for creating a PVC with another PVC as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmax-clone-pvc-demo namespace: test spec: storageClassName: powermax dataSource: name: pmax-pvc-demo kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi iSCSI CHAP With version 1.3.0, support has been added for unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI. To enable CHAP authentication:\n Create secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u003csymid\u003e -iscsi \u003chost iqn\u003e set chap -cred \u003chost IQN\u003e -secret \u003cCHAP secret\u003e\nWhere is the name of the iSCSI initiator of a host IQN, and is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which stored in the host initiator initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name (Experimental feature) With version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell EMC PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this experimental feature, set the following values under customDriverName in my-powermax-settings.yaml.\n Value: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template.  If enabled, the driver name is in the following format: \u003cnamespace\u003e.\u003cdriver name\u003e.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax , then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers NOTE: This is an experimental feature and should be used with extreme caution after consulting with Dell EMC Support.\nTo install multiple CSI Drivers for Dell EMC PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions which should be strictly adhered to:\n Only one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations  To install multiple CSI drivers, follow these steps:\n Create (or use) a new namespace. Ensure that all the pre-requisites are met:  powermax-creds secret is created in this namespace Optional) powermax-certs secret is created in this namespace   Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver.  Volume expansion Starting with v1.4, the CSI PowerMax driver supports expansion of Persistent Volumes (PVs). This expansion is done online, that is, when the PVC is attached to any node.\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThis is a sample manifest for a storage class which allows for Volume Expansion.\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-expand-sc annotations: storageclass.beta.kubernetes.io/is-default-class: false provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true #Set this attribute to true if you plan to expand any PVCs created using this storage class parameters: SYMID: \"000000000001\" SRP: \"DEFAULT_SRP\" ServiceLevel: \"Bronze\" To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5Gi, then you can resize it to 10 Gi by updating the PVC.\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pmax-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 10Gi #Updated size from 5Gi to 10Gi storageClassName: powermax-expand-sc NOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of volume, it cannot be used to shrink a volume.\nRaw block support Starting v1.4, CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powermaxtest namespace: {{ .Values.namespace }} spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: powermax resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy To get the maximum performance out of the CSI driver for PowerMax and Unisphere forPowerMax REST APIs, starting with v1.4 of the driver, you can deploy the optional CSI PowerMax Reverse Proxy application.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server which acts as a reverse proxy for the Unisphere forPowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and performance of the CSI PowerMax driver.\nOptionally you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation CSI PowerMax Reverse Proxy is installed as a Kubernetes deployment in the same namespace as the driver.\nIt is also configured as a Kubernetes “NodePort” service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is a HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be a X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nHere is an example to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Using Helm installer A new section, csireverseproxy, in the my-powermax-settings.yaml file can be used to deploy and configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. If it is enabled (using the enabled parameter in the csireverseproxy section of the my-powermax-settings.yaml file), the install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name “powermax-reverseproxy”. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two ‘%’ characters. String prefixing first ‘%’ and string suffixing second ‘%’ is used as is before and after every node identifier.\nAlso, there is a new setting, modifyHostName, which could be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u003cClusterPrefix\u003e-\u003cHostName\u003e*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1 , then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], ‘-’ and ‘_’, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in values.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. We recommend to not change this unless really required. Also, if controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in Pending state\n If you’re using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment \u0026 driver node Daemonset. There are two new sections in the values file - controller \u0026 node - where you can specify these values separately for the controller and node pods.\ncontroller If you want to apply nodeSelectors \u0026 tolerations for the controller pods, edit the controller section in the values file.\nHere are some examples:\n To schedule controller pods to worker nodes only (Default):  controller: nodeSelector: tolerations:  Set the following values for controller pods to tolerate the taint NoSchedule on master nodes:  controller: nodeSelector: tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\"  Set the following values for controller pods to be only scheduled on nodes labelled as master (node-role.kubernetes.io/master):  controller: nodeSelector: node-role.kubernetes.io/master: \"\" tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" node If you want to apply nodeSelectors \u0026 tolerations for the node pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add/remove tolerations to this list\n# \"node\" allows to configure node specific parameters node: # \"node.nodeSelector\" defines what nodes would be selected for pods of node daemonset # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \"\" # \"node.tolerations\" defines tolerations that would be applied to node daemonset # Add/Remove tolerations as per requirement # Leave as blank if you wish to not apply any tolerations tolerations: - key: \"node.kubernetes.io/memory-pressure\" operator: \"Exists\" effect: \"NoExecute\" - key: \"node.kubernetes.io/disk-pressure\" operator: \"Exists\" effect: \"NoExecute\" - key: \"node.kubernetes.io/network-unavailable\" operator: \"Exists\" effect: \"NoExecute\" Topology Support Starting from version 1.5, the CSI PowerMax driver supports Topology aware Volume Provisioning which helps Kubernetes scheduler place PVCs on worker nodes which have access to backend storage. When used in conjunction with nodeSelectors which can be specified for the driver node pods, it provides an effective way to provision applications on nodes which have access to the PowerMax array.\nAfter a successful installation of the driver, if a node pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\n csi-powermax.dellemc.com/\\\u003carray-id\\\u003e If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.iscsi  The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\n NOTE: The Topology support does not include any customer defined topology i.e. users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\n Topology Usage In order to utilize the Topology feature, the storage classes must be modified as follows:\n volumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section  For e.g. - A PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-fc parameters: SRP: \"SRP_1\" SYMID: \"000000000001\" ServiceLevel: \u003cService Level\u003e #Insert Service Level Name provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true allowedTopologies: - matchLabelExpressions: - key: csi-powermax.dellemc.com/000000000001 values: - csi-powermax.dellemc.com - key: csi-powermax.dellemc.com/000000000001.fc values: - csi-powermax.dellemc.com In the above example if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\n NOTE: The storage classes created during the driver installation (via Helm) do not contain any topology keys and have the volumeBindingMode set to Immediate. A set of sample storage class definitions to enable topology aware volume provisioning has been provided in the csi-powermax/helm/samples/storageclass folder\n For additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\nIf you are using dell-csi-operator to create storage classes while installing the CSI PowerMax 1.5 driver, you can set the allowedTopologies value appropriately. volumeBindingMode is set to WaitForFirstConsumer if not specified explicitly.\n","excerpt":"Volume Snapshot Feature The CSI PowerMax driver supports beta …","ref":"/csm-docs/v3/features/powermax/","title":"PowerMax"},{"body":"The CSI Driver for Dell EMC PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the powermax namespace:\n CSI Driver for Dell EMC PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace powermax:\n CSI Driver for Dell EMC PowerMax Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following requirements must be met before installing the CSI Driver for Dell EMC PowerMax:\n Install Kubernetes (1.17, 1.18, 1.19) or OpenShift (4.5 or 4.6) Install Helm 3 Fibre Channel requirements iSCSI requirements Certificate validation for Unisphere REST API calls Configure Mount propagation on container runtime (that is, Docker) Linux multipathing requirements Volume Snapshot requirements  Install Helm 3 Install Helm 3 on the master node before you install the CSI Driver for Dell EMC PowerMax.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.\nFibre Channel Requirements CSI Driver for Dell EMC PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install the CSI Driver:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs.  iSCSI Requirements The CSI Driver for Dell EMC PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\n All Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell EMC PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell EMC PowerMax array. All the port groups names supplied to the driver must exist on each Dell EMC PowerMax with the same name.  For information about configuring iSCSI, see Dell EMC PowerMax documentation on Dell EMC Support.\nCertificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\n  To fetch the certificate, run openssl s_client -showcerts -connect [Unisphere IP]:8443 \u003c/dev/null\u003e /dev/null | openssl x509 -outform PEM \u003e ca_cert.pem\nNOTE: The IP address varies for each user.\n  To create the secret, run kubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax\n  Ports in port group There are no restrictions around how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell EMC PowerMax to ensure that you have multiple paths to your data volumes.\nConfigure Mount Propagation on Container Runtime You must configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerMax. The following steps explain how to do this with Docker. If you use another container runtime please follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes. Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  Linux multipathing requirements CSI Driver for Dell EMC PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\n All the nodes must have Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file.  Volume Snapshot requirements Volume Snapshot CRDs The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\nAlternately, you can install the CRDs by supplying the option –snapshot-crd while installing the driver using the csi-install.sh script.\nVolume Snapshot Controller Starting with the beta Volume Snapshots, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests available on GitHub.\nNOTE:\n The manifests available on the GitHub repository for snapshot controller will install v3.0.2 of the snapshotter controller - (k8s.gcr.io/sig-storage/snapshot-controller:v3.0.2) Dell EMC recommends using the v3.0.2 image of the CSI external snapshotter - (k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2) The CSI external-snapshotter sidecar is still installed with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the `helm/secret.yaml, point to the correct namespace and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with PowerMax priviledges.\n Create the secret by running kubectl create -f helm/secret.yaml If you are going to install the new CSI PowerMax ReverseProxy service, create a TLS secret with the name - csireverseproxy-tls-secret which holds a SSL certificate and the corresponding private key in the namespace where you are installing the driver. Copy the default values.yaml file `cd helm \u0026\u0026 cp csi-powermax/values.yaml my-powermax-settings.yaml Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml     Parameter Description Required Default     unisphere Specifies the URL of the Unisphere for PowerMax server. If using the CSI PowerMax Reverse Proxy, leave this value unchanged at https://127.0.0.1:8443. Yes “https://127.0.0.1:8443”   clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes “ABC”   controller Allows configuration of the controller-specific parameters. - -   node Allows configuration of the node-specific parameters. - -   tolerations Add tolerations as per requirement No -   nodeSelector Add node selectors as per requirement No -   defaultFsType Used to set the default FS type for external provisioner Yes ext4   portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only “PortGroup1, PortGroup2, PortGroup3”   arrayWhitelist List of comma-separated array IDs. If this parameter remains empty, the driver manages all the arrays that are managed by the Unisphere instance that is configured for the driver. Specify the IDs of the arrays that you want to manage, using the driver. No Empty   symmetrixID Specify a Dell EMC PowerMax array that the driver manages. This value is used to create a default storage class. Yes “000000000000”   storageResourcePool Must mention one of the SRPs on the PowerMax array that the symmetrixID specifies. This value is used to create the default storage class. Yes “SRP_1”   serviceLevel This parameter must mention one of the Service Levels on the PowerMax array. This value is used to create the default storage class. Yes “Bronze”   skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No “True”   transportProtocol Set preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty   nodeNameTemplate Used to specify a template which will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, then leave this value empty. No Empty   csireverseproxy This section refers to configuration options for CSI PowerMax Reverse Proxy - -   enabled Boolean parameter which indicates if CSI PowerMax Reverse Proxy is going to be configured and installed.\nNOTE: If not enabled, then there is no requirement to configure any of the following values. No “False”   port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation No 2222   primary Mandatory section for Reverse Proxy - -   unisphere This must specify the URL of the Unisphere for PowerMax server Yes, if using Reverse Proxy “https://0.0.0.0:8443”   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. It is set to true by default. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty   backup Optional section for Reverse Proxy. Specify Unisphere server address which the Reverse Proxy can fall back to if the primary Unisphere is unreachable or unresponsive.\nNOTE: If you do not want to specify a backup Unisphere server, then remove the backup section from the file - -   unisphere Specify the IP address of the Unisphere for PowerMax server which manages the arrays being used by the CSI driver No “https://0.0.0.0:8443”   skipCertificateValidation This parameter should be set to false if you want to do client side TLS verification of Unisphere for PowerMax SSL certificates. It is set to true by default. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server No Empty    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml  Note:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option  Storage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and can’t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting in Q4 of 2020, an annotation \"helm.sh/resource-policy\": keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNOTE: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\n Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, make sure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You won’t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerMax can be deployed by using the …","ref":"/csm-docs/v3/installation/helm/powermax/","title":"PowerMax"},{"body":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell EMC PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Create secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. Please refer detailed documentation on how to create this secret here.\nInstall Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace. Create PowerMax credentials: Create a file called powermax-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:powermax-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards)# chapsecret: \u003cbase64 CHAP secret\u003eReplace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run kubectl create -f powermax-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerMax using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:    Parameter Description Required Default     replicas Controls the amount of controller Pods you deploy. If controller Pods are greater than number of available nodes, excess Pods will become stuck in pending. Defaults is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended onto all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ   X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443   X_CSI_TRANSPORT_PROTOCOL Choose what transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No -   X_CSI_POWERMAX_ARRAYS List of comma-separated array id(s) which will be managed by the driver No -   X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Leave blank if not using reverse proxy No -   X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4   X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature review related documentation No -   X_CSI_IG_NODENAME_TEMPLATE Template used for creating hosts on PowerMax. Example: “a-b-c-%foo%-xyz” where the text between the % symbols(foo) is replaced by the actual host name No -   X_CSI_IG_MODIFY_HOSTNAME Determines if node plugin can rename any existing host on the PowerMax array. Use it with the node name template to rename the existing hosts No false   X_CSI_POWERMAX_DEBUG Determines if HTTP Request/Response is logged No false   Node parameters      X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature review the related documentation No false   StorageClass parameters      SYMID Symmetrix ID Yes 000000000001   SRP Storage Resource Pool Name Yes DEFAULT_SRP   ServiceLevel Service Level No Bronze   FsType File System type (xfs/ext4) xfs    allowVolumeExpansion After the allowed topology is modified in storage class, pods/and volumes will always be scheduled on nodes that have access to the storage No false   allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to specify the PowerMax array ID and append .fc or .iscsi at the end of it to specify a protocol. For more details on this feature review the related documentation No “000000000001”     Execute the following command to create PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.  CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is an optional component which can be installed along with the CSI PowerMax driver. For more details on this feature review the related documentation.\nWhen you install CSI PowerMax ReverseProxy, dell-csi-operator is going to create a Deployment and ClusterIP service as part of the installation\nNote - If you wish to use the ReverseProxy with CSI PowerMax driver, the ReverseProxy service should be created before you install the CSIPowerMax driver.\nPre-requisites Create a TLS secret which holds a SSL certificate and a private key which is required by the reverse proxy server. Use a tool like openssl to generate this secret using the example below:\n openssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n powermax tls revproxy-certs --cert=tls.crt --key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs\nconfig : This section contains the details of the Reverse Proxy configuration\nmode : This value is set to Linked by default. Do not change this value\nlinkConfig : This section contains the configuration of the Linked mode\nprimary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if Primary Unisphere is unreachable\nurl : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false\nHere is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion: storage.dell.com/v1 kind: CSIPowerMaxRevProxy metadata: name: powermax-reverseproxy # \u003c- Name of the CSIPowerMaxRevProxy object namespace: test-powermax # \u003c- Set the namespace to where you will install the CSI PowerMax driver spec: # Image for CSI PowerMax ReverseProxy image: dellemc/csipowermax-reverseproxy:v1.0.0.000R # \u003c- CSI PowerMax Reverse Proxy image imagePullPolicy: Always # TLS secret which contains SSL certificate and private key for the Reverse Proxy server tlsSecret: csirevproxy-tls-secret config: # Mode for the proxy - only supported mode for now is \"Linked\" mode: Linked linkConfig: primary: url: https://0.0.0.0:8443 #Unisphere URL skipCertificateValidation: true # This setting determines if client side Unisphere certificate validation is to be skipped certSecret: \"\" # Provide this value if skipCertificateValidation is set to false backup: # This is an optional field and lets you configure a backup unisphere which can be used by proxy server url: https://0.0.0.0:8443 #Unisphere URL skipCertificateValidation: true Installation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service\nkubectl create -f powermax_reverseproxy.yaml  You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u003cnamespace\u003e kubectl get svc -n \u003cnamespace\u003e  ","excerpt":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell …","ref":"/csm-docs/v3/installation/operator/powermax/","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell EMC PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use the CSI Driver for Dell EMC PowerMax. These examples automate the creation of Pods using the default storage classes that were created during installation. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with different number of volumes. To test the installation of the CSI driver, perform these tests:\n Volume clone test Volume test Snapshot test  Volume test Use this procedure to perform a volume test.\n  Create a namespace with the name test.\n  Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\n  Run the starttest.sh script and provide it with a test name. The following is a sample command that can be used to run the 2vols test: ./starttest.sh -t 2vols -n test\nThis script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\n  Run the ./stoptest.sh -t 2vols -n test script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\n   NOTE: Helm tests have been designed assuming that users are using the default storageclass names (powermax and powermax-xfs). If your storageclass names differ from the default values, such as when deploying with the Operator, update the templates in 2vols accordingly (located in test/helm/2vols/templates/ directory). You can use kubectl get sc to check for the storageclass names.\n Volume clone test Use this procedure to perform a volume clone test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: bash volumeclonetest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test.  Snapshot test Use this procedure to perform a snapshot test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot on that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test.   NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass which is created by the Helm-based installer. If you have an operator-based deployment, the name of the snapshot class will differ. You must update the snapshot class name in the file betaSnap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\n Volume Expansion test Use this procedure to perform a volume expansion test.\n Create a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates one PVC and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC and then recalculates the checksum Cleans up all the resources that were created as part of the test  ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v3/installation/test/powermax/","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v1.5.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Ubuntu 20.04 Added support for Docker EE 3.1 Added support for Controller high availability (multiple-controllers) Added support for Topology Added support for mount options Changed driver base image to UBI 8.x  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.17 or 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.17.8+, 1.18.5+   Delete Volume fails with error message: volume is part of masking view This issue is due to limitations in Unisphere and occurs when Unisphere is overloaded. Currently, there is no workaround for this but can be avoided by making sure Unisphere is not overloaded during such operations. The Unisphere team is assessing a fix for this in a future Unisphere release    ","excerpt":"Release Notes - CSI PowerMax v1.5.0 New Features/Changes  Added …","ref":"/csm-docs/v3/release/powermax/","title":"PowerMax"},{"body":"   Symptoms Prevention, Resolution or Workaround     Warning about feature gates Double check that you have applied all the features to the indicated processes. Restart kubelet when remediated.   kubectl describe pod powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e indicates the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs shows the driver cannot authenticate Check your secret’s username and password   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs shows the driver failed to connect to the U4P because it couldn’t verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Warning about …","ref":"/csm-docs/v3/troubleshooting/powermax/","title":"PowerMax"},{"body":"The CSI Driver for Dell EMC PowerScale can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell EMC PowerScale:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerScale.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\n(Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\ncontroller:snapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here:v4.2.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v4.2.x\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 4.2.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone -b v2.0.0 https://github.com/dell/csi-powerscale.git to clone the git repository.\n  Ensure that you have created the namespace where you want to install the driver. You can run kubectl create namespace isilon to create a new one. The use of “isilon” as the namespace is just an example. You can choose any name for the namespace.\n  Collect information from the PowerScale Systems like IP address, IsiPath, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml.\n  Copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation.\n  Edit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     logLevel CSI driver log level No “debug”   certSecretCount Defines the number of certificate secrets, which the user is going to create for SSL authentication. (isilon-cert-0..isilon-cert-(n-1)); Minimum value should be 1. Yes 1   allowedNetworks Defines the list of networks that can be used for NFS I/O traffic, CIDR format must be used. No [ ]   maxIsilonVolumesPerNode Defines the default value for a maximum number of volumes that the controller can publish to the node. If the value is zero CO SHALL decide how many volumes of this type can be published by the controller to the node. This limit is applicable to all the nodes in the cluster for which node label ‘max-isilon-volumes-per-node’ is not set. Yes 0   imagePullPolicy Defines the policy to determine if the image should be pulled prior to starting the container Yes IfNotPresent   verbose Indicates what content of the OneFS REST API message should be logged in debug level logs Yes 1   kubeletConfigDir Specify kubelet config dir path Yes “/var/lib/kubelet”   enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish a connection to Array. This requires enableCustomTopology to be enabled. No false   controller Configure controller pod specific parameters     controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2   volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s”   snapshot.enabled Enable/Disable volume snapshot feature Yes true   snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot”   resizer.enabled Enable/Disable volume expansion feature Yes true   nodeSelector Define node selection constraints for pods of controller deployment No    tolerations Define tolerations for the controller deployment, if required No    node Configure node pod specific parameters     nodeSelector Define node selection constraints for pods of node daemonset No    tolerations Define tolerations for the node daemonset, if required No    dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet   PLATFORM ATTRIBUTES      endpointPort Define the HTTPs port number of the PowerScale OneFS API server. This value acts as a default value for endpointPort, if not specified for a cluster config in secret. No 8080   skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. This value acts as a default value for skipCertificateValidation, if not specified for a cluster config in secret. No true   isiAccessZone Define the name of the access zone a volume can be created in. If storageclass is missing with AccessZone parameter, then value of isiAccessZone is used for the same. No System   enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. No true   isiPath Define the base path for the volumes to be created on PowerScale cluster. This value acts as a default value for isiPath, if not specified for a cluster config in secret No /ifs/data/csi   noProbeOnStart Define whether the controller/node plugin should probe all the PowerScale clusters during driver initialization No false   autoProbe Specify if automatically probe the PowerScale cluster if not done already during CSI calls No true    NOTE:\n ControllerCount parameter value must not exceed the number of nodes in the Kubernetes cluster. Otherwise, some of the controller pods remain in a “Pending” state till new nodes are available for scheduling. The installer exits with a WARNING on the same. Whenever the certSecretCount parameter changes in my-isilon-setting.yaml user needs to reinstall the driver.    Edit following parameters in samples/secret/secret.yaml file and update/add connection/authentication information for one or more PowerScale clusters.\n   Parameter Description Required Default     clusterName Logical name of PoweScale cluster against which volume CRUD operations are performed through this secret. Yes -   username username for connecting to PowerScale OneFS API server Yes -   password password for connecting to PowerScale OneFS API server Yes -   endpoint HTTPS endpoint of the PowerScale OneFS API server Yes -   isDefault Indicates if this is a default cluster (would be used by storage classes without ClusterName parameter). Only one of the cluster config should be marked as default. No false   Optional parameters Following parameters are Optional. If specified will override default values from values.yaml.     skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. No default value from values.yaml   endpointPort Specify the HTTPs port number of the PowerScale OneFS API server No default value from values.yaml   isiPath The base path for the volumes to be created on PowerScale cluster. Note: IsiPath parameter in storageclass, if present will override this attribute. No default value from values.yaml    The username specified in secret.yaml must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\n   Privilege Type     ISI_PRIV_LOGIN_PAPI Read Only   ISI_PRIV_NFS Read Write   ISI_PRIV_QUOTA Read Write   ISI_PRIV_SNAPSHOT Read Write   ISI_PRIV_IFS_RESTORE Read Only   ISI_PRIV_NS_IFS_ACCESS Read Only   ISI_PRIV_IFS_BACKUP Read Only      Create isilon-creds secret using the following command:  kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl apply -f -\nNOTE:\n If any key/value is present in all my-isilon-settings.yaml, secret, and storageClass, then the values provided in storageClass parameters take precedence. The user has to validate the yaml syntax and array-related key/values while replacing or appending the isilon-creds secret. The driver will continue to use previous values in case of an error found in the yaml file. For the key isiIP/endpoint, the user can give either IP address or FQDN. Also, the user can prefix ‘https’ (For example, https://192.168.1.1) with the value. The isilon-creds secret has a mountEndpoint parameter which should not be updated by the user. This parameter is updated and used when the driver has been injected with CSM-Authorization.   Install OneFS CA certificates by following the instructions from the next section, if you want to validate OneFS API server’s certificates. If not, create an empty secret using the following command and an empty secret must be created for the successful installation of CSI Driver for Dell EMC PowerScale.\nkubectl create -f emptysecret.yaml This command will create a new secret called isilon-certs-0 in isilon namespace.\n  Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace isilon --values ../helm/my-isilon-settings.yaml (assuming that the current working directory is ‘helm’ and my-isilon-settings.yaml is also present under ‘helm’ directory)\n  Certificate validation for OneFS REST API calls The CSI driver exposes an install parameter ‘skipCertificateValidation’ which determines if the driver performs client-side verification of the OneFS certificates. The ‘skipCertificateValidation’ parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the ‘skipCertificateValidation’ is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the ‘skipCertificateValidation’ parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure  To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem To create the certs secret, run kubectl create secret generic isilon-certs-0 --from-file=cert-0=ca_cert_0.pem -n isilon Use the following command to replace the secret  kubectl create secret generic isilon-certs-0 -n isilon --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -  NOTES:\n The OneFS IP can be with or without a port, depends upon the configuration of OneFS API server. The commands are based on the namespace ‘isilon’ It is highly recommended that ca_cert.pem file(s) having the naming convention as ca_cert_number.pem (example: ca_cert_0, ca_cert_1), where this number starts from 0 and grows as the number of OneFS arrays grows. The cert secret created out of these pem files must have the naming convention as isilon-certs-number (example: isilon-certs-0, isilon-certs-1, and so on.); The number must start from zero and must grow in incremental order. The number of the secrets created out of pem files should match certSecretCount value in myvalues.yaml or my-isilon-settings.yaml.  Dynamic update of array details via secret.yaml CSI Driver for Dell EMC PowerScale now provides supports for Multi cluster. Now users can link the single CSI Driver to multiple OneFS Clusters by updating secret.yaml. Users can now update the isilon-creds secret by editing the secret.yaml and executing the following command\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl apply -f -\nNote: Updating isilon-certs-x secrets is a manual process, unlike isilon-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nStorage Classes The CSI driver for Dell EMC PowerScale version 1.5 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A sample storage class manifest is available at samples/storageclass/isilon.yaml. Use this sample manifest to create a storageclass to provision storage; uncomment/ update the manifest as per the requirements.\nWhat happens to my existing storage classes? Upgrading from CSI PowerScale v1.6 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver It is strongly recommended to upgrade the earlier versions of CSI PowerScale to 1.6 before upgrading to 2.0.\nNOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Volume Snapshot Class Starting CSI PowerScale v1.6, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. Sample volume snapshot class manifests are available at samples/volumesnapshotclass/. Use these sample manifests to create a volumesnapshotclass for creating volume snapshots; uncomment/ update the manifests as per the requirements.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerScale v1.6 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerScale to 1.6 before upgrading to 2.0.\n","excerpt":"The CSI Driver for Dell EMC PowerScale can be deployed by using the …","ref":"/csm-docs/docs/csidriver/installation/helm/isilon/","title":"PowerScale"},{"body":"Installing CSI Driver for PowerScale via Operator The CSI Driver for Dell EMC PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nNote: MKE (Mirantis Kubernetes Engine) does not support the installation of CSI-PowerScale via Operator.\nListing installed drivers with the CSI Isilon CRD User can query for CSI-PowerScale driver using the following command: kubectl get csiisilon --all-namespaces\nInstall Driver   Create namespace.\nExecute kubectl create namespace isilon to create the isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘isilon’.\n  Create isilon-creds secret by using secret.yaml file format only.\n2.1 Create a yaml file called secret.yaml with the following content:\n isilonClusters: # logical name of PowerScale Cluster - clusterName: \"cluster1\" # username for connecting to PowerScale OneFS API server # Default value: None username: \"user\" # password for connecting to PowerScale OneFS API server password: \"password\" # HTTPS endpoint of the PowerScale OneFS API server # Default value: None # Examples: \"1.2.3.4\", \"https://1.2.3.4\", \"https://abc.myonefs.com\" endpoint: \"1.2.3.4\" # Is this a default cluster (would be used by storage classes without ClusterName parameter) # Allowed values: # true: mark this cluster config as default # false: mark this cluster config as not default # Default value: false isDefault: true # Specify whether the PowerScale OneFS API server's certificate chain and host name should be verified. # Allowed values: # true: skip OneFS API server's certificate verification # false: verify OneFS API server's certificates # Default value: default value specified in values.yaml # skipCertificateValidation: true # The base path for the volumes to be created on PowerScale cluster # This will be used if a storage class does not have the IsiPath parameter specified. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Default value: default value specified in values.yaml # Examples: \"/ifs/data/csi\", \"/ifs/engineering\" # isiPath: \"/ifs/data/csi\" # The permissions for isi volume directory path # This will be used if a storage class does not have the IsiVolumePathPermissions parameter specified. # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" # isiVolumePathPermissions: \"0777\" - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" endpointPort: \"8080\" Replace the values for the given keys as per your environment. After creating the secret.yaml, the following command can be used to create the secret,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\nUse the following command to replace or update the secret\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the YAML syntax and array related key/values while replacing the isilon-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\n  Create isilon-certs-n secret. Please refer this section for creating cert-secrets.\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:isilon-certs-0namespace:isilontype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\n  Create a CR (Custom Resource) for PowerScale using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\n   Parameter Description Required Default     dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet   Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_ISI_SKIP_CERTIFICATE_VALIDATION Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty   X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISI_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    X_CSI_ISI_VOLUME_PATH_PERMISSIONS The permissions for isi volume directory path Yes 0777   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    Node parameters      X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes 0   X_CSI_MODE Driver starting mode No node      Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\n  Note :\n From CSI-PowerScale v1.6.0 and higher, Storage class and VolumeSnapshotClass will not be created as part of driver deployment. The user has to create Storageclass and Volume Snapshot Class. Node selector and node tolerations can be added in both controller parameters and node parameters section, based on the need. “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  ","excerpt":"Installing CSI Driver for PowerScale via Operator The CSI Driver for …","ref":"/csm-docs/docs/csidriver/installation/operator/isilon/","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\n  Creating a storage class:\nCreate a file storageclass.yaml using sample yaml file located at samples/storageclass/isilon.yaml. Update/uncomment the attributes in this sample file as per the requirements.\nExecute the following command to create a storage class:\nkubectl create -f $PWD/storageclass.yaml Result: After executing the above command storage class will be created in the default namespace, and the user can see the storage class by executing kubectl get sc.\n  Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at samples/persistentvolumeclaim/pvc.yaml\nExecute the following command to create volume:\nkubectl create -f $PWD/pvc.yaml\nResult: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing the command kubectl get pvc.\n  Note: The status of the volume can be either Bound or Pending depending on the VolumeBindingMode specified on the storage class.\n Attach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at samples/pod/.\nExecute the following command to mount the volume to the Kubernetes node:\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, a new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for the host to be part of clients/rootclients field of export created for volume and used by nginx application.\n  Create Snapshot\nVolumeSnapshotClass is needed for creating the volume snapshots. Starting from v1.6, CSI Driver for PowerScale will not create any default Volume Snapshot class.\nSo the user has to create a volume snapshot class. The required sample files are present under samples/volumesnapshotclass/. Choose the file based on Kubernetes version.\nExecute either one of the following commands to create a volume snapshot class.\nkubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1.yaml OR kubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1beta1.yaml\nThe above-said command will create a volume snapshotclass with the name isilon-snapclass.\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snapshot-of-test-pvc.yaml. The sample file for snapshot creation is located at samples/volumesnapshot/.\nExecute the following command to create snapshot:\nkubectl create -f samples/volumesnapshot/snapshot-of-test-pvc.yaml\nThe spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is test-pvc, then the created snapshot is named snapshot-of-test-pvc. Verify the PowerScale system for the newly created snapshot.\n  Note:\n User can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. User has to make sure that the IsiPath in the parameters section of the volume snapshot class is matching with a corresponding storage class.   Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in the spec dataSource field.\nThe sample file for volume creation from the snapshot is located at samples/persistentvolumeclaim/pvc-from-snapshot.yaml .\nExecute the following command to create a snapshot:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-snapshot.yaml Verify the PowerScale system for newly created volume from the snapshot.\n  Delete Snapshot\nExecute the following commands to delete the snapshot:\nkubectl get volumesnapshot kubectl delete volumesnapshot snapshot-of-test-pvc   Create a new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in the spec dataSource field.\nThe sample file for volume creation from volume is located at samples/persistentvolumeclaim/pvc-from-pvc.yaml\nExecute the following command to create a pvc from another pvc:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-pvc.yaml Verify the PowerScale system for newly created volume from volume.\n  To Unattach the volume from Host\nDelete the nginx application to Unattach the volume from the host:\nkubectl delete -f nginx.yaml\n  To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc   ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/docs/csidriver/installation/test/powerscale/","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v2.0.0 New Features/Changes  Added support for Kubernetes v1.22. Added support for OpenShift v4.8. Added support for CSI Spec v1.4. Added support for session-based authentication. Added support for consistent config parameters across CSI drivers. Added support for configurable security permissions for volume. Added the ability to enable/disable installation of resizer sidecar with driver installation. Added the ability to enable/disable installation of snapshotter sidecar with driver installation. Added support to make dnsPolicy of node component configurable via Operator as well. Added the ability to configure kubelet directory path. Updated support for dynamic logging configuration.  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Resolution or workaround, if known     If the length of the nodeID exceeds 128 characters, the driver fails to update the CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn’t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in the CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future Kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581 Note: In kubernetes 1.22 this limit has been relaxed to 192 characters.   If some older NFS exports /terminated worker nodes still in NFS export client list, CSI driver tries to add a new worker node it fails (For RWX volume). User need to manually clean the export client list from old entries to make successful additon of new worker nodes.    ","excerpt":"Release Notes - CSI Driver for PowerScale v2.0.0 New Features/Changes …","ref":"/csm-docs/docs/csidriver/release/powerscale/","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\n   Symptoms Prevention, Resolution or Workaround     The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret’s username and password for corresponding cluster   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn’t verify the certificates Check the isilon-certs- secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \"true\" for insecure connection. SSL validation is recommended in the production environment.   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver error: create volume failed, Access denied. create directory as requested This situation can happen when the user who created the base path is different from the user configured for the driver. Make sure the user used to deploy CSI-Driver must have enough rights on the base path (i.e. isiPath) to perform all operations.   Volume/filesystem is allowed to mount by any host in the network, though that host is not a part of the export of that particular volume under /ifs directory “Dell EMC PowerScale: OneFS NFS Design Considerations and Best Practices”: There is a default shared directory (ifs) of OneFS, which lets clients running Windows, UNIX, Linux, or Mac OS X access the same directories and files. It is recommended to disable the ifs shared directory in a production environment and create dedicated NFS exports and SMB shares for your workload.   Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class is not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same.   While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700.   If the hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to driver version 1.4.0 or later there is a possibility of “localhost” as a stale entry in export Recommended setup: User should not map a hostname to loopback IP in /etc/hosts file   CSI Driver installation fails with the error message “error getting FQDN”. Map IP address of host with its FQDN in /etc/hosts file.   Driver node pod is in “CrashLoopBackOff” as “Node ID” generated is not with proper FQDN. This might be due to “dnsPolicy” implemented on the driver node pod which may differ with different networks. This parameter is configurable in both helm and Operator installer and the user can try with different “dnsPolicy” according to the environment.    ","excerpt":"Here are some installation failures that might be encountered and how …","ref":"/csm-docs/docs/csidriver/troubleshooting/powerscale/","title":"PowerScale"},{"body":"Multicluster support You can connect a single CSI-PowerScale driver with multiple PowerScale clusters.\nPre-Requisites:\n Creation of secret.json or secret.yaml with credentials related to one or more Clusters. Creation of (at least) one Storage class for each cluster. Creation of custom-volumesnapshot classes with proper isiPath matching corresponding storage classes. Inclusion of cluster name in volume handle, if you want to provision existing static volumes.  Consuming existing volumes with static provisioning You can use existent volumes from the PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, the PowerScale cluster accessZone is assumed as ‘System’, storage class as ‘isilon’, cluster name as ‘pscale-cluster’ and volume’s internal name as ‘isilonvol’. The volume-handle should be in the format of \u003cvolume_name\u003e=_=_=\u003cexport_id\u003e=_=_==_=_=\u003ccluster_name\u003e  apiVersion:v1kind:PersistentVolumemetadata:name:isilonstaticpvnamespace:defaultspec:capacity:storage:5GiaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy:RetainstorageClassName:isiloncsi:driver:csi-isilon.dellemc.comvolumeAttributes:Path:\"/ifs/data/csi/isilonvol\"Name:\"isilonvol\"AzServiceIP:'XX.XX.XX.XX'volumeHandle:isilonvol=_=_=652=_=_=System=_=_=pscale-clusterclaimRef:name:isilonstaticpvcnamespace:defaultCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilonstaticpvcnamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:isilonstaticpvstorageClassName:isilonThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:isilonstaticpvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.  PVC Creation Feature Following yaml content can be used to create a PVC without referring any PV.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:testvolumenamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonVolume Snapshot Feature The CSI PowerScale driver version 1.3 and later supports managing beta snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller   For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerScale driver version 1.6, no default Volume Snapshot Class will get created.\nFollowing are the manifests for the Volume Snapshot Class:\n VolumeSnapshotClass - v1  # For kubernetes version 20 and above (v1 snaps)apiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:\"isilon-snapclass\"driver:csi-isilon.dellemc.com#The deletionPolicy of a volume snapshot class can either be Retain or Delete#If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object.#If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remaindeletionPolicy:Deleteparameters:#IsiPath should match with respective storageClass IsiPathIsiPath:\"/ifs/data/csi\"VolumeSnapshotClass - beta  # For kubernetes version 18 and 19 (beta snaps)apiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:\"isilon-snapclass\"driver:csi-isilon.dellemc.com#The deletionPolicy of a volume snapshot class can either be Retain or Delete#If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object.#If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remaindeletionPolicy:Deleteparameters:#IsiPath should match with respective storageClass IsiPathIsiPath:\"/ifs/data/csi\"### Create Volume SnapshotThe following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs; The following snippet assumes that the persistent volume claim name is testvolume.\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvcsnapnamespace:defaultspec:volumeSnapshotClassName:isilon-snapclasssource:persistentVolumeClaimName:testvolumeOnce the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:createfromsnapnamespace:defaultspec:storageClassName:isilondataSource:name:pvcsnapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteManyresources:requests:storage:5GiVolume Expansion The CSI PowerScale driver version 1.2 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:\"false\"provisioner:\"csi-isilon.dellemc.com\"reclaimPolicy:Deleteparameters:ClusterName:\u003cclusterNamespecifiedinsecret.json\u003e AccessZone: SystemisiPath:\"/ifs/data/csi\"AzServiceIP :'XX.XX.XX.XX'rootClientEnabled:\"true\"allowVolumeExpansion:truevolumeBindingMode:ImmediateTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilon-pvc-expansion-demospec:accessModes:- ReadWriteOnceresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Volume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing PVC:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:existing-pvcspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonThe following is a sample manifest for cloning:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:volume-from-volumenamespace:defaultspec:accessModes:- ReadWriteManyvolumeMode:Filesystemresources:requests:storage:50GistorageClassName:isilondataSource:kind:PersistentVolumeClaimname:existing-pvcapiGroup:\"\"Controller HA The CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two-controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if the controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in a Pending state.\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nEphemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves as use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumespec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data\"name:my-csi-volumevolumes:- name:my-csi-volumecsi:driver:csi-isilon.dellemc.comvolumeAttributes:size:\"2Gi\"ClusterName:\"cluster1\"This manifest creates a pod in a given cluster and attaches a newly created ephemeral inline CSI volume to it.\nTopology Topology Support The CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage, as a result of which the nodes which have access to PowerScale Array are appropriately labeled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, the CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and use them to initialize node pod with custom PowerScale FQDN/IP.\nNote: Only a single cluster can be configured as part of secret.json for custom topology.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that the Pod schedule takes advantage of the topology and the selected node has access to provisioned volumes.\nNote: Whenever a new storage cluster is being added in secret, even though it is dynamic, the new storage cluster IP address-related label is not added to worker nodes dynamically. The user has to spin off (bounce) driver-related pods (controller and node pods) in order to apply newly added information to be reflected in worker nodes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options.# PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon# Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server# Provide mount options through \"mountOptions\" attribute # to create PVCs with mount options.apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilonprovisioner:csi-isilon.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:trueparameters:AccessZone:SystemIsiPath:\"/ifs/data/csi\"# AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP.#AzServiceIP : 192.168.2.1# When a PVC is being created, it takes the storage class' value of \"storageclass.rootClientEnabled\", # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \"Root clients\" field (when true) or \"Clients\" field (when false) of the NFS export RootClientEnabled:\"false\"# Name of PowerScale cluster where pv will be provisioned# This name should match with name of one of the cluster configs in isilon-creds secret# If this parameter is not specified, then default cluster config in isilon-creds secret will be considered if available#ClusterName: \"\u003ccluster_name\u003e\"# volumeBindingMode controls when volume binding and dynamic provisioning should occur.# Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created# WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume# until a Pod using the PersistentVolumeClaim is createdvolumeBindingMode:WaitForFirstConsumer# allowedTopologies helps scheduling pod on worker nodes which match all of below expressions# If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologiesallowedTopologies:- matchLabelExpressions:- key:csi-isilon.dellemc.com/\u003cISILON_IP\u003e values:- csi-isilon.dellemc.commountOptions:[\"\u003cmountOption1\u003e\",\"\u003cmountOption2\u003e\",...,\"\u003cmountOptionN\u003e\"]For additional information, see the Kubernetes Topology documentation.\nSupport custom networks for NFS I/O traffic When allowedNetworks is specified for using custom networks to handle NFS traffic, and a user already has workloads scheduled, there is a possibility that it might lead to backward compatibility issues. For example, ControllerUnPublish might not be able to completely remove clients from the NFS exports of previously created pods.\nAlso, the previous workload will still be using the default network and not custom networks. For previous workloads to use custom networks, the recreation of pods is required.\nVolume Limit The CSI Driver for Dell EMC PowerScale allows users to specify the maximum number of PowerScale volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-isilon-volumes-per-node and specifying the volume limit for that node.  kubectl label node \u003cnode_name\u003e max-isilon-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxIsilonVolumesPerNode attribute in values.yaml.\n NOTE: The default value of maxIsilonVolumesPerNode is 0. If maxIsilonVolumesPerNode is set to zero, then CO shall decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxIsilonVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-isilon-volumes-per-node is not set.\n Node selector in helm template Now user can define in which worker node, the CSI node pod daemonset can run (just like any other pod in Kubernetes world.)For more information, refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector\nSimilarly, users can define the tolerations based on various conditions like memory pressure, disk pressure and network availability. Refer to https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#taints-and-tolerations for more information.\nDynamic log level change Log levels (debug, info, error, warning) were controlled only in my-isilon-settings.yaml which required restarting of csi-driver. Now the control has been transferred to secret definition (secret.json or secret.yaml). Changing the Log level in secret dynamically changes the log levels in controller and node logs.\nUsage of SmartQuotas to Limit Storage Consumption CSI driver for Dell EMC Isilon handles capacity limiting using SmartQuotas feature.\nTo use the SmartQuotas feature user can specify the boolean value ‘enableQuota’ in myvalues.yaml or my-isilon-settings.yaml.\nLet us assume the user creates a PVC with 3 Gi of storage and ‘SmartQuotas’ have already been enabled in PowerScale Cluster.\n  When ‘enableQuota’ is set to ‘true’\n The driver sets the hard limit of the PVC to 3Gi. The user adds data of 2Gi to the above said PVC (by logging into POD). It works as expected. The user tries to add 2Gi more data. Driver doesn’t allow the user to enter more data as total data to be added is 4Gi and PVC limit is 3Gi. The user can expand the volume from 3Gi to 6Gi. The driver allows it and sets the hard limit of PVC to 6Gi. User retries adding 2Gi more data (which has been errored out previously). The driver accepts the data.    When ‘enableQuota’ is set to ‘false’\n Driver doesn’t set any hard limit against the PVC created. The user adds data of 2Gi to the above said PVC, which is having the size 3Gi (by logging into POD). It works as expected. The user tries to add 2Gi more data. Now the total size of data is 4Gi. Driver allows the user to enter more data irrespective of the initial PVC size (since no quota is set against this PVC) The user can expand the volume from an initial size of 3Gi to 4Gi or more. The driver allows it.    ","excerpt":"Multicluster support You can connect a single CSI-PowerScale driver …","ref":"/csm-docs/v1/features/powerscale/","title":"PowerScale"},{"body":"The CSI Driver for Dell EMC PowerScale can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell EMC PowerScale:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerScale.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\n(Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20/1.21 (v1 snapshots) use v4.0.x  Volume Snapshot Controller The beta Volume Snapshots in Kubernetes version 1.17 and later, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20 and 1.21 (v1 snapshots) use v4.0.x  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.x quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 3.0.x version of snapshotter/snapshot-controller when using Kubernetes 1.19 When using Kubernetes 1.20/1.21 it is recommended to use 4.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone https://github.com/dell/csi-powerscale.git to clone the git repository.\n  Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace isilon to create a new one. The use of “isilon” as the namespace is just an example. You can choose any name for the namespace.\n  Collect information from the PowerScale Systems like IP address, IsiPath, username, and password. Make a note of the value for these parameters as they must be entered in the secret.json.\n  Copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation.\n  Edit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. (isilon-cert-0..isilon-cert-(n-1)); Minimum value should be 1. true 1   isiPort “isiPort” defines the HTTPs port number of the PowerScale OneFS API server. false 8080   allowedNetworks “allowedNetworks” defines the list of networks that can be used for NFS I/O traffic, CIDR format must be used. false [ ]   isiInsecure “isiInsecure” specifies whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. This value will affect the default storage class implementation. false true   isiAccessZone The name of the access zone a volume can be created in. false System   volumeNamePrefix “volumeNamePrefix” defines a string prepended to each volume created by the CSI driver. false k8s   controllerCount “controllerCount” defines the number of CSI PowerScale controller nodes to deploy to the Kubernetes release. true 2   enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. false true   noProbeOnStart Indicates whether the controller/node should probe during initialization. false false   isiPath The default base path for the volumes to be created, will be used if a storage class does not have the IsiPath parameter specified. false /ifs/data/csi   autoProbe Enable auto probe. false true   nfsV3 Specify whether to set the version to v3 when mounting an NFS export. If the value is “false”, then the default version supported will be used (that is, the mount command will not explicitly specify “-o vers=3” option). This flag has now been deprecated and will be removed in a future release. Use the StorageClass.mountOptions if you want to specify ‘vers=3’ as a mount option. false false   enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish a connection to Array. This requires enableCustomTopology to be enabled. false false   maxIsilonVolumesPerNode Specify the default value for a maximum number of volumes that the controller can publish to the node. If the value is zero CO SHALL decide how many volumes of this type can be published by the controller to the node. This limit is applicable to all the nodes in the cluster for which node label ‘max-isilon-volumes-per-node’ is not set. true 0   Controller parameters Set nodeSelector and tolerations for controller.     nodeSelector Define nodeSelector for the controllers, if required. false    tolerations Define tolerations for the controllers, if required. false    Node parameters Set nodeSelector and tolerations for node pods.     nodeSelector Define nodeSelector for the node pods, if required. false    tolerations Define tolerations for the node pods, if required. false     NOTE:\n User should provide all boolean values with double-quotes. This applies only for my-isilon-settings.yaml. Example: “true”/“false” ControllerCount parameter value must not exceed the number of nodes in the Kubernetes cluster. Otherwise, some of the controller pods remain in “Pending” state till new nodes are available for scheduling. The installer exits with a WARNING on the same. Whenever the certSecretCount parameter changes in my-isilon-setting.yaml user needs to reinstall the driver.    Create a secret file for the OneFS credentials by editing the secret.json or secret.yaml file present under helm directory. Either secret.json or secret.yaml can be used for adding the credentials of one or more OneFS storage arrays. The following table lists driver configuration parameters for a single storage array.\n   Parameter Description Required Default     isiIP “isiIP” defines the HTTPs endpoint of the PowerScale OneFS API server. true -   endpoint This is a new way of defining existing isiIP. User can use either isiIP or endpoint but not both. true -   clusterName PoweScale cluster against which volume CRUD operations are performed through this secret. This is a logical name. true -   username Username for accessing PowerScale OneFS system. true -   password Password for accessing PowerScale OneFS system. true -   isDefaultCluster Defines whether this storage array should be the default. This entry should be present only for one OneFS array and that array will be marked default for existing volumes. false false   isDefault This is a new way of defining the existing isDefaultCluster key. User can use either isDefaultCluster or isDefault key but not both. false false   Optional parameters Following parameters are Optional if provided will override default values of values.yaml .     isiPort isiPort defines the HTTPs port number of the PowerScale OneFS API server. false 8080   isiInsecure “isiInsecure” specifies whether the PowerScale OneFS API server’s certificate chain and hostname should be verified. false false   skipCertificateValidation This is a new way of defining the existing isiInsecure key. User can use either skipCertificateValidation or isiInsecure key but not both. false false   isiPath The base path for the volumes to be created. Note: isiPath value provided in the storage class will take the highest precedence while creating PVC. true -   LogLevel Log level of Drivers false “debug”    The username specified in secret.json / secret.yaml must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\n   Privilege Type     ISI_PRIV_LOGIN_PAPI Read Only   ISI_PRIV_NFS Read Write   ISI_PRIV_QUOTA Read Write   ISI_PRIV_SNAPSHOT Read Write   ISI_PRIV_IFS_RESTORE Read Only   ISI_PRIV_NS_IFS_ACCESS Read Only   ISI_PRIV_IFS_BACKUP Read Only      If user creates secret.json, then after editing the file, run the following command to create a secret called ‘isilon-creds’  kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.json\nAlternatively, if user creates secret.yaml, then the secret can be created by running the following command: kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\nNOTE:\n If any key/value is present in secret file and my-isilon-settings.yaml, then the values provided secret takes precedence. If any key/value is present in all my-isilon-settings.yaml, secret and storageClass, then the values provided in storageClass parameters takes precedence. User has to validate the JSON/ yaml syntax and array-related key/values while replacing or appending the isilon-creds secret. The driver will continue to use previous values in case of an error found in the JSON / yaml file. For the key isiIP/endpoint, the user can give either IP address or FQDN. Also user can prefix ‘https’ (For example, https://192.168.1.1) with the value.   Install OneFS CA certificates by following the instructions from the next section, if you want to validate OneFS API server’s certificates. If not, create an empty secret using the following command and an empty secret must be created for the successful installation of CSI Driver for Dell EMC PowerScale.\nkubectl create -f emptysecret.yaml This command will create a new secret called isilon-certs-0 in isilon namespace.\n  Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace isilon --values ../helm/my-isilon-settings.yaml (assuming that the current working directory is ‘helm’ and my-isilon-settings.yaml is also present under ‘helm’ directory)\n  Certificate validation for OneFS REST API calls The CSI driver exposes an install parameter ‘isiInsecure’ which determines if the driver performs client-side verification of the OneFS certificates. The ‘isiInsecure’ parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the ‘isiInsecure’ or ‘skipCertificateValidation’ is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the ‘isiInsecure’ or ‘skipCertificateValidation’ parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure  To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem To create the certs secret, run kubectl create secret generic isilon-certs-0 --from-file=cert-0=ca_cert_0.pem -n isilon Use the following command to replace the secret  kubectl create secret generic isilon-certs-0 -n isilon --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -  NOTES:\n The OneFS IP can be with or without a port, depends upon the configuration of OneFS API server. The commands are based on the namespace ‘isilon’ It is highly recommended that ca_cert.pem file(s) having the naming convention as ca_cert_number.pem (example: ca_cert_0, ca_cert_1), where this number starts from 0 and grows as the number of OneFS arrays grows. The cert secret created out of these pem files must have the naming convention as isilon-certs-number (example: isilon-certs-0, isilon-certs-1, and so on.); The number must start from zero and must grow in incremental order. The number of the secrets created out of pem files should match certSecretCount value in myvalues.yaml or my-isilon-settings.yaml.  Dynamic update of array details via secret.json CSI Driver for Dell EMC PowerScale now provides supports for Multi cluster. Now users can link the single CSI Driver to multiple OneFS Clusters by updating secret.json or secret.yaml. User can now update the isilon-creds secret by editing the secret.json or secret.yaml and executing the following command (replace secret.json with secret.yaml based on need)\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.json -o yaml --dry-run=client | kubectl replace -f - Note: Updating isilon-certs-x secrets is a manual process, unlike isilon-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nStorage Classes The CSI driver for Dell EMC PowerScale version 1.5 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the helm/samples folder. Use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerScale v1.5 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver It is strongly recommended to upgrade the earlier versions of CSI PowerScale to 1.5 before upgrading to 1.6.\nNOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerScale v1.6, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the helm/samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerScale v1.5 driver The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver : It is strongly recommended to upgrade the earlier versions of CSI PowerScale to 1.5 before upgrading to 1.6.\n","excerpt":"The CSI Driver for Dell EMC PowerScale can be deployed by using the …","ref":"/csm-docs/v1/installation/helm/isilon/","title":"PowerScale"},{"body":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell EMC PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nNote: MKE (Mirantis Kubernetes Engine) does not support the installation of CSI-PowerScale via Operator.\nListing installed drivers with the CSI Isilon CRD User can query for CSI-PowerScale driver using the following command: kubectl get csiisilon --all-namespaces\nInstall Driver   Create namespace.\nExecute kubectl create namespace isilon to create the isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘isilon’.\n  Create isilon-creds secret by first creating secret.json or secret.yaml file.\n2.1 Create a json file called secret.json with the following content:\n{ \"isilonClusters\": [ { \"clusterName\": \"cluster1\", \"username\": \"user\", \"password\": \"password\", \"isiIP\": \"1.2.3.4\", \"isDefaultCluster\": true }, { \"clusterName\": \"cluster2\", \"username\": \"user\", \"password\": \"password\", \"endpoint\": \"1.2.3.5\", \"isiPort\": \"8080\", \"skipCertificateValidation\": true, \"isDefault\": false, \"isiPath\": \"/ifs/data/csi\" } ] } Replace the values for the given keys as per your environment. This username/password value need not be encoded. You can refer here for more information about these isilon secret parameters. After creating the secret.json, the following command can be used to create the secret,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.json\n2.2. Alternately user can create a secret.yaml file in the following format and replace the values for the given keys as per your environment. The command\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml would be needed to create secret.\nisilonClusters: - clusterName: \"cluster1\" # logical name of PowerScale Cluster username: \"user\" # username for connecting to PowerScale OneFS API server password: \"password\" # password for connecting to PowerScale OneFS API server endpoint: \"1.2.3.4\" # HTTPS endpoint of the PowerScale OneFS API server isDefault: true # default cluster skipCertificateValidation: true # indicates if client side validation of server's SSL certificate can be skipped isiPath: \"/ifs/data/csi\" # base path for the volume(directory) to be created on PowerScale - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" isiPort: \"8080\" logLevel: \"debug\" # CSI log level; valid log levels- \"error\", \"warn\"/\"warning\", \"info\", \"debug\" After creating the above file, the user can use the following command to create a secret object,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\n  Create isilon-certs-n secret. Please refer this section for creating cert-secrets.\n  Create a CR (Custom Resource) for PowerScale using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_ISI_INSECURE Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty   X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISILON_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes    Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    Node parameters      X_CSI_ISILON_NFS_V3 Set the version to v3 when mounting an NFS export. If the value is “false”, then the default version supported will be used Yes    X_CSI_MODE Driver starting mode No node      Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\n  Note :\n From CSI-PowerScale v1.6.0 and higher, Storage class and VolumeSnapshotClass will not be created as part of driver deployment. The user has to create Storageclass and Volume Snapshot Class. Node selector and node tolerations can be added in both controller parameters and node parameters section, based on the need.  ","excerpt":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell …","ref":"/csm-docs/v1/installation/operator/isilon/","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\n  Creating a storage class:\nCreate a file storageclass.yaml using sample yaml file located at helm/samples/storageclass\nExecute the following command to create a storage class:\nkubectl create -f $PWD/storageclass.yaml Result: After executing the above command storage class will be created in the default namespace, and the user can see the storage class by executing kubectl get sc.\n  Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at test/sample_files/\nExecute the following command to create volume:\nkubectl create -f $PWD/pvc.yaml\nResult: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing the command kubectl get pvc.\n  Note: The status of the volume can be either Bound or Pending depending on the VolumeBindingMode specified on the storage class.\n Attach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at test/sample_files/.\nExecute the following command to mount the volume to the Kubernetes node:\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, a new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for the host to be part of clients/rootclients field of export created for volume and used by nginx application.\n  Create Snapshot\nVolumeSnapshotClass is needed for creating the volume snapshots. Starting from v1.6, CSI Driver for PowerScale will not create any default Volume Snapshot class.\nSo the user has to create a volume snapshot class. The required sample files are present under /helm/samples/volumesnapshotclass. Choose the file based on Kubernetes version.\nExecute either one of the following commands to create a volume snapshot class.\nkubectl create -f $PWD/volsnapclass_v1.yaml OR kubectl create -f $PWD/volsnapclass_beta.yaml\nThe above-said command will create a volume snapshotclass with the name isilon-snapclass.\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snap.yaml. The sample file for snapshot creation is located at test/sample_files/.\nExecute the following command to create snapshot:\nkubectl create -f $PWD/snap.yaml\nThe spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is testvolclaim1, then the created snapshot is named testvolclaim1-snap1. Verify the PowerScale system for the newly created snapshot.\n  Note:\n User can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. User has to make sure that the IsiPath in the parameters section of the volume snapshot class is matching with a corresponding storage class.   Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in the spec dataSource field.\nThe sample file for volume creation from the snapshot is located under test/sample_files/\nExecute the following command to create a snapshot:\nkubectl create -f $PWD/volume_from_snap.yaml Verify the PowerScale system for newly created volume from the snapshot.\n  Delete Snapshot\nExecute the following commands to delete the snapshot:\nkubectl get volumesnapshot kubectl delete volumesnapshot pvcsnap   Create a new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in the spec dataSource field.\nThe sample file for volume creation from volume is located at test/sample_files/\nExecute the following command to create a snapshot:\nkubectl create -f $PWD/volume_from_volume.yaml Verify the PowerScale system for newly created volume from volume.\n  To Unattach the volume from Host\nDelete the nginx application to Unattach the volume from the host:\nkubectl delete -f nginx.yaml\n  To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc   ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v1/installation/test/powerscale/","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v1.6.0 New Features/Changes  Added support for Kubernetes 1.21. Added support for Red Hat Enterprise Linux (RHEL) 8.4. Added support for CSI Spec 1.3. Added support for Volume Limit. Added support for node selector functionality to helm template. Added support for secret in YAML format. Added support for Dynamic log level changes. Added support to make dnsPolicy of node component configurable via Helm  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Resolution or workaround, if known     If the length of the nodeID exceeds 128 characters, the driver fails to update the CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn’t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in the CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future Kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581    ","excerpt":"Release Notes - CSI Driver for PowerScale v1.6.0 New Features/Changes …","ref":"/csm-docs/v1/release/powerscale/","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\n   Symptoms Prevention, Resolution or Workaround     The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret’s username and password for corresponding cluster   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn’t verify the certificates Check the isilon-certs- secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \"true\" for insecure connection. SSL validation is recommended in the production environment.   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver error: create volume failed, Access denied. create directory as requested This situation can happen when the user who created the base path is different from the user configured for the driver. Make sure the user used to deploy CSI-Driver must have enough rights on the base path (i.e. isiPath) to perform all operations.   Volume/filesystem is allowed to mount by any host in the network, though that host is not a part of the export of that particular volume under /ifs directory “Dell EMC PowerScale: OneFS NFS Design Considerations and Best Practices”: There is a default shared directory (ifs) of OneFS, which lets clients running Windows, UNIX, Linux, or Mac OS X access the same directories and files. It is recommended to disable the ifs shared directory in a production environment and create dedicated NFS exports and SMB shares for your workload.   Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class is not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same.   While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700.   If the hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to 1.4.0 there is a possibility of “localhost” as a stale entry in export Recommended setup: User should not map a hostname to loopback IP in /etc/hosts file   CSI Driver installation fails with the error message “error getting FQDN”. Map IP address of host with its FQDN in /etc/hosts file.   Driver node pod is in “CrashLoopBackOff” as “Node ID” generated is not with proper FQDN. This might be due to “dnsPolicy” implemented on the driver node pod which may differ with different networks. 1.This parameter is configurable in the helm installer and the user can try with different “dnsPolicy” according to the environment. (values.yaml). 2. In the case of Operator installation, this parameter is not configurable at present and will be available in upcoming release. To overcome this issue, try to use appropriate “dnsPolicy” ( ClusterFirst / ClusterFirstWithHostNet ) by patching Isilon node pods. Example : kubectl patch daemonset isilon-node -n isilon -p ‘{“spec”: {“template”: {“spec”:{“dnsPolicy”: “ClusterFirst”}}}}’    ","excerpt":"Here are some installation failures that might be encountered and how …","ref":"/csm-docs/v1/troubleshooting/powerscale/","title":"PowerScale"},{"body":"Multicluster support You can connect single CSI-PowerScale driver with multiple PowerScale clusters. Pre-Requisistes:\n Creation of secret.json with credentials related to one or more Clusters. Creation of (at least) one Custom Storage classes for each non-default clusters. Creation of custom-volumesnapshot classes, if corresponding isiPaths differ in custom storage classes. Inclusion of cluster name in volume handle, if you want to provision existing static volumes.  Consuming existing volumes with static provisioning You can use existent volumes from PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, the PowerScale cluster accessZone is assumed as ‘System’, cluster name is assumed as ‘pscale-cluster’ and volume’s internal name as ‘isilonvol’. The volume-handle shoulb be in the format of \u003cvolume_name\u003e=_=_=\u003cexport_id\u003e=_=_==_=_=\u003ccluster_name\u003e  apiVersion:v1kind:PersistentVolumemetadata:name:isilonstaticpvnamespace:defaultspec:capacity:storage:5GiaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy:RetainstorageClassName:isiloncsi:driver:csi-isilon.dellemc.comvolumeAttributes:Path:\"/ifs/data/csi/isilonvol\"Name:\"isilonvol\"AzServiceIP:'XX.XX.XX.XX'volumeHandle:isilonvol=_=_=652=_=_=System=_=_=pscale-clusterclaimRef:name:isilonstaticpvcnamespace:defaultCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilonstaticpvcnamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:isilonstaticpvstorageClassName:isilonThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:isilonstaticpvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerScale driver version 1.3 and later supports managing beta snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller   For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerScale driver version 1.3 and later, a Volume Snapshot Class is created using the new recommended snapshot APIs (depends upon Kubernetes version). This is the Volume Snapshot Class created for the default isiPath provided in my-isilon-settings.yaml (which is created based on values.yaml). For additional custom storage classes, separate custom volume snapshot class should be created (only if the isiPath is different from default storage class).\nFollowing are the manifests for the Volume Snapshot Class created during installation:\n VolumeSnapshotClass - v1  # For kubernetes version 20 (v1 snaps)# This is a sample manifest for creating snapshotclass with IsiPath other than default# pvc is created with sc which has some different IsiPath e.g. /ifs/custom# to create a snapshot for this pvc volumesnapshotclass must also be initilized with same IsiPath (i.e. /ifs/custom ) to work snapshot featureapiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:\"isilon-snapclass-custom\"driver:csi-isilon.dellemc.com#The deletionPolicy of a volume snapshot class can either be Retain or Delete#If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object.#If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remaindeletionPolicy:Deleteparameters:#IsiPath should match with respective storageClass IsiPathIsiPath:\"/ifs/custom\"VolumeSnapshotClass - beta  # For kubernetes version 18 and 19 (beta snaps)# This is a sample manifest for creating snapshotclass with IsiPath other than default# pvc is created with sc which has some different IsiPath e.g. /ifs/custom# to create a snapshot for this pvc volumesnapshotclass must also be initilized with same IsiPath (i.e. /ifs/custom ) to work snapshot featureapiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:\"isilon-snapclass-custom\"driver:csi-isilon.dellemc.com#The deletionPolicy of a volume snapshot class can either be Retain or Delete#If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object.#If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remaindeletionPolicy:Deleteparameters:#IsiPath should match with respective storageClass IsiPathIsiPath:\"/ifs/custom\"### Create Volume SnapshotThe following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvcsnapnamespace:defaultspec:volumeSnapshotClassName:isilon-snapclasssource:persistentVolumeClaimName:autotestvolumeOnce the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:createfromsnapnamespace:defaultspec:storageClassName:isilondataSource:name:newsnapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteManyresources:requests:storage:5GiVolume Expansion The CSI PowerScale driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:\"false\"provisioner:\"csi-isilon.dellemc.com\"reclaimPolicy:Deleteparameters:ClusterName:\u003cclusterNamespecifiedinsecret.json\u003e AccessZone: SystemisiPath:\"/ifs/data/csi\"AzServiceIP :'XX.XX.XX.XX'rootClientEnabled:\"true\"allowVolumeExpansion:truevolumeBindingMode:ImmediateTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilon-pvc-demospec:accessModes:- ReadWriteOnceresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Volume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:existing-pvcspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonThe following is a sample manifest for cloning:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:volume-from-volumenamespace:defaultspec:accessModes:- ReadWriteManyvolumeMode:Filesystemresources:requests:storage:50GistorageClassName:isilondataSource:kind:PersistentVolumeClaimname:existing-pvcapiGroup:\"\"Controller HA The CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in Pending state.\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nEphemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumespec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data\"name:my-csi-volumevolumes:- name:my-csi-volumecsi:driver:csi-isilon.dellemc.comvolumeAttributes:size:\"2Gi\"ClusterName:\"cluster1\"This manifest creates a pod in given cluster and attach newly created ephemeral inline csi volume to it.\nTopology Topology Support The CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage, as a result of which the nodes which have access to PowerScale Array are appropriately labelled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and use them to initialize node pod with custom PowerScale FQDN/IP. Note: Only a single cluster can be configured as part of secret.json for custom topology.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that Pod scheduling takes advantage of the topology and the selected node has access to provisioned volumes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options.# PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon# Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server# Provide mount options through \"mountOptions\" attribute # to create PVCs with mount options.apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilonprovisioner:csi-isilon.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:trueparameters:AccessZone:SystemIsiPath:\"/ifs/data/csi\"# AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP.#AzServiceIP : 192.168.2.1# When a PVC is being created, it takes the storage class' value of \"storageclass.rootClientEnabled\", # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \"Root clients\" field (when true) or \"Clients\" field (when false) of the NFS export RootClientEnabled:\"false\"# Name of PowerScale cluster where pv will be provisioned# This name should match with name of one of the cluster configs in isilon-creds secret# If this parameter is not specified, then default cluster config in isilon-creds secret will be considered if available#ClusterName: \"\u003ccluster_name\u003e\"# volumeBindingMode controls when volume binding and dynamic provisioning should occur.# Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created# WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume# until a Pod using the PersistentVolumeClaim is createdvolumeBindingMode:WaitForFirstConsumer# allowedTopologies helps scheduling pod on worker nodes which matches all of below expressions# If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologiesallowedTopologies:- matchLabelExpressions:- key:csi-isilon.dellemc.com/\u003cISILON_IP\u003e values:- csi-isilon.dellemc.commountOptions:[\"\u003cmountOption1\u003e\",\"\u003cmountOption2\u003e\",...,\"\u003cmountOptionN\u003e\"]For additional information, see the Kubernetes Topology documentation.\nSupport for Docker EE The CSI Driver for Dell EMC PowerScale supports Docker EE and deployment on clusters bootstrapped with UCP (Universal Control Plane) 3.3.5. *UCP version 3.3.5 supports kubernetes 1.20 and CSI driver can be installed on UCP 3.3.5 with Helm.\nThe installation process for the driver on such clusters remains the same as the installation process on upstream clusters.\nOn UCP based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes in UCP backed clusters may run any of the OSs which we support with upstream clusters.\nSupport custom networks for NFS I/O traffic When allowedNetworks is specified for using custom networks to handle NFS traffic, and a user already has workloads scheduled, there is a possibility that it might lead to backwards compatibility issues. For example, ControllerUnPublish might not be able to completely remove clients from the NFS exports of previously created pods. Also, previous workload will still be using default network and not custom networks, for previous workloads to use custom networks recreation of pods required.\n","excerpt":"Multicluster support You can connect single CSI-PowerScale driver with …","ref":"/csm-docs/v2/features/powerscale/","title":"PowerScale"},{"body":"The CSI Driver for Dell EMC PowerScale can be deployed by using the provided Helm v3 charts in upstream Kubernetes. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a Daemon Set:\n CSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for PowerScale, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes or OpenShift (see supported versions) Configure Docker service Install Helm v3 Install volume snapshot components Deploy PowerScale driver using Helm  NOTE: There is no feature gate that needs to be set explicitly for CSI drivers version 1.17 and later. All the required feature gates are either beta/GA.\nConfigure Docker service The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for PowerScale.\nProcedure  Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file as follows: [Service] ... MountFlags=shared  Restart the Docker service with the following commands: systemctl daemon-reload systemctl restart docker   NOTE: Some distribution, like Ubuntu, already has MountFlags set by default.\nInstall volume snapshot components Install Snapshot CRDs  For Kubernetes 1.18 and 1.19, SnapShot CRDs versioned 3.0.3 (https://github.com/kubernetes-csi/external-snapshotter/tree/v3.0.3/client/config/crd), must be installed. For Kubernetes 1.20, SnapShot CRDs versioned 4.0.0 (https://github.com/kubernetes-csi/external-snapshotter/tree/v4.0.0/client/config/crd) must be installed.  Install Snapshot Controller  For Kubernetes 1.18 and 1.19, Snapshot controller versioned 3.0.3 (https://github.com/kubernetes-csi/external-snapshotter/tree/v3.0.3/deploy/kubernetes/snapshot-controller) must be installed. For Kubernetes 1.20, Snapshot controller versioned 4.0.0 (https://github.com/kubernetes-csi/external-snapshotter/tree/v4.0.0/deploy/kubernetes/snapshot-controller) must be installed.  Install CSI Driver for PowerScale Before you begin\n You must clone the source code from git repository. In the dell-csi-helm-installer directory, there will be two shell scripts, csi-install.sh and csi-uninstall.sh. These scripts handle some of the pre and post operations that cannot be performed in the helm chart.  Steps\n  Collect information from the PowerScale Systems like IP address,IsiPath, username and password. Make a note of the value for these parameters as they must be entered in the secret.json.\n  Copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation.\n  Edit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     certSecretCount Represents number of certificate secrets, which user is going to create for ssl authentication. (isilon-cert-0..isilon-cert-(n-1)); Minimum value should be 1 true 1   isiPort “isiPort” defines the HTTPs port number of the PowerScale OneFS API server false 8080   allowedNetworks “allowedNetworks” defines list of networks which can be used for NFS I/O traffic, CIDR format must be used false -   isiInsecure “isiInsecure” specifies whether the PowerScale OneFS API server’s certificate chain and host name must be verified. This value will affect the default storage class implementation false true   isiAccessZone The name of the access zone a volume can be created in false System   volumeNamePrefix “volumeNamePrefix” defines a string prepended to each volume created by the CSI driver. false k8s   controllerCount “controllerCount” defines the number of CSI PowerScale controller nodes to deploy to the Kubernetes release. true 2   enableDebug Indicates whether debug level logs should be logged false true   verbose Indicates what content of the OneFS REST API message should be logged in debug level logs false 1   enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. false true   noProbeOnStart Indicates whether the controller/node should probe during initialization false false   isiPath The default base path for the volumes to be created, this will be used if a storage class does not have the IsiPath parameter specified false /ifs/data/csi   autoProbe Enable auto probe. false true   nfsV3 Specify whether to set the version to v3 when mounting an NFS export. If the value is “false”, then the default version supported will be used (that is, the mount command will not explicitly specify “-o vers=3” option). This flag has now been deprecated and will be removed in a future release. Use the StorageClass.mountOptions if you want to specify ‘vers=3’ as a mount option. false false   enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish connection to Array. This requires enableCustomTopology to be enabled. false false   Controller parameters Set nodeSelector and tolerations for controller     nodeSelector Define nodeSelector for the controllers, if required false    tolerations Define tolerations for the controllers, if required false     NOTES\n User should provide all boolean values with double quotes. This applicable only for my-isilon-settings.yaml. Example: “true”/“false” ControllerCount parameter value should not exceed number of nodes in the Kubernetes cluster. Otherwise some of the controller pods will be in “Pending” state till new nodes are available for scheduling. The installer will exit with a WARNING on the same. Whenever certSecretCount parameter changes in myvalues.yaml user needs to reinstall the driver.    Create namespace Run kubectl create namespace isilon to create the isilon namespace. Specify the same namespace name while installing the driver.\nNOTE: CSI PowerScale also supports installation of driver in custom namespace.\n  Create a secret file for the OneFS credentials by editing the secret.json present under helm directory. This secret.json can be used for adding the credentials of one or more OneFS storage arrays.The following table lists driver configuration parameters for a single storage array.\n   Parameter Description Required Default     isiIP “isiIP” defines the HTTPs endpoint of the PowerScale OneFS API server true -   clusterName PoweScale cluster against which volume CRUD operations are performed through this secret. This is a logical name. true -   username Username for accessing PowerScale OneFS system true -   password Password for accessing PowerScale OneFS system true -   isDefaultCluster defines whether this storage array should be the default.This entry should be present only for one OneFS array and that array will be marked default for existing volumes. true false   Optional parameters Following parameters are Optional, if provided , will override default values of values.yaml     isiPort isiPort defines the HTTPs port number of the PowerScale OneFS API server false -   isiInsecure “isiInsecure” specifies whether the PowerScale OneFS API server’s certificate chain and host name should be verified. false false   isiPath The base path for the volumes to be created. Note: isiPath value provided in the storage class will take the highest precedence while creating PVC true -    The username specified in secret.json must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\nISI_PRIV_LOGIN_PAPI ISI_PRIV_NFS ISI_PRIV_QUOTA ISI_PRIV_SNAPSHOT ISI_PRIV_IFS_RESTORE ISI_PRIV_NS_IFS_ACCESS After editing the file, run the following command to create a secret called isilon-creds  kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.json\nNOTES:\n If any key/value is present in both secret.json and my-isilon-settings.yaml, then the values provided secret.json will take precedence. If any key/value is present in both my-isilon-settings.yaml/secret.json and storageClass, then the values provided in storageClass parameters will take precedence. User has to validate the JSON syntax and array related key/values while replacing or appending the isilon-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.    Install OneFS CA certificates by following the instructions from next section, if you want to validate OneFS API server’s certificates. If not, create an empty secret using the following command and empty secret should be created for the successful CSI Driver for Dell EMC Powerscale installation.\nkubectl create -f emptysecret.yaml   Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace isilon --values ../helm/my-isilon-settings.yaml\n  In the case of OpenShift, the driver installation will fail because of a lack of privileges over clusterRole. To resolve this issue the command oc adm policy add-scc-to-user privileged -z isilon-node -n isilon and re-install the driver. This solution will be added in next release.\n  Certificate validation for OneFS REST API calls The CSI driver exposes an install parameter ‘isiInsecure’ which determines if the driver performs client-side verification of the OneFS certificates. The ‘isiInsecure’ parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the ‘isiInsecure’ is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the ‘isiInsecure’ parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure  To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem To create the certs secret, run kubectl create secret generic isilon-certs-0 --from-file=cert-0=ca_cert_0.pem -n isilon Use the following command to replace the secret  kubectl create secret generic isilon-certs-0 -n isilon --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -  NOTES:\n The OneFS IP can be with or without port , depends upon the configuration of OneFS API server. Above said commands is based on the namespace ‘isilon’ It is highly recommended that ca_cert.pem file(s) having the naming convention as ca_cert_number.pem (example: ca_cert_0, ca_cert_1), where this number starts from 0 and grows as number of OneFS arrays grows. The cert secret created out of these pem files should have the naming convention as isilon-certs-number (example: isilon-certs-0, isilon-certs-1 etc.); The number should start from zero and should grow in incremental order. The number of the secrets created out of pem files should match certSecretCount value in myvalues.yaml or my-isilon-settings.yaml.  Dynamic update of array details via secret.json CSI Driver for Dell EMC PowerScale now provides supports for Multi cluster. Now user can link the single CSI Driver to multiple OneFS Clusters by updating secret.json. User can now update the isilon-creds secret by editing the secret.json and executing following command:\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.json -o yaml --dry-run=client | kubectl replace -f - Storage Classes Storage Classes are an essential Kubernetes construct for Storage provisioning. To know more about Storage Classes, please refer: https://kubernetes.io/docs/concepts/storage/storage-classes/\nStarting from v1.5 of the driver, Storage Classes would no longer be created along with the installation of the driver. A wide set of annotated storage class manifests have been provided in the helm/samples/storageclass folder. Please use these samples to create new storage classes to provision storage.\nStarting in CSI PowerScale v1.5, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the helm/samples/storageclass folder. Please use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from CSI PowerScale v1.4 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so. Since in CSI-PowerScale 1.5 Multi array is supported. The existing storage class (of 1.4) should be treated as default storage class.\nUpgrading from an older version of the driver It is strongly recommended to upgrade older versions of CSI-PowerScale to CSI-PowerScale 1.4 before upgrading to 1.5.\nSteps to create storage class: There are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\nNOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You will not be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerScale can be deployed by using the …","ref":"/csm-docs/v2/installation/helm/isilon/","title":"PowerScale"},{"body":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell EMC PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nListing installed drivers with the CSI Isilon CRD User can query for csi-powerscale driver using the following command: kubectl get csiisilon --all-namespaces\nInstall Driver   Create namespace Run kubectl create namespace isilon to create the isilon namespace. Note that the namespace can be any user defined name , in this example, we assume that the namespace is ‘isilon’.\n  Create isilon-creds Create a json file called isilon-creds.json with the following content:\n{ \"isilonClusters\": [ { \"clusterName\": \"cluster1\", \"username\": \"user\", \"password\": \"password\", \"isiIP\": \"1.2.3.4\", \"isDefaultCluster\": true }, { \"clusterName\": \"cluster2\", \"username\": \"user\", \"password\": \"password\", \"isiIP\": \"1.2.3.5\", \"isiPort\": \"8080\", \"isiInsecure\": true, \"isiPath\": \"/ifs/data/csi\" } ] } Replace the values for the given keys as per your environment. This username / password value need not be encoded. You can refer here for more information about isilon secret parameters.\n  Create isilon-certs- secret Please refer this section for creating cert-secrets. Run kubectl create -f isilon-creds.yaml command to create the secret.\n  Create a CR (Custom Resource) for PowerScale using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_DEBUG To enable debug mode No false   X_CSI_ISI_ENDPOINT HTTPs endpoint of the PowerScale OneFS API server Yes    X_CSI_ISI_INSECURE Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISILON_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    Node parameters      X_CSI_ISILON_NFS_V3 Set the version to v3 when mounting an NFS export. If the value is “false”, then the default version supported will be used Yes    X_CSI_MODE Driver starting mode No node      Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver in the namespace specified in input yaml file.\n  ","excerpt":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell …","ref":"/csm-docs/v2/installation/operator/isilon/","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\n  Creating a storage class:\nCreate a file storageclass.yaml using sample yaml file located at helm/samples/storageclass\nExecute the following command to create storage class:\nkubectl create -f $PWD/storageclass.yaml Result: After executing the above command storage class will be created in the default namespace, and the user can see the storage class by executing kubectl get sc. Note: Verify system for the new storage class.\n  Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at test/sample_files/\nExecute the following command to create volume:\nkubectl create -f $PWD/pvc.yaml Result: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing kubectl get pvc. Note: Verify system for the new volume. Note that the status of the volume can be either Bound or Pending depending on the VolumeBindingMode specified on storage class.\n  Attach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at test/sample_files/.\nExecute the following command to mount the volume to Kubernetes node:\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for host to be part of clients/rootclients field of export created for volume and used by nginx application.\n  Create Snapshot\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snap.yaml. The sample file for snapshot creation is located at test/sample_files/\nExecute the following command to create snapshot:\nkubectl create -f $PWD/snap.yaml The spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is testvolclaim1, then the created snapshot is named testvolclaim1-snap1. Verify the PowerScale system for newly created snapshot.\nNote:\n User can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. The CSI Driver for PowerScale installation creates this class as its default snapshot class. You can see its definition using kubectl get volumesnapshotclasses isilon-snapclass -o yaml. The value of IsiPath in default VolumeSnapshotClass is taken from values.yaml. If user wants different path, she has to create custom volumesnapshot class with required IsiPath in parameters section. Sample VolumeSnapshotClass file is present under helm/samples/volumesnapshotclass    Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in spec dataSource field.\nThe sample file for volume creation from snapshot is located under test/sample_files/\nExecute the following command to create snapshot:\nkubectl create -f $PWD/volume_from_snap.yaml Verify the PowerScale system for newly created volume from snapshot.\n  Delete Snapshot\nExecute the following commands to delete the snapshot:\nkubectl get volumesnapshot kubectl delete volumesnapshot pvcsnap   Create new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in spec dataSource field.\nThe sample file for volume creation from volume is located at test/sample_files/\nExecute the following command to create snapshot:\nkubectl create -f $PWD/volume_from_volume.yaml Verify the PowerScale system for new created volume from volume.\n  To Unattach the volume from Host\nDelete the nginx application to unattach the volume from host:\nkubectl delete -f nginx.yaml\n  To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc   ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v2/installation/test/powerscale/","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v1.5.0 New Features/Changes  Added support for Kubernetes 1.20 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 8.x Added multi-cluster support through single instance of driver installation Added support for custom networks for NFS I/O traffic SSH permissions are no longer required. You can safely revoke the privilege ISI_PRIV_LOGIN_SSH for the CSI driver user.  Fixed Issues There are no Fixed issues in this release.\nKnown Issues    Issue Resolution or workaround, if known     Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class are not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same.   While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700.   If hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to 1.4.0 there is a possibility of “localhost” as stale entry in export We recommend you not to map hostname to loopback IP in /etc/hosts file   If the length of the nodeID exceeds 128 characters, driver fails to update CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn’t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581   Driver installation warning: “OpenShift version 4.7, is newer than the version that has been tested. Latest tested version is: 4.6” Ignore this warning and continue with the installation. v1.5.0 release of the driver supports OpenShift 4.6/4.7 .    ","excerpt":"Release Notes - CSI Driver for PowerScale v1.5.0 New Features/Changes …","ref":"/csm-docs/v2/release/powerscale/","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\n   Symptoms Prevention, Resolution or Workaround     The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret’s username and password for corresponding cluster   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn’t verify the certificates Check the isilon-certs- secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \"true\" for insecure connection. SSL validation is recommended in production environment.   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver error: create volume failed, Access denied. create directory as requested This situation can happen when the user who created the base path is different from the user configured for the driver. Make sure the user used to deploy CSI-Driver must have enough rights on the base path (i.e. isiPath) to perform all operations.    ","excerpt":"Here are some installation failures that might be encountered and how …","ref":"/csm-docs/v2/troubleshooting/powerscale/","title":"PowerScale"},{"body":"Consuming existing volumes with static provisioning You can use existent volumes from PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:isilonstaticpvnamespace:defaultspec:capacity:storage:5GiaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy:RetainstorageClassName:isiloncsi:driver:csi-isilon.dellemc.comvolumeAttributes:Path:\"/ifs/data/csi/isilonvol\"Name:\"isilonvol\"AzServiceIP:'XX.XX.XX.XX'volumeHandle:isilonvol=_=_=652=_=_=SystemclaimRef:name:isilonstaticpvcnamespace:defaultCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilonstaticpvcnamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:isilonstaticpvstorageClassName:isilonThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:isilonstaticpvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerScale driver version 1.3 and later supports managing beta snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller   For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerScale driver version 1.3 and later, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:isilon-snapclassdriver:csi-isilon.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvcsnapnamespace:defaultspec:volumeSnapshotClassName:isilon-snapclasssource:persistentVolumeClaimName:autotestvolumeOnce the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:createfromsnapnamespace:defaultspec:storageClassName:isilondataSource:name:newsnapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteManyresources:requests:storage:5GiVolume Expansion The CSI PowerScale driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:\"false\"provisioner:\"csi-isilon.dellemc.com\"reclaimPolicy:Deleteparameters:AccessZone:SystemisiPath:\"/ifs/data/csi\"AzServiceIP :'XX.XX.XX.XX'rootClientEnabled:\"true\"allowVolumeExpansion:truevolumeBindingMode:ImmediateTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilon-pvc-demospec:accessModes:- ReadWriteOnceresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Volume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:existing-pvcspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonThe following is a sample manifest for cloning:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:volume-from-volumenamespace:defaultspec:accessModes:- ReadWriteManyvolumeMode:Filesystemresources:requests:storage:50GistorageClassName:isilondataSource:kind:PersistentVolumeClaimname:existing-pvcapiGroup:\"\"Controller HA The CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in Pending state.\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nEphemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumespec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data\"name:my-csi-volumevolumes:- name:my-csi-volumecsi:driver:csi-isilon.dellemc.comvolumeAttributes:size:\"2Gi\"This manifest will create a pod and attach newly created ephemeral inline csi volume to it.\nTopology Topology Support The CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage, as a result of which the nodes which have access to PowerScale Array are appropriately labelled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and use them to initialize node pod with custom PowerScale FQDN/IP.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that Pod scheduling takes advantage of the topology and the selected node has access to provisioned volumes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options.# PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon# Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server# Provide mount options through \"mountOptions\" attribute # to create PVCs with mount options.apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilonprovisioner:csi-isilon.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:trueparameters:AccessZone:SystemIsiPath:\"/ifs/data/csi\"# AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP.#AzServiceIP : 192.168.2.1# When a PVC is being created, it takes the storage class' value of \"storageclass.rootClientEnabled\", # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \"Root clients\" field (when true) or \"Clients\" field (when false) of the NFS export RootClientEnabled:\"false\"# volumeBindingMode controls when volume binding and dynamic provisioning should occur.# Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created# WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume# until a Pod using the PersistentVolumeClaim is createdvolumeBindingMode:WaitForFirstConsumer# allowedTopologies helps scheduling pod on worker nodes which matches all of below expressions# If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologiesallowedTopologies:- matchLabelExpressions:- key:csi-isilon.dellemc.com/\u003cISILON_IP\u003e values:- csi-isilon.dellemc.commountOptions:[\"\u003cmountOption1\u003e\",\"\u003cmountOption2\u003e\",...,\"\u003cmountOptionN\u003e\"]For additional information, see the Kubernetes Topology documentation.\n","excerpt":"Consuming existing volumes with static provisioning You can use …","ref":"/csm-docs/v3/features/powerscale/","title":"PowerScale"},{"body":"The CSI Driver for Dell EMC PowerScale can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a Daemon Set:\n CSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for PowerScale, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes. Configure Docker service Install Helm v3 Install volume snapshot components Deploy PowerScale driver using Helm  Note: There is no feature gate that needs to be set explicitly for CSI drivers version 1.17 and later. All the required feature gates are either beta/GA.\nConfigure Docker service The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for PowerScale.\nProcedure  Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file as follows: [Service] ... MountFlags=shared  Restart the Docker service with systemctl daemon-reload and systemctl daemon-reload systemctl restart docker   Install volume snapshot components Install Snapshot Beta CRDs To install snapshot CRDs specify --snapshot-crd flag to driver installation script dell-csi-helm-installer/csi-install.sh during driver installation.\nInstall Common Snapshot Controller, if not already installed for the cluster.\n The manifests available on GitHub install v3.0.2 of the snapshotter image - quay.io/k8scsi/csi-snapshotter:v3.0.2 Dell recommends using v3.0.2 image of the snapshot-controller - quay.io/k8scsi/snapshot-controller:v3.0.2  Install CSI Driver for PowerScale Before you begin\n You must clone the source code from git repository. In the dell-csi-helm-installer directory, there should be two shell scripts, csi-install.sh and csi-uninstall.sh. These scripts handle some of the pre and post operations that cannot be performed in the helm chart.  Steps\n  Collect information from the PowerScale Systems like IP address, username and password. Make a note of the value for these parameters as they must be entered in the secret.yaml and values file.\n  Copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation.\n  Edit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     isiIP “isiIP” defines the HTTPs endpoint of the PowerScale OneFS API server true -   isiPort “isiPort” defines the HTTPs port number of the PowerScale OneFS API server false 8080   isiInsecure “isiInsecure” specifies whether the PowerScale OneFS API server’s certificate chain and host name should be verified. false true   isiAccessZone The name of the access zone a volume can be created in false System   volumeNamePrefix “volumeNamePrefix” defines a string prepended to each volume created by the CSI driver. false k8s   controllerCount “controllerCount” defines the number of CSI PowerScale controller nodes to deploy to the Kubernetes release. true 2   enableDebug Indicates whether debug level logs should be logged false true   verbose Indicates what content of the OneFS REST API message should be logged in debug level logs false 1   enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. false true   noProbeOnStart Indicates whether the controller/node should probe during initialization false false   isiPath The default base path for the volumes to be created, this will be used if a storage class does not have the IsiPath parameter specified false /ifs/data/csi   autoProbe Enable auto probe. false true   nfsV3 Specify whether to set the version to v3 when mounting an NFS export. If the value is “false”, then the default version supported will be used (that is, the mount command will not explicitly specify “-o vers=3” option). This flag has now been deprecated and will be removed in a future release. Use the StorageClass.mountOptions if you want to specify ‘vers=3’ as a mount option. false false   enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish connection to Array. This requires enableCustomTopology to be enabled. false false   Storage Class parameters Following parameters are related to Storage Class     name “storageClass.name” defines the name of the storage class to be defined. false isilon   isDefault “storageClass.isDefault” defines whether the primary storage class should be the default. false true   reclaimPolicy “storageClass.reclaimPolicy” defines what will happen when a volume is removed from the Kubernetes API. Valid values are “Retain” and “Delete”. false Delete   accessZone The Access Zone where the Volume would be created false System   AzServiceIP Access Zone service IP if different from isiIP, specify here and refer in storageClass false    rootClientEnabled When a PVC is being created, it takes the storage class’ value of “storageclass.rootClientEnabled” false false   Controller parameters Set nodeSelector and tolerations for controller     nodeSelector Define nodeSelector for the controllers, if required false    tolerations Define tolerations for the controllers, if required false     Note: User should provide all boolean values with double quotes. This is applicable only for my-isilon-settings.yaml. Example: “true”/“false”\nNote: controllerCount parameter value should not exceed number of nodes in the kubernetes cluster. Otherwise some of the controller pods will be in “Pending” state till new nodes are available for scheduling. The installer will exit with a WARNING on the same.\n  Create namespace Run kubectl create namespace isilon to create the isilon namespace. Specify the same namespace name while installing the driver.\nNote: CSI PowerScale also supports installation of driver in custom namespace.\n  Create a secret file for the OneFS credentials by editing the secret.yaml present under helm directory. Replace the values for the username and password parameters. Use the following command to convert username/password to base64 encoded string:\necho -n 'admin' | base64 echo -n 'password' | base64 Run kubectl create -f secret.yaml to create the secret.\nNote: The username specified in secret.yaml must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\nISI_PRIV_LOGIN_PAPI ISI_PRIV_NFS ISI_PRIV_QUOTA ISI_PRIV_SNAPSHOT ISI_PRIV_IFS_RESTORE ISI_PRIV_NS_IFS_ACCESS ISI_PRIV_LOGIN_SSH   Install OneFS CA certificates by following the instructions from next section, if you want to validate OneFS API server’s certificates. If not, create an empty secret using the following command and empty secret should be created for the successful CSI Driver for Dell EMC Powerscale installation.\nkubectl create -f emptysecret.yaml   Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace isilon --values ../helm/myvalues.yaml\n  Certificate validation for OneFS REST API calls The CSI driver exposes an install parameter ‘isiInsecure’ which determines if the driver performs client-side verification of the OneFS certificates. The ‘isiInsecure’ parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the ‘isiInsecure’ is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the ‘isiInsecure’ parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure  To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert.pem To create the secret, run kubectl create secret generic isilon-certs --from-file=ca_cert.pem -n isilon   Storage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and can’t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting in Q4 of 2020, an annotation \"helm.sh/resource-policy\": keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNOTE: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\n Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, make sure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You won’t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerScale can be deployed by using the …","ref":"/csm-docs/v3/installation/helm/isilon/","title":"PowerScale"},{"body":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell EMC PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nListing installed drivers with the CSI Isilon CRD User can query for csi-powerscale driver using the following command: kubectl get csiisilon --all-namespaces\nInstall Driver  Create namespace Run kubectl create namespace isilon to create the isilon namespace. Create isilon-creds Create a file called isilon-creds.yaml with the following content: apiVersion:v1kind:Secretmetadata:name:isilon-credsnamespace:isilontype:Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003eReplace the values for the username and password parameters. These values can be optioned using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 Run kubectl create -f isilon-creds.yaml command to create the secret.\n Create a CR (Custom Resource) for PowerScale using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:    Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_DEBUG To enable debug mode No false   X_CSI_ISI_ENDPOINT HTTPs endpoint of the PowerScale OneFS API server Yes    X_CSI_ISI_INSECURE Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISILON_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    Node parameters      X_CSI_ISILON_NFS_V3 Set the version to v3 when mounting an NFS export. If the value is “false”, then the default version supported will be used Yes    X_CSI_MODE Driver starting mode No node     Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver.  ","excerpt":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell …","ref":"/csm-docs/v3/installation/operator/isilon/","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\n  Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at test/sample_files/\nExecute the following command to create volume\nkubectl create -f $PWD/pvc.yaml Result: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing kubectl get pvc. Note: Verify system for the new volume\n  Attach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at test/sample_files/.\nExecute the following command to mount the volume to Kubernetes node\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for host to be part of clients/rootclients field of export created for volume and used by nginx application.\n  Create Snapshot\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snap.yaml. The sample file for snapshot creation is located at test/sample_files/\nExecute the following command to create snapshot\nkubectl create -f $PWD/snap.yaml The spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is testvolclaim1, then the created snapshot is named testvolclaim1-snap1. Verify the PowerScale system for newly created snapshot.\nNote:\n User can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. The CSI Driver for PowerScale installation creates this class as its default snapshot class. You can see its definition using kubectl get volumesnapshotclasses isilon-snapclass -o yaml.    Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in spec dataSource field.\nThe sample file for volume creation from snapshot is located under test/sample_files/\nExecute the following command to create snapshot\nkubectl create -f $PWD/volume_from_snap.yaml Verify the PowerScale system for newly created volume from snapshot.\n  Delete Snapshot\nExecute the following commands to delete the snapshot\nkubectl get volumesnapshot kubectl delete volumesnapshot testvolclaim1-snap1   Create new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in spec dataSource field.\nThe sample file for volume creation from volume is located at test/sample_files/\nExecute the following command to create snapshot\nkubectl create -f $PWD/volume_from_volume.yaml Verify the PowerScale system for new created volume from volume.\n  To Unattach the volume from Host\nDelete the nginx application to unattach the volume from host\nkubectl delete -f nginx.yaml\n  To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc   ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v3/installation/test/powerscale/","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v1.4.0 New Features/Changes  Added support for OpenShift 4.6 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Ubuntu 20.04 Added support for Controller high availability (multiple-controllers) Added Topology support Added support for CSI Ephemeral Inline Volumes Added support for mount options Enhancements to volume creation from data source Enhanced support for Docker EE 3.1  Fixed Issues    Problem summary Found in version Resolved in version     POD creation fails in OpenShift and Kubernetes environments, if hostname is not an FQDN v1.3.0 v1.4.0   When creating volume from a snapshot or volume from volume, the owner of the new files or folders that are copied from the source snapshot is the Isilon user who is specified in secret.yaml. So the original owner of a file or folder might not be the owner of the newly created file or folder.  v1.4.0    Known Issues    Issue Resolution or workaround, if known     Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class are not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same.   While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\n* ISI_PRIV_LOGIN_SSH\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700.   If hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to 1.4.0 there is a possibility of “localhost” as stale entry in export We recommend you not to map hostname to loopback IP in /etc/hosts file   If the length of the nodeID exceeds 128 characters, driver fails to update CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn’t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581    ","excerpt":"Release Notes - CSI Driver for PowerScale v1.4.0 New Features/Changes …","ref":"/csm-docs/v3/release/powerscale/","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\n   Symptoms Prevention, Resolution or Workaround     The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret’s username and password   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn’t verify the certificates Check the isilon-certs secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \"true\" for insecure connection. SSL validation is recommended in production environment.    ","excerpt":"Here are some installation failures that might be encountered and how …","ref":"/csm-docs/v3/troubleshooting/powerscale/","title":"PowerScale"},{"body":"The CSI Driver for Dell EMC PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell EMC PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers (Optional) Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell EMC PowerStore Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell EMC PowerStore:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator sections below. You can use NFS volumes without FC or iSCSI configuration.   You can use either the Fibre Channel or iSCSI protocol, but you do not need both.\n  If you want to use preconfigured iSCSI/FC hosts be sure to check that they are not part of any host group\n  Linux native multipathing requirements Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements Nonsecure registries are defined in Docker or other container runtimes, for CSI drivers that are hosted in a non-secure location. You can access your cluster with kubectl and helm. Ensure that your nodes support mounting NFS volumes.  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerStore.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell EMC PowerStore supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell EMC PowerStore:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done.  Set up the iSCSI Initiator The CSI Driver for Dell EMC PowerStore v1.4 and higher supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\n Ensure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell EMC PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi.  For information about configuring iSCSI, see Dell EMC PowerStore documentation on Dell EMC Support.\nLinux multipathing requirements Dell EMC PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell EMC PowerStore.\nSet up Linux multipathing as follows:\n Ensure that all nodes have the Device Mapper Multipathing package installed.   You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\n  Enable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes.  (Optional) Volume Snapshot Requirements Applicable only if you decided to enable the snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Use v4.2.x for the installation.\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available: Use v4.2.x for the installation.\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 4.2.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is installed along with the driver and does not involve any extra configuration.  (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication:enabled:trueReplication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\n  Run git clone -b v2.0.0 https://github.com/dell/csi-powerstore.git to clone the git repository.\n  Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one. “csi-powerstore” is just an example. You can choose any name for the namespace. But make sure to align to the same namespace during the whole installation.\n  Check helm/csi-powerstore/driver-image.yaml and confirm the driver image points to new image.\n  Edit samples/secret/secret.yaml file and configure connection information for your PowerStore arrays changing following parameters:\n endpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what SCSI transport protocol we should use (FC, ISCSI, None, or auto). nasName: defines what NAS should be used for NFS volumes.  Add more blocks similar to above for each PowerStore array if necessary.\n  Create storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n If you do not specify arrayID parameter in the storage class then the array that was specified as the default would be used for provisioning volumes.\n   Create the secret by running kubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml\n  Copy the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml\n  Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:\n     Parameter Description Required Default     logLevel Defines CSI driver log level No “debug”   logFormat Defines CSI driver log format No “JSON”   externalAccess Defines additional entries for hostAccess of NFS volumes, single IP address and subnet are valid entries No \" \"   kubeletConfigDir Defines kubelet config path for cluster Yes “/var/lib/kubelet”   imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Yes “IfNotPresent”   connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No False   controller.controllerCount Defines number of replicas of controller deployment Yes 2   controller.volumeNamePrefix Defines the string added to each volume that the CSI driver creates No “csivol”   controller.snapshot.enabled Allows to enable/disable snapshotter sidecar with driver installation for snapshot feature No “true”   controller.snapshot.snapNamePrefix Defines prefix to apply to the names of a created snapshots No “csisnap”   controller.resizer.enabled Allows to enable/disable resizer sidecar with driver installation for volume expansion feature No “true”   controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \" \"   controller.tolerations Defines tolerations that would be applied to controller deployment Yes \" \"   node.nodeNamePrefix Defines the string added to each node that the CSI driver registers No “csi-node”   node.nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No “/etc/machine-id”   node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \" \"   node.tolerations Defines tolerations that would be applied to node daemonset Yes \" \"    Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml  After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n csi-powerstore    NOTE:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using node.nodeNamePrefix and the ID read from the file pointed to by node.nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. A hostname the driver uses for registration of adapters is in the form \u003cnodeNamePrefix\u003e-\u003cnodeID\u003e-\u003cnodeIP\u003e. By default, these are csi-node and the machine ID read from the file /etc/machine-id. To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes The CSI driver for Dell EMC PowerStore version 1.3 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class:\nThere are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\n Edit the sample storage class yaml file and update following parameters:   arrayID: specifies what storage cluster the driver should use, if not specified driver will use storage cluster specified as default in samples/secret/secret.yaml FsType: specifies what filesystem type driver should use, possible variants ext4, xfs, nfs, if not specified driver will use ext4 by default. allowedTopologies (Optional): If you want you can also add topology constraints.  allowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/12.34.56.78-iscsi# replace \"-iscsi\" with \"-fc\" or \"-nfs\" at the end to use FC or NFS enabled hosts# replace \"12.34.56.78\" with PowerStore endpoint IPvalues:- \"true\"Create your storage class by using kubectl:  kubectl create -f \u003cpath_to_storageclass_file\u003e NOTE: Deleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerStore v1.4, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerStore v1.4 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerStore to 1.4 before upgrading to 2.0.\nDynamically update the powerstore secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\nkubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\n","excerpt":"The CSI Driver for Dell EMC PowerStore can be deployed by using the …","ref":"/csm-docs/docs/csidriver/installation/helm/powerstore/","title":"PowerStore"},{"body":"Installing CSI Driver for PowerStore via Operator The CSI Driver for Dell EMC PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note: The deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver   Create namespace:\nRun kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\n  Create PowerStore array connection config:\nCreate a file called config.yaml with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIglobalID:\"unique\"# unique id of the PowerStore arrayusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIskipCertificateValidation:true# indicates if client side validation of (management)server's certificate can be skippedisDefault:true# treat current array as a default (would be used by storage classes without arrayID parameter)blockProtocol:\"auto\"# what SCSI transport protocol use on node side (FC, ISCSI, None, or auto)nasName:\"nas-server\"# what NAS should be used for NFS volumesChange the parameters with relevant values for your PowerStore array.\nAdd more blocks similar to above for each PowerStore array if necessary.\n  Create Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLCombine both files and create Kubernetes secret by running the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f -   Create a Custom Resource (CR) for PowerStore using the sample files provided here.\n  Users must configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:\n   Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be pending state till new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2   namespace Specifies namespace where the drive will be installed Yes “test-powerstore”   Common parameters for node and controller      X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node”   X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter”   Controller parameters      X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No \" \"   Node parameters      X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false      Execute the following command to create PowerStore custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerStore driver.\n After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e    Dynamic Logging Configuration This feature is introduced in CSI Driver for unity version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params Note :\n “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  ","excerpt":"Installing CSI Driver for PowerStore via Operator The CSI Driver for …","ref":"/csm-docs/docs/csidriver/installation/operator/powerstore/","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs, and nfs storage classes and automatically mounts them to the pod.\n It assumes that you’ve created the same basic three storage classes from samples/storageclass folder without changing their names. If you’ve created different storage classes please edit tests/simple/simple.yaml and change PersistentVolumeClaim definitions to point to correct storage classes.\n Steps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/   You can find all the created resources in testpowerstore namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/csm-docs/docs/csidriver/installation/test/powerstore/","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v2.0.0 New Features/Changes  Added support for Kubernetes v1.22. Added support for OpenShift 4.8. Added the ability to change log level and log format of CSI driver and change them dynamically. Added the ability to configure kubelet directory path. Added the ability to enable/disable installation of resizer sidecar with driver installation. Added the ability to enable/disable installation of snapshotter sidecar with driver installation. Added support for consistent config parameters across CSI drivers.  Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\n","excerpt":"Release Notes - CSI PowerStore v2.0.0 New Features/Changes  Added …","ref":"/csm-docs/docs/csidriver/release/powerstore/","title":"PowerStore"},{"body":"   Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods powerstore-controller-\u003csuffix\u003e –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n csi-powerstore powerstore-node-\u003csuffix\u003e driver logs show that the driver can’t connect to PowerStore API. Check if you’ve created a secret with correct credentials   Installation of the driver on Kubernetes supported versions fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.20/1.21/v1.22 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements   If PVC is not getting created and getting the following error in PVC description: failed to provision volume with StorageClass \"powerstore-iscsi\": rpc error: code = Internal desc = : Unknown error: Check if you’ve created a secret with correct credentials    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/csm-docs/docs/csidriver/troubleshooting/powerstore/","title":"PowerStore"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command creates a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset are created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset’s pods by running kubectl get pods -n testpowerstore\nThe pod must be Ready and Running\n If Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run, use the command:\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u003cpowerstore.api.ip\u003e/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:existingvolspec:accessModes:- ReadWriteOncecapacity:storage:30Gicsi:driver:csi-powerstore.dellemc.comvolumeHandle:0055558c-5ae1-4ed1-b421-6f5a9475c19fpersistentVolumeReclaimPolicy:RetainstorageClassName:powerstorevolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30GistorageClassName:powerstoreThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:quay.io/centos/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:pvolAfter the pod is Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes version 1.17 and was generally available (v1) in Kubernetes version \u003e= 1.20.\nThe CSI PowerStore driver version 1.4 supports v1beta1 snapshots on Kubernetes 1.19 and v1 snapshots on Kubernetes 1.20/1.21.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class   Note: From v1.4, the CSI PowerStore driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the helm/samples folder\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powerstore-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueSnapshot feature is optional for the installation CSI PowerStore driver version 1.4 makes the snapshot feature optional for the installation.\nTo enable or disable this feature, specify the following in values.yaml\nsnapshot:enable:trueExternal Snapshotter and its CRDs are not installed even if the Snapshot feature is enabled. These have to be installed manually before the installation.\nDisabling the Snapshot feature will opt out of the snapshotter sidecar from the installation.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP The CSI PowerStore driver Version 1.3.0 and later extends Challenge Handshake Authentication Protocol (CHAP) support by adding automatic credentials generation.\nThis means that you no longer need to provide chapsecret/chapuser credentials, they will be automatically generated by the driver for each host.\nTo enable this feature you need to set connection.enableCHAP to true when installing with helm or set X_CSI_POWERSTORE_ENABLE_CHAP to true in your PowerStore CustomResource when installing using operator.\nThe driver uses the generated chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating a new host on powerstore array driver will populate host chap credentials with generated values. When re-using already existing hosts driver must override existing CHAP credentials with newly generated ones.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pstore-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:powerstoretestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powerstoreresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvol0namespace:testpowerstorespec:storageClassName:powerstoreaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clonedpvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI PowerStore driver version 1.2 and later supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind:PodapiVersion:v1metadata:name:powerstore-inline-volumespec:containers:- name:test-containerimage:quay.io/centos/centoscommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"20Gi\"This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI PowerStore driver version 1.2 and later introduces the controller HA feature. Instead of StatefulSet, controller pods are deployed as a Deployment.\nBy default number of replicas is set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in the cluster, each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods must be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As mentioned earlier, you can configure where node driver pods would be assigned in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they must use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology features user must create their own storage classes similar to those that can be found in helm/samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-fcprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/127.0.0.1-fcvalues:- \"true\"This example matches all nodes where the driver has a connection to PowerStore with an IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS and iSCSI.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels\n Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work.\n For any additional information about the topology, see the Kubernetes Topology documentation.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as a Host on the storage array before. It will check if Host initiators and node initiators (FC or iSCSI) match. If they do, the driver will not create a new storage class and will take the existing name of the Host as nodeID.\nMultiarray support The CSI PowerStore driver version 1.3.0 and later support managing multiple PowerStore arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration Create a file called config.yaml and populate it with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIglobalID:\"unique\"# global ID to identify arrayusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIskipCertificateValidation:true# use insecure connection or notdefault:true# treat current array as a default (would be used by storage classes without arrayIP parameter)blockProtocol:\"ISCSI\"# what SCSI transport protocol use on node side (FC, ISCSI, None, or auto)nasName:\"nas-server\"# what NAS must be used for NFS volumes- endpoint:\"https://10.0.0.2/api/rest\"globalID:\"unique\"username:\"user\"password:\"password\"skipCertificateValidation:trueblockProtocol:\"FC\"Here we specify that we want to CSI driver to manage two arrays: one with an IP 10.0.0.1 and the other with an IP 10.0.0.2, we want to connect to the first array with iSCSI protocol and with FC to the second array. Also, we want to be able to create NFS-based volume so we provide the name of the NAS to the first array.\nTo use this config we need to create a Kubernetes secret from it, to do so create a file called secret.yaml in the same folder and populate it with the following content:\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLApply the secret by running following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Creating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nCreate file storageclass.yaml and populate it with the following content:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-1provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayID:\"GlobalUniqueID\"FsType:\"ext4\"---apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-2provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayID:\"GlobalUniqueID\"FsType:\"xfs\"Here we specify two storage classes: one of them uses the first array and ext4 filesystem, and the other uses the second array and xfs filesystem.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use powerstore-1 storage class to create volumes on the first array and powerstore-2 storage class to create volumes on the second array.\nDynamic secret change detection CSI PowerStore driver version 1.3.0 and later supports the ability to detect changes to array configuration Kubernetes secret. This essentially means that you can change credentials for your PowerStore arrays in-flight (without restarting the driver).\nTo do so just change your configuration file config.yaml and apply it again using the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - After Kubernetes remounts secret to driver containers (this usually takes around one minute), a driver must detect the change and start using this new configuration information.\nConfiguring custom access to NFS exports CSI PowerStore driver Version 1.3.0 and later supports the ability to configure NFS access to nodes that use dedicated storage networks.\nTo enable this feature you need to specify externalAccess parameter in your helm values.yaml file or X_CSI_POWERSTORE_EXTERNAL_ACCESS variable when creating CustomResource using an operator.\nThe value of that parameter is added as an additional entry to NFS Export host access.\nFor example the following notation:\nexternalAccess:\"10.0.0.0/24\"This means that we allow for NFS Export created by driver to be consumed by address range 10.0.0.0-10.0.0.255.\nArray identification based on GlobalID CSI PowerStore driver version 1.4.0 slightly changes the way arrays are being identified in runtime. In previous versions of the driver, a management IP address was used to identify an array. The address change could lead to an invalid state of PV. From version 1.4.0 a unique GlobalID string is used for an array identification. It has to be specified in config.yaml and in Storage Classes.\nThe change provides backward compatibility with previously created PVs. However, to provision new volumes, make sure to delete old Storage Classes and create new ones with arrayID instead of arrayIP specified.\n NOTE: It is recommended to migrate the PVs to new identifiers before changing management IPs of storage systems. The recommended way to do it is to clone the existing volume and delete the old one. The cloned volume will automatically switch to using globalID instead of management IP.\n Root squashing CSI PowerStore driver version 1.4.0 allows users to enable root squashing for NFS volumes provisioned by the driver.\nRoot squashing rule prevents root users on NFS clients from exercising root privileges on the NFS server.\nTo enable this rule, you need to set parameter allowRoot to false in your NFS storage class.\nYour storage class definition must look similar to this:\napiVersion:storage.k8s.io/v1kind:StorageClass...parameters:...allowRoot:\"false\"# enables or disables root squashing The 1.4 version of the driver also enables any container user, to have full access to provisioned NFS volume, in earlier versions only root user had access\n ","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/csm-docs/v1/features/powerstore/","title":"PowerStore"},{"body":"The CSI Driver for Dell EMC PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell EMC PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers (Optional) Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell EMC PowerStore Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell EMC PowerStore:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator sections below. You can use NFS volumes without FC or iSCSI configuration.   You can use either the Fibre Channel or iSCSI protocol, but you do not need both.\n  If you want to use preconfigured iSCSI/FC hosts be sure to check that they are not part of any host group\n  Linux native multipathing requirements Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements Nonsecure registries are defined in Docker or other container runtimes, for CSI drivers that are hosted in a non-secure location. You can access your cluster with kubectl and helm. Ensure that your nodes support mounting NFS volumes.  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerStore.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell EMC PowerStore supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell EMC PowerStore:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done.  Set up the iSCSI Initiator The CSI Driver for Dell EMC PowerStore v1.4 supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\n Ensure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell EMC PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi.  For information about configuring iSCSI, see Dell EMC PowerStore documentation on Dell EMC Support.\nLinux multipathing requirements Dell EMC PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell EMC PowerStore.\nSet up Linux multipathing as follows:\n Ensure that all nodes have the Device Mapper Multipathing package installed.   You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\n  Enable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes.  (Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20/1.21 (v1 snapshots) use v4.0.x  Volume Snapshot Controller The beta Volume Snapshots in Kubernetes version 1.17 and later, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20 and 1.21 (v1 snapshots) use v4.0.x  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.x quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 3.0.x version of snapshotter/snapshot-controller when using Kubernetes 1.19 When using Kubernetes 1.20/1.21 it is recommended to use 4.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository.\n  Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one. “csi-powerstore” is just an example. You can choose any name for the namespace. But make sure to align to the same namespace during the whole installation.\n  Check helm/csi-powerstore/driver-image.yaml and confirm the driver image points to new image.\n  Edit helm/secret.yaml, correct namespace field to point to your desired namespace.\n  Edit helm/config.yaml file and configure connection information for your PowerStore arrays changing following parameters:\n endpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what SCSI transport protocol we should use (FC, ISCSI, None, or auto). nasName: defines what NAS should be used for NFS volumes.  Add more blocks similar to above for each PowerStore array if necessary.\n  Create storage classes using ones from helm/samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n If you do not specify arrayID parameter in the storage class then the array that was specified as the default would be used for provisioning volumes.\n   Create the secret by running sed \"s/CONFIG_YAML/`cat helm/config.yaml | base64 -w0`/g\" helm/secret.yaml | kubectl apply -f -\n  Copy the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml\n  Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:\n     Parameter Description Required Default     volumeNamePrefix Defines the string added to each volume that the CSI driver creates No “csi”   nodeNamePrefix Defines the string added to each node that the CSI driver registers No “csi-node”   nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No “/etc/machine-id”   externalAccess Defines additional entries for hostAccess of NFS volumes, single IP address and subnet are valid entries No \" \"   connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No False   controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \" \"   controller.tolerations Defines tolerations that would be applied to controller deployment Yes \" \"   controller.replicas Defines number of replicas of controller deployment Yes 2   node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \" \"   node.tolerations Defines tolerations that would be applied to node daemonset Yes \" \"    Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml  After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n csi-powerstore    NOTE:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using nodeNamePrefix and the ID read from the file pointed to by nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. A hostname the driver uses for registration of adapters is in the form \u003cnodeNamePrefix\u003e-\u003cnodeID\u003e-\u003cnodeIP\u003e. By default, these are csi-node and the machine ID read from the file /etc/machine-id. To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes The CSI driver for Dell EMC PowerStore version 1.3 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the helm/samples folder. Use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerStore v1.2 driver: The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNOTE: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class:\nThere are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\n Edit the sample storage class yaml file and update following parameters:   arrayID: specifies what storage cluster the driver should use, if not specified driver will use storage cluster specified as default in helm/config.yaml FsType: specifies what filesystem type driver should use, possible variants ext4, xfs, nfs, if not specified driver will use ext4 by default. allowedTopologies (Optional): If you want you can also add topology constraints.  allowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/12.34.56.78-iscsi# replace \"-iscsi\" with \"-fc\" or \"-nfs\" at the end to use FC or NFS enabled hosts# replace \"12.34.56.78\" with PowerStore endpoint IPvalues:- \"true\"Create your storage class by using kubectl:  kubectl create -f \u003cpath_to_storageclass_file\u003e NOTE: Deleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerStore v1.4, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the helm/samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\n","excerpt":"The CSI Driver for Dell EMC PowerStore can be deployed by using the …","ref":"/csm-docs/v1/installation/helm/powerstore/","title":"PowerStore"},{"body":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell EMC PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note: The deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver   Create namespace:\nRun kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\n  Create PowerStore array connection config:\nCreate a file called config.yaml with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIglobalID:\"unique\"# unique id of the PowerStore arrayusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIskipCertificateValidation:true# indicates if client side validation of (management)server's certificate can be skippedisDefault:true# treat current array as a default (would be used by storage classes without arrayID parameter)blockProtocol:\"auto\"# what SCSI transport protocol use on node side (FC, ISCSI, None, or auto)nasName:\"nas-server\"# what NAS should be used for NFS volumesChange the parameters with relevant values for your PowerStore array.\nAdd more blocks similar to above for each PowerStore array if necessary.\n  Create Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLCombine both files and create Kubernetes secret by running the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f -   Create a Custom Resource (CR) for PowerStore using the sample files provided here.\n  Users must configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:\n   Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be pending state till new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2   namespace Specifies namespace where the drive will be installed Yes “test-powerstore”   Common parameters for node and controller      X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node”   X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter”   Controller parameters      X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No \" \"   Node parameters      X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false      Execute the following command to create PowerStore custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerStore driver.\n After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e    ","excerpt":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell …","ref":"/csm-docs/v1/installation/operator/powerstore/","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs, and nfs storage classes and automatically mounts them to the pod.\n It assumes that you’ve created the same basic three storage classes from helm/samples/storageclass folder without changing their names. If you’ve created different storage classes please edit tests/simple/simple.yaml and change PersistentVolumeClaim definitions to point to correct storage classes.\n Steps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/   You can find all the created resources in testpowerstore namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/csm-docs/v1/installation/test/powerstore/","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v1.4.0 New Features/Changes  Added support for Kubernetes v1.21 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Added support for SLES 15.2 and RHEL 8.4 as a host operating system Added support for enabling root-squashing for NFS shares Added support for disabling snapshot feature during installation Added the ability to configure nasName per Storage Class Refactored configuration files to use more generic naming  Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\n","excerpt":"Release Notes - CSI PowerStore v1.4.0 New Features/Changes  Added …","ref":"/csm-docs/v1/release/powerstore/","title":"PowerStore"},{"body":"   Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods powerstore-controller-\u003csuffix\u003e –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n csi-powerstore powerstore-node-\u003csuffix\u003e driver logs show that the driver can’t connect to PowerStore API. Check if you’ve created a secret with correct credentials   Installation of the driver on Kubernetes v1.20/1.21 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.20/1.21 requires v1 version of snapshot CRDs. If on Kubernetes 1.20/1.21 (v1 snapshots) install CRDs from v4.0.0, see the Volume Snapshot Requirements    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/csm-docs/v1/troubleshooting/powerstore/","title":"PowerStore"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command creates a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset are created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset’s pods by running kubectl get pods -n testpowerstore Pod should be Ready and Running\n If Pod is in CrashLoopback or PVCs is in Pending state then driver installation is not successful, check logs of node and controller\n Deleting volumes To delete volumes, pod and statefulset run\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u003cpowerstore.api.ip\u003e/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:existingvolspec:accessModes:- ReadWriteOncecapacity:storage:30Gicsi:driver:csi-powerstore.dellemc.comvolumeHandle:0055558c-5ae1-4ed1-b421-6f5a9475c19fpersistentVolumeReclaimPolicy:RetainstorageClassName:powerstorevolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30GistorageClassName:powerstoreThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:quay.io/centos/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:pvolAfter the pod is Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerStore driver version 1.1 and later supports managing beta snapshots.\nTo use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  You can install them by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller  For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerStore driver version 1.1 and later, a Volume Snapshot Class is created. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1# or v1beta1, depends on your k8s versionkind:VolumeSnapshotClassmetadata:name:powerstore-snapshotdriver:csi-powerstore.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 (or v1beta1) snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1# or v1beta1, depends on your k8s versionkind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:testpowerstorespec:volumeSnapshotClassName:powerstore-snapshotsource:persistentVolumeClaimName:pvolOnce the VolumeSnapshot is successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP The CSI PowerStore driver Version 1.3.0 extends Challenge Handshake Authentication Protocol (CHAP) support by adding automatic credentials generation.\nThis means that you no longer need to provide chapsecret/chapuser credentials, they will be automatically generated by the driver for each host.\nTo enable this feature you need to set connection.enableCHAP to true when installing with helm or set X_CSI_POWERSTORE_ENABLE_CHAP to true in your PowerStore CustomResource when installing using operator.\nThe driver uses the generated chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating new host on powerstore array driver will populate host chap credentials with generated values. When re-using already existing hosts driver should override existing CHAP credentials with newly generated ones.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pstore-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:powerstoretestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powerstoreresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing pvc, for example, pvol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvol0namespace:testpowerstorespec:storageClassName:powerstoreaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clonedpvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI PowerStore driver version 1.2 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind:PodapiVersion:v1metadata:name:powerstore-inline-volumespec:containers:- name:test-containerimage:quay.io/centos/centoscommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"20Gi\"This manifest creates a pod and attach newly created ephemeral inline csi volume to it.\nTo create NFS volume you need to provide nasName: parameters that points to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI PowerStore driver version 1.2 introduces controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default number of replicas is set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in cluster, each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As mentioned earlier, you can configure where node driver pods would be assinged in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology features user must create their own storage classes similar to those that can be found in helm/samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-fcprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/127.0.0.1-fcvalues:- \"true\"This example matches all nodes where driver has a connection to PowerStore with IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS and iSCSI.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels\n Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for topology feature to work.\n For any additional information about topology, see the Kubernetes Topology documentation.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as Host on storage array before. It will check if Host initiators and node initiators (FC or iSCSI) match. If they do, the driver will not create a new storage class and will take the existing name of the Host as nodeID.\nMultiarray support The CSI PowerStore driver version 1.3.0 adds support for managing multiple PowerStore arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration Create a file called config.yaml and populate it with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIinsecure:true# use insecure connection or notdefault:true# treat current array as a default (would be used by storage classes without arrayIP parameter)block-protocol:\"ISCSI\"# what SCSI transport protocol use on node side (FC, ISCSI, None, or auto)nas-name:\"nas-server\"# what NAS should be used for NFS volumes- endpoint:\"https://10.0.0.2/api/rest\"username:\"user\"password:\"password\"insecure:trueblock-protocol:\"FC\"Here we specify that we want to CSI driver to manage two arrays: one with an IP 10.0.0.1 and the other with an IP 10.0.0.2, we want to connect to the first array with iSCSI protocol and with FC to the second array. Also, we want to be able to create NFS-based volume so we provide the name of the NAS to the first array.\nTo use this config we need to create a Kubernetes secret from it, to do so create a file called secret.yaml in the same folder and populate it with the following content:\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLApply the secret by running following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Creating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nCreate file storageclass.yaml and populate it with the following content:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-1provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayIP:\"10.0.0.1\"FsType:\"ext4\"---apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-2provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayIP:\"10.0.0.2\"FsType:\"xfs\"Here we specify two storage classes: one of them uses the first array and ext4 filesystem, and the other uses the second array and xfs filesystem.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use powerstore-1 storage class to create volumes on first array and powerstore-2 storage class to create volumes on the second array.\nDynamic secret change detection CSI PowerStore driver version 1.3.0 adds the ability to detect changes to array configuration Kubernetes secret. This essentially means that you can change credentials for your PowerStore arrays in-flight (without restarting the driver).\nTo do so just change your configuration file config.yaml and apply it again using the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - After Kubernetes remounts secret to driver containers (this usually takes around one minute), a driver should detect the change and start using this new configuration information.\nConfiguring custom access to NFS exports CSI PowerStore driver Version 1.3.0 adds the ability to configure NFS access to nodes that use dedicated storage networks.\nTo enable this feature you need to specify externalAccess parameter in your helm values.yaml file or X_CSI_POWERSTORE_EXTERNAL_ACCESS variable when creating CustomResource using an operator.\nThe value of that parameter will be added as an additional entry to NFS Export host access.\nFor example the following notation:\nexternalAccess:\"10.0.0.0/24\"This would mean that we allow for NFS Export created by driver to be consumed by address range 10.0.0.0-10.0.0.255.\n","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/csm-docs/v2/features/powerstore/","title":"PowerStore"},{"body":"The CSI Driver for Dell EMC PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the namespace csi-powerstore:\n CSI Driver for Dell EMC PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace csi-powerstore:\n CSI Driver for Dell EMC PowerStore Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell EMC PowerStore:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator sections below. You can use NFS volumes without FC or iSCSI configuration.   You can use either the Fibre Channel or iSCSI protocol, but you do not need both.\n  Linux native multipathing requirements Configure Mount propagation on container runtime (i.e. Docker) Volume Snapshot requirements The nonsecure registries are defined in Docker or other container runtimes, for CSI drivers that are hosted in a non-secure location. You can access your cluster with kubectl and helm. Ensure that your nodes support mounting NFS volumes.  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell EMC PowerStore supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell EMC PowerStore:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be done.  Set up the iSCSI Initiator The CSI Driver for Dell EMC PowerStore v1.3 supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\n Ensure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI director on the Dell EMC PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi.  For information about configuring iSCSI, see Dell EMC PowerStore documentation on Dell EMC Support.\nLinux multipathing requirements Dell EMC PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell EMC PowerStore.\nSet up Linux multipathing as follows:\n Ensure that all nodes have the Device Mapper Multipathing package installed.   You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\n  Enable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes.  Configure Mount Propagation on Container Runtime It is required to configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerStore. The following is instruction on how to do this with Docker. If you use another container runtime please follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  NOTE: Some distribution, like Ubuntu, already has MountFlags set by default.\nVolume Snapshot Requirements Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  Volume Snapshot Controller The beta Volume Snapshots in Kubernetes version 1.17 and later, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.3 quay.io/k8scsi/csi-snapshotter:v4.0.0   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use v3.0.3 version of snapshotter/snapshot-controller when using Kubernetes v1.18, v1.19 When using Kubernetes v1.20 it is recommended to use v4.0.0 version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository.\n  Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one.\n  Check helm/csi-powerstore/driver-image.yaml and confirm the driver image points to new image.\n  Edit helm/secret.yaml, correct namespace field to point to your desired namespace.\n  Edit helm/config.yaml file and configure connection information for your PowerStore arrays changing following parameters:\n endpoint: defines the full URL path to the PowerStore API. username, password: defines credentials for connecting to array. insecure: defines if we should use insecure connection or not. default: defines if we should treat the current array as a default. block-protocol: defines what SCSI transport protocol we should use (FC, ISCSI, None, or auto). nas-name: defines what NAS should be used for NFS volumes.  Add more blocks similar to above for each PowerStore array if necessary.\n  Create storage classes using ones from helm/samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n If you do not specify arrayIP parameter in the storage class then the array that was specified as the default would be used for provisioning volumes.\n   Create the secret by running sed \"s/CONFIG_YAML/`cat helm/config.yaml | base64 -w0`/g\" helm/secret.yaml | kubectl apply -f -\n  Copy the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml\n  Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:\n     Parameter Description Required Default     volumeNamePrefix Defines the string added to each volume that the CSI driver creates No “csi”   nodeNamePrefix Defines the string added to each node that the CSI driver registers No “csi-node”   nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No “/etc/machine-id”   externalAccess Defines additional entries for hostAccess of NFS volumes, single IP address and subnet are valid entries No \" \"   connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No False   controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \" \"   controller.tolerations Defines tolerations that would be applied to controller deployment Yes \" \"   controller.replicas Defines number of replicas of controller deployment Yes 2   node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \" \"   node.tolerations Defines tolerations that would be applied to node daemonset Yes \" \"    Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml  After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n csi-powerstore    NOTE:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using nodeNamePrefix and the ID read from the file pointed to by nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. A hostname the driver uses for registration of adapters is in the form \u003cnodeNamePrefix\u003e-\u003cnodeID\u003e-\u003cnodeIP\u003e. By default, these are csi-node and the machine ID read from the file /etc/machine-id. To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes The CSI driver for Dell EMC PowerStore version 1.3 and later, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the helm/samples folder. Please use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerStore v1.2 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\nNOTE: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class:\nThere are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\n Edit the sample storage class yaml file and update following parameters:   arrayIP: specifies what array driver should use to provision volumes, if not specified driver will use array specified as default in helm/config.yaml FsType: specifies what filesystem type driver should use, possible variants ext4, xfs, nfs, if not specified driver will use ext4 by default allowedTopologies (Optional): If you want you can also add topology constraints.  allowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/12.34.56.78-iscsi# replace \"-iscsi\" with \"-fc\" or \"-nfs\" at the end to use FC or NFS enabled hosts# replace \"12.34.56.78\" with PowerStore endpoint IPvalues:- \"true\"Create your storage class by using kubectl:  kubectl create -f \u003cpath_to_storageclass_file\u003e NOTE: Deleting a storage class has no impact on a running Pod with mounted PVCs. You won’t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerStore can be deployed by using the …","ref":"/csm-docs/v2/installation/helm/powerstore/","title":"PowerStore"},{"body":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell EMC PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note: The deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver   Create namespace:\nRun kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\n  Create PowerStore array connection config:\nCreate a file called config.yaml with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIinsecure:true# use insecure connection or notdefault:true# treat current array as a default (would be used by storage classes without arrayIP parameter)block-protocol:\"auto\"# what SCSI transport protocol use on node side (FC, ISCSI, None, or auto)nas-name:\"nas-server\"# what NAS should be used for NFS volumesChange the parameters with relevant values for your PowerStore array.\nAdd more blocks similar to above for each PowerStore array if necessary.\n  Create Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLCombine both files and create Kubernetes secret by running following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f -   Create a Custom Resource (CR) for PowerStore using the sample files provided here.\n  Users must configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:\n   Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be pending state till new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2   namespace Specifies namespace where the drive will be installed Yes “test-powerstore”   Common parameters for node and controller      X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node”   X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter”   Controller parameters      X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No \" \"   Node parameters      X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false   StorageClass parameters      FsType Specifies what filesystem type driver should use, possible variants ext4, xfs, nfs No “ext4”   arrayIP Specifies what array driver should use to provision volumes No “default”   allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to replace the “127.0.0.1-nfs” portion in the key with PowerStore endpoint IP with its value and append -nfs, -fc or -iscsi at the end of it No “127.0.0.1-nfs”      Execute the following command to create PowerStore custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerStore driver.\n After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e    ","excerpt":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell …","ref":"/csm-docs/v2/installation/operator/powerstore/","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs and nfs storage classes and automatically mounts them to the pod.\n It assumes that you’ve created the same basic three storage classes from helm/samples/storageclass folder without changing their names. If you’ve created different storage classes please edit tests/simple/simple.yaml and change PersistentVolumeClaim definitions to point to correct storage classes.\n Steps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/   You can find all the created resources in testpowerstore namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/csm-docs/v2/installation/test/powerstore/","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v1.3.0 New Features/Changes  Added support for Kubernetes v1.20 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 8.3 Added support for managing multiple PowerStore arrays from one driver Added support for configuring custom IPs/sub-networks for NFS exports Added support for automatic generation of CHAP credentials Changed code structure of the project Removed storage classes from helm template  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.17 or 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.17.8+, 1.18.5+   Topology related node labels are not removed automatically. Currently, when the driver is uninstalled, topology related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released you need to manually remove the node labels    ","excerpt":"Release Notes - CSI PowerStore v1.3.0 New Features/Changes  Added …","ref":"/csm-docs/v2/release/powerstore/","title":"PowerStore"},{"body":"   Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods powerstore-controller-\u003csuffix\u003e –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n csi-powerstore powerstore-node-\u003csuffix\u003e driver logs shows that the driver can’t connect to PowerStore API. Check if you’ve created secret with correct credentials   Installation of the driver on Kubernetes v1.20 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.20 requires v1 version of snapshot CRDs. If on Kubernetes 1.20 (v1 snapshots) install CRDs from v4.0.0, see the Volume Snapshot Requirements    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/csm-docs/v2/troubleshooting/powerstore/","title":"PowerStore"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command will create a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset will be created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset’s pods by running kubectl get pods -n testpowerstore Pod should be Ready and Running\n If Pod is in CrashLoopback or PVCs is in Pending state then driver installation is not successful, check logs of node and controller\n Deleting volumes To delete volumes, pod and statefulset run\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u003cpowerstore.api.ip\u003e/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:existingvolspec:accessModes:- ReadWriteOncecapacity:storage:30Gicsi:driver:csi-powerstore.dellemc.comvolumeHandle:0055558c-5ae1-4ed1-b421-6f5a9475c19fpersistentVolumeReclaimPolicy:RetainstorageClassName:powerstorevolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30GistorageClassName:powerstoreThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:pvolAfter the pod is Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerStore driver version 1.1 and later supports managing beta snapshots.\nTo use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  You can install them by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-3.0 kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller  For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerStore driver version 1.1 and later, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:powerstore-snapshotdriver:csi-powerstore.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:testpowerstorespec:volumeSnapshotClassName:powerstore-snapshotsource:persistentVolumeClaimName:pvolOnce the VolumeSnapshot has been successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP The CSI PowerStore driver Version 1.2.0 adds support for unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI protocol.\nTo enable CHAP authentication:\n Create secret powerstore-creds with the key chapsecret and chapuser set to base64 values. chapsecret must be between 12 and 60 symbols. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter connection.enableCHAP in my-powerstore-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating new host on powerstore array driver will populate host chap credentials with provided values. When re-using already existing hosts be sure to check that provided credentials in powerstore-creds match earlier preconfigured host credentials.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pstore-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:powerstoretestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powerstoreresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing pvc, for example, pvol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvol0namespace:testpowerstorespec:storageClassName:powerstoreaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clonedpvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI PowerStore driver version 1.2 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind:PodapiVersion:v1metadata:name:powerstore-inline-volumespec:containers:- name:test-containerimage:busyboxcommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"20Gi\"This manifest will create a pod and attach newly created ephemeral inline csi volume to it.\nTo create NFS volume you need to provide nasName: parameters that points to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI PowerStore driver version 1.2 introduces controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default number of replicas set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in cluster each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As mentioned earlier, you can configure where node driver pods would be assinged in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology feaure user needs to create their own storage classes similar to those that can be found in helm/samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-fcprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/127.0.0.1-fcvalues:- \"true\"This example will match all nodes where driver has a connection to PowerStore with IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS and iSCSI.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels\n Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for topology feature to work.\n For any additional information about topology, see the Kubernetes Topology documentation.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as Host on storage array before. It will check if Host initiators and node initiators (FC or iSCSI) match. If they do, the driver will not create a new storage class and will take the existing name of the Host as nodeID.\n","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/csm-docs/v3/features/powerstore/","title":"PowerStore"},{"body":"The CSI Driver for Dell EMC PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, please review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the namespace csi-powerstore:\n CSI Driver for Dell EMC PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace csi-powerstore:\n CSI Driver for Dell EMC PowerStore Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell EMC PowerStore:\n Install Kubernetes (1.17, 1.18, 1.19) or OpenShift (4.5 or 4.6) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator sections below. You can use NFS volumes without FC or iSCSI configuration.   You can use either the Fibre Channel or iSCSI protocol, but you do not need both.\n  Linux native multipathing requirements Configure Mount propagation on container runtime (i.e. Docker) Volume Snapshot requirements The nonsecure registries are defined in Docker or other container runtime, for CSI drivers that are hosted in a nonsecure location. You can access your cluster with kubectl and helm.  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell EMC PowerStore supports Fibre Channel communication. If you will use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell EMC PowerStore:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be done.  Set up the iSCSI Initiator The CSI Driver for Dell EMC PowerStore v1.2 supports iSCSI connectivity.\nIf you will use the iSCSI protocol, set up the iSCSI initiators as follows:\n Make sure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Make sure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi.  For information about configuring iSCSI, see Dell EMC PowerStore documentation on Dell EMC Support.\nLinux multipathing requirements Dell EMC PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell EMC PowerStore.\nSet up Linux multipathing as follows:\n Ensure that all nodes have the Device Mapper Multipathing package installed.   You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\n  Enable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes.  Configure Mount Propagation on Container Runtime It is required to configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerStore. The following is instruction on how to do this with Docker. If you use another container runtime please follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  Volume Snapshot requirements Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\nAlternately, you can install the CRDs by supplying the option –snapshot-crd while installing the driver using the csi-install.sh script. If you are installing the driver using the Dell CSI Operator, there is a helper script provided to install the snapshot CRDs - scripts/install_snap_crds.sh.\nVolume Snapshot Controller Starting with beta Volume Snapshots in Kubernetes 1.17, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 onwards, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests available on GitHub.\nNOTE:\n The manifests available on GitHub install v3.0.2 of the snapshotter image - quay.io/k8scsi/csi-snapshotter:v3.0.2 Dell EMC recommends using v3.0.2 image of the snapshot-controller - quay.io/k8scsi/snapshot-controller:v3.0.2 The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository Ensure that you’ve created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one Check helm/csi-powerstore/driver-image.yaml and confirm the driver image points to new image. Edit the helm/secret.yaml, point to correct namespace and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername \u0026 mypassword are credentials that would be used for accessing PowerStore API. NOTE: If you want to use iSCSI CHAP you need fill chapsecret and chapuser fields in similar manner\n Create the secret by running kubectl create -f helm/secret.yaml Copy the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:     Parameter Description Required Default     powerStoreApi Defines the full URL path to the PowerStore API Yes \" \"   volumeNamePrefix Defines the string added to each volume that the CSI driver creates No “csi”   nodeNamePrefix Defines the string added to each node that the CSI driver registers No “csi-node”   nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No “/etc/machine-id”   connection.scsiProtocol Defines which transport protocol to use (FC, ISCSI, None, or auto). - By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using nodeNamePrefix and the ID read from the file pointed to by nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. - A hostname the driver uses for registration of adapters is in the form \u003cnodeNamePrefix\u003e-\u003cnodeID\u003e-\u003cnodeIP\u003e. By default, these are csi-node and the machine ID read from the file /etc/machine-id. - To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. - For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. Yes “auto”   connection.nfs.enable Enables or disables NFS support No FALSE   connection.nfs.nasServerName Points to the NAS server that would be used - If you have nfs.enabled set to true, it will try to use nfs.nasServerName. This will fail if you do not provide nfs.nasServerName. No “nas-server”   connection.nfs.version Defines version of NFS protocol No “v3”   connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No FALSE   controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \" \"   controller.tolerations Defines tolerations that would be applied to controller deployment Yes \" \"   controller.replicas Defines number of replicas of controller deployment Yes 2   node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \" \"   node.tolerations Defines tolerations that would be applied to node daemonset Yes \" \"    Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml  After that the driver should be installed, you can check condition of driver pods by running kubectl get all -n csi-powerstore    NOTE:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and can’t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting in Q4 of 2020, an annotation \"helm.sh/resource-policy\": keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNOTE: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\n Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, make sure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You won’t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerStore can be deployed by using the …","ref":"/csm-docs/v3/installation/helm/powerstore/","title":"PowerStore"},{"body":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell EMC PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator doesn’t use any Helm charts and the installation \u0026 configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace. Create PowerStore credentials: Create a file called powerstore-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:powerstore-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003eReplace the values for the username and password parameters. These values can be optioned using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 Run kubectl create -f powerstore-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerStore using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:    Parameter Description Required Default     replicas Controls the amount of controller pods you deploy. If controller pods is greater than number of available nodes, excess pods will become stuck in pending. Defaults is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_POWERSTORE_ENDPOINT Must provide a PowerStore HTTPS API url Yes https://127.0.0.1/api/rest   X_CSI_TRANSPORT_PROTOCOL Choose what transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node”   X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provide list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter”   StorageClass parameters      allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to replace the “127.0.0.1-nfs” portion in the key with PowerStore endpoint IP with its value and append -nfs, -fc or -iscsi at the end of it No “127.0.0.1-nfs”     Execute the following command to create PowerStore custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerStore driver.  ","excerpt":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell …","ref":"/csm-docs/v3/installation/operator/powerstore/","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs and nfs storage classes, and automatically mounts them to the pod. Note that nfs storage class is optional and will not be created if you haven’t turned it on in myvalues.yaml.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/   You can find all the created resources in testpowerstore namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/csm-docs/v3/installation/test/powerstore/","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v1.2.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Ubuntu 20.04 Added support for Docker EE 3.1 Added support for Controller high availability (multiple-controllers) Added support for Topology Added support for ephemeral volumes Added support for mount options Changed driver base image to UBI 8.x  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.17 or 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.17.8+, 1.18.5+    ","excerpt":"Release Notes - CSI PowerStore v1.2.0 New Features/Changes  Added …","ref":"/csm-docs/v3/release/powerstore/","title":"PowerStore"},{"body":"   Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods powerstore-controller-\u003csuffix\u003e –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n csi-powerstore powerstore-node-\u003csuffix\u003e driver logs shows that the driver can’t connect to PowerStore API. Check if you’ve created secret with correct credentials    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/csm-docs/v3/troubleshooting/powerstore/","title":"PowerStore"},{"body":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI Operator deployment in OpenShift   ","excerpt":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI …","ref":"/csm-docs/docs/grasp/video/","title":"Quick video lessons"},{"body":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI Operator deployment in OpenShift   ","excerpt":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI …","ref":"/csm-docs/v1/grasp/video/","title":"Quick video lessons"},{"body":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI Operator deployment in OpenShift   ","excerpt":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI …","ref":"/csm-docs/v2/grasp/video/","title":"Quick video lessons"},{"body":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI Operator deployment in OpenShift   ","excerpt":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI …","ref":"/csm-docs/v3/grasp/video/","title":"Quick video lessons"},{"body":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE) v1.2.8.\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters. Installation on this cluster is done using helm and via Operator has not been qualified.\nRKE Examples ","excerpt":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE) v1.2.8. …","ref":"/csm-docs/docs/csidriver/partners/rancher/","title":"RKE"},{"body":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters. Installation on this cluster is done using helm and via Operator has not been qualified.\nRKE v1.2.8 supported with Kubernetes client (kubectl) v1.21\nRKE Examples ","excerpt":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE).\nThe …","ref":"/csm-docs/v1/partners/rancher/","title":"RKE"},{"body":"","excerpt":"","ref":"/csm-docs/search/","title":"Search Results"},{"body":"","excerpt":"","ref":"/csm-docs/docs/csidriver/installation/test/","title":"Testing Drivers"},{"body":"","excerpt":"","ref":"/csm-docs/v1/installation/test/","title":"Testing Drivers"},{"body":"","excerpt":"","ref":"/csm-docs/v2/installation/test/","title":"Testing Drivers"},{"body":"","excerpt":"","ref":"/csm-docs/v3/installation/test/","title":"Testing Drivers"},{"body":"The CSI Driver for Dell EMC Unity can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for Unity Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet:\n CSI Driver for Unity Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for Unity, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes or OpenShift (see supported versions) Install Helm v3 To use FC protocol, the host must be zoned with Unity array and Multipath needs to be configured To use iSCSI protocol, iSCSI initiator utils packages needs to be installed and Multipath needs to be configured To use NFS protocol, NFS utility packages needs to be installed Mount propagation is enabled on container runtime that is being used  Install CSI Driver Install CSI Driver for Unity using this procedure.\nBefore you begin\n You must have the downloaded files, including the Helm chart from the source git repository with the command git clone -b v2.0.0 https://github.com/dell/csi-unity.git, ready for this procedure. In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. Ensure unity namespace exists in Kubernetes cluster. Use the kubectl create namespace unity command to create the namespace if the namespace is not present.  Procedure\n  Collect information from the Unity Systems like Unique ArrayId, IP address, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml and myvalues.yaml file.\nNote:\n ArrayId corresponds to the serial number of Unity array. Unity Array username must have role as Storage Administrator to be able to perform CRUD operations.    Copy the helm/csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.\n  Edit myvalues.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     version helm version true -   logLevel LogLevel is used to set the logging level of the driver true info   allowRWOMultiPodAccess Flag to enable multiple pods to use the same PVC on the same node with RWO access mode. false false   kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet   syncNodeInfoInterval Time interval to add node info to the array. Default 15 minutes. The minimum value should be 1 minute. false 15   maxUnityVolumesPerNode Maximum number of volumes that controller can publish to the node. false 0   certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. (unity-cert-0..unity-cert-n). The minimum value should be 1. false 1   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent   podmon.enabled service to monitor failing jobs and notify false -   podmon.image pod man image name false -   controller Allows configuration of the controller-specific parameters. - -   controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2   volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s”   snapshot.enabled Enable/Disable volume snapshot feature Yes true   snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot”   resizer.enabled Enable/Disable volume expansion feature Yes true   nodeSelector Define node selection constraints for pods of controller deployment No    tolerations Define tolerations for the controller deployment, if required No    node Allows configuration of the node-specific parameters. - -   tolerations Define tolerations for the node daemonset, if required No    dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet    Note:\n  User should provide all boolean values with double-quotes. This applies only for myvalues.yaml. Example: “true”/“false”\n  controllerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails.\n  User can a create separate StorageClass (with topology-related keys) by referring to existing default storage classes.\n  Host IO Limit must have a minimum bandwidth of 1 MBPS to discover the volumes on node successfully.\n  User must not change the value of allowRWOMultiPodAccess to true unless intended to use the feature and is aware of the consequences. Enabling multiple pods to access the same PVC with RWO access mode on the same node might cause data to be overwritten and therefore leading to data loss in some cases.\n  Example myvalues.yaml\nlogLevel:\"info\"imagePullPolicy:AlwayscertSecretCount:1kubeletConfigDir:/var/lib/kubeletcontroller:controllerCount:2volumeNamePrefix :csivolsnapshot:enabled:truesnapNamePrefix:csi-snapresizer:enabled:falseallowRWOMultiPodAccess:falsesyncNodeInfoInterval:5maxUnityVolumesPerNode:0  For certificate validation of Unisphere REST API calls refer here. Otherwise, create an empty secret with file helm/emptysecret.yaml file by running the kubectl create -f helm/emptysecret.yaml command.\n  Prepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     storageArrayList.username Username for accessing Unity system true -   storageArrayList.password Password for accessing Unity system true -   storageArrayList.endpoint REST API gateway HTTPS endpoint Unity system true -   storageArrayList.arrayId ArrayID for Unity system true -   storageArrayList.skipCertificateValidation “skipCertificateValidation \" determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface. If it is set to false, then a secret unity-certs has to be created with an X.509 certificate of CA which signed the Unisphere certificate. true true   storageArrayList.isDefault An array having isDefault=true or isDefaultArray=true will be considered as the default array when arrayId is not specified in the storage class. This parameter should occur only once in the list. false false    Example: secret.yaml\nstorageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:trueUse the following command to create a new secret unity-creds from secret.yaml file.\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the yaml syntax and array-related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the yaml file.\nAlternatively, users can configure and use secret.yaml for driver configuration. The parameters remain the same as in the above table and below is a sample of secret.yaml. Samples of secret.yaml is available in the directory csi-unity/samples/secret/ .\nExample: secret.yaml\nstorageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:true```**Note:***Parameters\"allowRWOMultiPodAccess\"and\"syncNodeInfoTimeInterval\"havebeenenabledforconfigurationinvalues.yamlandthishelpsuserstodynamicallychangethesevalueswithouttheneedfordriverre-installation.  Setup for snapshots.\nApplicable only if you decided to enable snapshot feature in values.yaml\ncontroller:snapshot:enabled:trueIn order to use the Kubernetes Volume Snapshot feature, you must ensure the following components have been deployed on your Kubernetes cluster\nVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Use v4.2.x for the installation.\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  Use v4.2.x for the installation.\nNote:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller Note:\n It is recommended to use 4.2.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.    Run the ./csi-install.sh --namespace unity --values ./myvalues.yaml command to proceed with the installation.\nA successful installation must display messages that look similar to the following samples:\n------------------------------------------------------ \u003e Installing CSI Driver: csi-unity on 1.20 ------------------------------------------------------ ------------------------------------------------------ \u003e Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u003e Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.20 | |- Driver: csi-unity | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that required secrets have been created Success | |- Verifying alpha snapshot resources | |--\u003e Verifying that alpha snapshot CRDs are not installed Success | |- Verifying sshpass installation.. | |- Verifying iSCSI installation Enter the root password of 10.**.**.**: Enter the root password of 10.**.**.**: Success | |- Verifying snapshot support | |--\u003e Verifying that snapshot CRDs are available Success | |--\u003e Verifying that the snapshot controller is available Success | |- Verifying helm version Success | |- Verifying helm values version Success ------------------------------------------------------ \u003e Verification Complete - Success ------------------------------------------------------ | |- Installing Driver Success | |--\u003e Waiting for Deployment unity-controller to be ready Success | |--\u003e Waiting for DaemonSet unity-node to be ready Success ------------------------------------------------------ \u003e Operation complete ------------------------------------------------------ Results:\nAt the end of the script unity-controller Deployment and DaemonSet unity-node will be ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\n One or more Unity Controller (based on controllerCount) with 5/5 containers ready, and status displayed as Running. Agent pods with 2/2 containers and the status displayed as Running.    Certificate validation for Unisphere REST API calls This topic provides details about setting up the certificate validation for the CSI Driver for Dell EMC Unity.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on the “.Values.certSecretCount” parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\n  To fetch the certificate, run the following command. openssl s_client -showcerts -connect \u003cUnisphere IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem\n  Run the following command to create the cert secret with index ‘0’: kubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret: kubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -\n  Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)\nNote:\n  “unity” is the namespace for helm-based installation but namespace can be user-defined in operator-based installation.\n  User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation.\n  Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\n    Volume Snapshot Class For CSI Driver for Unity version 1.6 and later, dell-csi-helm-installer does not create any Volume Snapshot classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI Unity v1.6 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI Unity to 1.6 before upgrading to 2.0.\nStorage Classes Storage Classes are an essential Kubernetes construct for Storage provisioning. To know more about Storage Classes, refer to https://kubernetes.io/docs/concepts/storage/storage-classes/\nA wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nFor CSI Driver for Unity version 2.0, a wide set of annotated storage class manifests have been provided in the csi-unity/samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class: There are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\n Pick any of unity-fc.yaml, unity-iscsi.yaml or unity-nfs.yaml Copy the file as unity-\u003cARRAY_ID\u003e-fc.yaml, unity-\u003cARRAY_ID\u003e-iscsi.yaml or unity-\u003cARRAY_ID\u003e-nfs.yaml Replace \u003cARRAY_ID\u003e with the Array Id of the Unity Array to be used Replace \u003cSTORAGE_POOL\u003e with the storage pool you have Replace \u003cTIERING_POLICY\u003e with the Tiering policy that is to be used for provisioning Replace \u003cHOST_IO_LIMIT_NAME\u003e with the Host IO Limit Name that is to be used for provisioning Replace \u003cmountOption1\u003e with the necessary mount options. If not required, this can be removed from the storage class Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f unity-\u003cARRAY_ID\u003e-fc.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-iscsi.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-nfs.yaml  Note:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nDynamically update the unity-creds secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\n kubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Note: Updating unity-certs-x secrets is a manual process, unlike unity-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nDynamic Logging Configuration This feature is introduced in CSI Driver for unity version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\n","excerpt":"The CSI Driver for Dell EMC Unity can be deployed by using the …","ref":"/csm-docs/docs/csidriver/installation/helm/unity/","title":"Unity"},{"body":"CSI Driver for Unity Pre-requisites Create secret to store Unity credentials Create a namespace called unity (it can be any user-defined name; But commands in this section assumes that the namespace is unity) Prepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing Unity system true -   password Password for accessing Unity system true -   restGateway REST API gateway HTTPS endpoint Unity system true -   arrayId ArrayID for Unity system true -   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. false false    Ex: secret.yaml\nstorageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:truekubectl create secret generic unity-creds -n unity --from-file=config=secret.secret\nUse the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the YAML syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\nCreate secret for client side TLS verification Please refer detailed documentation on how to create this secret here\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:unity-certs-0namespace:unitytype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\nModify/Set the following optional environment variables Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT Specifies the HTTP endpoint for Unity. No /var/run/csi/csi.sock   X_CSI_UNITY_ALLOW_MULTI_POD_ACCESS Flag to enable multiple pods use the same pvc on the same node with RWO access mode No false   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true   Node parameters      X_CSI_MODE Driver starting mode No node   X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands. No /noderoot    Example CR for Unity Refer samples from here. Below is an example CR:\napiVersion:storage.dell.com/v1kind:CSIUnitymetadata:name:test-unitynamespace:test-unityspec:driver:configVersion:v2.0.0replicas:2dnsPolicy:ClusterFirstWithHostNetforceUpdate:falsecommon:image:\"dellemc/csi-unity:v2.0.0\"imagePullPolicy:IfNotPresentsideCars:- name:provisionerargs:[\"--volume-name-prefix=csiunity\",\"--default-fstype=ext4\"]- name:snapshotterargs:[\"--snapshot-name-prefix=csiunitysnap\"]---apiVersion:v1kind:ConfigMapmetadata:name:unity-config-paramsnamespace:test-unitydata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"info\"ALLOW_RWO_MULTIPOD_ACCESS:\"false\"MAX_UNITY_VOLUMES_PER_NODE:\"0\"SYNC_NODE_INFO_TIME_INTERVAL:\"0\"Dynamic Logging Configuration This feature is introduced in CSI Driver for unity version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params Note :\n Prior to CSI Driver for unity version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object. “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  ","excerpt":"CSI Driver for Unity Pre-requisites Create secret to store Unity …","ref":"/csm-docs/docs/csidriver/installation/operator/unity/","title":"Unity"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes and automatically mounts them to the pod.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./test/sample.yaml   You can find all the created resources in test-unity namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n test-unity If it is in CrashLoopback state then the driver installation was not successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./test/sample.yaml   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/csm-docs/docs/csidriver/installation/test/unity/","title":"Test Unity CSI Driver"},{"body":"Release Notes - CSI Unity v2.0.0 New Features/Changes  Added support for Kubernetes v1.22. Added support for Openshift 4.8. Added the ability to change log level and log format of CSI driver and change them dynamically. Added the ability to configure kubelet directory path. Added the ability to enable/disable installation of resizer sidecar with driver installation. Added the ability to enable/disable installation of snapshotter sidecar with driver installation. Added support for consistent config parameters across CSI drivers.  Fixed Issues Known Issues    Issue Workaround     Topology-related node labels are not removed automatically. Currently, when the driver is uninstalled, topology-related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released, remove the labels manually after the driver un-installation using command kubectl label node \u003cnode_name\u003e - - … Example: kubectl label node  csi-unity.dellemc.com/array123-iscsi- Note: there must be - at the end of each label to remove it.   NFS Clone - Resize of the snapshot is not supported by Unity Platform. Currently, when the driver takes a clone of NFS volume, it succeeds. But when the user tries to resize the NFS volumesnapshot, the driver will throw an error. The user should never try to resize the cloned NFS volume.    ","excerpt":"Release Notes - CSI Unity v2.0.0 New Features/Changes  Added support …","ref":"/csm-docs/docs/csidriver/release/unity/","title":"Unity"},{"body":"    Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods unity-controller-\u003csuffix\u003e –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   The kubectl logs -n unity unity-node-\u003csuffix\u003e driver logs show that the driver can’t connect to Unity - Authentication failure. Check if you have created a secret with correct credentials   fsGroup specified in pod spec is not reflected in files or directories at mounted path of volume. fsType of PVC must be set for fsGroup to work. fsType can be specified while creating a storage class. For NFS protocol, fsType can be specified as nfs. fsGroup doesn’t work for ephemeral inline volumes.   Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver controller and node pod should be restarted with command kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}'| xargs kubectl delete -n unity pod when topology-based storage classes are used. For dynamic array addition without topology, the driver will detect the newly added or removed arrays automatically   If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in the cluster but on array, it will still be present and marked for deletion. All the cloned PVC should be deleted in order to delete the source PVC from the array.   PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass “unity-iscsi”: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}'| xargs kubectl delete -n unity pod   On deleting pods sometimes the corresponding ‘volumeattachment’ will not get removed. This issue is intermittent and happens with one specific protocol (FC, iSCSI, or NFS) based storageclasses. This issue occurs in Kubernetes versions 1.19 and both versions of OpenShift (4.5/4.6). On deleting the stale volumeattachment manually, Controller Unpublish gets invoked and then the corresponding PVCs can be deleted.    ","excerpt":"    Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/csm-docs/docs/csidriver/troubleshooting/unity/","title":"Unity"},{"body":"Creating volumes and consuming them Create a file sample.yaml using sample yaml files located at test/sample.yaml\nThe following command creates a statefulset that consumes three volumes of default storage classes:\nkubectl create -f tests/sample.yaml After executing this command 3 PVC and statefulset are created in the test-unity namespace. You can check created PVCs by running kubectl get pvc -n test-unity and check statefulset’s pods by running kubectl get pods -n test-unity command. The pod should be Ready and Running.\n If Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f tests/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\n Open your volume in Unity Management UI (Unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:static-1annotations:pv.kubernetes.io/provisioned-by:csi-unity.dellemc.comspec:accessModes:- ReadWriteOncecapacity:storage:5Gicsi:driver:csi-unity.dellemc.comvolumeHandle:existingvol-\u003cprotocol\u003e-\u003carray_id\u003e-\u003cvolume-id\u003e persistentVolumeReclaimPolicy: RetainclaimRef:namespace:defaultname:static-pvc1storageClassName:unityvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:static-pvc1spec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:static-1storageClassName:unityThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:static-pvc1After the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes version 1.17 and generally available (v1) in Kubernetes version \u003e=1.20.\nThe CSI Unity driver version 1.5 supports v1beta1 snapshots on Kubernetes 1.19 and v1 snapshots on Kubernetes 1.20 and 1.21.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of the CSI Unity 1.5 driver, a Volume Snapshot Class is created. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for a Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1#For beta snapshots the apiVersion will be snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:unity-snapclassdriver:csi-unity.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1#For beta snapshots the apiVersion will be snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:test-unityspec:volumeSnapshotClassName:unity-snapclasssource:persistentVolumeClaimName:pvolOnce the VolumeSnapshot is successfully created by the CSI Unity driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:test-unityspec:storageClassName:unity-iscsidataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiVolume Expansion The CSI Unity driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:unity-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support The CSI Unity driver version 1.4 and later supports Raw Block Volumes. Raw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. The following is an example configuration:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:rawblockpvcnamespace:defaultspec:accessModes:- ReadWriteOncevolumeMode:Blockresources:requests:storage:5GistorageClassName:unity-iscsiapiVersion:v1kind:Podmetadata:name:rawblockpodnamespace:defaultspec:containers:- name:task-pv-containerimage:nginxports:- containerPort:80name:\"http-server\"volumeDevices:- devicePath:/usr/share/nginx/html/devicename:nov-eleventh-1-pv-storagevolumes:- name:nov-eleventh-1-pv-storagepersistentVolumeClaim:claimName:rawblockpvcAccess modes allowed are ReadWriteOnce and ReadWriteMany. Raw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device.\nRaw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size. Access mode ReadOnlyMany is not supported with raw block since we cannot restrict volumes to be readonly from Unity.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:vol0namespace:test-unityspec:storageClassName:unity-nfsaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:cloned-pvcnamespace:test-unityspec:storageClassName:unity-nfsdataSource:name:vol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI Unity driver version 1.4 and later supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity driver.\nkind:PodapiVersion:v1metadata:name:test-unity-ephemeral-volumespec:containers:- name:test-containerimage:busyboxcommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"10Gi\"arrayId:APM************protocol:iSCSIthinProvisioned:\"true\"isDataReductionEnabled:\"false\"tieringPolicy:\"1\"storagePool:pool_2This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI Unity driver version 1.4 and later supports the controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default, number of replicas is set to 2, you can set the controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator you can change the replicas parameter in the spec.driver section in your Unity Custom Resource.\nWhen multiple replicas of controller pods are in a cluster each sidecar (Attacher, Provisioner, Resizer, and Snapshotter) tries to get a lease so only one instance of each sidecar is active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As said before you can configure where node driver pods would be assigned in a similar way in the node section of myvalues.yaml\nTopology The CSI Unity driver version 1.4 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage User can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-topology-fcprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/\u003carray_id\u003e-fcvalues:- \"true\"This example matches all nodes where the driver has a connection to the Unity array with array ID mentioned via Fiber Channel. Similarly, by replacing fc with iscsi in the key checks for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\n Note that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work properly.\n For any additional information about the topology, see the Kubernetes Topology documentation.\nSupport for SLES 15 SP2 The CSI Driver for Dell EMC Unity requires the following set of packages installed on all worker nodes that run on SLES 15 SP2.\n open-iscsi open-iscsi is required in order to make use of iSCSI protocol for provisioning nfs-utils nfs-utils is required in order to make use of NFS protocol for provisioning multipath-tools multipath-tools is required in order to make use of FC and iSCSI protocols for provisioning  After installing open-iscsi, ensure “iscsi” and “iscsid” services have been started and /etc/isci/initiatorname.iscsi is created and has the host initiator id. The pre-requisites are mandatory for provisioning with the iSCSI protocol to work.\nVolume Limit The CSI Driver for Dell EMC Unity allows users to specify the maximum number of Unity volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-unity-volumes-per-node and specifying the volume limit for that node.  kubectl label node \u003cnode_name\u003e max-unity-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxUnityVolumesPerNode attribute in secret.json or secret.yaml file.\n NOTE: To reflect the changes after setting the value either via node label or in secret.json/secret.yaml file, user has to bounce the driver controller and node pods using the command kubectl get pods -n unity --no-headers=true | awk '/unity-/{print $1}'| xargs kubectl delete -n unity pod. If the value is set both by node label and secret.json/secret.yaml file then node label value will get the precedence and user has to remove the node label in order to reflect the secret.json/secret.yaml value. The default value of maxUnityVolumesPerNode is 0. If maxUnityVolumesPerNode is set to zero, then CO SHALL decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxUnityVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-unity-volumes-per-node is not set.\n Log Levels The CSI Driver for Dell EMC Unity allows users to configure different log levels using logLevel parameter.\nThe logLevel parameter needs to be configured in unity-creds secret created from secret.json or secret.yaml. The supported log levels are Info/Debug/Warn/Error. The default level is Info if logLevel is not configured by the user. The parameter can be changed dynamically without the need of driver re-installation or restart.\n","excerpt":"Creating volumes and consuming them Create a file sample.yaml using …","ref":"/csm-docs/v1/features/unity/","title":"Unity"},{"body":"The CSI Driver for Dell EMC Unity can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for Unity Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet:\n CSI Driver for Unity Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for Unity, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes or OpenShift (see supported versions) Install Helm v3 To use FC protocol, the host must be zoned with Unity array and Multipath needs to be configured To use iSCSI protocol, iSCSI initiator utils packages needs to be installed and Multipath needs to be configured To use NFS protocol, NFS utility packages needs to be installed Mount propagation is enabled on container runtime that is being used  Install CSI Driver Install CSI Driver for Unity using this procedure.\nBefore you begin\n You must have the downloaded files, including the Helm chart from the source git repository with the command git clone https://github.com/dell/csi-unity.git, ready for this procedure. In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. Ensure unity namespace exists in Kubernetes cluster. Use the kubectl create namespace unity command to create the namespace if the namespace is not present.  Procedure\n  Collect information from the Unity Systems like Unique ArrayId, IP address, username, and password. Make a note of the value for these parameters as they must be entered in the secret.json or secret.yaml and myvalues.yaml file.\nNote:\n ArrayId corresponds to the serial number of Unity array. Unity Array username must have role as Storage Administrator to be able to perform CRUD operations.    Copy the helm/csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.\n  Edit myvalues.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. (unity-cert-0..unity-cert-n). The minimum value should be 1. false 1   controllerCount Controller replication count to maintain high availability. yes 2   volumeNamePrefix String to prepend to any volumes created by the driver. false csivol   snapNamePrefix String to prepend to any snapshot created by the driver. false csi-snap   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. false IfNotPresent   allowRWOMultiPodAccess Flag to enable multiple pods to use the same PVC on the same node with RWO access mode. false false   syncNodeInfoInterval Time interval to add node info to the array. Default 15 minutes. The minimum value should be 1 minute. false 15    Note:\n  User should provide all boolean values with double-quotes. This applies only for myvalues.yaml. Example: “true”/“false”\n  controllerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails.\n  User can a create separate StorageClass (with topology-related keys) by referring to existing default storage classes.\n  Host IO Limit must have a minimum bandwidth of 1 MBPS to discover the volumes on node successfully.\n  User must not change the value of allowRWOMultiPodAccess to true unless intended to use the feature and is aware of the consequences. Enabling multiple pods to access the same PVC with RWO access mode on the same node might cause data to be overwritten and therefore leading to data loss in some cases.\n Parameters allowRWOMultiPodAccess and syncNodeInfoInterval have been deprecated from myvalues.yaml and are removed from myvalues.yaml in a future releases. They can be configured in secret.json or secret.yaml as they facilitate the user to dynamically change these values without the need for driver re-installation.    Example myvalues.yaml\ncsiDebug:\"true\"volumeNamePrefix :csivolsnapNamePrefix:csi-snapimagePullPolicy:AlwayscertSecretCount:1controllerCount:2allowRWOMultiPodAccess:falsesyncNodeInfoInterval:5  For certificate validation of Unisphere REST API calls refer here. Otherwise, create an empty secret with file helm/emptysecret.yaml file by running the kubectl create -f helm/emptysecret.yaml command.\n  Prepare the secret.json or secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     storageArrayList.username Username for accessing Unity system true -   storageArrayList.password Password for accessing Unity system true -   storageArrayList.endpoint REST API gateway HTTPS endpoint Unity system true -   storageArrayList.arrayId ArrayID for Unity system true -   storageArrayList.skipCertificateValidation “skipCertificateValidation \" determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface. If it is set to false, then a secret unity-certs has to be created with an X.509 certificate of CA which signed the Unisphere certificate. true true   storageArrayList.isDefault An array having isDefault=true or isDefaultArray=true will be considered as the default array when arrayId is not specified in the storage class. This parameter should occur only once in the list. false false   logLevel This parameter is used to set the logging level in the driver. Log level can be Info/Debug/Warn/Error. false Info   allowRWOMultiPodAccess Flag to enable multiple pods to use the same PVC on the same node with RWO access mode. false false   syncNodeInfoInterval Time interval to add node info to array. Default 15 minutes. Minimum value should be 1 minute. false 15   maxUnityVolumesPerNode The maximum number of volumes that can be provisioned to a single node beyond which the driver would return an error on a provisioning request. Default value 0 stands for unlimited volumes. false 0    Example: secret.json\n{ \"storageArrayList\": [ { \"username\": \"user\", \"password\": \"password\", \"endpoint\": \"https://10.1.1.1\", \"arrayId\": \"APM00******1\", \"skipCertificateValidation\": true, \"isDefaultArray\": true }, { \"username\": \"user\", \"password\": \"password\", \"endpoint\": \"https://10.1.1.2\", \"arrayId\": \"APM00******2\", \"skipCertificateValidation\": true } ], \"logLevel\": \"Info\", \"allowRWOMultiPodAccess\": false, \"syncNodeInfoTimeInterval\": 15, \"maxUnityVolumesPerNode\": 0 } Use the following command to create a new secret unity-creds from secret.json file.\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.json\nUse the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the JSON syntax and array-related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.\nAlternatively, users can configure and use secret.yaml for driver configuration. The parameters remain the same as in the above table and below is a sample of secret.yaml. Samples of both secret.json and secret.yaml are available in the directory csi-unity/helm/samples.\nExample: secret.yaml\nlogLevel:\"Info\"syncNodeInfoInterval:15allowRWOMultiPodAccess:\"false\"maxUnityVolumesPerNode:0storageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:true``\nUse the following command to create a new secret unity-creds from secret.yaml file.\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml --dry-run | kubectl replace -f -\nNote:\n  “restGateway” parameter has been changed to “endpoint” as restgateway is deprecated and will be removed from use in secret.json or secret.yaml in a future release. Users can continue to use any one of “restGateway” or “endpoint” for now. The driver would return an error if both parameters are used.\n  “insecure” parameter has been changed to “skipCertificateValidation” as insecure is deprecated and will be removed from use in secret.json or secret.yaml in a future release. Users can continue to use any one of “insecure” or “skipCertificateValidation” for now. The driver would return an error if both parameters are used.\n  “isDefaultArray” parameter has been changed to “isDefault” as isDefaultArray is deprecated and will be removed from use in secret.json or secret.yaml in a future release. Users can continue to use any one of “isDefaultArray” or “isDefault” for now. The driver would return error if both parameters are used.\n  Parameters “allowRWOMultiPodAccess” and “syncNodeInfoTimeInterval” have been enabled for configuration in secret.json or secret.yaml and this helps users to dynamically change these values without the need for driver re-installation. If these parameters are specified in myvalues.yaml as well as here then the values from secret.json / secret.yaml takes precedence and they will reflect in the driver after unity-creds secret is updated.\n    Setup for snapshots.\nThe Kubernetes Volume Snapshot feature is beta in Kubernetes 1.19 and moved to GA in \u003e=v1.20.\n The following section summarizes the changes in the GA Applicable only if you decided to enable snapshot feature in values.yaml  snapshot:enabled:trueIn order to use the Kubernetes Volume Snapshot feature, you must ensure the following components have been deployed on your Kubernetes cluster\nVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20/1.21 (v1 snapshots) use v4.0.x  Volume Snapshot Controller The beta Volume Snapshots in Kubernetes version 1.17 and later, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.19 (beta snapshots) use v3.0.x If on Kubernetes 1.20 and 1.21 (v1 snapshots) use v4.0.x  Note:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.x quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller Note:\n It is recommended to use 3.0.x version of snapshotter/snapshot-controller when using Kubernetes 1.19 When using Kubernetes 1.20/1.21 it is recommended to use 4.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.    Run the ./csi-install.sh --namespace unity --values ./myvalues.yaml command to proceed with the installation.\nA successful installation must display messages that look similar to the following samples:\n------------------------------------------------------ \u003e Installing CSI Driver: csi-unity on 1.20 ------------------------------------------------------ ------------------------------------------------------ \u003e Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u003e Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.20 | |- Driver: csi-unity | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that required secrets have been created Success | |- Verifying alpha snapshot resources | |--\u003e Verifying that alpha snapshot CRDs are not installed Success | |- Verifying sshpass installation.. | |- Verifying iSCSI installation Enter the root password of 10.**.**.**: Enter the root password of 10.**.**.**: Success | |- Verifying snapshot support | |--\u003e Verifying that snapshot CRDs are available Success | |--\u003e Verifying that the snapshot controller is available Success | |- Verifying helm version Success ------------------------------------------------------ \u003e Verification Complete - Success ------------------------------------------------------ | |- Installing Driver Success | |--\u003e Waiting for Deployment unity-controller to be ready Success | |--\u003e Waiting for DaemonSet unity-node to be ready Success ------------------------------------------------------ \u003e Operation complete ------------------------------------------------------ Results:\nAt the end of the script unity-controller Deployment and DaemonSet unity-node will be ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\n One or more Unity Controller (based on controllerCount) with 5/5 containers ready, and status displayed as Running. Agent pods with 2/2 containers and the status displayed as Running.  The script also creates one or more volumesnapshotclasses based on the number of arrays . “unity-snapclass” will be the volumesnapshotclass for the default array. The output will be similar to the following:\n[root@host ~]# kubectl get volumesnapshotclass NAME AGE unity-apm***********-snapclass 12m unity-snapclass 12m\n  Certificate validation for Unisphere REST API calls This topic provides details about setting up the certificate validation for the CSI Driver for Dell EMC Unity.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on the “.Values.certSecretCount” parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nThe CSI driver exposes an install parameter in secret.json, which is like storageArrayList[i].insecure, which determines if the driver performs client-side verification of the Unisphere certificates.\nThe storageArrayList[i].insecure parameter set to true by default, and the driver does not verify the Unisphere certificates.\nIf the storageArrayList[i].insecure set to false, then the secret unity-certs-n must contain the CA certificate for Unisphere.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the storageArrayList[i].insecure parameter set to false and a previous installation attempt created the empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\n  To fetch the certificate, run the following command. openssl s_client -showcerts -connect \u003cUnisphere IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem\n  Run the following command to create the cert secret with index ‘0’: kubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret: kubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -\n  Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)\nNote:\n  “unity” is the namespace for helm-based installation but namespace can be user-defined in operator-based installation.\n  User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation.\n  Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\n    Storage Classes Storage Classes are an essential Kubernetes construct for Storage provisioning. To know more about Storage Classes, refer to https://kubernetes.io/docs/concepts/storage/storage-classes/\nA wide set of annotated storage class manifests have been provided in the helm/samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nFor CSI Driver for Unity version 1.6 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the csi-unity/helm/samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from CSI Unity v1.5 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver It is strongly recommended to upgrade the earlier versions of CSI Unity to 1.5 before upgrading to 1.6.\nSteps to create storage class: There are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\n Pick any of unity-fc.yaml, unity-iscsi.yaml or unity-nfs.yaml Copy the file as unity-\u003cARRAY_ID\u003e-fc.yaml, unity-\u003cARRAY_ID\u003e-iscsi.yaml or unity-\u003cARRAY_ID\u003e-nfs.yaml Replace \u003cARRAY_ID\u003e with the Array Id of the Unity Array to be used Replace \u003cSTORAGE_POOL\u003e with the storage pool you have Replace \u003cTIERING_POLICY\u003e with the Tiering policy that is to be used for provisioning Replace \u003cHOST_IO_LIMIT_NAME\u003e with the Host IO Limit Name that is to be used for provisioning Replace \u003cmountOption1\u003e with the necessary mount options. If not required, this can be removed from the storage class Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f unity-\u003cARRAY_ID\u003e-fc.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-iscsi.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-nfs.yaml  Note:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nDynamically update the unity-creds secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\n kubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run=client | kubectl replace -f - Note: Updating unity-certs-x secrets is a manual process, unlike unity-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\n","excerpt":"The CSI Driver for Dell EMC Unity can be deployed by using the …","ref":"/csm-docs/v1/installation/helm/unity/","title":"Unity"},{"body":"CSI Unity Pre-requisites Create secret to store Unity credentials Create a namespace called unity (it can be any user-defined name; But commands in this section assumes that the namespace is unity) Prepare the secret.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing Unity system true -   password Password for accessing Unity system true -   restGateway REST API gateway HTTPS endpoint Unity system true -   arrayId ArrayID for Unity system true -   insecure “unityInsecure” determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface If it is set to false, then a secret unity-certs has to be created with a X.509 certificate of CA which signed the Unisphere certificate true true   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. false false    Ex: secret.json\n{ \"storageArrayList\": [ { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.1\", \"arrayId\": \"APM00******1\", \"insecure\": true, \"isDefaultArray\": true }, { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.2\", \"arrayId\": \"APM00******2\", \"insecure\": true } ] } kubectl create secret generic unity-creds -n unity --from-file=config=secret.json\nUse the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the JSON syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.\nCreate secret for client side TLS verification Please refer detailed documentation on how to create this secret here\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:unity-certs-0namespace:unitytype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\nModify/Set the following optional environment variables Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT Specifies the HTTP endpoint for Unity. No /var/run/csi/csi.sock   X_CSI_UNITY_ALLOW_MULTI_POD_ACCESS Flag to enable multiple pods use the same pvc on the same node with RWO access mode No false   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true   Node parameters      X_CSI_MODE Driver starting mode No node   X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands. No /noderoot    Example CR for Unity Refer samples from here. Below is an example CR:\napiVersion:storage.dell.com/v1kind:CSIUnitymetadata:name:test-unitynamespace:test-unityspec:driver:configVersion:v5replicas:2common:image:\"dellemc/csi-unity:v1.6.0\"imagePullPolicy:IfNotPresentenvs:- name:X_CSI_UNITY_DEBUGvalue:\"true\"- name:X_CSI_UNITY_ALLOW_MULTI_POD_ACCESSvalue:\"false\"- name:X_CSI_MAX_VOLUMES_PER_NODEvalue:\"0\"sideCars:- name:provisionerargs:[\"--volume-name-prefix=csiunity\",\"--default-fstype=ext4\"]- name:snapshotterargs:[\"--snapshot-name-prefix=csiunitysnap\"]","excerpt":"CSI Unity Pre-requisites Create secret to store Unity credentials …","ref":"/csm-docs/v1/installation/operator/unity/","title":"Unity"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes and automatically mounts them to the pod.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./test/sample.yaml   You can find all the created resources in test-unity namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n test-unity If it is in CrashLoopback state then the driver installation was not successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./test/sample.yaml   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/csm-docs/v1/installation/test/unity/","title":"Test Unity CSI Driver"},{"body":"Release Notes - CSI Unity v1.6.0 New Features/Changes  Added support for Kubernetes v1.21 Added support for Red Hat Enterprise Linux (RHEL) 8.4 Added support for MKE 3.4.0 Added support for RKE v1.2.8 Added support for VMware Tanzu Added support for CSI Spec 1.3 Added Volume limit feature Added support for secret in YAML format Added support for Dynamic log level changes  Fixed Issues  The flag allowRWOMultiPodAccess: false is not applicable for Raw Block volumes and the driver allows the creation of multiple pods on the same node with RWO access mode.  Known Issues    Issue Workaround     Topology-related node labels are not removed automatically. Currently, when the driver is uninstalled, topology-related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released, remove the labels manually after the driver un-installation using command kubectl label node \u003cnode_name\u003e - - … Example: kubectl label node  csi-unity.dellemc.com/array123-iscsi- Note: there must be - at the end of each label to remove it.    ","excerpt":"Release Notes - CSI Unity v1.6.0 New Features/Changes  Added support …","ref":"/csm-docs/v1/release/unity/","title":"Unity"},{"body":"    Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods unity-controller-\u003csuffix\u003e –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   The kubectl logs -n unity unity-node-\u003csuffix\u003e driver logs show that the driver can’t connect to Unity - Authentication failure. Check if you have created a secret with correct credentials   Installation of the driver on Kubernetes v1.20/1.21 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.20/1.21 requires v1 version of snapshot CRDs. If on Kubernetes 1.20/1.21 (v1 snapshots) install CRDs from v4.0.0, see point 6 here   fsGroup specified in pod spec is not reflected in files or directories at mounted path of volume. fsType of PVC must be set for fsGroup to work. fsType can be specified while creating a storage class. For NFS protocol, fsType can be specified as nfs. fsGroup doesn’t work for ephemeral inline volumes.   Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver controller and node pod should be restarted with command kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}'| xargs kubectl delete -n unity pod when topology-based storage classes are used. For dynamic array addition without topology, the driver will detect the newly added or removed arrays automatically   If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in the cluster but on array, it will still be present and marked for deletion. All the cloned PVC should be deleted in order to delete the source PVC from the array.   PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass “unity-iscsi”: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}'| xargs kubectl delete -n unity pod   On deleting pods sometimes the corresponding ‘volumeattachment’ will not get removed. This issue is intermittent and happens with one specific protocol (FC, iSCSI, or NFS) based storageclasses. This issue occurs in Kubernetes versions 1.19 and both versions of OpenShift (4.5/4.6). On deleting the stale volumeattachment manually, Controller Unpublish gets invoked and then the corresponding PVCs can be deleted.    ","excerpt":"    Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/csm-docs/v1/troubleshooting/unity/","title":"Unity"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/sample.yaml\nThe following command creates a statefulset that consumes three volumes of default storage classes:\nkubectl create -f tests/sample.yaml After executing this command 3 PVC and statefulset are created in the test-unity namespace. You can check created PVCs by running kubectl get pvc -n test-unity and check statefulset’s pods by running kubectl get pods -n test-unitycommand. Pod should be Ready and Running.\n If Pod is in CrashLoopback or PVCs is in Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f tests/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\n Open your volume in Unity Management UI (unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:static-1annotations:pv.kubernetes.io/provisioned-by:csi-unity.dellemc.comspec:accessModes:- ReadWriteOncecapacity:storage:5Gicsi:driver:csi-unity.dellemc.comvolumeHandle:existingvol-\u003cprotocol\u003e-\u003carray_id\u003e-\u003cvolume-id\u003e persistentVolumeReclaimPolicy: RetainclaimRef:namespace:defaultname:static-pvc1storageClassName:unityvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:static-pvc1spec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:static-1storageClassName:unityThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:static-pvc1After the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes version 1.17 and generally available (v1) in Kubernetes version 1.20.\nThe CSI Unity driver version 1.5 supports v1beta1 snapshots on Kubernetes 1.18/1.19 and v1 snapshots on Kubernetes 1.20.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of CSI Unity 1.5 driver, a Volume Snapshot Class is created. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for a Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1#For beta snapshots the apiVersion will be snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:unity-snapclassdriver:csi-unity.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1#For beta snapshots the apiVersion will be snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:test-unityspec:volumeSnapshotClassName:unity-snapclasssource:persistentVolumeClaimName:pvolOnce the VolumeSnapshot is successfully created by the CSI Unity driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:test-unityspec:storageClassName:unity-iscsidataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiVolume Expansion The CSI Unity driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:unity-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support The CSI Unity driver supports Raw Block Volumes from v1.4 onwards. Raw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:rawblockpvcnamespace:defaultspec:accessModes:- ReadWriteOncevolumeMode:Blockresources:requests:storage:5GistorageClassName:unity-iscsiapiVersion:v1kind:Podmetadata:name:rawblockpodnamespace:defaultspec:containers:- name:task-pv-containerimage:nginxports:- containerPort:80name:\"http-server\"volumeDevices:- devicePath:/usr/share/nginx/html/devicename:nov-eleventh-1-pv-storagevolumes:- name:nov-eleventh-1-pv-storagepersistentVolumeClaim:claimName:rawblockpvcAccess modes allowed are ReadWriteOnce and ReadWriteMany. Raw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size. Access mode ReadOnlyMany is not supported with raw block since we cannot restrict volumes to be readonly from Unity.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity driver version 1.3 supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:vol0namespace:test-unityspec:storageClassName:unity-nfsaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:cloned-pvcnamespace:test-unityspec:storageClassName:unity-nfsdataSource:name:vol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI Unity driver version 1.4 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity driver.\nkind:PodapiVersion:v1metadata:name:test-unity-ephemeral-volumespec:containers:- name:test-containerimage:busyboxcommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"10Gi\"This manifest creates a pod and attach newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that points to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI Unity driver version 1.4 introduces controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default number of replicas set to 2, you can set controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your Unity Custom Resource.\nWhen multiple replicas of controller pods are in cluster each sidecar (Attacher, Provisioner, Resizer and Snapshotter) tries to get a lease so only one instance of each sidecar is active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As said before you can configure where node driver pods would be assigned in the similar way in node section of myvalues.yaml\nTopology The CSI Unity driver version 1.4 supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology feature, user can install driver by setting createStorageClassesWithTopology to true in the myvalues.yaml which will create default storage classes by adding topology keys (based on the arrays specified in myvalues.yaml) and with WaitForFirstConsumer binding mode.\nAnother option is the user can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-topology-fcprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/\u003carray_id\u003e-fcvalues:- \"true\"This example will match all nodes where driver has a connection to Unity array with array ID mentioned via Fiber Channel. Similarly by replacing fc with iscsi in the key will check for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\n Note that volumeBindingMode: is set to WaitForFirstConsumer this is required for topology feature to work properly.\n For any additional information about topology, see the Kubernetes Topology documentation.\nSupport for Docker EE The CSI Driver for Dell EMC Unity supports Docker EE and deployment on clusters bootstrapped with UCP (Universal Control Plane).\n*UCP version 3.3.3 supports Kubernetes 1.18 and CSI driver can be installed on UCP 3.3 with Helm.\nThe installation process for the driver on such clusters remains the same as the installation process on upstream clusters.\nOn UCP based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes in UCP backed clusters may run any of the OSs which we support with upstream clusters.\nSupport for SLES 15 SP2 The CSI Driver for Dell EMC Unity requires the following set of packages installed on all worker nodes that run on SLES 15 SP2.\n open-iscsi open-iscsi is required in order to make use of iSCSI protocol for provisioning nfs-utils nfs-utils is required in order to make use of NFS protocol for provisioning multipath-tools multipath-tools is required in order to make use of FC and iSCSI protocols for provisioning  After installing open-iscsi, ensure “iscsi” and “iscsid” services have been started and /etc/isci/initiatorname.iscsi is created and has the host initiator id. The pre-requisites are mandatory for provisioning with iSCSI protocol to work.\n","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/csm-docs/v2/features/unity/","title":"Unity"},{"body":"The CSI Driver for Dell EMC Unity can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for Unity Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet:\n CSI Driver for Unity Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for Unity, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes or OpenShift (see supported versions) Configure Docker service Install Helm v3 To use FC protocol, the host must be zoned with Unity array and Multipath needs to be configured To use iSCSI protocol, iSCSI initiator utils packages needs to be installed and Multipath needs to be configured To use NFS protocol, NFS utility packages needs to be installed  Configure Docker service The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for Unity.\nProcedure   Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file as follows:\n[Service] ... MountFlags=shared   Restart the Docker service with following commands:\nsystemctl daemon-reload systemctl restart docker   Install CSI Driver Install CSI Driver for Unity using this procedure.\nBefore you begin\n You must have the downloaded files, including the Helm chart from the source git repository with command git clone https://github.com/dell/csi-unity.git, ready for this procedure. In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. Ensure “unity” namespace exists in kubernetes cluster. Use kubectl create namespace unity command to create the namespace, if the namespace is not present.  Procedure\n  Collect information from the Unity Systems like Unique ArrayId, IP address, username and password. Make a note of the value for these parameters as they must be entered in the secret.json and myvalues.yaml file.\n  Copy the helm/csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.\n  Edit myvalues.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     certSecretCount Represents number of certificate secrets, which user is going to create for ssl authentication. (unity-cert-0..unity-cert-n). Minimum value should be 1 false 1   syncNodeInfoInterval Time interval to add node info to array. Default 15 minutes. Minimum value should be 1 minute false 15   controllerCount Controller replication count to maintain high availability yes 2   volumeNamePrefix String to prepend to any volumes created by the driver false csivol   snapNamePrefix String to prepend to any snapshot created by the driver false csi-snap   csiDebug To set the debug log policy for CSI driver false “false”   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. false IfNotPresent   createStorageClassesWithTopology Flag to enable or disable topology. true false   allowRWOMultiPodAccess Flag to enable multiple pods use the same pvc on the same node with RWO access mode. false false   Storage Array List Following parameters is a list of parameters to provide multiple storage arrays     storageArrayList[i].name Name of the storage class to be defined. A suffix of ArrayId and protocol will be added to the name. No suffix will be added to default array. false unity   storageArrayList[i].isDefaultArray To handle the existing volumes created in csi-unity v1.0, 1.1 and 1.1.0.1. The user needs to provide “isDefaultArray”: true in secret.json. This entry should be present only for one array and that array will be marked default for existing volumes. false “false”   Storage Class parameters Following parameters are not present in values.yaml     storageArrayList[i].storageClass.storagePool Unity Storage Pool CLI ID to use with in the Kubernetes storage class true -   storageArrayList[i].storageClass.thinProvisioned To set volume thinProvisioned false “true”   storageArrayList[i].storageClass.isDataReductionEnabled To set volume data reduction false “false”   storageArrayList[i].storageClass.volumeTieringPolicy To set volume tiering policy false 0   storageArrayList[i].storageClass.FsType Block volume related parameter. To set File system type. Possible values are ext3,ext4,xfs. Supported for FC/iSCSI protocol only. false ext4   storageArrayList[i].storageClass.hostIOLimitName Block volume related parameter. To set unity host IO limit. Supported for FC/iSCSI protocol only. false \"”   storageArrayList[i].storageClass.nasServer NFS related parameter. NAS Server CLI ID for filesystem creation. true \"”   storageArrayList[i].storageClass.hostIoSize NFS related parameter. To set filesystem host IO Size. false “8192”   storageArrayList[i].storageClass.reclaimPolicy What should happen when a volume is removed false Delete   Snapshot Class parameters Following parameters are not present in values.yaml     storageArrayList[i] .snapshotClass.retentionDuration TO set snapshot retention duration. Format:“1:23:52:50” (number of days:hours:minutes:sec) false \"”    Note:\n  User should provide all boolean values with double quotes. This applicable only for myvalues.yaml. Example: “true”/“false”\n  controllerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails.\n  ‘createStorageClassesWithTopology’ key is applicable only in the helm based installation but not with the operator based installation. In operator based installation, however user can create custom storage class with topology related key/values.\n  User can create separate storage class (with topology related keys) by referring to existing default storageclasses.\n  Host IO Limit must have a minimum bandwidth of 1 MBPS to discover the volumes on node successfully.\n  User must not change the value of allowRWOMultiPodAccess to true unless intended to use the feature and is aware of the consequences. Enabling multiple pods access the same pvc with RWO access mode on the same node might cause data to be overwritten and therefore leading to data loss in some cases.\n  Example myvalues.yaml\ncsiDebug:\"true\"volumeNamePrefix :csivolsnapNamePrefix:csi-snapimagePullPolicy:AlwayscertSecretCount:1syncNodeInfoInterval:5controllerCount:2createStorageClassesWithTopology:trueallowRWOMultiPodAccess:falsestorageClassProtocols:- protocol:\"FC\"- protocol:\"iSCSI\"- protocol:\"NFS\"storageArrayList:- name:\"APM00******1\"isDefaultArray:\"true\"storageClass:storagePool:pool_1FsType:ext4nasServer:\"nas_1\"thinProvisioned:\"true\"isDataReductionEnabled:truehostIOLimitName:\"value_from_array\"tieringPolicy:\"2\"snapshotClass:retentionDuration:\"2:2:23:45\"- name:\"APM001******2\"storageClass:storagePool:pool_1reclaimPolicy:DeletehostIoSize:\"8192\"nasServer:\"nasserver_2\"  Create an empty secret with file helm/emptysecret.yaml file by running the kubectl create -f helm/emptysecret.yaml command.\n  Prepare the secret.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing unity system true -   password Password for accessing unity system true -   restGateway REST API gateway HTTPS endpoint Unity system true -   arrayId ArrayID for unity system true -   insecure “unityInsecure” determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface. If it is set to false, then a secret unity-certs has to be created with a X.509 certificate of CA which signed the Unisphere certificate true true   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. false false    Example: secret.json\n{ \"storageArrayList\": [ { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.1\", \"arrayId\": \"APM00******1\", \"insecure\": true, \"isDefaultArray\": true }, { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.2\", \"arrayId\": \"APM00******2\", \"insecure\": true } ] } kubectl create secret generic unity-creds -n unity --from-file=config=secret.json\nUse the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the JSON syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.\nNote: “isDefaultArray” parameter in values.yaml and secret.json should match each other.\n  Setup for snapshots.\nThe Kubernetes Volume Snapshot feature is beta in Kubernetes v1.18 and v1.19 and move to GA in v1.20.\n The following section summarizes the changes in the GA  In order to use the Kubernetes Volume Snapshot feature, you must ensure the following components have been deployed on your Kubernetes cluster.\n  Install Snapshot CRDs:\nFor Kubernetes 1.18 and 1.19, Snapshot CRDs versioned 3.0.3 must be installed. CRDs\nFor Kubernetes 1.20 , Snapshot CRDs versioned 4.0.0 must be installed. CRDs\n  Install Snapshot Controller:\nFor Kubernetes 1.18 and 1.19, Snapshot Controller versioned 3.0.3 must be installed. Controller\nFor Kubernetes 1.20, Snapshot Controller versioned 4.0.0 must be installed.\nController\n    Run the ./csi-install.sh --namespace unity --values ./myvalues.yaml command to proceed with the installation.\nA successful installation must display messages that look similar to the following samples:\n------------------------------------------------------ \u003e Installing CSI Driver: csi-unity on 1.20 ------------------------------------------------------ ------------------------------------------------------ \u003e Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u003e Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.20 | |- Driver: csi-unity | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that required secrets have been created Success | |- Verifying alpha snapshot resources | |--\u003e Verifying that alpha snapshot CRDs are not installed Success | |- Verifying sshpass installation.. | |- Verifying iSCSI installation Enter the root password of 10.**.**.**: Enter the root password of 10.**.**.**: Success | |- Verifying snapshot support | |--\u003e Verifying that snapshot CRDs are available Success | |--\u003e Verifying that the snapshot controller is available Success | |- Verifying helm version Success ------------------------------------------------------ \u003e Verification Complete - Success ------------------------------------------------------ | |- Installing Driver Success | |--\u003e Waiting for Deployment unity-controller to be ready Success | |--\u003e Waiting for DaemonSet unity-node to be ready Success ------------------------------------------------------ \u003e Operation complete ------------------------------------------------------ Results At the end of the script unity-controller Deployment and DaemonSet unity-node will be ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\n One or more Unity Controller (based on controllerCount) with 5/5 containers ready, and status displayed as Running. Agent pods with 2/2 containers and the status displayed as Running.  Finally, the script creates storageclasses such as, “unity”. Additional storage classes can be created for different combinations of file system types and Unity storage pools.\nThe script also creates one or more volumesnapshotclasses based on number of arrays . “unity-snapclass” will be the volumesnapshotclass for default array. The output will be similar to following:\n[root@host ~]# kubectl get volumesnapshotclass NAME AGE unity-apm***********-snapclass 12m unity-snapclass 12m\n  Certificate validation for Unisphere REST API calls This topic provides details about setting up the certificate validation for the CSI Driver for Dell EMC Unity.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on “.Values.certSecretCount” parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nThe CSI driver exposes an install parameter in secret.json, which is like storageArrayList[i].insecure, which determines if the driver performs client-side verification of the Unisphere certificates.\nThe storageArrayList[i].insecure parameter set to true by default, and the driver does not verify the Unisphere certificates.\nIf the storageArrayList[i].insecure set to false, then the secret unity-certs-n must contain the CA certificate for Unisphere.\nIf this secret is empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the storageArrayList[i].insecure parameter set to false and a previous installation attempt created the empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\n To fetch the certificate, run the following command. openssl s_client -showcerts -connect \u003cUnisphere IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Run the following command to create the cert secret with index ‘0’: kubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret: kubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)  Note: “unity” is the namespace for helm based installation but namespace can be user defined in operator based installation.\nNote: User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to kubernetes secret size limitation.\nNote: Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\nStorage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and cannot be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting from CSI Unity v1.5, an annotation “helm.sh/resource-policy”: keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNote: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\nError: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command. Deleting a storage class has no impact on a running Pod with mounted PVCs. You will not be able to provision new PVCs until at least one storage class is newly created.\nDynamically update the unity-creds secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\n`kubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run=client | kubectl replace -f - `  Note: Updating unity-certs-x secrets is a manual process, unlike unity-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\n","excerpt":"The CSI Driver for Dell EMC Unity can be deployed by using the …","ref":"/csm-docs/v2/installation/helm/unity/","title":"Unity"},{"body":"CSI Unity Pre-requisites Create secret to store Unity credentials Create a namespace called unity (it can be any user-defined name; But commands in this section assumes that the namespace is unity) Prepare the secret.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing unity system true -   password Password for accessing unity system true -   restGateway REST API gateway HTTPS endpoint Unity system true -   arrayId ArrayID for unity system true -   insecure “unityInsecure” determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface If it is set to false, then a secret unity-certs has to be created with a X.509 certificate of CA which signed the Unisphere certificate true true   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. false false    Ex: secret.json\n{ \"storageArrayList\": [ { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.1\", \"arrayId\": \"APM00******1\", \"insecure\": true, \"isDefaultArray\": true }, { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.2\", \"arrayId\": \"APM00******2\", \"insecure\": true } ] } kubectl create secret generic unity-creds -n unity --from-file=config=secret.json\nUse the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the JSON syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.\nCreate secret for client side TLS verification Please refer detailed documentation on how to create this secret here\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:unity-certs-0namespace:unitytype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\nModify/Set the following optional environment variables Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT Specifies the HTTP endpoint for Unity. No /var/run/csi/csi.sock   X_CSI_DEBUG To enable debug mode No false   GOUNITY_DEBUG To enable debug mode for gounity library No false   X_CSI_UNITY_ALLOW_MULTI_POD_ACCESS Flag to enable multiple pods use the same pvc on the same node with RWO access mode No false   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true   Node parameters      X_CSI_MODE Driver starting mode No node   X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands. No /noderoot    StorageClass Parameters    Parameter Description Required Default     storagePool Unity Storage Pool CLI ID to use with in the Kubernetes storage class true -   thinProvisioned To set volume thinProvisioned false “true”   isDataReductionEnabled To set volume data reduction false “false”   volumeTieringPolicy To set volume tiering policy false 0   FsType Block volume related parameter. To set File system type. Possible values are ext3,ext4,xfs. Supported for FC/iSCSI protocol only. false ext4   hostIOLimitName Block volume related parameter. To set unity host IO limit. Supported for FC/iSCSI protocol only. false \"”   nasServer NFS related parameter. NAS Server CLI ID for filesystem creation. true \"”   hostIoSize NFS related parameter. To set filesystem host IO Size. false “8192”   reclaimPolicy What should happen when a volume is removed false Delete    SnapshotClass parameters Following parameters are not present in values.yaml in the Helm based installer\n   Parameter Description Required Default     snapshotRetentionDuration TO set snapshot retention duration. Format:“1:23:52:50” (number of days:hours:minutes:sec) false \"”    Example CR for Unity Refer samples from here. Below is an example CR:\napiVersion:storage.dell.com/v1kind:CSIUnitymetadata:name:test-unitynamespace:test-unityspec:driver:configVersion:v4replicas:2common:image:\"dellemc/csi-unity:v1.5.0\"imagePullPolicy:IfNotPresentenvs:- name:X_CSI_UNITY_DEBUGvalue:\"true\"- name:X_CSI_UNITY_ALLOW_MULTI_POD_ACCESSvalue:\"false\"sideCars:- name:provisionerargs:[\"--volume-name-prefix=csiunity\"]storageClass:- name:virt2016****-fcdefault:truereclaimPolicy:\"Delete\"allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\"VIRT2016****\"protocol:\"FC\"- name:virt2017****-iscsireclaimPolicy:\"Delete\"allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\"VIRT2017****\"protocol:\"iSCSI\"- name:virt2017****-nfsreclaimPolicy:\"Delete\"allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\"VIRT2017****\"protocol:\"NFS\"hostIoSize:\"8192\"nasServer:nas_1- name:virt2017****-iscsi-topologyreclaimPolicy:\"Delete\"allowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/virt2017****-iscsivalues:- \"true\"parameters:storagePool:pool_1arrayId:\"VIRT2017****\"protocol:\"iSCSI\"snapshotClass:- name:test-snapparameters:retentionDuration:\"\"","excerpt":"CSI Unity Pre-requisites Create secret to store Unity credentials …","ref":"/csm-docs/v2/installation/operator/unity/","title":"Unity"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes, and automatically mounts them to the pod.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/sample.yaml   You can find all the created resources in test-unity namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n test-unity If it is in CrashLoopback state then the driver installation was not successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/sample.yaml   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/csm-docs/v2/installation/test/unity/","title":"Test Unity CSI Driver"},{"body":"Release Notes - CSI Unity v1.5.0 New Features/Changes  Added support for Kubernetes v1.20 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Changed driver base image to UBI 8.x Added support for Red Hat Enterprise Linux (RHEL) 8.3 Qualified with Docker - UCP 3.3.5 Added support for SLES 15SP2  Fixed Issues  Raw-Block volume with accessmode RWX can be mounted to multiple nodes. PVC creation fails on a cluster with only NFS protocol enabled by adding topology keys for NFS protocol.  Known Issues    Issue Workaround     Topology related node labels are not removed automatically. Currently, when the driver is uninstalled, topology related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released we need to manually remove the node labels mentioned here https://github.com/dell/csi-unity#known-issues (Point 1)   Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver should be restarted with the below command only if the topology-based storage classes are used. Otherwise, the driver will automatically detect the newly added or removed arrays https://github.com/dell//csi-unity#known-issues (Point 2)   If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in cluster but on array it will still be present and marked for deletion. All the cloned pvc should be deleted in order to delete the source pvc from array.   PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass “unity-iscsi”: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity --no-headers=true   On deleting pods sometimes the corresponding ‘volumeattachment’ will not get removed. This issue is intermittent and happens with one specific protocol (FC, iSCSI or NFS) based storageclasses. This issue occurs across kubernetes versions 1.18 and 1.19 and both versions of OpenShift (4.5/4.6). On deleting the stale volumeattachment manually, Controller Unpublish gets invoked and then the corresponding PVCs can be deleted.   The flag allowRWOMultiPodAccess:false is not applicable for Raw Block volumes and the driver allows creation of multiple pods on the same node with RWO access mode. Workaround not necessary as this issue does not block any usecase.   Driver installation warning: “OpenShift version 4.7, is newer than the version that has been tested. Latest tested version is: 4.6” Ignore this warning and continue with the installation. v1.5.0 release of the driver supports OpenShift 4.6/4.7 .    ","excerpt":"Release Notes - CSI Unity v1.5.0 New Features/Changes  Added support …","ref":"/csm-docs/v2/release/unity/","title":"Unity"},{"body":"    Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods unity-controller-\u003csuffix\u003e –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   The kubectl logs -n unity unity-node-\u003csuffix\u003e driver logs shows that the driver can’t connect to Unity - Authentication failure. Check if you have created secret with correct credentials   Installation of the driver on Kubernetes v1.20 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.20 requires v1 version of snapshot CRDs. If on Kubernetes 1.20 (v1 snapshots) install CRDs from v4.0.0, see point 6 here    ","excerpt":"    Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/csm-docs/v2/troubleshooting/unity/","title":"Unity"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/sample.yaml\nThis command will create a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/sample.yaml After executing this command 3 PVC and statefulset will be created in the test-unity namespace. You can check created PVCs by running kubectl get pvc -n test-unity and check statefulset’s pods by running kubectl get pods -n test-unitycommand. Pod should be Ready and Running.\n If Pod is in CrashLoopback or PVCs is in Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f tests/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\n Open your volume in Unity Management UI (unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:static-1annotations:pv.kubernetes.io/provisioned-by:csi-unity.dellemc.comspec:accessModes:- ReadWriteOncecapacity:storage:5Gicsi:driver:csi-unity.dellemc.comvolumeHandle:existingvol-\u003cprotocol\u003e-\u003carray_id\u003e-\u003cvolume-id\u003e persistentVolumeReclaimPolicy: RetainclaimRef:namespace:defaultname:static-pvc1storageClassName:unityvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:static-pvc1spec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:static-1storageClassName:unityThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:static-pvc1After the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI Unity driver version 1.4 and later supports managing beta snapshots.\nTo use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  You can install them by copy pasting the following commands (Copy entire thing in one shot and paste it in terminal):\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml \u0026\u0026 kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml \u0026\u0026 kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml \u0026\u0026 kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.2/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml \u0026\u0026 kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.2/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml  For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of the CSI Unity driver version 1.3 and later, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:unity-snapclassdriver:csi-unity.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:test-unityspec:volumeSnapshotClassName:unity-snapclasssource:persistentVolumeClaimName:pvolOnce the VolumeSnapshot is successfully created by the CSI Unity driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:test-unityspec:storageClassName:unity-iscsidataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiVolume Expansion The CSI Unity driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:unity-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support The CSI Unity driver version 1.3 and later supports managing Raw Block volumes.\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:unitytestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:unityresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nNote: Raw block volume creation supports only for FC and iSCSI protocols\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity driver version 1.3 supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:vol0namespace:test-unityspec:storageClassName:unity-nfsaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:cloned-pvcnamespace:test-unityspec:storageClassName:unity-nfsdataSource:name:vol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI Unity driver version 1.4 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity driver.\nkind:PodapiVersion:v1metadata:name:test-unity-ephemeral-volumespec:containers:- name:test-containerimage:busyboxcommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"10Gi\"This manifest will create a pod and attach newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that points to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI Unity driver version 1.4 introduces controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default number of replicas set to 2, you can set controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your Unity Custom Resource.\nWhen multiple replicas of controller pods are in cluster each sidecar (Attacher, Provisioner, Resizer and Snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As said before you can configure where node driver pods would be assigned in the similar way in node section of myvalues.yaml\nTopology The CSI Unity driver version 1.4 supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology feature user can install driver by setting createStorageClassesWithTopology to true in the myvalues.yaml which will create default storage classes by adding topology keys (based on the arrays specified in myvalues.yaml) and with WaitForFirstConsumer binding mode.\nAnother option is the user can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-topology-fcprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/\u003carray_id\u003e-fcvalues:- \"true\"This example will match all nodes where driver has a connection to Unity array with array ID mentioned via Fiber Channel. Similarly by replacing fc with iscsi in the key will check for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\n Note that volumeBindingMode: is set to WaitForFirstConsumer this is required for topology feature to work properly.\n For any additional information about topology, see the Kubernetes Topology documentation.\n","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/csm-docs/v3/features/unity/","title":"Unity"},{"body":"The CSI Driver for Dell EMC Unity can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for Unity Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet:\n CSI Driver for Unity Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for Unity, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes Configure Docker service Install Helm v3 To use FC protocol, host must be zoned with Unity array To use iSCSI and NFS protocol, iSCSI initiator and NFS utility packages need to be installed  Configure Docker service The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for Unity.\nProcedure   Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file as follows:\n[Service] ... MountFlags=shared   Restart the Docker service with systemctl daemon-reload and\nsystemctl daemon-reload systemctl restart docker   Install CSI Driver Install CSI Driver for Unity using this procedure.\nBefore you begin\n You must have the downloaded files, including the Helm chart from the source git repository, ready for this procedure. In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. These scripts handle some of the pre and post operations that cannot be performed in the helm chart, such as creating Custom Resource Definitions (CRDs), if needed. Make sure “unity” namespace exists in kubernetes cluster. Use kubectl create namespace unity command to create the namespace, if the namespace is not present.  Procedure\n  Collect information from the Unity Systems like Unique ArrayId, IP address, username and password. Make a note of the value for these parameters as they must be entered in the secret.json and myvalues.yaml file.\n  Copy the csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.\n  Edit myvalues.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     certSecretCount Represents number of certificate secrets, which user is going to create for ssl authentication. (unity-cert-0..unity-cert-n). Minimum value should be 1 false 1   syncNodeInfoInterval Time interval to add node info to array. Default 15 minutes. Minimum value should be 1 minute false 15   controllerCount Controller replication count to maintain high availability yes 2   volumeNamePrefix String to prepend to any volumes created by the driver false csivol   snapNamePrefix String to prepend to any snapshot created by the driver false csi-snap   csiDebug To set the debug log policy for CSI driver false “false”   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. false IfNotPresent   createStorageClassesWithTopology Flag to enable or disable topology. true false   Storage Array List Following parameters is a list of parameters to provide multiple storage arrays     storageArrayList[i].name Name of the storage class to be defined. A suffix of ArrayId and protocol will be added to the name. No suffix will be added to default array. false unity   storageArrayList[i].isDefaultArray To handle the existing volumes created in csi-unity v1.0, 1.1 and 1.1.0.1. The user needs to provide “isDefaultArray”: true in secret.json. This entry should be present only for one array and that array will be marked default for existing volumes. false “false”   Storage Class parameters Following parameters are not present in values.yaml     storageArrayList[i].storageClass.storagePool Unity Storage Pool CLI ID to use with in the Kubernetes storage class true -   storageArrayList[i].storageClass.thinProvisioned To set volume thinProvisioned false “true”   storageArrayList[i].storageClass.isDataReductionEnabled To set volume data reduction false “false”   storageArrayList[i].storageClass.volumeTieringPolicy To set volume tiering policy false 0   storageArrayList[i].storageClass.FsType Block volume related parameter. To set File system type. Possible values are ext3,ext4,xfs. Supported for FC/iSCSI protocol only. false ext4   storageArrayList[i].storageClass.hostIOLimitName Block volume related parameter. To set unity host IO limit. Supported for FC/iSCSI protocol only. false \"”   storageArrayList[i].storageClass.nasServer NFS related parameter. NAS Server CLI ID for filesystem creation. true \"”   storageArrayList[i].storageClass.hostIoSize NFS related parameter. To set filesystem host IO Size. false “8192”   storageArrayList[i].storageClass.reclaimPolicy What should happen when a volume is removed false Delete   Snapshot Class parameters Following parameters are not present in values.yaml     storageArrayList[i] .snapshotClass.retentionDuration TO set snapshot retention duration. Format:“1:23:52:50” (number of days:hours:minutes:sec) false \"”    Note: User should provide all boolean values with double quotes. This applicable only for myvalues.yaml. Example: “true”/“false”.\nExample myvalues.yaml\ncsiDebug: \"true\" volumeNamePrefix : csivol snapNamePrefix: csi-snap imagePullPolicy: Always certSecretCount: 1 syncNodeInfoInterval: 5 controllerCount: 2 createStorageClassesWithTopology: true storageClassProtocols: - protocol: \"FC\" - protocol: \"iSCSI\" - protocol: \"NFS\" storageArrayList: - name: \"APM00******1\" isDefaultArray: \"true\" storageClass: storagePool: pool_1 FsType: ext4 nasServer: \"nas_1\" thinProvisioned: \"true\" isDataReductionEnabled: true hostIOLimitName: \"value_from_array\" tieringPolicy: \"2\" snapshotClass: retentionDuration: \"2:2:23:45\" - name: \"APM001******2\" storageClass: storagePool: pool_1 reclaimPolicy: Delete hostIoSize: \"8192\" nasServer: \"nasserver_2\"   Create an empty secret by navigating to helm folder that contains emptysecret.yaml file and running the kubectl create -f emptysecret.yaml command.\n  Prepare the secret.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing unity system true -   password Password for accessing unity system true -   restGateway REST API gateway HTTPS endpoint Unity system true -   arrayId ArrayID for unity system true -   insecure “unityInsecure” determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface If it is set to false, then a secret unity-certs has to be created with a X.509 certificate of CA which signed the Unisphere certificate true true   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. false false    Example: secret.json\n{ \"storageArrayList\": [ { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.1\", \"arrayId\": \"APM00******1\", \"insecure\": true, \"isDefaultArray\": true }, { \"username\": \"user\", \"password\": \"password\", \"restGateway\": \"https://10.1.1.2\", \"arrayId\": \"APM00******2\", \"insecure\": true } ] } kubectl create secret generic unity-creds -n unity --from-file=config=secret.json\nUse the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the JSON syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.\nNote: “isDefaultArray” parameter in values.yaml and secret.json should match each other.\n  Setup for snapshots\nThe Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17.\n  The following section summarizes the changes in the beta release.\nTo use the Kubernetes Volume Snapshot feature, ensure the following components have been deployed on your Kubernetes cluster.\n Install Snapshot Beta CRDs using the following command  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml   Volume snapshot controller\n The manifests available on GitHub install v3.0.2 of the snapshotter image - quay.io/k8scsi/csi-snapshotter:v3.0.2 Dell recommends using v3.0.2 image of the snapshot-controller - quay.io/k8scsi/snapshot-controller:v3.0.2  After executing these commands, a snapshot-controller pod should be up and running.\n      Run the ./csi-install.sh --namespace unity --values ./myvalues.yaml command to proceed with the installation.\nA successful installation should emit messages that look similar to the following samples:\n------------------------------------------------------ \u003e Installing CSI Driver: csi-unity on 1.19 ------------------------------------------------------ ------------------------------------------------------ \u003e Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u003e Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.18 | |- Driver: csi-unity | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that required secrets have been created Success | |- Verifying snapshot support | |--\u003e Verifying that beta snapshot CRDs are available Success | |--\u003e Verifying that beta snapshot controller is available Success | |- Verifying helm version Success ------------------------------------------------------ \u003e Verification Complete ------------------------------------------------------ | |- Installing Driver Success | |--\u003e Waiting for statefulset unity-controller to be ready Success | |--\u003e Waiting for daemonset unity-node to be ready Success ------------------------------------------------------ \u003e Operation complete ------------------------------------------------------ Results: At the end of the script statefulset unity-controller and daemonset unity-node is ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\n unity-controller-xxxx with 5/5 containers ready, and status displayed as Running.      Agent pods with 2/2 containers and the status displayed as Running.\nFinally, the script creates storageclasses such as, “unity”. Additional storage classes can be created for different combinations of file system types and Unity storage pools. The script also creates volumesnapshotclass “unity-snapclass”.\n  Certificate validation for Unisphere REST API calls This topic provides details about setting up the certificate validation for the CSI Driver for Dell EMC Unity.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on “.Values.certSecretCount” parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nThe CSI driver exposes an install parameter in secret.json, which is like storageArrayList[i].insecure, which determines if the driver performs client-side verification of the Unisphere certificates.\nThe storageArrayList[i].insecure parameter set to true by default, and the driver does not verify the Unisphere certificates.\nIf the storageArrayList[i].insecure set to false, then the secret unity-certs-n must contain the CA certificate for Unisphere.\nIf this secret is empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the storageArrayList[i].insecure parameter set to false and a previous installation attempt created the empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\n To fetch the certificate, run the following command. openssl s_client -showcerts -connect \u003cUnisphere IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Run the following command to create the cert secret with index ‘0’ kubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret kubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)  Note: “unity” is the namespace for helm based installation but namespace can be user defined in operator based installation.\nNote: User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to kubernetes secret size limitation.\nNote: Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\n","excerpt":"The CSI Driver for Dell EMC Unity can be deployed by using the …","ref":"/csm-docs/v3/installation/helm/unity/","title":"Unity"},{"body":"Installing Unity CSI Driver via Operator The CSI Driver for Dell EMC Unity can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver   Create namespace run kubectl create namespace test-unity to create the a namespace called test-unity. It can be any user-defined name.\n  Create unity-creds\nCreate secret mentioned in Install csi-driver section. The secret should be created in user-defined namespace (test-unity, in this case)\n  Create certificate secrets\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n in the user-defined namespace (test-unity, in this case) Create certificate procedure explained in the link\nNote: ‘certSecretCount’ parameter is not required for operator. Based on secret name pattern (unity-certs-*) operator reads all the secrets. Secret name suffix should have 0 to N order to read the secrets. Secrets will not be considered, if any number missing in suffix.\nExample: If unity-certs-0, unity-certs-1, unity-certs-3 are present in the namespace, then only first two secrets are considered for SSL verification.\n  Create a CR (Custom Resource) for unity using the sample provided below\n  Create a new file csiunity.yaml by referring the following content. Replace the given sample values according to your environment. You can find may CRDs under deploy/crds folder when you install dell-csi-operator\napiVersion:storage.dell.com/v1kind:CSIUnitymetadata:name:test-unitynamespace:test-unityspec:driver:configVersion:v3replicas:2common:image:\"dellemc/csi-unity:v1.4.0.000R\"imagePullPolicy:IfNotPresentenvs:- name:X_CSI_UNITY_DEBUGvalue:\"true\"sideCars:- name:provisionerargs:[\"--volume-name-prefix=csiunity\"]storageClass:- name:virt2016****-fcdefault:truereclaimPolicy:\"Delete\"allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\"VIRT2016****\"protocol:\"FC\"- name:virt2017****-iscsireclaimPolicy:\"Delete\"allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\"VIRT2017****\"protocol:\"iSCSI\"- name:virt2017****-nfsreclaimPolicy:\"Delete\"allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\"VIRT2017****\"protocol:\"NFS\"hostIoSize:\"8192\"nasServer:nas_1- name:virt2017****-iscsi-topologyreclaimPolicy:\"Delete\"allowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/virt2017****-iscsivalues:- \"true\"parameters:storagePool:pool_1arrayId:\"VIRT2017****\"protocol:\"iSCSI\"snapshotClass:- name:test-snapparameters:retentionDuration:\"\"  Execute the following command to create unity custom resource kubectl create -f csiunity.yaml. This command will deploy the csi-unity driver in the test-unity namespace.\n  Any deployment error can be found out by logging the operator pod which is in default namespace (example, kubectl logs dell-csi-operator-64c58559f6-cbgv7)\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT Specifies the HTTP endpoint for Unity. No /var/run/csi/csi.sock   X_CSI_DEBUG To enable debug mode No false   GOUNITY_DEBUG To enable debug mode for gounity library No false   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true   Node parameters      X_CSI_MODE Driver starting mode No node   X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands. No /noderoot      ","excerpt":"Installing Unity CSI Driver via Operator The CSI Driver for Dell EMC …","ref":"/csm-docs/v3/installation/operator/unity/","title":"Unity"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes, and automatically mounts them to the pod.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/sample.yaml   You can find all the created resources in test-unity namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n test-unity If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/sample.yaml   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/csm-docs/v3/installation/test/unity/","title":"Test Unity CSI Driver"},{"body":"Release Notes - CSI Unity v1.4.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL and CoreOS worker nodes Added support for Controller high availability (multiple-controllers) Added support for Ubuntu 20.04 Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Docker EE 3.1 Added support for Topology Added support for ephemeral volumes Added raw-block volume creation capability for iSCSI and FC based volumes. Added support for Mount options Changed driver base image to UBI 8.x  Fixed Issues  Source NFS PVC cannot be deleted if cloned NFS PVC exists.  Known Issues    Issue Workaround     Topology related node labels are not removed automatically. Currently, when the driver is uninstalled, topology related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released we need to manually remove the node labels mentioned here https://github.com/dell/csi-unity#known-issues (Point 1)   Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver should be restarted with the below command only if the topology-based storage classes are used. Otherwise, the driver will automatically detect the newly added or removed arrays https://github.com/dell//csi-unity#known-issues (Point 2)   If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in cluster but on array it will still be present and marked for deletion. All the cloned PVC should be deleted in order to delete the source PVC from array.   PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass “unity-iscsi”: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity --no-headers=true   PVC creation fails on a cluster with only NFS protocol enabled with error failed to provision volume with StorageClass “unity-nfs”: error generating accessibility requirements: no available topology found. For NFS volume and pod creation to succeed there must be minimum one worker node with iSCSI support and with a successful iSCSI login in to the array. Following commands can be used as a reference (which should be executed on worker node with iSCSI support) iscsiadm -m discovery -t st -p \u003ciscsi-interface-ip\u003e iscsiadm -m node -T \u003ctarget-iqn\u003e -l   On deleting pods sometimes the corresponding volumeattachment will not get removed. This issue is intermittent and happens with one specific protocol (FC, iSCSI or NFS) based StorageClasses. This issue occurs across Kubernetes versions 1.18 and 1.19 and both versions of OpenShift (4.5/4.6). On deleting the stale volumeattachment manually, Controller Unpublish gets invoked and then the corresponding PVCs can be deleted.    ","excerpt":"Release Notes - CSI Unity v1.4.0 New Features/Changes  Added support …","ref":"/csm-docs/v3/release/unity/","title":"Unity"},{"body":"    Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods unity-controller-\u003csuffix\u003e –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   The kubectl logs -n unity unity-node-\u003csuffix\u003e driver logs shows that the driver can’t connect to Unity - Authentication failure. Check if you’ve created secret with correct credentials    ","excerpt":"    Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/csm-docs/v3/troubleshooting/unity/","title":"Unity"},{"body":"The CSI Driver for Dell EMC Unity and PowerScale supports VMware Tanzu and deployment of these Tanzu clusters is done using the VMware Tanzu supervisor cluster and supervisor namespace.\nCurrently, VMware Tanzu with normal configuration(without NAT) supports Kubernetes 1.20. The CSI driver can be installed on this cluster using Helm. Installation of CSI drivers in Tanzu via Operator has not been qualified.\nTo login to the Tanzu cluster, download kubectl and kubectl vsphere binaries to any of the system\nRefer: https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-0F6E45C4-3CB1-4562-9370-686668519FCA.html\nConnect to the VCenter using kubectl vSphere commands as shown below.\nkubectl vsphere login --insecure-skip-tls-verify --vsphere-username vSphere username --server=https://\u003ctanzu-server-ip\u003e/ -v 5  Once login is done to the Tanzu cluster, the installation of CSI driver is done using kubectl binary similar to how we do for other systems.\nTanzu example ","excerpt":"The CSI Driver for Dell EMC Unity and PowerScale supports VMware Tanzu …","ref":"/csm-docs/docs/csidriver/partners/tanzu/","title":"VMware Tanzu"},{"body":"The CSI Driver for Dell EMC Unity and PowerScale supports VMware Tanzu and deployment of these Tanzu clusters is done using the VMware Tanzu supervisor cluster and supervisor namespace.\nCurrently, VMware Tanzu with normal configuration(without NAT) supports Kubernetes 1.19 and 1.20. The CSI driver can be installed on this cluster using Helm. Installation of CSI drivers in Tanzu via Operator has not been qualified.\nTo login to the Tanzu cluster, download kubectl and kubectl vsphere binaries to any of the system\nRefer: https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-0F6E45C4-3CB1-4562-9370-686668519FCA.html\nConnect to the VCenter using kubectl vSphere commands as shown below.\nkubectl vsphere login --insecure-skip-tls-verify --vsphere-username vSphere username --server=https://\u003ctanzu-server-ip\u003e/ -v 5  Once login is done to the Tanzu cluster, the installation of CSI driver is done using kubectl binary similar to how we do for other systems.\nTanzu example ","excerpt":"The CSI Driver for Dell EMC Unity and PowerScale supports VMware Tanzu …","ref":"/csm-docs/v1/partners/tanzu/","title":"VMware Tanzu"}]