---
title: "Release Notes"
linkTitle: "Release Notes"
no_list: true
weight: 20
Description: >
  Container Storage Modules release notes
---

## Notifications:

**General:**

> * <span><span/>{{< message text="8" >}}
> * <span><span/>{{< message text="7" >}}
> * <span><span/>{{< message text="1" >}}

**Deprecation:**

> * <span><span/>{{< message text="5" >}}
> * <span><span/>{{< message text="11" >}}
> * <span><span/>{{< message text="12" >}}

## Release Notes for v1.14.0

### New Features/Changes

- [#1614 - [FEATURE]: CSM PowerMax - Mount credentials secret to the reverse-proxy](https://github.com/dell/csm/issues/1614)
- [#1743 - [FEATURE]: Add support for PowerScale 9.11](https://github.com/dell/csm/issues/1743)
- [#1744 - [FEATURE]: Add support for Unity 5.5](https://github.com/dell/csm/issues/1744)
- [#1748 - [FEATURE]: CSM PowerMax - Multi-Availability Zone (AZ) support with multiple storage systems - dedicated storage systems in each AZ](https://github.com/dell/csm/issues/1748)
- [#1749 - [FEATURE]: CSM Operator - CSM Operator must manage the CRD only on the K8S cluster where the Operator is deployed](https://github.com/dell/csm/issues/1749)
- [#1750 - [FEATURE]: Kubernetes 1.33 Qualification](https://github.com/dell/csm/issues/1750)
- [#1751 - [FEATURE]: CSM RBAC rules](https://github.com/dell/csm/issues/1751)
- [#1752 - [FEATURE]: CSM PowerFlex - Expose the SFTP settings to automatically pull the scini.ko kernel module](https://github.com/dell/csm/issues/1752)- 
- [#1753 - [FEATURE]: CSM PowerStore - Host Registration for PowerStore Metro](https://github.com/dell/csm/issues/1753)
- [#1755 - [FEATURE]: CSM Observability - Grafana Migration from Angular JS to React](https://github.com/dell/csm/issues/1755)
- [#1758 - [FEATURE]: CSM PowerStore - Multiple NAS Servers Support](https://github.com/dell/csm/issues/1758)
- [#1844 - [FEATURE]: Add Openshift Virtualization support for resiliency module](https://github.com/dell/csm/issues/1844)
- [#1850 - [FEATURE]: Controller reattach failover PV to PVC automatically on stretched cluster](https://github.com/dell/csm/issues/1850)
- [#1862 - [FEATURE]: CSM Replication - Add claimRef to the target PV](https://github.com/dell/csm/issues/1862)
- [#1882- Â [FEATURE]: Metro Replication Qualification - Support PowerMax and PowerStore with OpenShift Virtualization](https://github.com/dell/csm/issues/1882)

### Fixed Issues

- [#1732 - [BUG]: Updating approveSDC in tenant CR doesn't reflect in backend](https://github.com/dell/csm/issues/1732)
- [#1804 - [BUG]: Support multiple sidecar headers in the Authorization Proxy Server](https://github.com/dell/csm/issues/1804)
- [#1740 - [BUG]: CSI PowerFlex does not list volumes from non-default systems](https://github.com/dell/csm/issues/1740)
- [#1774 - [BUG]: CSI-PowerFlex - DriverConfigMap is using hard-coded value](https://github.com/dell/csm/issues/1774)
- [#1782 - [BUG]: Pods Stuck in Terminating State After PowerFlex CSI Node Pod Restart When Deployments Share Same Node](https://github.com/dell/csm/issues/1782)
- [#1841 - [BUG]: "unable to get node list" error="csinodes.storage.k8s.io is forbidden](https://github.com/dell/csm/issues/1841)
- [#1858 - [BUG]: Resiliency Tests Fail on GOARCH=386 Due to Type Mismatch in gofsutil (statfs.Bsize)](https://github.com/dell/csm/issues/1858)
- [#1689 - [BUG]: Auto select protocol makes the node driver to crash](https://github.com/dell/csm/issues/1689)
- [#1698 - [BUG]: 1.13 documentation | PowerMax | CSI PowerMax Reverse Proxy](https://github.com/dell/csm/issues/1698)
- [#1711 - [BUG]: Unable to provision PowerMax Metro volumes with replication module not enabled](https://github.com/dell/csm/issues/1711)
- [#1725 - [BUG]: Scale test fails with powermax nvmetcp protocol when X_CSI_TRANSPORT_PROTOCOL= ""](https://github.com/dell/csm/issues/1725)
- [#1760 - [BUG]: [csi-powermax]: Yaml error in configmap generation](https://github.com/dell/csm/issues/1760)
- [#1769 - [BUG]: PowerMax node pods are crashing even though the second array is reachable](https://github.com/dell/csm/issues/1769)
- [#1826 - [BUG]: PowerMax CSI Driver attempts to create Port Group with Mixed FC and NVMe/FC Ports](https://github.com/dell/csm/issues/1826)
- [#1870 - [BUG]: PowerMax Resiliency E2E Tests Failing Due to Incorrect Image Patch in Driver Node Pod](https://github.com/dell/csm/issues/1870)
- [#1775 - [BUG]: CSI+Rep using Operator for PMAX failing during deployment.](https://github.com/dell/csm/issues/1775)
- [#1726 - [BUG]: Panic Error During Volume Expansion When Hard Limit is Not Set for CSI PowerScale Driver](https://github.com/dell/csm/issues/1726)
- [#1763 - [BUG]: Cloned PVC remains in a Pending state when cloning a large PVC in PowerScale](https://github.com/dell/csm/issues/1763)
- [#1773 - [BUG]: gopowerscale CopyIsiVolume* functions ignoring error cases](https://github.com/dell/csm/issues/1773)
- [#1823 - [BUG]: Powerscale CSI driver not setting snapshot restoreSize field](https://github.com/dell/csm/issues/1823)
- [#1714 - [BUG]: PowerStore CSI driver version 2.12 - only supports the default interface for iSCSI discovery. ](https://github.com/dell/csm/issues/1714)
- [#1729 - [BUG]: [CSI-Powerstore] Clarify protocol value use in PV](https://github.com/dell/csm/issues/1729)
- [#1845 - [BUG]: PowerStore driver 2.12 error with OCP 4.14 on NVMe](https://github.com/dell/csm/issues/1845)
- [#1717 - [BUG]: CSM Unity XT protocol/host registration documentation is not clear](https://github.com/dell/csm/issues/1717)
- [#1736 - [BUG]: failed to provision volume with StorageClass error generating accessibility requirements: no available topology found](https://github.com/dell/csm/issues/1736)
- [#1762 - [BUG]: CSM Operator samples are incomplete](https://github.com/dell/csm/issues/1762)
- [#1861 - [BUG]: Update the OTEL image version in operator and helm sample files](https://github.com/dell/csm/issues/1861)
- [#1930 - [BUG]: Update the OTEL image version in operator and helm sample files](https://github.com/dell/csm/issues/1930)
- [#1896 - [BUG]: Update the OTEL image version in operator and helm sample files](https://github.com/dell/csm/issues/1896)


### Known Issues


| Issue                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Workaround                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| <div style="text-align: left"> When CSM Operator creates a deployment that includes secrets (e.g., application-mobility, observability, cert-manager, velero), these secrets are not deleted on uninstall and will be left behind. For example, the `karavi-topology-tls`, `otel-collector-tls`, and `cert-manager-webhook-ca` secrets will not be deleted.                                                                                                                                                                                                                                                                                 | <div style="text-align: left"> This should not cause any issues on the system, but all secrets present on the cluster can be found with `kubectl get secrets -A`, and any unwanted secrets can be deleted with `kubectl delete secret -n <secret-namespace> <secret-name>`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| <div style="text-align: left"> In certain environments, users have encountered difficulties in installing drivers using the CSM Operator due to the 'OOM Killed' issue. This issue is attributed to the default resource requests and limits configured in the CSM Operator, which fail to meet the resource requirements of the user environments. OOM error occurs when a process in the container tries to consume more memory than the limit specified in resource configuration.                                                                                                                                                       | <div style="text-align: left"> Before deploying the CSM Operator, it is crucial to adjust the memory and CPU requests and limits in the files [config/manager.yaml](https://github.com/dell/csm-operator/blob/main/config/manager/manager.yaml#L100), [deploy/operator.yaml](https://github.com/dell/csm-operator/blob/main/deploy/operator.yaml#L1330) to align with the user's environment requirements. If the containers running on the pod exceed the specified CPU and memory limits, the pod may get evicted. Currently CSM Operator do not support updating this configuration dynamically. CSM Operator needs to be redeployed for these updates to take effect in case it is already installed. Steps to manually update the resource configuration and then redeploy CSM Operator are available [here](https://dell.github.io/csm-docs/docs/deployment/csmoperator/#installation)|
| <div style="text-align: left">  Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | <div style="text-align: left">  Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| <div style="text-align: left">  When a node goes down, the block volumes attached to the node cannot be attached to another node                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | <div style="text-align: left">  This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: <br /> 1. Force delete the pod running on the node that went down <br /> 2. Delete the volumeattachment to the node that went down. <br /> Now the volume can be attached to the new node.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| <div style="text-align: left">  sdc:3.6.0.6 is causing issues while installing the csi-powerflex driver on ubuntu,RHEL8.3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | <div style="text-align: left">   Workaround: <br /> Change the powerflexSdc to sdc:3.6 in values.yaml https://github.com/dell/csi-powerflex/blob/72b27acee7553006cc09df97f85405f58478d2e4/helm/csi-vxflexos/values.yaml#L13 <br />                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| <div style="text-align: left">  sdc:3.6.1 is causing issues while installing the csi-powerflex driver on ubuntu.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | <div style="text-align: left">   Workaround: <br /> Change the powerflexSdc to sdc:3.6 in values.yaml https://github.com/dell/csi-powerflex/blob/72b27acee7553006cc09df97f85405f58478d2e4/helm/csi-vxflexos/values.yaml#L13 <br />                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| <div style="text-align: left"> A CSI ephemeral pod may not get created in OpenShift 4.13 and fail with the error `"error when creating pod: the pod uses an inline volume provided by CSIDriver csi-vxflexos.dellemc.com, and the namespace has a pod security enforcement level that is lower than privileged."`                                                                                                                                                                                                                                                                                                                           | <div style="text-align: left">  This issue occurs because OpenShift 4.13 introduced the CSI Volume Admission plugin to restrict the use of a CSI driver capable of provisioning CSI ephemeral volumes during pod admission. Therefore, an additional label `security.openshift.io/csi-ephemeral-volume-profile` in [csidriver.yaml](https://github.com/dell/helm-charts/blob/csi-vxflexos-2.10.0/charts/csi-vxflexos/templates/csidriver.yaml) file with the required security profile value should be provided. Follow [OpenShift 4.13 documentation for CSI Ephemeral Volumes](https://docs.openshift.com/container-platform/4.13/storage/container_storage_interface/ephemeral-storage-csi-inline.html) for more information.                                                                                                                                                            |
| <div style="text-align: left">   <div style="text-align: left">  If the volume limit is exhausted and there are pending pods and PVCs due to `exceed max volume count`, the pending PVCs will be bound to PVs and the pending pods will be scheduled to nodes when the driver pods are restarted.                                                                                                                                                                                                                                                                                                                                           | <div style="text-align: left">  It is advised not to have any pending pods or PVCs once the volume limit per node is exhausted on a CSI Driver. There is an open issue reported with kubernetes at https://github.com/kubernetes/kubernetes/issues/95911 with the same behavior.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| <div style="text-align: left">  Resource quotas may not work properly with the CSI PowerFlex driver. PowerFlex is only able to assign storage in 8Gi chunks, so if a create volume call is made with a size not divisible by 8Gi, CSI-PowerFlex will round up to the next 8Gi boundary when it provisions storage -- however, the resource quota will not record this size but rather the original size in the create request. This means that, for example, if a 10Gi resource quota is set, and a user provisions 10 1Gi PVCs, 80Gi of storage will actually be allocated, which is well over the amount specified in the resource quota. | <div style="text-align: left">  For now, users should only provision volumes in 8Gi-divisible chunks if they want to use resource quotas.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| <div style="text-align: left">  Unable to update Host: A problem occurred modifying the host resource                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | <div style="text-align: left">  This issue occurs when the nodes do not have unique hostnames or when an IP address/FQDN with same sub-domains are used as hostnames. The workaround is to use unique hostnames or FQDN with unique sub-domains                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| <div style="text-align: left">  When a node goes down, the block volumes attached to the node cannot be attached to another node                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | <div style="text-align: left">  This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: <br /> 1. Force delete the pod running on the node that went down <br /> 2. Delete the volumeattachment to the node that went down. <br /> Now the volume can be attached to the new node                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| <div style="text-align: left">  Automatic SRDF group creation is failing with "Unable to get Remote Port on SAN for Auto SRDF" for PowerMaxOS 10.1 arrays                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | <div style="text-align: left">  Create the SRDF Group and add it to the storage class                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| <div style="text-align: left">  [Node stage is failing with error "wwn for FC device not found"](https://github.com/dell/csm/issues/1070)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | <div style="text-align: left">  This is an intermittent issue, rebooting the node will resolve this issue                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| <div style="text-align: left">  When the driver is installed using CSM Operator , few times, pods created using block volume are getting stuck in containercreating/terminating state or devices are not available inside the pod.                                                                                                                                                                                                                                                                                                                                                                                                          | <div style="text-align: left">  Update the daemonset with parameter `mountPropagation: "Bidirectional"` for volumedevices-path under volumeMounts section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| <div style="text-align: left">  When running CSI-PowerMax with Replication in a multi-cluster configuration, the driver on the target cluster fails and the following error is seen in logs: `error="CSI reverseproxy service host or port not found, CSI reverseproxy not installed properly"`                                                                                                                                                                                                                                                                                                                                             | <div style="text-align: left">  The reverseproxy service needs to be created manually on the target cluster. Follow [the instructions here](docs/getting-started/installation/kubernetes/powermax/csmoperator/csm-modules/replication/#configuration-steps) to create it.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| <div style="text-align: left">  Storage capacity tracking does not return `MaximumVolumeSize` parameter. PowerScale is purely NFS based meaning it has no actual volumes. Therefore `MaximumVolumeSize` cannot be implemented if there is no volume creation.                                                                                                                                                                                                                                                                                                                                                                               | <div style="text-align: left">  CSI PowerScale 2.9.1 is compliant with CSI 1.6 specification since the field `MaximumVolumeSize` is optional.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| <div style="text-align: left">  If the length of the nodeID exceeds 128 characters, the driver fails to update the CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn't allow nodeID to be greater than 128 characters.                                                                                                                                                                                                                                                                                                                                                                         | <div style="text-align: left">  The CSI PowerScale driver uses the hostname for building the nodeID which is set in the CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future Kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581 <br><br> **Note:** In kubernetes 1.22 this limit has been relaxed to 192 characters.                                                                                                                                                                                                                                                                                                                                              |
| <div style="text-align: left">  If some older NFS exports /terminated worker nodes still in NFS export client list, CSI driver tries to add a new worker node it fails (For RWX volume).                                                                                                                                                                                                                                                                                                                                                                                                                                                    | <div style="text-align: left">  User need to manually clean the export client list from old entries to make successful addition of new worker nodes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| <div style="text-align: left">  Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | <div style="text-align: left">  Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| <div style="text-align: left">  fsGroupPolicy may not work as expected without root privileges for NFS only<br/>https://github.com/kubernetes/examples/issues/260                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | <div style="text-align: left">  To get the desired behavior set "RootClientEnabled" = "true" in the storage class parameter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| <div style="text-align: left">  Driver logs shows "VendorVersion=2.3.0+dirty"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | <div style="text-align: left">  Update the driver to csi-powerscale 2.4.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| <div style="text-align: left">  PowerScale 9.5.0, Driver installation fails with session based auth, "HTTP/1.1 401 Unauthorized"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | <div style="text-align: left">  Fix is available in PowerScale >= 9.5.0.4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| <div style="text-align: left">  Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style="text-align: left">  Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| <div style="text-align: left">  fsGroupPolicy may not work as expected without root privileges for NFS only https://github.com/kubernetes/examples/issues/260                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | <div style="text-align: left">  To get the desired behavior set "allowRoot: "true" in the storage class parameter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| <div style="text-align: left">  If the NVMeFC pod is not getting created and the host looses the ssh connection, causing the driver pods to go to error state                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | <div style="text-align: left">  remove the nvme_tcp module from the host in case of NVMeFC connection                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| <div style="text-align: left">  When a node goes down, the block volumes attached to the node cannot be attached to another node                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | <div style="text-align: left">  This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: <br /> 1. Force delete the pod running on the node that went down <br /> 2. Delete the volumeattachment to the node that went down.  Now the volume can be attached to the new node.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| <div style="text-align: left">  When driver node pods enter CrashLoopBackOff and PVC remains in pending state with one of the following events:<br /> 1. failed to provision volume with StorageClass `<storage-class-name>`: error generating accessibility requirements: no available topology found <br /> 2. waiting for a volume to be created, either by external provisioner "csi-powerstore.dellemc.com" or manually created by system administrator.                                                                                                                                                                               | <div style="text-align: left">  Check whether all array details present in the secret file are valid and remove any invalid entries if present. <br/>Redeploy the driver.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| <div style="text-align: left">  If an ephemeral pod is not being created in OpenShift 4.13 and is failing with the error "error when creating pod: the pod uses an inline volume provided by CSIDriver csi-powerstore.dellemc.com, and the namespace has a pod security enforcement level that is lower than privileged."                                                                                                                                                                                                                                                                                                                   | <div style="text-align: left">  This issue occurs because OpenShift 4.13 introduced the CSI Volume Admission plugin to restrict the use of a CSI driver capable of provisioning CSI ephemeral volumes during pod admission https://docs.openshift.com/container-platform/4.13/storage/container_storage_interface/ephemeral-storage-csi-inline.html . Therefore, an additional label "security.openshift.io/csi-ephemeral-volume-profile" needs to be added to the CSIDriver object to support inline ephemeral volumes.                                                                                                                                                                                                                                                                                                                                                                    |
| <div style="text-align: left">  In OpenShift 4.13, the root user is not allowed to perform write operations on NFS shares, when root squashing is enabled.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style="text-align: left">  The workaround for this issue is to disable root squashing by setting allowRoot: "true" in the NFS storage class.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| <div style="text-align: left">  If the volume limit is exhausted and there are pending pods and PVCs due to `exceed max volume count`, the pending PVCs will be bound to PVs, and the pending pods will be scheduled to nodes when the driver pods are restarted.                                                                                                                                                                                                                                                                                                                                                                           | <div style="text-align: left">  It is advised not to have any pending pods or PVCs once the volume limit per node is exhausted on a CSI Driver. There is an open issue reported with Kubernetes at https://github.com/kubernetes/kubernetes/issues/95911 with the same behavior.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| <div style="text-align: left">  If two separate networks are configured for ISCSI and NVMeTCP, the driver may encounter difficulty identifying the second network (e.g., NVMeTCP).                                                                                                                                                                                                                                                                                                                                                                                                                                                          | <div style="text-align: left">  This is a known issue, and the workaround involves creating a single network on the array to serve both ISCSI and NVMeTCP purposes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| <div style="text-align: left">  When a PV/PVC is deleted in Kubernetes, it will trigger the deletion of the underlying volume and snapshot on the array as a default behaviour. This can result in a situation where the VolumeSnapshot and VolumeSnapshotContent will still show "readyToUse: true", but leaves them unusable because it is no longer backed by underlying storage snapshot. This will not allow the creation of a PVC from snapshot and this could also lead to a data loss situations.                                                                                                                                   | <div style="text-align: left">  This is a known issue, and the workaround is use of **retain** policy on the various PV, VolumeSnapshot and VolumeSnapshotContent that you wish to use for cloning.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| <div style="text-align: left">  Nodes not getting registered on Unity XT.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | <div style="text-align: left">  Creating wrapper around `hostname` command inside the node pod's driver container, that fails when `-I` flag is used. This will triggrer fallback behaviour in driver and should fix the issue.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| <div style="text-align: left">  Topology-related node labels are not removed automatically.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | <div style="text-align: left">  Currently, when the driver is uninstalled, topology-related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released, remove the labels manually after the driver un-installation using command **kubectl label node <node_name> <label1>- <label2>- ...** Example: **kubectl label node <hostname> csi-unity.dellemc.com/array123-iscsi-** Note: there must be - at the end of each label to remove it.                                                                                                                                                                                                                                                                                                                                                                          |
| <div style="text-align: left">  NFS Clone - Resize of the snapshot is not supported by Unity XT Platform, however, the user should never try to resize the cloned NFS volume.                                                                                                                                                                                                                                                                                                                                                                                                                                                               | <div style="text-align: left">  Currently, when the driver takes a clone of NFS volume, it succeeds but if the user tries to resize the NFS volumesnapshot, the driver will throw an error.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| <div style="text-align: left">  Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | <div style="text-align: left">  Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| <div style="text-align: left"> A CSI ephemeral pod may not get created in OpenShift 4.13 and fail with the error `"error when creating pod: the pod uses an inline volume provided by CSIDriver csi-unity.dellemc.com, and the namespace has a pod security enforcement level that is lower than privileged."`                                                                                                                                                                                                                                                                                                                              | <div style="text-align: left">  This issue occurs because OpenShift 4.13 introduced the CSI Volume Admission plugin to restrict the use of a CSI driver capable of provisioning CSI ephemeral volumes during pod admission. Therefore, an additional label `security.openshift.io/csi-ephemeral-volume-profile` in [csidriver.yaml](https://github.com/dell/helm-charts/blob/csi-unity-2.8.0/charts/csi-unity/templates/csidriver.yaml) file with the required security profile value should be provided. Follow [OpenShift 4.13 documentation for CSI Ephemeral Volumes](https://docs.openshift.com/container-platform/4.13/storage/container_storage_interface/ephemeral-storage-csi-inline.html) for more information.                                                                                                                                                                   |
| <div style="text-align: left">  fsGroupPolicy may not work as expected without root privileges for NFS only [https://github.com/kubernetes/examples/issues/260](https://github.com/kubernetes/examples/issues/260)                                                                                                                                                                                                                                                                                                                                                                                                                          | <div style="text-align: left">  To get the desired behavior set âRootClientEnabledâ = âtrueâ in the storage class parameter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| <div style="text-align: left">  Controller publish is taking too long to complete/ Health monitoring is causing Unity array to panic by opening multiple sessions/ There are error messages in the log `context deadline exceeded`, when health monitoring is enabled                                                                                                                                                                                                                                                                                                                                                                       | <div style="text-align: left">  Disable volume health monitoring on the node and keep it only at the controller level. Refer [here](https://dell.github.io/csm-docs/docs/csidriver/features/unity/#volume-health-monitoring) for more information about enabling/disabling volume health monitoring                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| <div style="text-align: left"> Minimal installation of the driver via Operator does not create driver pods                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style="text-align: left"> This issue has been resolved in the latest Operator image, but if using a released image, there is a workaround. Add the following to the CSM object manifest YAML (for example, `minimal-samples/powerflex_v2140.yaml`) file:<br><br><pre style="color: black; background-color: white">node:<br>Â  envs:<br>Â Â Â  - name: X_CSI_SDC_SFTP_REPO_ENABLED<br>Â Â Â Â Â  value: "false"</pre></div>                                                                                                                                                                                                                                                                                                                                                                                                                                          |
